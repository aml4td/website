---
knitr:
  opts_chunk:
    cache.path: "../_cache/removals/"
---

# Removing Predictors {#sec-removing-predictors}

```{r}
#| label: removals-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(colino)
library(doParallel)

# ------------------------------------------------------------------------------
# set options
tidymodels_prefer()
theme_set(theme_transparent())
set_options()
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```

```{r}
#| label: data-import
#| include: false
load("../RData/noise_simulation_results.RData")
source("../R/setup_deliveries.R")
num_train <- nrow(delivery_train)
chr_train <- format(num_train, big.mark = ",")
```

When starting with a new data set, you may not know which columns have predictive power. You may also add features, such as splines, and you need to evaluate if they or others are relevant. 

**Feature selection** is the process of determining how many (and which) predictors should be retained in the model so that it performs satisfactorily. _Supervised_ feature selection, discussed below, is very complex. Unfortunately, it is often conducted inappropriately, partly because it can be computationally onerous to do the right thing.

This chapter _does not_ provide a comprehensive discussion of this topic. Due to the issue’s complexity, we suggest @fes for more detailed discussions of specific feature selection tools and algorithms. 

## Do Extra Predictor Matter? {#sec-effect-of-extra-features}

As with most questions, “It depends on the model.” Some models have specific limitations on the number of predictors. For example, ordinary linear regression cannot estimate more parameters than data points in the training set. As will be seen in later chapters, some models can naturally eliminate predictors with no effect relationship to the outcome. In general, it is better to have fewer predictors than an overabundance. 

To demonstrate the potential issue, a regression problem was simulated^[A similar analysis with a different simulation system can be found in @fes.] using equations used in @hooker2004discovering and @Sorokina2008

$$ 
y_i = \pi^{x_{i1}  x_{i2}}  \sqrt{2 x_{i3}} - \arcsin(x_{i4}) + \log(x_{i3} + x_{i5}) - (x_{i9}/x_{i10})  \sqrt{x_{i7}/x_{i8}} - (x_{i2} x_{i7}) + \epsilon_i
$$

where the predictors have independent random uniform distributions and the error term is $N(0.00, 0.25)$. Training sets were simulated with 1,000 samples as well as a test set of 10,000 data points. In addition to the original 10 predictors, extra sets of $p^*$ random normal variables were added with $p^* = 10, 20, \ldots, 100$. These columns were created to have no relationship to the outcome or the other predictors. A collection of models where optimized for these data and trained, including: 

* Generalized additive models (GAMs, section TODO) with nonlinear terms for all predictors. 
* Support vector machines (SVMs, section TODO) using a radial basis function kernel. 
* K-nearest neighbors (section TODO).
* Single-layer neural networks (section TODO).
* Penalized linear regression (section TODO). For this model, five interaction pairs were added from the model equation (e.g., $x_2\times x_7$), assuming that we would have discovered these during data analysis. 
* Multivariate adaptive regression splines (MARS) (section TODO) with second-order interaction effects. 
* Bagged regression trees (section TODO).
* Boosted regression trees (section TODO).
* Bayesian adaptive regression trees (BART, section TODO).
* Random forests (section TODO).
* Cubist rule ensembles (section TODO).
* RuleFit rule ensembles (section TODO).

The first four models do not intrinsically eliminate non-informative predictors. In actuality, this is a bit more complicated; many ensemble methods will over-select predictors so that, _in theory_, they can eliminate predictors. This is discussed in more detail for each model in their individual sections. 

After tuning these models on the training set (using 10-fold cross-validation), they were evaluated on the test set and the RMSE statistic was computed^[The details and code for this simulation can be found at [`https://github.com/topepo/noise_features_sim`](https://github.com/topepo/noise_features_sim).]. 

```{r}
#| label: feature-selection-notes
#| include: false

knn_res <- noise_simulation_results_pct %>% filter(class == "train.kknn") %>% slice_max(increase)
mlp_res <- noise_simulation_results_pct %>% filter(class == "brulee_mlp") %>% slice_max(increase)
bag_res <- noise_simulation_results_pct %>% filter(class == "bagger") %>% slice_max(increase)
svm_res <- noise_simulation_results_pct %>% filter(class == "ksvm") %>% slice_max(increase)
rf_res <- noise_simulation_results_pct %>% filter(class == "ranger") %>% slice_max(increase)
mars_res <- noise_simulation_results_pct %>% filter(class == "earch") %>% slice_max(increase)
```

Each simulation was repeated 20 times. The averaged test set RMSE results are shown in @fig-feature-selection, where the model curves are colored by whether they can automatically remove useless predictors. First, note that a few models did a relatively poor job predicting these data: K-nearest neighbors, generalized additive models, bagged trees, and random forests. 

Since the models have different performance levels at baseline (i.e., $p^*=0$), we’ll measure the effect of the extra variables via a percent difference from baseline. K-nearest neighbors, support vector machines, and neural networks performed particularly poorly when additional predictors were added. The percent increases in RMSE for these models were `r round(knn_res$increase * 100, 1)`%, `r round(svm_res$increase * 100, 1)`%, and `r round(mlp_res$increase * 100, 1)`%, respectively. Bagged trees (change of `r round(bag_res$increase * 100, 1)`%) and random forest (`r round(rf_res$increase * 100, 1)`%) showed moderate erosions in RMSE. 

```{r}
#| label: fig-feature-selection 
#| echo: false
#| warning: false
#| out-width: 70%
#| fig-width: 7
#| fig-height: 7
#| fig-cap: "Results of the feature selection simulation. The different curves show the erosion of performance (measured by RMSE) as more non-informative predictors are added. The dotted horizontal line indicates the theoretical best RMSE value that could be achieved and the colored regions around the curves are 95% confidence intervals."

noise_simulation_results %>%
  mutate(
    .lower = RMSE - qt(0.975, n) * std_err,
    .upper = RMSE + qt(0.975, n) * std_err
  ) %>%
  ggplot(aes(num_extra, RMSE)) +
  geom_hline(lty = 3, yintercept = 0.25) +
  geom_line(aes(col = `Intrinsic Feature Selection`)) +
  geom_ribbon(aes(ymin = .lower, ymax = .upper, fill = `Intrinsic Feature Selection`), 
              alpha = 1 / 5) +
  facet_wrap(~ label) +
  labs(x = "# Noise Features", y = "RMSE (Testing)")
```

The other models showed almost no real change in RMSE as new predictors were added. This reinforces the unfortunate truth that it is difficult to make generalizations regarding what types of preprocessing should be done prior to top modeling; it very much depends on the model. Appendix TODO shows a table of recommended preprocessing techniques for many models described in this work. 

## Unsupervised {#sec-unsupervised-filters}

The most uncontroversial filter is one that removes if predictors have a single unique value. Given that there is no information in these data, it is better to remove them so that errors do not occur when training models. However, some models are tolerant of this data characteristic.  

We can view columns with a single unique value to have zero variance. However, what should we do if a predictor has an odd sample or two that are different? For example, suppose in the food delivery data, the training set has an item that was rarely ordered and had five orders out of the `r chr_train` training set points had a single item ordered, and one additional order had a count of two.  That predictor is unlikely to be important but would not be caught in a zero-variance filter. 

@apm describes a _near_ zero-variance filter that attempts to find predictors with very granular values (i.e., few unique values) and very unbalanced frequency distributions. It computes two characteristics: 

- The percentage of unique values. In the example above, this would be (3 / `r num_train` = `r signif(3 / num_train * 100, 3)`%). 
- The count ratio of the two most frequent values (`r num_train` / 5 $\approx$ `r round(num_train / 5, 0)`). 

There are suggested cutoffs for each of these criteria (10% and 19, respectively), with the rule that if the unique value percentage is less than 10% and the frequency value is greater than 19, the predictor is removed. 

```{r}
#| label: nzv-text
#| include: false

nzv_res <- 
  recipe(time_to_delivery ~ ., data = delivery_train) %>% 
  step_nzv(starts_with("item")) %>% 
  prep() %>% 
  tidy(number = 1) %>% 
  pluck("terms")

item_rm_list <- knitr::combine_words(gsub("item_", "", nzv_res))

counts <- 
  delivery_train %>% 
  count(!!sym(nzv_res[1])) %>% 
  setNames(c("item", "n"))

pct_un <- nrow(counts) / nrow(delivery_train) * 100
fr_ratio <- round(counts$n[1] / counts$n[2], 1)

counts_txt <- 
  counts %>% 
  mutate(
    orders = ifelse(n != 1, "orders", "order"),
    txt = paste0(item,  " (", format(n, big.mark = ","), " ", orders, ")")
  ) %>% 
  pluck("txt") %>% 
  knitr::combine_words()
```

Recall that there are 27 predictors in the food delivery data that represent the counts of specific items in an order. These values are mostly zero and less than three. When the near-zero variance filter is applied to those predictors, `r length(nzv_res)` were slated for removal: items `r item_rm_list`. Of these, `r gsub("_", " ", nzv_res[1])` had the following frequency distribution: `r counts_txt`. The percentage of unique values was `r round(pct_un, 3)`% and the ratio of frequencies was `r fr_ratio`. 

Another potential filter reduces between-pair correlations (a.k.a. multicollinearity). For some threshold for the absolute value of the pairwise correlations, the filter removes the minimal subset of predictors to meet this standard. This can be a crude but effective method of reducing colinearity. Other tools, such as PCA, might be more advantageous for the model but do not reduce the dimensions of the predictors. This filter is considered in more detail in @sec-colinearity and compared to other approaches. 

others

## Supervised {#sec-unsupervised-feature-selection}

There are three main classes of supervised methods for filtering predictors. The first is model-based. As previously mentioned, some models can remove irrelevant predictors during training. For example, a decision tree effectively ignores predictors that were not involved in any of the model’s splits. This approach is generally called _embedded feature selection_^[Not to be confused with embeddings discussed in @sec-embeddings.]. For many of these models, no further filtering is required. However, this is not always the case. Tree-based ensembles tend to over-split on irrelevant predictors (often by design) and, for this reason, the _can_ exclude irrelevant predictors but do not in practice. Random forests is a prime example. 

### Filter Methods

Second, _simple filters_ can be applied to remove predictors before serving them to the model. For example, for a regression model, we might screen out numeric predictors whose correlation with the outcome is less than some threshold. This can be quick and effective but also has many limitations. For example, the correlation filter is univariate, and if the predictors are highly correlated with one another, this filter will either under- or over-filter the predictors. Another issue is related to data usage. If we use the same data to filter the predictors as will be used to train the model, there is the distinct possibility of overfitting the filter to the training data. Otherwise, we will need to either reserve a separate pool of data for filtering or use a nested resampling scheme (described in @sec-resampling). 

simple cut; no "where do we stop" but what should the threshold be? optimization via tuning? 


### Search Methods 

The third general approach uses a search procedure to iteratively provide subsets of the predictor set to the model. For each iteration, model performance is used to guide the next step of the optimization. As an example, recursive feature elimination (RFE) initially ranks the predictors in terms of their importance to the outcome. The search then successively reduces the number of predictors by prioritizing less relevant predictors for removal. 

two approaches to determine stopping point; nested resampling; re=running variable scoring


:::: {.columns}

::: {.column width="15%"}

:::

::: {.column width="70%"}


```pseudocode
#| label: alg-rfe
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
\begin{algorithm}
\caption{Recursive feature elimination (no parameter tuning)}
\begin{algorithmic}
\State $D^{model}$: data used for modeling: $p$ predictors $X$ and outcome $y$
\State $D^{pred}$: data used for performance estimation (same column dimensions)
\State $f()$: modeling function including preprocessing
\Procedure{RFE}{$D^{model}, D^{pred}, f()$}
  \For{$j =p$ \To $1$}
    \State Rank each predictor. 
    \State Select the top $j$ predictors and subset $D^{model}$ and $D_{j}^{pred}$.
    \State Train model $f$ on the analysis set to produce $\hat{f}_{j}(D^{model})$.
    \State Generate assessment set predictions $\hat{y}$ by applying model $\hat{f}_{j}$ to $D_{j}^{pred}$.
    \State Estimate performance statistic $\hat{Q}_{j}$.
  \EndFor    
\Return $\hat{Q}_s$.
\Endprocedure
\end{algorithmic}
\end{algorithm}
```

:::

::: {.column width="15%"}

:::

::::



## What Could Go Wrong?

## Proper Data Usage

```{r}
#| label: proper-rfe
#| include: false
#| cache: true

n_train <- 10^3
n_test <- 10^5
num_extra <- 100
seed_val <- 187

search_sizes <- c(1:9, (1:11) * 10)

# ------------------------------------------------------------------------------

set.seed(seed_val)
sim_tr <- sim_regression(n_train, method = "hooker_2004")
sim_te <- sim_regression(n_test, method = "hooker_2004")

set.seed(seed_val + 1)
sim_tr <- sim_tr %>% bind_cols(sim_noise(n_train, num_extra))
sim_te <- sim_te %>% bind_cols(sim_noise(n_test,  num_extra))

sim_rs <- vfold_cv(sim_tr)

# ------------------------------------------------------------------------------

base_model <- rand_forest(mode = "regression", trees = 1000) %>%
  set_engine("ranger", importance = "permutation")

rf_rec <- recipe(outcome ~ ., data = sim_tr) %>% 
  step_select_vip(
    all_predictors(),
    outcome = "outcome",
    model = base_model,
    top_p = tune()
  )

rf_filter_spec <- 
  rand_forest(mode = "regression", trees = 1000) %>%
  set_engine("randomForest")

rf_filter_wflow <- 
  workflow() %>% 
  add_model(rf_filter_spec) %>% 
  add_recipe(rf_rec)

rf_filter_res <- 
  rf_filter_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    grid = tibble(top_p = search_sizes)
  )

met_avg <- collect_metrics(rf_filter_res) %>% filter(.metric == "rmse")
met_rs <- collect_metrics(rf_filter_res, summarize = FALSE) %>% filter(.metric == "rmse")
```


```{r}
#| label: fig-rfe 
#| echo: false
#| warning: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 4.25
#| fig-cap: "Recursive feature elimination results for one of the simulated sets."

met_rs %>% 
  ggplot(aes(top_p)) + 
  geom_line(aes(y = .estimate, group = id, col = id), show.legend = FALSE, alpha = 1 / 2) +
  geom_line(data = met_avg, aes(y = mean), linewidth = 1) +
  geom_point(data = met_avg, aes(y = mean)) +
  labs(y = "RMSE", x = "# Predictors")
```

## Chapter References {.unnumbered}
