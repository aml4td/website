<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Applied Machine Learning for Tabular Data - 6&nbsp; Embeddings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/interactions-nonlinear.html" rel="next">
<link href="../chapters/categorical-predictors.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<meta name="shinylive:serviceworker_dir" content="..">
<script src="../site_libs/quarto-contrib/shinylive-0.2.3/shinylive/load-shinylive-sw.js" type="module"></script>
<script src="../site_libs/quarto-contrib/shinylive-0.2.3/shinylive/run-python-blocks.js" type="module"></script>
<link href="../site_libs/quarto-contrib/shinylive-0.2.3/shinylive/shinylive.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/shinylive-quarto-css/shinylive-quarto.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title"><span id="sec-embeddings" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Embeddings</span></span></h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../"></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/aml4td/website/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/news.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">News</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/contributing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/whole-game.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Whole Game</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Preparation</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/initial-data-splitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Initial Data Splitting</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/numeric-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Transforming Numeric Predictors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/categorical-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Working with Categorical Predictors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/embeddings.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/interactions-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interactions and Nonlinear Features</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Optimization</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/overfitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Overfitting</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Classification</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Regression</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Characterization</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Finalization</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-barley" id="toc-sec-barley" class="nav-link active" data-scroll-target="#sec-barley"><span class="header-section-number">6.1</span> Example: Predicting Barley Amounts</a></li>
  <li><a href="#sec-linear-embed" id="toc-sec-linear-embed" class="nav-link" data-scroll-target="#sec-linear-embed"><span class="header-section-number">6.2</span> Linear Transformations</a>
  <ul class="collapse">
  <li><a href="#principal-component-analysis" id="toc-principal-component-analysis" class="nav-link" data-scroll-target="#principal-component-analysis"><span class="header-section-number">6.2.1</span> Principal Component Analysis</a></li>
  <li><a href="#independent-component-analysis" id="toc-independent-component-analysis" class="nav-link" data-scroll-target="#independent-component-analysis"><span class="header-section-number">6.2.2</span> Independent Component Analysis</a></li>
  <li><a href="#numeric-pls" id="toc-numeric-pls" class="nav-link" data-scroll-target="#numeric-pls"><span class="header-section-number">6.2.3</span> Partial Least Squares</a></li>
  <li><a href="#overall-comparisons" id="toc-overall-comparisons" class="nav-link" data-scroll-target="#overall-comparisons"><span class="header-section-number">6.2.4</span> Overall Comparisons</a></li>
  </ul></li>
  <li><a href="#sec-mds" id="toc-sec-mds" class="nav-link" data-scroll-target="#sec-mds"><span class="header-section-number">6.3</span> Multidimensional Scaling</a>
  <ul class="collapse">
  <li><a href="#sec-isomap" id="toc-sec-isomap" class="nav-link" data-scroll-target="#sec-isomap"><span class="header-section-number">6.3.1</span> Isomap</a></li>
  <li><a href="#sec-eigenmaps" id="toc-sec-eigenmaps" class="nav-link" data-scroll-target="#sec-eigenmaps"><span class="header-section-number">6.3.2</span> Laplacian Eigenmaps</a></li>
  <li><a href="#sec-umap" id="toc-sec-umap" class="nav-link" data-scroll-target="#sec-umap"><span class="header-section-number">6.3.3</span> UMAP</a></li>
  </ul></li>
  <li><a href="#sec-centroids" id="toc-sec-centroids" class="nav-link" data-scroll-target="#sec-centroids"><span class="header-section-number">6.4</span> Centroid-Based Methods</a></li>
  <li><a href="#other-methods" id="toc-other-methods" class="nav-link" data-scroll-target="#other-methods"><span class="header-section-number">6.5</span> Other Methods</a></li>
  <li><a href="#chapter-references" id="toc-chapter-references" class="nav-link" data-scroll-target="#chapter-references">Chapter References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span id="sec-embeddings" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Embeddings</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>When there are a multitude of predictors, it might be advantegous to condense them into a smaller number of artificial features. To be useful, this smaller set should represent what is essential in the original data. This process is often called <em>feature extraction</em>, <em>dimension reduction</em>, or <em>manifold learning</em>. We’ll use a more general term currently en vogue: <strong>embeddings</strong>. While this chapter focuses on feature extraction, embeddings can be used for other purposes, such as converting non-numeric data (e.g., text) into a more usable numeric format.</p>
<p>This chapter will examine several primary classes of embedding methods that can achieve multiple purposes. First, we’ll consider linear methods that take a numeric input matrix <span class="math inline">\(\boldsymbol{X}\)</span> that is <span class="math inline">\(n \times p\)</span> and create a different, probably smaller set of features <span class="math inline">\(\boldsymbol{X}^*\)</span> (<span class="math inline">\(n \times m\)</span>) using the transformation <span class="math inline">\(\boldsymbol{X}^* = \boldsymbol{X}\boldsymbol{Q}\)</span>. We hope that we can find appropriate embeddings so that <span class="math inline">\(m &lt;&lt; p\)</span>.</p>
<p>After describing linear methods, we will consider a different class of transformations that focuses on the distances between data points called <em>multidimensional scaling</em> (MDS). MDS creates a new set of <span class="math inline">\(m\)</span> features that are not necessarily linear combinations of the original features but often use some of the same math as the linear techniques.</p>
<p>Finally, some embedding techniques specific to classification are discussed. These are based on <em>class centroids</em>.</p>
<p>Before beginning, we’ll introduce another data set that will be used here and in forthcoming chapters.</p>
<section id="sec-barley" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="sec-barley"><span class="header-section-number">6.1</span> Example: Predicting Barley Amounts</h2>
<p><span class="citation" data-cites="larsen2019deep">Larsen and Clemmensen (<a href="#ref-larsen2019deep" role="doc-biblioref">2019</a>)</span> and <span class="citation" data-cites="pierna2020applicability">Fernández P et al. (<a href="#ref-pierna2020applicability" role="doc-biblioref">2020</a>)</span> describe a data set where laboratory measurements are used to predict what percentage of a liquid was lucerne, soy oil, or barley oil<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. An instrument is used to measure how much of particular wavelengths of light are absorbed by the mixture to help determine chemical composition. We will focus on using the lab measurements to predict the percentage of barley oil in the mixture. The distribution of these values is shown in <a href="#fig-barley-data" class="quarto-xref">Figure&nbsp;<span>6.1</span></a>(a).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-barley-data" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-barley-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/fig-barley-data-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-barley-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: (a) The distribution of the outcome for the entire data set. The bar colors reflect the percent barley distribution and are used in subsequent sections. (b) Selected training set spectra for four barley samples. Each line represents the set of 550 predictors in the data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note that most of the data have very little barley oil. About 27% of the data are less than 1%, and the median barley oil percentage is 5.96%.</p>
<p>The 550 predictors are the light absorbance for sequential values in the light region of interest (believed to be from wavelengths between 1300 and 2398 nm). <a href="#fig-barley-data" class="quarto-xref">Figure&nbsp;<span>6.1</span></a>(b) shows a selection of four samples from the data. The darker lines represent samples with lower barley content.</p>
<p>These predictor values, called <em>spectra</em>, have a very high serial correlation between predictors; median correlation between the predictors was 0.98. The high degree of between-predictor correlation can be a major complication for some models and can degrade predictive performance. Therefore, we need methods that will simultaneously decorrelate predictors while extracting useful predictive information for the outcome.</p>
<p>Analyses of similar data sets can be found in <a href="https://bookdown.org/max/FES/illustrative-data-pharmaceutical-manufacturing-monitoring.html">Section 9.1</a> of <span class="citation" data-cites="fes">Kuhn and Johnson (<a href="#ref-fes" role="doc-biblioref">2019</a>)</span> and <span class="citation" data-cites="wtf2024">Johnson and Kuhn (<a href="#ref-wtf2024" role="doc-biblioref">2024</a>)</span>.</p>
<p>In the following computations, each predictor was standardized using the orderNorm transformation mentioned earlier (unless otherwise noted).</p>
<p>The data originated from a modeling competition to find the most accurate model and specific samples were allocated to training and test sets. However, there were no public outcome values for the test set; our analysis will treat the 6,915 samples in their training set as the overall pool of samples. This is enough data to split into separate training (<span class="math inline">\(n_{tr} =\)</span> 4,839), validation (<span class="math inline">\(n_{val} =\)</span> 1,035), and test sets (<span class="math inline">\(n_{te} =\)</span> 1,041). The allocation of samples to each of the three data sets utilized stratified sampling based on the outcome data.</p>
</section>
<section id="sec-linear-embed" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="sec-linear-embed"><span class="header-section-number">6.2</span> Linear Transformations</h2>
<p>The barley data set presents two common challenges for many machine learning techniques:</p>
<ol type="1">
<li>The number of original predictors (550) is fairly large .</li>
<li>The features are highly correlated. Techniques that require inverting the covariance matrix of the features, such as linear regression, will become numerically unstable or may not be able to be fit when features are highly correlated.</li>
</ol>
<p>What can we do if we desire to use a modeling technique that is adversely affected by either (or both) of these characteristics?</p>
<p>Dimensionality reduction is one technique that can help with the first issue (an abundance of columns). As we’ll see below, we might be able to extract new features (<span class="math inline">\(X^*\)</span>) such that the new features optimally summarize information from the original data (<span class="math inline">\(X\)</span>).</p>
<p>For the problem of highly correlated predictors, some embedding methods can additionally <em>decorrelated</em> the predictors by deriving embedded features with minimal correlation.</p>
<p>The three embedding methods we’ll discuss first are linear in a mathematical sense because they transform a table of numeric features into new features that are linear combinations of the original features. These new linear combinations are often called <em>scores</em>. This transformation uses the equation.</p>
<p><span class="math display">\[ \underset{n\times m}{\boldsymbol{X}^*} = \underset{n\times p}{\boldsymbol{X}}\ \underset{p\times m}{\boldsymbol{Q}} \]</span></p>
<p>where <span class="math inline">\(\boldsymbol{Q}\)</span> is a matrix that translates or embeds the original features into a potentially lower dimensional space (<span class="math inline">\(m &lt; p\)</span>) without losing much information. It is easy to see why this matrix operation is linear when the elements of the <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{Q}\)</span> matrices are expanded. For example, the first score for the <span class="math inline">\(i^{th}\)</span> sample would be:</p>
<p><span id="eq-pca-linear-combo"><span class="math display">\[
x^*_{i1} = q_{11} x_{i1} + q_{21}x_{i2} + \ldots +  q_{p1}x_{ip}
\tag{6.1}\]</span></span></p>
<p>The <span class="math inline">\(q_{ij}\)</span> values are often referred to as the <em>loadings</em> for each predictor in the linear combination.</p>
<p>In this section, we will review principal components analysis (PCA), independent component analysis (ICA), and partial least squares (PLS), which are fundamental linear embedding techniques that are effective at identifying meaningful embeddings but estimate <span class="math inline">\(A\)</span> in different ways. PCA and ICA extract embedded features using only information from the original features; they are unsupervised. Conversely, PLS uses the predictors and outcome; it is a supervised technique.</p>
<p>There are many other linear embedding methods that can be used. For example, non-negative matrix factorization <span class="citation" data-cites="lee1999learning lee2000algorithms">(<a href="#ref-lee1999learning" role="doc-biblioref">Lee and Seung 1999</a>, <a href="#ref-lee2000algorithms" role="doc-biblioref">2000</a>)</span>is an embedding method that is useful when the data in <span class="math inline">\(X\)</span> are integer counts or other values that cannot be negative.</p>
<p>To start, we’ll focus on the most often used embedding method: PCA.</p>
<section id="principal-component-analysis" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="principal-component-analysis"><span class="header-section-number">6.2.1</span> Principal Component Analysis</h3>
<p>Karl Pearson introduced PCA over a century ago, yet it remains a fundamental dimension reduction method <span class="citation" data-cites="jolliffe2016principal">(<a href="#ref-jolliffe2016principal" role="doc-biblioref">Jolliffe and Cadima 2016</a>)</span>. Its relevance stems from the underlying objective of the technique: to find linear combinations of the original predictors that maximize amount of variation in the new features<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<div class="note-box">
<p>From a statistical perspective, variation is synonymous with information.</p>
</div>
<p>Simultaneously, the scores produced by PCA (i.e., <span class="math inline">\(X^*\)</span>) are required to be orthogonal to each other. The important side benefit of this technique is that <em>PCA scores are uncorrelated</em>. This is very useful for modeling techniques that need the predictors to be relatively uncorrelated: multiple linear regression, neural networks, support vector machines, and others.</p>
<p>In a way, PCA components have an order or hierarchy. The first principal component has a higher variance than any other component. The second component has a higher variance than subsequent components, and so on. We can think of this process as one of deflation; the first component extracts the largest sources of information/variation in the predictors set. The second component extracts as much as possible for whatever is left behind by the first, and so on. For this reason, we can track how much variation in the original data each component accounts for. We often use the phrase that “PCA chases variation” to achieve its goals.</p>
<p>Suppose we start with <span class="math inline">\(p\)</span> columns<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> in our data. In that case, we can produce up to <span class="math inline">\(p\)</span> PCA components, and their accumulated variation will eventually add up to the total variation in the original <span class="math inline">\(X\)</span>.</p>
<p>While PCA can deliver new features with the desirable characteristics, the technique must be used with care. Specifically, we must understand that PCA seeks to find embeddings without regard to any further understanding of the original features (such as the response). Hence, PCA may generate embeddings that are ignorant of the modeling objective (i.e., they lack predictive information).</p>
<section id="preparing-for-pca" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="preparing-for-pca">Preparing for PCA</h4>
<p>Because PCA seeks linear combinations of predictors that maximize variability, it will naturally first be drawn to summarizing predictors with more variation. If the original predictors are on measurement scales that differ in magnitude, then the first components will focus on summarizing the higher magnitude predictors. This means that the PCA scores will focus on the scales of the measurements (rather than their intrinsic value). If, by chance, the most relevant feature for predicting the response also has the largest numerical values, the new features created by PCA will retain the essential information for predicting the response. However, if the most important features are in the smallest units, then the top features created by PCA will be much less effective.</p>
<p>In most practical machine learning applications, features are on vastly different scales. We suggest using the tools from <a href="numeric-predictors.html#sec-common-scale" class="quarto-xref"><span>Section 4.3</span></a> to transform the data so that they have the same units. This prevents PCA from focusing dimension reduction simply on the measurement scale.</p>
<p>Additionally, the distributions of each feature may show considerable skewness or outliers. While there are no specific distributional requirements to use PCA, it is focused on variance and variance calculations often involve squared terms (e.g., <span class="math inline">\((x_i - \bar{x})^2\)</span>). This type of computation can be very sensitive to outliers and skewness; resolving these issues prior to PCA is recommended. See the methods previously described in <a href="numeric-predictors.html#sec-skewness" class="quarto-xref"><span>Section 4.2</span></a>. When outliers are a specific concern, the spatial-sign transformation might be a good idea <span class="citation" data-cites="visuri2000sign croux2002sign">(<a href="#ref-visuri2000sign" role="doc-biblioref">Visuri, Koivunen, and Oja 2000</a>; <a href="#ref-croux2002sign" role="doc-biblioref">Croux, Ollila, and Oja 2002</a>)</span>.</p>
</section>
<section id="how-does-pca-work" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="how-does-pca-work">How does PCA work?</h4>
<p>Principal component analysis is focused on understanding the relationships between the predictor columns. PCA is only effective when there are correlations between predictors. Otherwise, each new PCA feature would only highlight a single predictor, and you would need the full set of PCA features to approximate the original columns. Conversely, data sets with a handful of intense between-predictor relationships require very few predictors to represent the source columns.</p>
<p>The barley data set has abnormally high correlations between each predictor. These tend to be autoregressive; the correlations between predictors at adjacent wavelengths are very high, and the correlation between a pair of predictors diminishes as you move away from their locations on the spectrum. For example, the correlation between the first and second predictor (in their raw form) is essentially 1 while the corresponding correlation between the first and fiftieth columns is 0.67. We’ll explore the latter pair of columns below.</p>
<p>Before proceeding, let’s go on a small “side-quest” to talk about the mathematics of PCA.</p>
<div class="mathy-box">
<p>The remainder of this subsection discussed a mathematical operation called the singular value decomposition (SVD). PCA and many other computations rely on on the SVD. It is a fundamental linear algebra calculation.</p>
<p>We’ll describe it loosely with a focus on how it is used. The concept of eigenvalues will come up again in the next section and in subsequent chapters. <span class="citation" data-cites="banerjee2014linear">Banerjee and Roy (<a href="#ref-banerjee2014linear" role="doc-biblioref">2014</a>)</span> and <span class="citation" data-cites="aggarwal2020linear">Aggarwal (<a href="#ref-aggarwal2020linear" role="doc-biblioref">2020</a>)</span> are thorough but helpful resources for learning more.</p>
</div>
<p>The SVD process is intended to find a way to <em>rotate</em> the original data in a way that the resulting columns are orthogonal to one another. It takes a square matrix as an input and can decompose it into several pieces:</p>
<p><span class="math display">\[\underset{p\times p}{A} = \underset{p\times p}{Q}\quad\underset{p\times p}{\Lambda}\quad \underset{p\times p}{Q'}\]</span></p>
<p>The results of this computation are the <span class="math inline">\(p\)</span> eigenvectors <span class="math inline">\(\mathbf{q}_j\)</span> and eigenvalues <span class="math inline">\(\lambda_j\)</span>. The eignevectors tell you about the directions in which values of <span class="math inline">\(A\)</span> are moving and the eigenvalues describe the corresponding magnitudes (e.g.&nbsp;how far they go in their corresponding direction).</p>
<p>The transformation works with PCA by using the covariance matrix as the input <span class="math inline">\(A\)</span>. We are trying to find trends in the relationships between variables and the SVD is designed to translate the original matrix to one that is orthogonal (that is, uncorrelated). This aspect of the SVD is how it connects to PCA. If we want to approximate our original data with new features that are uncorrelated, we need a transformation to orthogonality.</p>
<p>The matrix <span class="math inline">\(Q\)</span> houses the <span class="math inline">\(p\)</span> possible eigenvectors. These are the values by which you multiply the original matrix <span class="math inline">\(A\)</span> to achieve an orthogonal version. For PCA, they are the loading values shown in <a href="#eq-pca-linear-combo" class="quarto-xref">Equation&nbsp;<span>6.1</span></a>. The matrix <span class="math inline">\(\Lambda\)</span> is a diagonal matrix whose <span class="math inline">\(p\)</span> values are the eigenvalues. It turns out that the eigenvalues represent the amount of variation (i.e., information) captured by each component.</p>
<p>If we conduct the SVD (and PCA) on the covariance matrix of the data, we are again subject to issues around potentially different units and scales of the predictors. Previously, we suggested to center and scale the data prior to PCA. The equivalent approach here is to conduct the SVD on the correlation matrix of the data. Similar to our recommendations on standardization, we recommend using the correlation as the input to PCA.</p>
<p>One interesting side-effect of using the covariance or correlation matrix as the input to PCA is related to missing data (discussed in more detail in <span class="quarto-unresolved-ref">?sec-missing-data</span>). The conventional sample covariance (or correlation) matrices can be computed for each matrix element (i.e., without using matrix multiplication). Each covariance or correlation can be computed with whatever rows of the two predictors have pairwise-complete results. This means that we do not have to globally drop specific rows of the data when a small number of columns contain missing data.</p>
</section>
<section id="a-two-dimensional-example" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="a-two-dimensional-example">A two dimensional example</h4>
<p>To demonstrate, we chose two predictors in the barley data. They correspond to wavelengths<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> 1,300 and 1,398 shown in <a href="#fig-barley-data" class="quarto-xref">Figure&nbsp;<span>6.1</span></a>(b). In the data, these two predictors were preprocessed using the ORQ procedure (<a href="numeric-predictors.html#sec-skewness" class="quarto-xref"><span>Section 4.2</span></a>). The standardized versions of the predictors have a correlation of 0.68 and are shown in <a href="#fig-pca-rotation" class="quarto-xref">Figure&nbsp;<span>6.2</span></a>(a).</p>
<p>PCA was performed on these data, and the results are animated in <a href="#fig-pca-rotation" class="quarto-xref">Figure&nbsp;<span>6.2</span></a>(b). This visualization shows the original data (colored by their outcome data) and then rotates it around the center position. The PCA transformation corresponds to the rotation where the x-axis has the largest spread. The variance is largest at <em>two</em> angles (135<span class="math inline">\(^{\circ}\)</span> and 315<span class="math inline">\(^{\circ}\)</span>) since the solution for PCA is <em>unique up to sign</em>. Both possible solutions are correct, and we usually pick one. For one solution, the PCA loading coefficients are:</p>
<p><span class="math display">\[\begin{align}
x^*_{i1} &amp;= -0.71 x_{i1} -0.71 x_{i2} \notag \\
x^*_{i2} &amp;= -0.71 x_{i1} + 0.71 x_{i2} \notag
\end{align}\]</span></p>
<p>Because of this illustrative example’s low-dimensional aspect, this solution is excessively simple. Ordinarily, the loading values take a great many values.</p>
<div id="fig-pca-rotation" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-rotation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figures/barley-ord-orig-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<div class="cell quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../premade/anime_barley_pca.gif" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-rotation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: A demonstration that PCA is just a rotation of the data. The original predictors (those at the first and fiftieth wavelengths) are in Panel (a). The animation in (b) shows the rotation of two of the predictors, which stops at the two optimal rotations corresponding to the principal components and the original data.
</figcaption>
</figure>
</div>
<p>Again,the remarkable characteristic of this specific set of linear combinations is that they are orthogonal (i.e., uncorrelated) with one another.</p>
<div class="note-box">
<p>Using PCA to transform the data into a full set of uncorrelated variables is often called “whitening the data.” It retains all of the original information and is more pliable to many computations. It can be used as a precursor to other algorithms.</p>
</div>
<p>Now that we’ve seen PCA on a small scale, let’s look at an analysis of the full set of predictors</p>
</section>
<section id="pca-on-the-barley-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="pca-on-the-barley-data">PCA on the barley data</h4>
<p>We’ll repeat the analysis in the preceding section; PCA was calculated on the training set using all predictors. There are 550 feature columns and the same number of possible principal components. The number of new features to retain is a parameter that must be determined. In practice, the number of new features is often selected based on the percentage of variability that the new features summarize relative to the original features. <a href="#fig-barley-pca-scree" class="quarto-xref">Figure&nbsp;<span>6.3</span></a> displays the new PCA feature number (x-axis) versus the percent of total variability across the original features (commonly called a “scree plot”). In this graph, three components summarize 99.4% of the total variability. We often select the smallest number of new features that summarize the greatest variability. Again, this is an unsupervised optimization. To understand how the number of components affects supervised predictors (e.g., RMSE), the number of features can be treated as a tuning parameter .</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-barley-pca-scree" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-barley-pca-scree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/fig-barley-pca-scree-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-barley-pca-scree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: A scree plot of the PCA component number versus the percent of variance explained for the first 10 components of the barley data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note that the first component alone captured 92.3% of the variation in the 550 training data columns. This reflects the extreme correlation seen in the predictors.</p>
<p><a href="#fig-linear-scores" class="quarto-xref">Figure&nbsp;<span>6.4</span></a> shows a scatterplot of the principal components colored by the percentage of barley oil. The figure reveals that the first PCA component delineates the higher barley oil samples from those with less oil. There also appear to be three different clusters of samples with very low (if any) barley oil. It might be helpful to investigate these samples to ascertain if they are artifacts or caused by some systematic, underlying factors not in the current set of columns. The pairing of the second and fourth components appears to help differentiate lower barely samples from the broader set. The pairing of the third and fourth components doesn’t offer much predictive power on their own.</p>
<div id="fig-linear-scores" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-linear-scores-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| label: fig-linear-scores
#| out-width: "80%"
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)
library(dplyr)
library(tidyr)
library(purrr)
library(ggforce)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-linear-scores.R")

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-linear-scores-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: A visualization of the four new features for different linear embedding methods. The data shown are the validation set results.
</figcaption>
</figure>
</div>
<p>One note about this visualization. The axis scales are not common across panels (for ease of illustration). If we were to keep a common scale, the later components would appear to have very little effect due to the flatness of the resulting figures. When looking at the scores, we suggest keeping a common scale, which will most likely be the scale of the first component. This will help avoid over-interpreting patterns in the later components.</p>
<p>For PCA, it can be very instructive to visualize the loadings for each component. This can help in several different ways. First, it can tell which predictors dominate each specific new PCA component. If a particular component ends up having a strong relationship with the outcome, this can aid our explanation of how the model works. Second, we can examine the magnitude of the predictors to determine some of the relationships between predictors. If a group of predictors have approximately the same loading value, this implies that they have a common relationship with one another. This, on its own, can be a significant aid when conducting exploratory data analysis on a high-dimensional data set. <a href="#fig-linear-loadings" class="quarto-xref">Figure&nbsp;<span>6.5</span></a> shows the relationship between the PCA loadings and each predictor’s position on the spectrum.</p>
<div id="fig-linear-loadings" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-linear-loadings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| label: fig-linear-loadings
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)
library(dplyr)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-linear-loadings.R")

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-linear-loadings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: The loadings for the first four components of each linear embedding method as a function of wavelength.
</figcaption>
</figure>
</div>
<p>For the first component, wavelengths at the low ends of the spectrum have a relatively small impact that increases as you get to about 1,400 nm. At this point, the predictors have a constant effect on that component. The second PCA component is almost the opposite; it emphasizes the smaller wavelengths while predictors above 1,400 nm fluctuate around zero. The third and fourth components emphasize different areas of the spectrum and are generally different than zero.</p>
</section>
<section id="how-to-incorporate-pca-into-the-model" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="how-to-incorporate-pca-into-the-model">How to incorporate PCA into the model</h4>
<p>The barley data demonstrates that PCA can effectively capture the training set information in a smaller predictor set when there is a correlation between predictors. The number of components to retain depends on the data. It is a good idea to optimize the number of components to retain via model tuning.</p>
<div class="note-box">
<p>Note that “dimension reduction” is <em>not</em> the same as feature selection. The PCA components described above are functions of all of the predictors. Even if you only need a handful of PCA components to approximate the training set, the original predictor set is still required when predicting new samples.</p>
</div>
<p>Another aspect of using PCA for feature extraction is the scale of the resulting scores. <a href="#fig-linear-scores" class="quarto-xref">Figure&nbsp;<span>6.4</span></a> demonstrates that the first component has the largest variance, resulting in a wider range of training set values. For the validation set scores, the variance of PC<sub>1</sub> is 14-fold larger than the variance of PC<sub>2</sub>. Even though the scores are unitless, there may be issues for models that expect the predictors to have a common scale, similar to the requirement for PCA itself. In these cases, it may be advisable to standardize the PCA components to have the same range before serving them to the supervised ML model.</p>
</section>
<section id="non-standard-pca" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="non-standard-pca">Non-standard PCA</h4>
<p>Various methods exist for estimating the PCA loadings. Some are well-suited to different dimensions of the predictor’s data. For example, “small <span class="math inline">\(n\)</span>, large <span class="math inline">\(p\)</span>” data sizes can make PCA computations difficult. <span class="citation" data-cites="wu1997kernel">Wu, Massart, and De Jong (<a href="#ref-wu1997kernel" role="doc-biblioref">1997</a>)</span> and others describe a <em>kernel</em> method more appropriate for very wide data sets. <span class="citation" data-cites="scholkopf1998nonlinear">Schölkopf, Smola, and Müller (<a href="#ref-scholkopf1998nonlinear" role="doc-biblioref">1998</a>)</span> extended this even further but using nonlinear kernel methods<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. Still, others use modified objective functions to compensate for some potential deficiencies of the canonical technique.</p>
<p>For example, the standard loading matrix (i.e., the eigenvectors) are <em>dense</em>. It is possible that the loadings for some predictors shouldn’t affect specific components. The SVD might estimate these to be close to zero but not exactly zero. Sparse PCA techniques can, through various means, estimate some loading values to be exactly zero, indicating that the predictor has no functional effect on the embedding.</p>
<p>For example, <span class="citation" data-cites="shen2008sparse">Shen and Huang (<a href="#ref-shen2008sparse" role="doc-biblioref">2008</a>)</span> take a penalization approach that puts a high price for loadings to have very large values. This regularization method can shrink the values towards zero and make some absolute zero. This is similar to the regularization approach seen in <a href="categorical-predictors.html#sec-effect-encodings" class="quarto-xref"><span>Section 5.4.3</span></a> and is directly related to techniques discussed in <span class="quarto-unresolved-ref">?sec-logistic-reg</span>, <span class="quarto-unresolved-ref">?sec-nnet-cls</span>, and <span class="quarto-unresolved-ref">?sec-linear-reg</span>. This option can coerce values across the loadings to be zero. It is unlikely to force a particular predictor’s values to be zero across all components. In other words, it may not completely erase the effect of a predictor from the embedding altogether.</p>
<p>Another method is based on a Bayesian approach <span class="citation" data-cites="ning2021spike">(<a href="#ref-ning2021spike" role="doc-biblioref">Ning and Ning 2021</a>)</span>, where the loading values are assumed to come from a mixture of two different distributions. One has a sharp “spike” around zero, and the other is a flat, wide distribution that encompasses a wide range of values (called the “slab”). The method then estimates the parameters to favor one distribution or the other and sets some proportion of the loadings to zero.</p>
<p>Additionally, there are PCA variants for non-numeric data, such as categorical predictors. Probabilistic PCA <span class="citation" data-cites="tipping1999probabilistic pmlrvR4schein03a">(<a href="#ref-tipping1999probabilistic" role="doc-biblioref">Tipping and Bishop 1999</a>; <a href="#ref-pmlrvR4schein03a" role="doc-biblioref">Schein, Saul, and Ungar 2003</a>)</span> uses a PCA generalization to reduce the dimensions of qualitative data.</p>
</section>
</section>
<section id="independent-component-analysis" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="independent-component-analysis"><span class="header-section-number">6.2.2</span> Independent Component Analysis</h3>
<p>ICA has roots in signal processing, where the observed signal that is measured is a combination of several different input signals.</p>
<p>For example, electroencephalography (EEG) uses a set of electrodes on a patient’s scalp to noninvasively measure the brain’s electrical activity over time. The electrodes are placed on specific parts of the scalp and measure different brain regions (e.g., prefrontal cortex etc.). However, since the electrodes are near others, they can often measure mixtures of multiple underlying signals.</p>
<p>The objective of independent component analysis is to identify linear combinations of the original features that are <em>statistically independent</em> of each other <span class="citation" data-cites="nordhausen2018independent hyvarinen2000independent">(<a href="#ref-hyvarinen2000independent" role="doc-biblioref">Hyvärinen and Oja 2000</a>; <a href="#ref-nordhausen2018independent" role="doc-biblioref">Nordhausen and Oja 2018</a>)</span>.</p>
<div class="note-box">
<p>Two variables can be uncorrelated but still have a relationship (most likely nonlinear). For example, suppose we have a variable <span class="math inline">\(z\)</span> that is a sequence of evenly spaced values between -2 and 2. The correlation between <span class="math inline">\(z\)</span> and <span class="math inline">\(z^2\)</span> is very close to zero but there is a clear and precise relationship between them, therefore the variables are not independent.</p>
</div>
<p>In practice, satisfying the statistically independence requirement of ICA can be done several ways. One approach maximizes the “non-Gaussianity” of the resulting components. As shown by <span class="citation" data-cites="hyvarinen2000independent">Hyvärinen and Oja (<a href="#ref-hyvarinen2000independent" role="doc-biblioref">2000</a>)</span>,</p>
<blockquote class="blockquote">
<p>The fundamental restriction in ICA is that the independent components must be nongaussian for ICA to be possible.</p>
</blockquote>
<p>There are different ways to measure non-Gaussianity. The <em>fastICA</em> algorithm uses an information theory statistic called <em>negentropy</em> <span class="citation" data-cites="hyvarinen2000independent">(<a href="#ref-hyvarinen2000independent" role="doc-biblioref">Hyvärinen and Oja 2000</a>)</span>. Another approach called InfoMax uses neural networks to estimate a different information theory statistic.</p>
<p>Because ICA’s objective requires statistical independence among components, it will generate features different from those of PCA. Unlike PCA, which orders new components based on summarized variance, ICA’s components have no natural ordering. This means that ICA may require more components than PCA to find the ones related to the outcome.</p>
<p>The predictors should be centered (and perhaps scaled) before estimating the ICA loadings. Additionally, some ICA algorithms internally add a PCA step to “whiten” the data before computing the ICA loadings (using all possible PCA components). Since independent component analysis thrives on non-Gaussian data, we should avoid transformations that induce a more symmetric distribution. For this reason, we will not use the ORD transformation to preprocess the predictors. Finally, the ICA loadings are often initialized before training using random values. If this randomness is not controlled, different results will likely occur from training run to training run.</p>
<p><a href="#fig-linear-scores" class="quarto-xref">Figure&nbsp;<span>6.4</span></a> contains a scatterplot matrix of the first 4 ICA components colored by the outcome. In this figure, Two components that appear to differentiate levels of barley by themselves (components two and three). Unsurprisingly, their interaction appears to be the one that visually differentiates the different amounts of barley oil in the validation set.</p>
<p>The loadings are shown in <a href="#fig-linear-loadings" class="quarto-xref">Figure&nbsp;<span>6.5</span></a> and tell an interesting story. The pattern of loadings one and three have similar patterns over the wavelengths. The same situation is the case for components two and four. These two pairs of trends in the loadings are somewhat oppositional to one another. Also, the patterns have little in common with the PCA loadings. For example, the first PCA loading was relatively constant across wavelengths. None of the ICA components show a constant pattern.</p>
</section>
<section id="numeric-pls" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="numeric-pls"><span class="header-section-number">6.2.3</span> Partial Least Squares</h3>
<p>Both PCA and ICA focus strictly on summarizing information based on the features exclusively. When the outcome is related to the way that the unsupervised techniques extract the embedded features, then PCA and/or ICA can be effective dimension reduction tools. However, when the qualities of variance summarization or statistical independence are not related to the outcome, then these techniques may struggle to identify appropriate embedded features that are useful for predicting the outcome.</p>
<p>Partial least squares (PLS) <span class="citation" data-cites="Wold1983">(<a href="#ref-Wold1983" role="doc-biblioref">Wold, Martens, and Wold 1983</a>)</span> is a dimension reduction technique that identifies embedded features that are optimally linearly related to the outcome. While the objective of PCA is to find linear combinations of the original features that best summarize variability, the objective of PLS is to do the same <em>and</em> to find the linear combinations that are correlated with the outcome <span class="citation" data-cites="stone1990continuum">(<a href="#ref-stone1990continuum" role="doc-biblioref">Stone and Brooks 1990</a>)</span>. This process can be thought of as a constrained optimization problem in which PLS is forced to compromise between maximizing information from the predictors and simultaneously maximizing the prediction of the outcome. This means that the outcome is used to focus the dimension reduction process such that the new features have the greatest correlation with the outcome. For more detailed information, we suggest <span class="citation" data-cites="Geladi1986">Geladi and Kowalski (<a href="#ref-Geladi1986" role="doc-biblioref">1986</a>)</span> and <span class="citation" data-cites="esposito2013partial">Esposito Vinzi and Russolillo (<a href="#ref-esposito2013partial" role="doc-biblioref">2013</a>)</span>.</p>
<p>Because one of the objectives of PLS is to summarize variance among the original features, the features should be pre-processed in the same way as PCA.</p>
<p>For the barley data, the first few sets of PLS loads are fairly similar to those generated by PCA. The first three components have almost identical values as their PCA analogs. The fourth component has different loadings, and the correlation between the fourth PCA and PLS scores is -0.63. The similarities in the first three scores and loadings between the methods can be seen in <a href="#fig-linear-scores" class="quarto-xref">Figure&nbsp;<span>6.4</span></a> and <a href="#fig-linear-loadings" class="quarto-xref">Figure&nbsp;<span>6.5</span></a>, respectively.</p>
<p>If our goal is to maximize predictive performance, fewer features are necessary when using PLS. As we’ll see below, we can sometimes achieve similar performance using PCA or ICA but we will need more features to match what PLS can do with less.</p>
<p>Like principal component analysis, there are many modified versions of partial least squares. For example, <span class="citation" data-cites="le2008sparse">Lê Cao et al. (<a href="#ref-le2008sparse" role="doc-biblioref">2008</a>)</span> and <span class="citation" data-cites="le2008sparse">Lê Cao et al. (<a href="#ref-le2008sparse" role="doc-biblioref">2008</a>)</span> describe a sparse estimation routine that can set some loadings equal to absolute zero. Also, <span class="citation" data-cites="barker2003partial">Barker and Rayens (<a href="#ref-barker2003partial" role="doc-biblioref">2003</a>)</span> and <span class="citation" data-cites="Liu2007b">Y. Liu and Rayens (<a href="#ref-Liu2007b" role="doc-biblioref">2007</a>)</span> adapt the algorithm for classification problems (i.e., qualitative outcomes).</p>
</section>
<section id="overall-comparisons" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="overall-comparisons"><span class="header-section-number">6.2.4</span> Overall Comparisons</h3>
<p>How do these three approaches differ in terms of predictive performance? To evaluate this, these embeddings were computed with up to 25 new features and used as the inputs for a linear regression model. After the embeddings and regression parameters were estimated, the validation set RMSE was computed. <a href="#fig-barley-linear-bakeoff" class="quarto-xref">Figure&nbsp;<span>6.6</span></a> has the results. The bands around each curve are 90% confidence intervals for the RMSE.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-barley-linear-bakeoff" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-barley-linear-bakeoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/fig-barley-linear-bakeoff-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-barley-linear-bakeoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.6: Results for linear regressions using three different linear embeddings.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Overall, the patterns are similar. By the time 25 components are added, there is parity between the methods. However, PLS appears to be more efficient at finding predictive features since its curve has uniformly smaller RMSE values than the others . This isn’t surprising since it is the only supervised embedding of the three.</p>
<p>PCA and PLS will be discussed in more detail in <span class="quarto-unresolved-ref">?sec-colinearity</span>.</p>
</section>
</section>
<section id="sec-mds" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec-mds"><span class="header-section-number">6.3</span> Multidimensional Scaling</h2>
<p>Multidimensional scaling <span class="citation" data-cites="torgerson1952multidimensional">(<a href="#ref-torgerson1952multidimensional" role="doc-biblioref">Torgerson 1952</a>)</span> is a feature extraction tool that creates embeddings that try to preserve the geometric distances between training set points. In other words, the distances between points in the smaller dimensions should be comparable to those in the original dimensions. Since the methods in this section use distances, the predictors should be standardized to equivalent units before the embedding is trained. As with PCA, we also recommend transformations to resolve skewness.</p>
<p>Take <a href="#fig-mds-example" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>(a) as an example. There are ten points in two dimensions (colored by three outcome classes). If we were to project these points down to a single dimension, we’d like points that are close in the original two dimensions to remain close when projected down to a single dimension. Panel (c) shows two such solutions. Each does reasonably well with some exceptions (i.e., points six and nine are too close for non-Metric MDS).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mds-example" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mds-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/fig-mds-example-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mds-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.7: (a) A collection of samples associated with three outcome classes. (b) A diagram of the two nearest neighbors for each data point. (c) A one-dimensional projection using two different methods: non-metric MDS and Isomap.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Here we present a few MDS methods, but there are many more. <span class="citation" data-cites="Ghojogh2023">Ghojogh et al. (<a href="#ref-Ghojogh2023" role="doc-biblioref">2023</a>)</span> has an excellent review of an assortment of methods and their nuances.</p>
<p>Some MDS methods compute all pairwise distances between points and use this as the input to the embedding algorithm. This is similar to how PCA can be estimated using the covariance or correlation matrices. One technique, <em>Non-Metric MDS</em> <span class="citation" data-cites="kruskal1964multidimensional kruskal1964nonmetric sammon1969nonlinear">(<a href="#ref-kruskal1964multidimensional" role="doc-biblioref">Kruskal 1964a</a>, <a href="#ref-kruskal1964nonmetric" role="doc-biblioref">1964b</a>; <a href="#ref-sammon1969nonlinear" role="doc-biblioref">Sammon 1969</a>)</span>, finds embeddings that minimize an objective function called “stress”:</p>
<p><span class="math display">\[
\text{Stress} = \sqrt{\frac{\sum\limits^{n_{tr}}_{i = 1}\;\sum\limits^{n_{tr}}_{j = i+1}\left(d(x_i, x_j) - d(x^*_i, x^*_j)\right)^2}{\sum\limits^{n_{tr}}_{i = 1}\;\sum\limits^{n_{tr}}_{j = i+1}d(x_i, x_j)^2}}
\]</span></p>
<p>The numerator uses the squared difference between the pairwise distances in the original values (<span class="math inline">\(x\)</span>) and the smaller embedded dimension (<span class="math inline">\(x^*\)</span>). The summations only move along the upper triangle of the distance matrices to reduce redundant computations. <a href="#fig-mds-example" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>(c, top row) has the resulting one dimensional projection of our two-dimensional data.</p>
<p>This can be an effective dimension reduction procedure, although there are a few issues. First, the entire matrix of distances is required (with <span class="math inline">\(n_{tr}(n_{tr}-1)/2\)</span> entries). For large training sets, this can be unwieldy and time-consuming. Second, like PCA, it is a global method that uses all data in the computations. We might be able to achieve more nuanced embeddings by focusing on local structures. Finally, it is challenging to apply metric MDS to project new data onto the space in which the original data was projected.</p>
<p>For these reasons, let’s take a look at more modern versions of multidimensional scaling.</p>
<section id="sec-isomap" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="sec-isomap"><span class="header-section-number">6.3.1</span> Isomap</h3>
<p>To start, we’ll focus on <em>Isomap</em> <span class="citation" data-cites="tenenbaum2000global">(<a href="#ref-tenenbaum2000global" role="doc-biblioref">Tenenbaum, Silva, and Langford 2000</a>)</span>. This nonlinear MDS method uses a specialized distance function to find the embedded features. First, the <em>K</em> nearest neighbors are determined for each training set point using standard functions, such as Euclidean distance. <a href="#fig-mds-example" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>(b) shows the <em>K</em> = 2 nearest neighbors for our example data. Many nearest-neighbor algorithms can be very computationally efficient and their use eliminates the need to compute all of the pairwise distances.</p>
<p>The connections between neighbors form a <em>graph structure</em> that qualitatively defines which data points are closely related to one another. From this, a new metric called <em>geodesic distance</em> can be approximated. For a graph, we can compute the approximate geodesic distance using the shortest path between two points on the graph. With our example data, the Euclidean distance between points four and five is not large. However, its approximate geodesic distance is greater because the shortest path is through points nine, eight, and seven. <span class="citation" data-cites="Ghojogh2023">Ghojogh et al. (<a href="#ref-Ghojogh2023" role="doc-biblioref">2023</a>)</span> use a wonderful analogy:</p>
<blockquote class="blockquote">
<p>A real-world example is the distance between Toronto and Athens. The Euclidean distance is to dig the Earth from Toronto to reach Athens directly. The geodesic distance is to move from Toronto to Athens on the curvy Earth by the shortest path between two cities. The approximated geodesic distance is to dig the Earth from Toronto to London in the UK, then dig from London to Frankfurt in Germany, then dig from Frankfurt to Rome in Italy, then dig from Rome to Athens.</p>
</blockquote>
<p>The Isomap embeddings are a function of the eigenvalues computed on the geodesic distance matrix. The <span class="math inline">\(m\)</span> embedded features are functions of the first <span class="math inline">\(m\)</span> eigenvectors. Although eigenvalues are associated with linear embeddings (e.g., PCA), nonlinear geodesic distance results in a local nonlinear embedding. <a href="#fig-mds-example" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>(c, bottom row) shows the 1D results for the example data set. For a new data point, its nearest-neighbors in the training set are determined so that the approximate geodesic distance can be computed. The estimated eigenvectors and eigenvalues are used to project the new point into the embedding space.</p>
<p>For Isomap, the number of nearest neighbors and the number of embeddings are commonly optimized. <a href="#fig-barley-isomap" class="quarto-xref">Figure&nbsp;<span>6.8</span></a> shows a two-dimensional Isomap embedding for the barley data with varying numbers of neighbors. In each configuration, the higher barley values are differentiated from the small (mostly zero) barley samples. We again see the two or three clusters of data associated with small outcome values. The new features become more densely packed as the number of neighbors increases.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-barley-isomap" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-barley-isomap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/fig-barley-isomap-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-barley-isomap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.8: Isomap for the barley data for different numbers of nearest neighbors. The training set was used to fit the model and these results show the projections on the validation set. Lighter colors indicate larger values of the outcome.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-eigenmaps" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="sec-eigenmaps"><span class="header-section-number">6.3.2</span> Laplacian Eigenmaps</h3>
<p>There are many other approaches to preserve local distances. One is <em>Laplacian eigenmaps</em> <span class="citation" data-cites="belkin2001laplacian">(<a href="#ref-belkin2001laplacian" role="doc-biblioref">Belkin and Niyogi 2001</a>)</span>. Like Isomap, it uses nearest neighbors to define a graph of connected training set points. For each connected point, a weight between graph nodes is computed that becomes smaller as the distance between points in the input space increases. The radial basis kernel (also referred to as the “heat kernel”) is a good choice for the weighting function<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>:</p>
<p><span class="math display">\[
w_{ij} = \exp\left(\frac{-||\boldsymbol{x}_i - \boldsymbol{x}_j||^2}{\sigma}\right)
\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is a scaling parameter that can be tuned. If two points are not neighbors, or if <span class="math inline">\(i = j\)</span>, then <span class="math inline">\(w_{ij} = 0\)</span>. Note that the equation above uses Euclidean distance. For the 2-nearest neighbor graph shown in <a href="#fig-mds-example" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>(b) and <span class="math inline">\(\sigma = 1 / 2\)</span>, the weight matrix is roughly</p>
<div class="cell" data-layout-align="center">
<p><span class="math display">\[
\newcommand{\0}{{\color{lightgray} 0.0}}
\boldsymbol{W} = \begin{bmatrix}
\0 &amp; 0.1 &amp; \0 &amp; 0.2 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0\\
&amp; \0 &amp; 0.4 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0\\
&amp;  &amp; \0 &amp; 0.1 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0\\
&amp;  &amp;  &amp; \0 &amp; \0 &amp; \0 &amp; 0.1 &amp; \0 &amp; \0 &amp; \0\\
&amp;  &amp;  &amp;  &amp; \0 &amp; 0.4 &amp; \0 &amp; \0 &amp; 0.1 &amp; \0\\
&amp;  &amp; sym &amp;  &amp;  &amp; \0 &amp; \0 &amp; \0 &amp; 0.1 &amp; \0\\
&amp;  &amp;  &amp;  &amp;  &amp;  &amp; \0 &amp; 0.4 &amp; \0 &amp; 0.1\\
&amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp; \0 &amp; 0.1 &amp; 0.3\\
&amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp; \0 &amp; \0\\
&amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp; \0
\end{bmatrix}
\]</span></p>
</div>
<p>The use of nearest neighbors means that the matrix can be very sparse and the zero values help define locality for each data point. Recall that samples 2 and 3 are fairly close to one another, while samples 1 and 2 are farther away. The weighting scheme gives the former pair a 4-fold larger weight in the graph than the latter pair.</p>
<p>Laplacian eigenmaps rely heavily on graph theory. This method computes a <em>graph Laplacian</em> matrix, defined as <span class="math inline">\(\boldsymbol{L} = \boldsymbol{D} - \boldsymbol{W}\)</span> where the matrix <span class="math inline">\(\boldsymbol{D}\)</span> has zero non-diagonal entries and diagonals equal to the sum of the weights for each row. For our example data:</p>
<div class="cell" data-layout-align="center">
<p><span class="math display">\[
\newcommand{\0}{{\color{lightgray} 0.0}}
L = \begin{bmatrix}
0.3 &amp; -0.1 &amp; \0 &amp; -0.2 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0\\
&amp;  0.5 &amp; -0.4 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0\\
&amp;  &amp;  0.5 &amp; -0.1 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0 &amp; \0\\
&amp;  &amp;  &amp;  0.4 &amp; \0 &amp; \0 &amp; -0.1 &amp; \0 &amp; \0 &amp; \0\\
&amp;  &amp;  &amp;  &amp;  0.5 &amp; -0.4 &amp; \0 &amp; \0 &amp; -0.1 &amp; \0\\
&amp;  &amp; sym &amp;  &amp;  &amp;  0.5 &amp; \0 &amp; \0 &amp; -0.1 &amp; \0\\
&amp;  &amp;  &amp;  &amp;  &amp;  &amp;  0.6 &amp; -0.4 &amp; \0 &amp; -0.1\\
&amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  0.8 &amp; -0.1 &amp; -0.3\\
&amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  0.3 &amp; \0\\
&amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  0.4
\end{bmatrix}
\]</span></p>
</div>
<p>The eigenvalues and eigenvectors of this matrix are used as the main ingredients for the embeddings. <span class="citation" data-cites="Bengio2003advances">Bengio et al. (<a href="#ref-Bengio2003advances" role="doc-biblioref">2003</a>)</span> shows that, since these methods eventually use eigenvalues in the embeddings, they can be easily used to project new data.</p>
</section>
<section id="sec-umap" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="sec-umap"><span class="header-section-number">6.3.3</span> UMAP</h3>
<p>The Uniform Manifold Approximation and Projection (UMAP) <span class="citation" data-cites="sainburg2020parametric">(<a href="#ref-sainburg2020parametric" role="doc-biblioref">Sainburg, McInnes, and Gentner 2020</a>)</span> technique is one of the most popular distance-based methods. Its precursors, stochastic neighbor embedding (SNE) <span class="citation" data-cites="hinton2002stochastic">(<a href="#ref-hinton2002stochastic" role="doc-biblioref">Hinton and Roweis 2002</a>)</span> and Student’s t-distributed stochastic neighbor embedding (t-SNE) <span class="citation" data-cites="van2008visualizing">(<a href="#ref-van2008visualizing" role="doc-biblioref">Van der Maaten and Hinton 2008</a>)</span>, redefined feature extraction, particularly for visualizations. UMAP borrows significantly from Laplacian eigenmaps and t-SNE but has a more theoretically sound motivation.</p>
<p>As with Laplacian eigenmaps, UMAP converts the training data points to a sparse graph structure. Given a set of <em>K</em> nearest neighbors, it computes values similar to the previously shown weights (<span class="math inline">\(\boldsymbol{W}\)</span> matrix), which we will think of as the probability that point <span class="math inline">\(j\)</span> is a neighbor of point <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
p_{j|i} = \exp\left(\frac{-\left(||\boldsymbol{x}_i - \boldsymbol{x}_j||^2 - \rho_i\right)}{\sigma_i}\right)
\]</span></p>
<p>where <span class="math inline">\(\rho_i\)</span> is the distance from <span class="math inline">\(\boldsymbol{x}_i\)</span> to its closest neighbor, and <span class="math inline">\(\sigma_i\)</span> is a scale parameter that now varies with each sample (<span class="math inline">\(i\)</span>). To compute <span class="math inline">\(\sigma_i\)</span>, we can solve the equation</p>
<p><span class="math display">\[
\sum_{i=1}^K  \exp\left(\frac{-\left(||\boldsymbol{x}_i - \boldsymbol{x}_j||^2 - \rho_i\right)}{\sigma_i}\right) = \log_2(K)
\]</span></p>
<p>Unlike the previous weighting system, the resulting <span class="math inline">\(n \times n\)</span> matrix may not be symmetric, so the final weights are computed using <span class="math inline">\(p_{ij} = p_{j|i} + p_{i|j} - p_{j|i}p_{i|j}\)</span>.</p>
<p>UMAP performs a similar weight calculation for the embedded values <span class="math inline">\(\boldsymbol{x}^*\)</span>. We’ll denote the probability that embedded points <span class="math inline">\(\boldsymbol{x}_i^*\)</span> and <span class="math inline">\(\boldsymbol{x}_j^*\)</span> are connected as <span class="math inline">\(p_{ij}^*\)</span>. We’d like the algorthim to estimate these values such that <span class="math inline">\(p_{ij} \approx p_{ij}^*\)</span>.</p>
<p>Numerical optimization methods<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> used to estimate the <span class="math inline">\(n \times m\)</span> values <span class="math inline">\(x^*_{ij}\)</span>. The process is initialized using a very sparse Laplacian eigenmap, the first few PCA components, or random uniform numbers. The objective function is based on cross-entropy and attempts to make the graphs in the input and embedded dimensions as similar as possible by minimizing:</p>
<p><span class="math display">\[
CE = \sum_{i=1}^{n_{tr}}\sum_{j=i+1}^{n_{tr}} \left[p_{ij}\, \log\frac{p_{ij}}{p_{ij}^*} + (1 - p_{ij})\log\frac{1-p_{ij}}{1-p_{ij}^*}\right]
\]</span></p>
<p>Unlike the other embedding methods shown in this section, UMAP can also create supervised embeddings so that the resulting features are more predictive of a qualitative or quantitative outcome value. See <span class="citation" data-cites="sainburg2020parametric">Sainburg, McInnes, and Gentner (<a href="#ref-sainburg2020parametric" role="doc-biblioref">2020</a>)</span>.</p>
<p>Besides the number of neighbors and embedding dimensions, several more tuning parameters exist. The optimization process’s number of optimization iterations (i.e., epochs) and the learning rate can significantly affect the final results. A distance-based tuning parameter, often called <em>min-dist</em>, specifies how “packed” points should be in the reduced dimensions. Values typically range from zero to one. However, the original authors state:</p>
<blockquote class="blockquote">
<p>We view min-dist as an essentially aesthetic parameter governing the appearance of the embedding, and thus is more important when using UMAP for visualization.</p>
</blockquote>
<p>As will be seen below, the initialization scheme is an important tuning parameter.</p>
<p>For supervised UMAP, there is an additional weighting parameter (between zero and one) that is used to balance the importance of the supervised and unsupervised aspects of the results. A value of zero specifies a completely unsupervised embedding.</p>
<p><a href="#fig-umap" class="quarto-xref">Figure&nbsp;<span>6.9</span></a> shows an interactive visualization of how UMAP can change with different tuning parameters. Each combination was trained for 1,000 epochs and used a learning rate of 1.0. For illustrative purposes, the resulting embeddings were scaled to a common range.</p>
<div id="fig-umap" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-umap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| label: fig-umap
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)
library(dplyr)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-umap.R")

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-umap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.9: A visualization of UMAP results for the barley data using different values for several tuning parameters. The points are the validation set values.
</figcaption>
</figure>
</div>
<p>There are a few notable patterns in these results:</p>
<ul>
<li>The initialization method can heavily impact the patterns in the embeddings.</li>
<li>As with Isomap, there are two or three clusters of data points with small barley values.</li>
<li>When the amount of supervision increases, one or more circular structures form that are associated with small outcome values.</li>
<li>The minimum distance parameter can drastically change the results.</li>
</ul>
<p>t-SNE and UMAP have become very popular tools for visualizing complex data. Visually, they often show interesting patterns that linear methods such as PCA cannot. However, they are computationally slow and unstable over different tuning parameter values. Also, it is easy to believe that the UMAP distances between embedding points are important or quantitatively predictive. That is not the case; the distances can be easily manipulated using the tuning parameters (especially the minimum distance).</p>
</section>
</section>
<section id="sec-centroids" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="sec-centroids"><span class="header-section-number">6.4</span> Centroid-Based Methods</h2>
<p>Another common approach to creating features is to calculate distances between landmarks or pairs of predictors. For example, the distance to desirable grade schools (or the university campus) could affect house prices in the Ames housing data. Potential predictors can be created that measure how close each home is to these points of interest. To calculate distance, a simple Euclidean metric can be used. However, for spatial data across longer distances, the Haversine metric <span class="citation" data-cites="sinnott1984virtues">(<a href="#ref-sinnott1984virtues" role="doc-biblioref">Sinnott 1984</a>)</span> is a better alternative because it takes into account the curvature of the earth.</p>
<p>Note that distance-based features are often right skewed. When a metric produces a right-skewed distribution, the log-transformation often helps improve predictive performance when the predictor is truly informative.</p>
<p>Distance-based features can also be effective for classification models. A centroid is another name for the multivariate mean of a collection of data. It is possible to compute class-specific centroids using only the data from each class. When a new sample is predicted, the distance to each class centroid can be used as a predictor. These features would be helpful when a model could be better at detecting/emulating linear class boundaries. An example of this would be tree-based models; these models have to work hard to approximate linear trends in the data. Supplementing the data with simple centroid features might improve performance.</p>
<p>Again, there are several choices for the distance metric. Mahalanobis distance is a good choice when there is not an overwhelming number of predictors:</p>
<p><span class="math display">\[
D_c(\boldsymbol{x}_0) = (\boldsymbol{x}_0 - \boldsymbol{\bar{x}}_{c})' \boldsymbol{S}^{-1}_c (\boldsymbol{x}_0 - \boldsymbol{\bar{x}}_{c})
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{x}_0\)</span> is the new data point being predicted, <span class="math inline">\(\boldsymbol{\bar{x}}\)</span> is a vector of sample means, <span class="math inline">\(\boldsymbol{S}\)</span> is the estimated covariance matrix (the subscript of <span class="math inline">\(c\)</span> denotes the class-specific statistics). This metric requires more data points within each class than the number of predictors being used. It also assumes that there are no linear dependencies between the predictors.</p>
<p>When the model has many features, regularizing the centroid distances can be a good approach. This approach, similar to the tools described in <a href="categorical-predictors.html#sec-effect-encodings" class="quarto-xref"><span>Section 5.4.3</span></a>, will shrink the class-specific centroids towards the overall (class-nonspecific) centroid at different rates. If a predictor does have any discriminative ability in the training set, its contribution to the class-specific centroids can be removed.</p>
<p>We’ll let <span class="math inline">\(x_{ij}\)</span> denote sample <span class="math inline">\(i\)</span> (<span class="math inline">\(i=1\ldots n_{tr}\)</span>) for predictor <span class="math inline">\(j\)</span> (<span class="math inline">\(j=1\ldots p\)</span>). The approach by <span class="citation" data-cites="tibshirani2003class">Tibshirani et al. (<a href="#ref-tibshirani2003class" role="doc-biblioref">2003</a>)</span> estimates the standardized difference between the class-specific centroid and the global centroid using the following:</p>
<p><span class="math display">\[\begin{align}
\delta_{jc} &amp;= \frac{\bar{x}_{jc} - \bar{x}_j}{w_c s_j} &amp;&amp;\text{ where } \notag \\
\bar{x}_{jc} &amp;= \frac{1}{{n_{tr}^c}}\sum_{i=1}^{{n_{tr}^c}} x_{ij}\, I(y_i = c) &amp;&amp; \text{\textcolor{grey}{(class-specific centroid elements)}} \notag \\
\bar{x}_{j} &amp;= \frac{1}{n_{tr}}\sum_{i=1}^{n_{tr}} x_{ij}  &amp;&amp; \text{\textcolor{grey}{(global centroid elements)}}\notag \\
w_c &amp;= \sqrt{\frac{1}{{n_{tr}^c}}  - \frac{1}{n_{tr}}}  &amp;&amp; \text{\textcolor{grey}{{(weights)}}} \notag \\
s_j &amp;= \frac{1}{n_{tr}-C}\sum_{c=1}^C\sum_{i=1}^{n_{tr}} \left(x_{ij} - \bar{x}_{jc}\right)^2 I(y_i = c)  &amp;&amp; \text{\textcolor{grey}{(pooled standard deviation for predictor $j$)}}\notag
\end{align}\]</span></p>
<p>where <span class="math inline">\(n_{tr}^c\)</span> is the number of training set points for class <span class="math inline">\(c\)</span> and <span class="math inline">\(I(x)\)</span> is a function that returns a value of one when <span class="math inline">\(x\)</span> is true.</p>
<p>To shrink this difference towards zero, a tuning parameter <span class="math inline">\(\lambda\)</span> is used to create a modified version of each predictor’s contribution to the difference:</p>
<p><span class="math display">\[\begin{equation}
\delta^*_{jc} = sign(\delta_{jc})\,h(|\delta_{jc}| - \lambda)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(h(x) = x\)</span> when <span class="math inline">\(x &gt; 0\)</span> and zero otherwise. If the difference between the class-specific and global centroid is small (relative to <span class="math inline">\(\lambda\)</span>), <span class="math inline">\(\delta^*_{jc} = 0\)</span> and predictor <span class="math inline">\(j\)</span> does not functionally affect the calculations for class <span class="math inline">\(c\)</span>. The class-specific shrunken centroid is then</p>
<p><span class="math display">\[\begin{equation}
\bar{x}^*_{jc}= \bar{x}_j + w_c\, s_j\, \delta^*_{jc}
\end{equation}\]</span></p>
<p>New features are added to the model based on the distance between <span class="math inline">\(\boldsymbol{x}_0\)</span> and <span class="math inline">\(\boldsymbol{\bar{x}}^*_{c}\)</span>. The amount of shrinkage is best optimized using the tuning methods described in later chapters. There are several variations of this specific procedure. <span class="citation" data-cites="wangImprovedCentroids">Wang and Zhu (<a href="#ref-wangImprovedCentroids" role="doc-biblioref">2007</a>)</span> describe several different approaches and <span class="citation" data-cites="efron2009empirical">Efron (<a href="#ref-efron2009empirical" role="doc-biblioref">2009</a>)</span> demonstrates the connection to Bayesian methods.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nsc" class="quarto-figure quarto-figure-center quarto-float anchored" alt="Nearest centroids" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nsc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/fig-nsc-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%" alt="Nearest centroids">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nsc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.10: An example of shrunken, class-specific centroids. Panel (a) shows data where there are three classes. Panel (b) demonstrates how, with increasing regularization, the centorids converge towards the global centroid (the black open circle).
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-nsc" class="quarto-xref">Figure&nbsp;<span>6.10</span></a> shows the impact of regularization for a simple data set with two predictors and three classes. The data are shown in Panel (a), while Panel (b) displays the raw, class-specific centroids. As regularization is added, the lines indicate the path each takes toward the global centroid (in black). Notice that the centroid for the first class eventually does not use predictor <span class="math inline">\(x_1\)</span>; the path becomes completely vertical when that predictor is removed from the calculations. As a counter-example, the centroid for the third class moves in a straight line towards the center. This indicates that both predictors showed a strong signal, and neither was removed as regularization increased.</p>
<p>Like distance-based methods, predictors based on <em>data depth</em> <span class="citation" data-cites="liu1999multivariate ghosh2005data mozharovskyi2015classifying">(<a href="#ref-liu1999multivariate" role="doc-biblioref">R. Liu, Parelius, and Singh 1999</a>; <a href="#ref-ghosh2005data" role="doc-biblioref">Ghosh and Chaudhuri 2005</a>; <a href="#ref-mozharovskyi2015classifying" role="doc-biblioref">Mozharovskyi, Mosler, and Lange 2015</a>)</span> can be helpful for separating classes. The depth of the data was initially defined by <span class="citation" data-cites="tukey1975mathematics">Tukey (<a href="#ref-tukey1975mathematics" role="doc-biblioref">1975</a>)</span>, and it can be roughly understood as the inverse of the distance to a centroid. For example, Mahalanobis depth would be:</p>
<p><span class="math display">\[
Depth_c(\boldsymbol{x}_0) = \left[1 + D_c(\boldsymbol{x}_0)\right]^{-1}
\]</span></p>
<p>There are more complex depth methods, and some are known for their robustness to outliers. Again, like distances, class-specific depth features can be used as features in a model.</p>
</section>
<section id="other-methods" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="other-methods"><span class="header-section-number">6.5</span> Other Methods</h2>
<p>There are numerous other methods to create embeddings such as autoencoders <span class="citation" data-cites="michelucci2022introduction borisov2022deep">(<a href="#ref-borisov2022deep" role="doc-biblioref">Borisov et al. 2022</a>; <a href="#ref-michelucci2022introduction" role="doc-biblioref">Michelucci 2022</a>)</span>. <span class="citation" data-cites="Boykis_What_are_embeddings_2023">Boykis (<a href="#ref-Boykis_What_are_embeddings_2023" role="doc-biblioref">2023</a>)</span> is an excellent survey and discussion of modern embedding methods, highlighting methods for text and deep learning.</p>
</section>
<section id="chapter-references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="chapter-references">Chapter References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-aggarwal2020linear" class="csl-entry" role="listitem">
Aggarwal, C. 2020. <em>Linear Algebra and Optimization for Machine Learning</em>. Vol. 156. Springer.
</div>
<div id="ref-banerjee2014linear" class="csl-entry" role="listitem">
Banerjee, S, and A Roy. 2014. <em>Linear Algebra and Matrix Analysis for Statistics</em>. Vol. 181. CRC Press.
</div>
<div id="ref-barker2003partial" class="csl-entry" role="listitem">
Barker, M, and W Rayens. 2003. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Partial+least+squares+for+discrimination&amp;as_ylo=2003&amp;as_yhi=2003&amp;btnG=">Partial Least Squares for Discrimination</a>.”</span> <em>Journal of Chemometrics: A Journal of the Chemometrics Society</em> 17 (3): 166–73.
</div>
<div id="ref-belkin2001laplacian" class="csl-entry" role="listitem">
Belkin, M, and P Niyogi. 2001. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Laplacian+eigenmaps+and+spectral+techniques+for+embedding+and+clustering&amp;as_ylo=2001&amp;as_yhi=2001&amp;btnG=">Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a>.”</span> <em>Advances in Neural Information Processing Systems</em> 14.
</div>
<div id="ref-Bengio2003advances" class="csl-entry" role="listitem">
Bengio, Y, JF Paiement, P Vincent, O Delalleau, N Roux, and M Ouimet. 2003. <span>“<a href="\href{https://proceedings.neurips.cc/paper_files/paper/2003/file/cf05968255451bdefe3c5bc64d550517-Paper.pdf}{Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering}"><span>Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering</span></a>.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by S. Thrun, L. Saul, and B. Schölkopf. Vol. 16. MIT Press.
</div>
<div id="ref-borisov2022deep" class="csl-entry" role="listitem">
Borisov, V, T Leemann, K Seßler, J Haug, M Pawelczyk, and G Kasneci. 2022. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Deep+neural+networks+and+tabular+data+A+survey&amp;as_ylo=2022&amp;as_yhi=2022&amp;btnG=">Deep Neural Networks and Tabular Data: A Survey</a>.”</span> <em>IEEE Transactions on Neural Networks and Learning Systems</em>.
</div>
<div id="ref-Boykis_What_are_embeddings_2023" class="csl-entry" role="listitem">
Boykis, V. 2023. <span>“<span class="nocase">What are embeddings?</span>”</span> <a href="https://doi.org/10.5281/zenodo.8015029">https://doi.org/10.5281/zenodo.8015029</a>.
</div>
<div id="ref-croux2002sign" class="csl-entry" role="listitem">
Croux, C, E Ollila, and H Oja. 2002. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Sign+and+rank+covariance+matrices+statistical+properties+and+application+to+principal+components+analysis&amp;as_ylo=2002&amp;as_yhi=2002&amp;btnG=">Sign and Rank Covariance Matrices: Statistical Properties and Application to Principal Components Analysis</a>.”</span> In <em>Statistical Data Analysis Based on the L1-Norm and Related Methods</em>, 257–69. Springer.
</div>
<div id="ref-efron2009empirical" class="csl-entry" role="listitem">
Efron, B. 2009. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Empirical+Bayes+estimates+for+large+scale+prediction+problems&amp;as_ylo=2009&amp;as_yhi=2009&amp;btnG=">Empirical <span>Bayes</span> Estimates for Large-Scale Prediction Problems</a>.”</span> <em>Journal of the American Statistical Association</em> 104 (487): 1015–28.
</div>
<div id="ref-esposito2013partial" class="csl-entry" role="listitem">
Esposito Vinzi, V, and G Russolillo. 2013. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Partial+least+squares+algorithms+and+methods&amp;as_ylo=2013&amp;as_yhi=2013&amp;btnG=">Partial Least Squares Algorithms and Methods</a>.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 5 (1): 1–19.
</div>
<div id="ref-pierna2020applicability" class="csl-entry" role="listitem">
Fernández P, J. A., A. Laborde, L. Lakhal, M. Lesnoff, M. Martin, Y. Roggo, and P. Dardenne. 2020. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+applicability+of+vibrational+spectroscopy+and+multivariate+analysis+for+the+characterization+of+animal+feed+where+the+reference+values+do+not+follow+a+normal+distribution+A+new+chemometric+challenge+posed+at+the+Chimiométrie+2019+congress&amp;as_ylo=2020&amp;as_yhi=2020&amp;btnG=">The Applicability of Vibrational Spectroscopy and Multivariate Analysis for the Characterization of Animal Feed Where the Reference Values Do Not Follow a Normal Distribution: A New Chemometric Challenge Posed at the ’Chimiométrie 2019’ Congress</a>.”</span> <em>Chemometrics and Intelligent Laboratory Systems</em> 202: 104026.
</div>
<div id="ref-Geladi1986" class="csl-entry" role="listitem">
Geladi, P., and B Kowalski. 1986. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Partial+Least+Squares+Regression+A+Tutorial&amp;as_ylo=1986&amp;as_yhi=1986&amp;btnG=">Partial Least-Squares Regression: A Tutorial</a>.”</span> <em>Analytica Chimica Acta</em> 185: 1–17.
</div>
<div id="ref-Ghojogh2023" class="csl-entry" role="listitem">
Ghojogh, B, M Crowley, F Karray, and A Ghodsi. 2023. <span>“Elements of Dimensionality Reduction and Manifold Learning.”</span> In, 185–205. Springer International Publishing.
</div>
<div id="ref-ghosh2005data" class="csl-entry" role="listitem">
Ghosh, A K, and P Chaudhuri. 2005. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=On+data+depth+and+distribution+free+discriminant+analysis+using+separating+surfaces&amp;as_ylo=2005&amp;as_yhi=2005&amp;btnG=">On Data Depth and Distribution-Free Discriminant Analysis Using Separating Surfaces</a>.”</span> <em>Bernoulli</em> 11 (1): 1–27.
</div>
<div id="ref-hinton2002stochastic" class="csl-entry" role="listitem">
Hinton, G, and S Roweis. 2002. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Stochastic+neighbor+embedding&amp;as_ylo=2002&amp;as_yhi=2002&amp;btnG=">Stochastic Neighbor Embedding</a>.”</span> <em>Advances in Neural Information Processing Systems</em> 15.
</div>
<div id="ref-hyvarinen2000independent" class="csl-entry" role="listitem">
Hyvärinen, Aapo, and Erkki Oja. 2000. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Independent+component+analysis+algorithms+and+applications&amp;as_ylo=2000&amp;as_yhi=2000&amp;btnG=">Independent Component Analysis: Algorithms and Applications</a>.”</span> <em>Neural Networks</em> 13 (4-5): 411–30.
</div>
<div id="ref-wtf2024" class="csl-entry" role="listitem">
Johnson, K, and M Kuhn. 2024. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=What+they+forgot+to+tell+you+about+machine+learning+with+an+application+to+pharmaceutical+manufacturing&amp;as_ylo=2024&amp;as_yhi=2024&amp;btnG=">What They Forgot to Tell You about Machine Learning with an Application to Pharmaceutical Manufacturing</a>.”</span> <em>Pharmaceutical Statistics</em> n/a (n/a).
</div>
<div id="ref-jolliffe2016principal" class="csl-entry" role="listitem">
Jolliffe, I, and J Cadima. 2016. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Principal+component+analysis+a+review+and+recent+developments&amp;as_ylo=2016&amp;as_yhi=2016&amp;btnG=">Principal Component Analysis: A Review and Recent Developments</a>.”</span> <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em> 374 (2065): 20150202.
</div>
<div id="ref-kruskal1964multidimensional" class="csl-entry" role="listitem">
Kruskal, J. 1964a. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Multidimensional+scaling+by+optimizing+goodness+of+fit+to+a+nonmetric+hypothesis&amp;as_ylo=1964&amp;as_yhi=1964&amp;btnG=">Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis</a>.”</span> <em>Psychometrika</em> 29 (1): 1–27.
</div>
<div id="ref-kruskal1964nonmetric" class="csl-entry" role="listitem">
———. 1964b. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Nonmetric+multidimensional+scaling+a+numerical+method&amp;as_ylo=1964&amp;as_yhi=1964&amp;btnG=">Nonmetric Multidimensional Scaling: A Numerical Method</a>.”</span> <em>Psychometrika</em> 29 (2): 115–29.
</div>
<div id="ref-fes" class="csl-entry" role="listitem">
Kuhn, M, and K Johnson. 2019. <em><span><a href="https://bookdown.org/max/FES">Feature Engineering and Selection</a></span>: <span class="nocase">A Practical Approach for Predictive Models</span></em>. CRC Press.
</div>
<div id="ref-larsen2019deep" class="csl-entry" role="listitem">
Larsen, J, and L Clemmensen. 2019. <span>“Deep Learning for Chemometric and Non-Translational Data.”</span> <a href="https://arxiv.org/abs/1910.00391">https://arxiv.org/abs/1910.00391</a>.
</div>
<div id="ref-le2008sparse" class="csl-entry" role="listitem">
Lê Cao, KA, D Rossouw, C Robert-Granié, and P Besse. 2008. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+sparse+PLS+for+variable+selection+when+integrating+omics+data&amp;as_ylo=2008&amp;as_yhi=2008&amp;btnG=">A Sparse PLS for Variable Selection When Integrating Omics Data</a>.”</span> <em>Statistical Applications in Genetics and Molecular Biology</em> 7 (1).
</div>
<div id="ref-lee1999learning" class="csl-entry" role="listitem">
Lee, D, and S Seung. 1999. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Learning+the+parts+of+objects+by+non+negative+matrix+factorization&amp;as_ylo=1999&amp;as_yhi=1999&amp;btnG=">Learning the Parts of Objects by Non-Negative Matrix Factorization</a>.”</span> <em>Nature</em> 401 (6755): 788–91.
</div>
<div id="ref-lee2000algorithms" class="csl-entry" role="listitem">
———. 2000. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Algorithms+for+non+negative+matrix+factorization&amp;as_ylo=2000&amp;as_yhi=2000&amp;btnG=">Algorithms for Non-Negative Matrix Factorization</a>.”</span> <em>Advances in Neural Information Processing Systems</em> 13.
</div>
<div id="ref-liu1999multivariate" class="csl-entry" role="listitem">
Liu, R, J Parelius, and K Singh. 1999. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Multivariate+analysis+by+data+depth+D+escriptive+statistics+graphics+and+inference&amp;as_ylo=1999&amp;as_yhi=1999&amp;btnG=">Multivariate Analysis by Data Depth: <span>D</span>escriptive Statistics, Graphics and Inference</a>.”</span> <em>The Annals of Statistics</em> 27 (3): 783–858.
</div>
<div id="ref-Liu2007b" class="csl-entry" role="listitem">
Liu, Y, and W Rayens. 2007. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=PLS+and+Dimension+Reduction+for+Classification&amp;as_ylo=2007&amp;as_yhi=2007&amp;btnG=">PLS and Dimension Reduction for Classification</a>.”</span> <em>Computational Statistics</em>, 189–208.
</div>
<div id="ref-michelucci2022introduction" class="csl-entry" role="listitem">
Michelucci, I. 2022. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=An+introduction+to+autoencoders&amp;as_ylo=2022&amp;as_yhi=2022&amp;btnG=">An Introduction to Autoencoders</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-mozharovskyi2015classifying" class="csl-entry" role="listitem">
Mozharovskyi, P, K Mosler, and T Lange. 2015. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Classifying+real+world+data+with+the+DD+alpha+procedure&amp;as_ylo=2015&amp;as_yhi=2015&amp;btnG=">Classifying Real-World Data with the <span>DD</span><span class="math inline">\(\alpha\)</span>-Procedure</a>.”</span> <em>Advances in Data Analysis and Classification</em> 9 (3): 287–314.
</div>
<div id="ref-ning2021spike" class="csl-entry" role="listitem">
Ning, B, and N Ning. 2021. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Spike+and+slab+Bayesian+sparse+principal+component+analysis&amp;as_ylo=2021&amp;as_yhi=2021&amp;btnG=">Spike and Slab Bayesian Sparse Principal Component Analysis</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-nordhausen2018independent" class="csl-entry" role="listitem">
Nordhausen, K, and H Oja. 2018. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Independent+component+analysis+A+statistical+perspective&amp;as_ylo=2018&amp;as_yhi=2018&amp;btnG=">Independent Component Analysis: A Statistical Perspective</a>.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 10 (5): e1440.
</div>
<div id="ref-sainburg2020parametric" class="csl-entry" role="listitem">
Sainburg, T, L McInnes, and TQ Gentner. 2020. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Parametric+UMAP+learning+embeddings+with+deep+neural+networks+for+representation+and+semi+supervised+learning&amp;as_ylo=2020&amp;as_yhi=2020&amp;btnG=">Parametric <span>UMAP</span>: Learning Embeddings with Deep Neural Networks for Representation and Semi-Supervised Learning</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-sammon1969nonlinear" class="csl-entry" role="listitem">
Sammon, J. 1969. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+nonlinear+mapping+for+data+structure+analysis&amp;as_ylo=1969&amp;as_yhi=1969&amp;btnG=">A Nonlinear Mapping for Data Structure Analysis</a>.”</span> <em>IEEE Transactions on Computers</em> 100 (5): 401–9.
</div>
<div id="ref-pmlrvR4schein03a" class="csl-entry" role="listitem">
Schein, A, L Saul, and L Ungar. 2003. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+Generalized+Linear+Model+for+Principal+Component+Analysis+of+Binary+Data&amp;as_ylo=2003&amp;as_yhi=2003&amp;btnG=">A Generalized Linear Model for Principal Component Analysis of Binary Data</a>.”</span> In <em>Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics</em>, edited by C Bishop and B Frey, R4:240–47. Proceedings of Machine Learning Research.
</div>
<div id="ref-scholkopf1998nonlinear" class="csl-entry" role="listitem">
Schölkopf, B, A Smola, and KR Müller. 1998. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Nonlinear+component+analysis+as+a+kernel+eigenvalue+problem&amp;as_ylo=1998&amp;as_yhi=1998&amp;btnG=">Nonlinear Component Analysis as a Kernel Eigenvalue Problem</a>.”</span> <em>Neural Computation</em> 10 (5): 1299–319.
</div>
<div id="ref-shen2008sparse" class="csl-entry" role="listitem">
Shen, H, and JZ Huang. 2008. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Sparse+principal+component+analysis+via+regularized+low+rank+matrix+approximation&amp;as_ylo=2008&amp;as_yhi=2008&amp;btnG=">Sparse Principal Component Analysis via Regularized Low Rank Matrix Approximation</a>.”</span> <em>Journal of Multivariate Analysis</em> 99 (6): 1015–34.
</div>
<div id="ref-sinnott1984virtues" class="csl-entry" role="listitem">
Sinnott, R. 1984. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Virtues+of+the+Haversine+&amp;as_ylo=1984&amp;as_yhi=1984&amp;btnG=">Virtues of the <span>Haversine</span></a>.”</span> <em>Sky and Telescope</em> 68 (2): 158.
</div>
<div id="ref-stone1990continuum" class="csl-entry" role="listitem">
Stone, M, and R Brooks. 1990. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Continuum+regression+cross+validated+sequentially+constructed+prediction+embracing+ordinary+least+squares+partial+least+squares+and+principal+components+regression&amp;as_ylo=1990&amp;as_yhi=1990&amp;btnG=">Continuum Regression: Cross-Validated Sequentially Constructed Prediction Embracing Ordinary Least Squares, Partial Least Squares and Principal Components Regression</a>.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 52 (2): 237–58.
</div>
<div id="ref-tenenbaum2000global" class="csl-entry" role="listitem">
Tenenbaum, J, V Silva, and J Langford. 2000. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+global+geometric+framework+for+nonlinear+dimensionality+reduction&amp;as_ylo=2000&amp;as_yhi=2000&amp;btnG=">A Global Geometric Framework for Nonlinear Dimensionality Reduction</a>.”</span> <em>Science</em> 290 (5500): 2319–23.
</div>
<div id="ref-tibshirani2003class" class="csl-entry" role="listitem">
Tibshirani, R, T Hastie, B Narasimhan, and G Chu. 2003. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Class+prediction+by+nearest+shrunken+centroids+with+applications+to+DNA+microarrays&amp;as_ylo=2003&amp;as_yhi=2003&amp;btnG=">Class Prediction by Nearest Shrunken Centroids, with Applications to <span>DNA</span> Microarrays</a>.”</span> <em>Statistical Science</em>, 104–17.
</div>
<div id="ref-tipping1999probabilistic" class="csl-entry" role="listitem">
Tipping, M, and C Bishop. 1999. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Probabilistic+principal+component+analysis&amp;as_ylo=1999&amp;as_yhi=1999&amp;btnG=">Probabilistic Principal Component Analysis</a>.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 61 (3): 611–22.
</div>
<div id="ref-torgerson1952multidimensional" class="csl-entry" role="listitem">
Torgerson, W. 1952. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Multidimensional+scaling+I+Theory+and+method&amp;as_ylo=1952&amp;as_yhi=1952&amp;btnG=">Multidimensional Scaling: <span>I</span>. Theory and Method</a>.”</span> <em>Psychometrika</em> 17 (4): 401–19.
</div>
<div id="ref-tukey1975mathematics" class="csl-entry" role="listitem">
Tukey, J. 1975. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Mathematics+and+the+Picturing+of+Data&amp;as_ylo=1975&amp;as_yhi=1975&amp;btnG=">Mathematics and the Picturing of Data</a>.”</span> In <em><span class="nocase">Proceedings of the International Congress of Mathematicians</span></em>, 2:523–31.
</div>
<div id="ref-van2008visualizing" class="csl-entry" role="listitem">
Van der Maaten, L, and G Hinton. 2008. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Visualizing+data+using+t+SNE+&amp;as_ylo=2008&amp;as_yhi=2008&amp;btnG=">Visualizing Data Using <span class="nocase">t-SNE</span></a>.”</span> <em>Journal of Machine Learning Research</em> 9 (11).
</div>
<div id="ref-visuri2000sign" class="csl-entry" role="listitem">
Visuri, S, V Koivunen, and H Oja. 2000. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Sign+and+rank+covariance+matrices&amp;as_ylo=2000&amp;as_yhi=2000&amp;btnG=">Sign and Rank Covariance Matrices</a>.”</span> <em>Journal of Statistical Planning and Inference</em> 91 (2): 557–75.
</div>
<div id="ref-wangImprovedCentroids" class="csl-entry" role="listitem">
Wang, S, and J Zhu. 2007. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Improved+centroids+estimation+for+the+nearest+shrunken+centroid+classifier&amp;as_ylo=2007&amp;as_yhi=2007&amp;btnG=">Improved Centroids Estimation for the Nearest Shrunken Centroid Classifier</a>.”</span> <em>Bioinformatics</em> 23 (8): 972–79.
</div>
<div id="ref-Wold1983" class="csl-entry" role="listitem">
Wold, S, H Martens, and H Wold. 1983. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+Multivariate+Calibration+Problem+in+Chemistry+Solved+by+the+PLS+Method&amp;as_ylo=1983&amp;as_yhi=1983&amp;btnG=">The Multivariate Calibration Problem in Chemistry Solved by the PLS Method</a>.”</span> In <em>Proceedings from the Conference on Matrix Pencils</em>. Heidelberg: Springer-Verlag.
</div>
<div id="ref-wu1997kernel" class="csl-entry" role="listitem">
Wu, W, DL Massart, and S De Jong. 1997. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+kernel+PCA+algorithms+for+wide+data+Part+I+theory+and+algorithms&amp;as_ylo=1997&amp;as_yhi=1997&amp;btnG=">The Kernel <span>PCA</span> Algorithms for Wide Data. Part <span>I</span>: Theory and Algorithms</a>.”</span> <em>Chemometrics and Intelligent Laboratory Systems</em> 36 (2): 165–72.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Retreived from <a href="https://chemom2019.sciencesconf.org/resource/page/id/13.html"><code>https://chemom2019.sciencesconf.org/resource/page/id/13.html</code></a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>As is, this is far too general. We could maximize the variance by making every value of <span class="math inline">\(Q\)</span> equal to <span class="math inline">\(\infty\)</span>. PCA, and other methods, impose an implicit constraint to limit the elements of <span class="math inline">\(Q\)</span>. In this case, PCA constrains the loading vectors to have unit length.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Assuming that the <span class="math inline">\(p\)</span> columns are linearly independent.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>These wavelengths correspond to the 1<sup>st</sup> and 50<sup>th</sup> predictors.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>We will see similar methods in <span class="quarto-unresolved-ref">?sec-svm-cls</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>A note about some notation… We commonly think of the <em>norm</em> notation as <span class="math inline">\(\|\boldsymbol{x}\|_p = \left(|x_1|^p + |x_2|^p + \ldots + |x_n|^p\right)^{1/p}\)</span>. So what does the lack of a subscript in <span class="math inline">\(||\boldsymbol{x}||^2\)</span> mean? The convention is the sum of squares: <span class="math inline">\(||\boldsymbol{x}||^2 = x_1^2 + x_2^2 + \ldots + x_n^2\)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Specifically gradient descent with a user-defined learning rate.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/aml4td\.org");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/categorical-predictors.html" class="pagination-link" aria-label="Working with Categorical Predictors">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Working with Categorical Predictors</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/interactions-nonlinear.html" class="pagination-link" aria-label="Interactions and Nonlinear Features">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Interactions and Nonlinear Features</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>