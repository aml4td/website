<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>8&nbsp; Interactions and Nonlinear Features – Applied Machine Learning for Tabular Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/overfitting.html" rel="next">
<link href="../chapters/embeddings.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ac94f01b84e0cf733daf4ed4b084c36a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<meta name="shinylive:serviceworker_dir" content="..">
<script src="../site_libs/quarto-contrib/shinylive-0.9.1/shinylive/load-shinylive-sw.js" type="module"></script>
<script src="../site_libs/quarto-contrib/shinylive-0.9.1/shinylive/run-python-blocks.js" type="module"></script>
<link href="../site_libs/quarto-contrib/shinylive-0.9.1/shinylive/shinylive.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/shinylive-quarto-css/shinylive-quarto.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-7T996NL20Z"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-7T996NL20Z', { 'anonymize_ip': true});
</script>
<script src="../site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="../site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title"><span id="sec-interactions-nonlinear" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Interactions and Nonlinear Features</span></span></h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../"></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/aml4td/website/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/news.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">News</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/contributing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributing</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/whole-game.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Whole Game</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Preparation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/initial-data-splitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Initial Data Splitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/missing-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Missing Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/numeric-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transforming Numeric Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/categorical-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Working with Categorical Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/interactions-nonlinear.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Interactions and Nonlinear Features</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/overfitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Overfitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/resampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Measuring Performance with Resampling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/grid-search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Grid Search</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/iterative-search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Iterative Search</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/feature-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/comparing-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Comparing Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Classification</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Regression</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Characterization</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Finalization</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-interactions" id="toc-sec-interactions" class="nav-link active" data-scroll-target="#sec-interactions"><span class="header-section-number">8.1</span> Interactions</a>
  <ul class="collapse">
  <li><a href="#sec-interactions-principles" id="toc-sec-interactions-principles" class="nav-link" data-scroll-target="#sec-interactions-principles"><span class="header-section-number">8.1.1</span> How Likely are Interactions?</a></li>
  <li><a href="#sec-interactions-detection" id="toc-sec-interactions-detection" class="nav-link" data-scroll-target="#sec-interactions-detection"><span class="header-section-number">8.1.2</span> Detecting Interactions</a></li>
  </ul></li>
  <li><a href="#sec-polynomials" id="toc-sec-polynomials" class="nav-link" data-scroll-target="#sec-polynomials"><span class="header-section-number">8.2</span> Polynomial Basis Expansions</a></li>
  <li><a href="#sec-splines" id="toc-sec-splines" class="nav-link" data-scroll-target="#sec-splines"><span class="header-section-number">8.3</span> Spline Functions</a>
  <ul class="collapse">
  <li><a href="#sec-define-knots" id="toc-sec-define-knots" class="nav-link" data-scroll-target="#sec-define-knots"><span class="header-section-number">8.3.1</span> Defining the Knots</a></li>
  <li><a href="#sec-natural-splines" id="toc-sec-natural-splines" class="nav-link" data-scroll-target="#sec-natural-splines"><span class="header-section-number">8.3.2</span> Natural Cubic Splines</a></li>
  </ul></li>
  <li><a href="#sec-variance-bias" id="toc-sec-variance-bias" class="nav-link" data-scroll-target="#sec-variance-bias"><span class="header-section-number">8.4</span> Sidebar: The Variance-Bias Tradeoff</a></li>
  <li><a href="#discretization" id="toc-discretization" class="nav-link" data-scroll-target="#discretization"><span class="header-section-number">8.5</span> Discretization</a></li>
  <li><a href="#chapter-references" id="toc-chapter-references" class="nav-link" data-scroll-target="#chapter-references">Chapter References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span id="sec-interactions-nonlinear" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Interactions and Nonlinear Features</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>We saw in previous chapters that preprocessing is necessary because models can be sensitive to specific predictor characteristics. For instance, some models:</p>
<ul>
<li>are sensitive to highly correlated predictors.</li>
<li>cannot consume qualitative predictors as non-numeric data.</li>
<li>cannot be built when predictors have no or near-zero variability.</li>
</ul>
<p>Preprocessing methods address aspects of the predictors that place them in a form so that models can be built. This is <em>what the model needs</em> to function.</p>
<p>While preprocessing techniques allow models to be built, they do not necessarily transform them in ways that help the model to identify predictive relationships with the outcome. This is the fundamental concept of <strong>feature engineering</strong> which addresses the question: what can we do to make it easier for the model to understand and predict the outcome.</p>
<p>In this chapter we will discuss techniques that address this question. For example, if the relationship between a predictor is nonlinear, how can we represent that predictor so that the relationship can be modeled? Or if two predictors work in conjunction with each other, then how can this information be engineered for a model? While feature engineering is not necessary for models to be built, it contains a crucial set of tools for improving model performance.</p>
<p>Let’s look at a simple example. Occasionally, predictor pairs work better in unison rather than as main effects. For example, consider the data in <a href="#fig-two-class-corr" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-two-class-corr</span></a>(a), where two predictors are:</p>
<ul>
<li>strictly positive,</li>
<li>significantly right-skewed, and</li>
<li>highly correlated.</li>
</ul>
<p>In the predictors’ original form, there is a significant overlap between the two classes of samples. However, when the ratio of the predictors is used, the newly derived predictor better discriminates between classes (shown in <a href="#fig-two-class-corr" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-two-class-corr</span></a>(b)). While not a general rule, the three data characteristics above suggest that the modeler attempts to form ratios from two or more predictors <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-two-class-corr" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" alt="Panel (a): two highly correlated, right-skewed predictors with two classes. Panel (b): separation of classes using $log(A/B)$.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-two-class-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-two-class-corr-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%" alt="Panel (a): two highly correlated, right-skewed predictors with two classes. Panel (b): separation of classes using $log(A/B)$.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-two-class-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: Panel (a): two highly correlated, right-skewed predictors with two classes. Panel (b): separation of classes using <span class="math inline">\(log(A/B)\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>It is essential to understand that these <em>data</em> require transformation. We can put the original predictors into a model as-is. The model won’t produce an error but won’t have good predictive performance. We can induce a separation of the classes when a joint feature is used.</p>
<p>This aspect of feature engineering depends on the data set so it is difficult to enumerate all possible techniques. In this chapter, we’ll describe a few commonly used methods: splines, interactions, and discretization (a.k.a. binning).</p>
<section id="sec-interactions" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="sec-interactions"><span class="header-section-number">8.1</span> Interactions</h2>
<p>When building models for prediction, the majority of variation in the outcome is generally explained by the cumulative effect of the important individual predictors. For many problems, additional variation in the response can be explained by the effect of two or more predictors working in conjunction with each other.</p>
<p>The healthcare industry has long understood the concept of interactions among drugs for treating specific diseases <span class="citation" data-cites="singh2017suppressive">Altorki et al. (<a href="#ref-altorki2021neoadjuvant" role="doc-biblioref">2021</a>)</span>. As an example of an interaction, consider treatment for the disease non-small cell lung cancer (NSCLC). In a recent study, patients with an advanced stage of NSCLC with an EGFR mutation were given either osimertinib alone or osimertinib in combination with traditional chemotherapy <span class="citation" data-cites="NEJMoa2306434">(<a href="#ref-NEJMoa2306434" role="doc-biblioref">Planchard et al. 2023</a>)</span>. Patients taking the combination treatment had a significantly longer progression-free survival time than patients taking osimertinib alone. Hence, the interaction of the treatments is more effective than the single treatment. To summarize, two (or more) predictors interact if their combined impact is different (less or greater) than what would be expected from the added impact of each predictor alone.</p>
<p>We’ve already encountered an example of an interaction in <a href="#sec-eda-whole-game" class="quarto-xref"><span class="quarto-unresolved-ref">sec-eda-whole-game</span></a> where the relationship between delivery time and the time of order differed across days of the week. This trend is reproduced in <a href="#fig-delivery-no-interact" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-delivery-no-interact</span></a>(a). The telltale sign of the interaction is that the trendlines are not parallel with one another; they have different rates of increase and cross.</p>
<p>How would this plot change if there was no interaction between the order time and day? To illustrate, we estimated trendlines in a way that coerced the nonlinear trend for order time to be the same. <a href="#fig-delivery-no-interact" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-delivery-no-interact</span></a>(b) shows the results: parallel lines for each day of the week.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-delivery-no-interact" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-delivery-no-interact-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-delivery-no-interact-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-delivery-no-interact-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: Delivery times versus the time of the order, colored by the day of the week. (a) A duplicate of a panel from <a href="#fig-delivery-predictors" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-delivery-predictors</span></a>. (b) The same data with trendlines where the underlying model did <em>not</em> allow an interaction between the delivery day and hour.
</figcaption>
</figure>
</div>
</div>
</div>
<p>This example demonstrates an interaction between a numeric predictor (hour of order) and a categorical predictor (day of the week). Interactions can also occur between two (or more) numeric or categorical predictors. Using the delivery data, let’s examine potential interactions solely between categorical predictor columns.</p>
<p>For regression problems, one visualization technique is to compute the means (or medians) of the outcome for all combinations of variables and then plot these means in a manner similar to the previous figure. Let’s look at the 27 predictors columns for whether a specific item was included in the order. The original column contains counts, but the data are mostly zero or one. We’ll look at two variables at a time and plot the four combinations of whether the item was ordered at all (versus not at all). <a href="#fig-delivery-items" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-delivery-items</span></a> shows two potential sets of interactions. The x-axis indicates whether Item 1 was in the order or not. The y-axis is the mean delivery time with 90% confidence intervals<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>The left panel shows the joint effect of items 1 and 9. The lines connecting the means are parallel. This indicates that each of these two predictors affects the outcome independently of one another. Specifically, the incremental change in delivery time is the same when item 9 is or is not included with item 1. The right panel has means for items 1 and 10. The mean delivery times are very similar when neither is contained in the order. Also, when only one of the two items is in the order, the average time is similarly small. However, when both are included, the delivery time becomes much larger. This means that you cannot consider the effect or either item 1 or 10 alone; their effect on the outcome occurs jointly.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-delivery-items" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-delivery-items-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-delivery-items-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-delivery-items-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: Interaction examples with two categorical predictors. Items 1 and 9 (left panel) do not interaction, while items 1 and 10 appear to have a strong interaction (right panel).
</figcaption>
</figure>
</div>
</div>
</div>
<p>As a third example, interactions may occur between continuous predictors. It can be difficult to discover the interaction via visualization for two numeric predictors without converting one of the predictors to categorical bins. To illustrate the <em>concept</em> of an interaction between two numeric predictors, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, let’s use a simple linear equation:</p>
<p><span id="eq-two-way-int"><span class="math display">\[
y = \beta_0 + \beta_{1}x_1 + \beta_{2}x_2 + \beta_{3}x_{1}x_{2} + \epsilon
\tag{8.1}\]</span></span></p>
<p>The <span class="math inline">\(\beta\)</span> coefficients represent the overall average response (<span class="math inline">\(\beta_0\)</span>), the average rate of change for each individual predictor (<span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>), and the incremental rate of change due to the combined effect of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> (<span class="math inline">\(\beta_3\)</span>) that goes beyond what <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> can explain alone. The parameters for this equation can be estimated using a technique such as ordinary least squares (REF). The sign and magnitude of <span class="math inline">\(\beta_3\)</span> indicate how and the extent to which the two predictors interact:</p>
<ul>
<li>When <span class="math inline">\(\beta_3\)</span> is positive, the interaction is <em>synergistic</em> since the response increases beyond the effect of either predictor alone.<br>
</li>
<li>Alternatively, when <span class="math inline">\(\beta_3\)</span> is negative, the interaction is <em>antagonistic</em> since the response decreases beyond the effect of either predictor alone.<br>
</li>
<li>A third scenario is when <span class="math inline">\(\beta_3\)</span> is essentially zero. In this case, there is no interaction between the predictors and the relationship between the predictors is <em>additive</em>.</li>
<li>Finally, for some data sets, we may find that neither predictor is important individually (i.e., <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are zero). However, the coefficient on the interaction term is not zero. Because this case occurs very infrequently, it is called <em>atypical</em>.</li>
</ul>
<p>To understand this better, <a href="#fig-interaction-contours" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-interaction-contours</span></a> shows a contour plot of a predicted linear regression model with various combinations of the model slope parameters. The two predictors are centered at zero with values ranging within <span class="math inline">\(x_j \pm 4.0\)</span>). The default setting shows a moderate synergistic interaction effect since all of the <span class="math inline">\(\beta_j = 1.0\)</span>). In the plot, darker values indicate smaller predicted values.</p>
<div id="fig-interaction-contours" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interaction-contours-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| label: shiny-interaction-contours
#| viewerHeight: 600
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-interaction-contours.R")

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interaction-contours-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.4: Prediction contours for a linear regression model with <a href="#eq-two-way-int" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-two-way-int</span></a> with <span class="math inline">\(\beta_0 = 0\)</span>. Darker values indicate smaller predictions.
</figcaption>
</figure>
</div>
<p>Interaction effects between predictors are sometimes confused with correlation between predictors. They are not the same thing. Interactions are defined by their relationship to the outcome while between-predictors correlations are unrelated to it. An interaction could occur independent of the amount of correlation between predictors.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/interactions-nonlinear.html#sec-interactions"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
<section id="sec-interactions-principles" class="level3" data-number="8.1.1">
<h3 data-number="8.1.1" class="anchored" data-anchor-id="sec-interactions-principles"><span class="header-section-number">8.1.1</span> How Likely are Interactions?</h3>
<p>Within the context of statistical experimental design, <span class="citation" data-cites="hamada1992analysis">Hamada and Wu (<a href="#ref-hamada1992analysis" role="doc-biblioref">1992</a>)</span> discuss some probabilistic aspects of predictor importance. The effect sparsity principle is that there are often few predictors that are relevant for predicting the outcome. Similarly, the effect hierarchy principle states that “main effects” (i.e.&nbsp;a feature involving only predictor) are more likely to occur than interactions. Also, as more predictors are involved in the interaction, the less likely they become. Finally, the heredity principle conjectures that if an interaction is important, it is very likely that the corresponding main effects are likely too. <span class="citation" data-cites="chipman1996bayesian">Chipman (<a href="#ref-chipman1996bayesian" role="doc-biblioref">1996</a>)</span> further expands this principle.</p>
<p>The original context of these principles was envisioned for screening large numbers of predictors in experimental designs; they are still very relevant for the analysis of tabular data.</p>
</section>
<section id="sec-interactions-detection" class="level3" data-number="8.1.2">
<h3 data-number="8.1.2" class="anchored" data-anchor-id="sec-interactions-detection"><span class="header-section-number">8.1.2</span> Detecting Interactions</h3>
<p>An ideal scenario would be that we would know which predictors interact before modeling the data. If this would be the case, then these terms could be included in a model. This would be ideal because the model could more easily find the relationship with the response, thus leading to better predictive performance. Unfortunately, knowledge of which predictors interact is usually not available prior to initiating the modeling process.</p>
<p>If meaningful interactions are unknown before modeling, can models still discover and utilize these potentially important features? Recent studies using various advanced modeling techniques have shown that some methods can inherently detect interactions. For example, tree-based models <span class="citation" data-cites="elith2008working">(<a href="#ref-elith2008working" role="doc-biblioref">Elith, Leathwick, and Hastie 2008</a>)</span>, random forests <span class="citation" data-cites="garcia2009evaluating">(<a href="#ref-garcia2009evaluating" role="doc-biblioref">García-Magariños et al. 2009</a>)</span>, boosted trees <span class="citation" data-cites="lampa2014identification">(<a href="#ref-lampa2014identification" role="doc-biblioref">Lampa et al. 2014</a>)</span>, and support vector machines <span class="citation" data-cites="chen2008support">(<a href="#ref-chen2008support" role="doc-biblioref">Chen et al. 2008</a>)</span> are effective at uncovering them.</p>
<p>If modern modeling techniques can naturally find and utilize interactions, then why is it necessary to spend any time uncovering these relationships? The first reason is to improve interpretability. Recall the trade-off between prediction and interpretation that was discussed in Section TODO. More complex models are less interpretable and generally more predictive, while simpler models are more interpretable and less predictive. Therefore, if we know which interaction(s) are important, we can include these in a simpler model to enable a better interpretation. A second reason to spend time uncovering interactions is to help improve the predictive performance of models.</p>
<p>What should we do if we have a candidate set of interactions? If we are using a more formal statistical model, such as linear or logistic regression, the traditional tool for evaluating whether additional model terms in a set of nested models are worth including is the conventional analysis of variance (ANOVA). This uses the statistical likelihood value as the objective function, which is equivalent to comparing the RMSE values between models for linear regression <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. The error reduction and how many additional terms are responsible for the improvement are computed. Using these results and some probability assumptions about the data, we can formally test the null hypothesis that the additional parameters all have coefficient values of zero.</p>
<p>Looking at the delivery data, our model in <a href="#sec-model-development-whole-game" class="quarto-xref"><span class="quarto-unresolved-ref">sec-model-development-whole-game</span></a> included a single set of interactions (e.g., hour-by-day). What if we included one more interaction: item 1 <span class="math inline">\(\times\)</span> item 10? <a href="#tbl-interaction-anova" class="quarto-xref">Table&nbsp;<span class="quarto-unresolved-ref">tbl-interaction-anova</span></a> shows the ANOVA results. The RMSE <em>computed on the training set</em> is listed in the first and second columns. The reduction by including this additional model term is 0.18 (decimal minutes). Assuming normality of the model residuals, the p-value for this test is exceedingly small. This indicates that there is no evidence that this parameter is truly zero. In other words, there is strong evidence that the inclusion of the interaction helps explain the response. This statistical difference may not make much of a practical difference. However, machine learning models often behave like “a game of inches” where every small improvement adds up to an overall improvement that matters.</p>
<div class="columns">
<div class="column" style="width:15%;">

</div><div class="column" style="width:70%;">
<div id="tbl-interaction-anova" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-interaction-anova-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="table-responsive">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: left; empty-cells: hide; border-bottom: hidden;"></th>
<th colspan="2" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Training Set RMSE
</div></th>
<th colspan="2" data-quarto-table-cell-role="th" style="text-align: right; empty-cells: hide; border-bottom: hidden;"></th>
<th colspan="2" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Validation Set
</div></th>
</tr>
<tr class="even">
<th style="text-align: left;" data-quarto-table-cell-role="th">Interactions</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Decimal</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">Time</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Deg. Free.</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">p-Value</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">MAE</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left; width: 100px;">Hour x Day</td>
<td style="text-align: right;">2.30</td>
<td style="text-align: left;">(2m, 17s)</td>
<td style="text-align: right;">5,890</td>
<td style="text-align: left; width: 80px;"></td>
<td style="text-align: right;">1.61</td>
<td style="text-align: right;">2.29</td>
</tr>
<tr class="even">
<td style="text-align: left; width: 100px;">Add One Item Interaction</td>
<td style="text-align: right;">2.12</td>
<td style="text-align: left;">(2m, 6s)</td>
<td style="text-align: right;">5,889</td>
<td style="text-align: left; width: 80px;">10<sup>-219.8</sup></td>
<td style="text-align: right;">1.56</td>
<td style="text-align: right;">2.10</td>
</tr>
<tr class="odd">
<td style="text-align: left; width: 100px;">Add All Item Interactions</td>
<td style="text-align: right;">2.06</td>
<td style="text-align: left;">(2m, 3s)</td>
<td style="text-align: right;">5,539</td>
<td style="text-align: left; width: 80px;">10<sup>-18.4</sup></td>
<td style="text-align: right;">1.60</td>
<td style="text-align: right;">2.13</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-interaction-anova-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.1: ANOVA table and validation set statistics for three models with different interaction sets.
</figcaption>
</figure>
</div>
</div><div class="column" style="width:15%;">

</div>
</div>
<p>What about the other two-way interactions between the other item predictors? Since the training set is fairly large (compared to the current number of model parameters), it is feasible to include the remaining 350 pairwise interactions and use the ANOVA method to validate this choice. These results are contained in the third row of <a href="#tbl-interaction-anova" class="quarto-xref">Table&nbsp;<span class="quarto-unresolved-ref">tbl-interaction-anova</span></a> where the RMSE dropped further by 0.05 minutes. The p-value is also very small, indicating that there is no evidence that <em>all</em> of the 350 parameter estimates are zero. More extensive testing would be required to determine which actually are zero. This can be tedious and a potentially dangerous “fishing expedition” that could result in serious bias creeping into the modeling process.</p>
<p>One problem with the ANOVA method is that it calculates the model error using the training set, which we know may not indicate what would occur with previously unseen data. In fact, it is well known that, for linear regression via ordinary least squares estimation, it is impossible for the training set error to ever increase when adding new model terms. Therefore, was the drop in RMSE when all interactions were included due to this fact? Or was it due to other important interactions that were included? To understand this we can turn to the validation set to provide confirmation. You can see this in <a href="#tbl-interaction-anova" class="quarto-xref">Table&nbsp;<span class="quarto-unresolved-ref">tbl-interaction-anova</span></a> where the validation set RMSE and MAE values are included. Note that, when the large set of interactions were added, these metrics both <em>increase</em> in the validation set, a result contrary to what the ANOVA results tell us.</p>
<p>The observed and predicted visualizations for each model in <a href="#tbl-interaction-anova" class="quarto-xref">Table&nbsp;<span class="quarto-unresolved-ref">tbl-interaction-anova</span></a> are shown in <a href="#fig-lin-reg-interactions" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-lin-reg-interactions</span></a>. Adding the additional interaction yielded a slight numerical improvement. However, the visualization in the middle panel shows fewer very large residuals. The figure also shows the model results that include the full set of 351 two-way interactions; this panel shows no significant reduction in large residuals, further casting doubt on using the entire set.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-lin-reg-interactions" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lin-reg-interactions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-lin-reg-interactions-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lin-reg-interactions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.5: Validation set predicted vs.&nbsp;observed delivery times for linear regression models with and without additional interactions.
</figcaption>
</figure>
</div>
</div>
</div>
<p>So far, we have used visualizations to unearth potential interactions. This can be an effective strategy when the number of potential interactions is large, but the visual nature of this process is subjective. There are some specialized quantitative tools for identifying interactions. For example, <span class="citation" data-cites="lim2015learning">Lim and Hastie (<a href="#ref-lim2015learning" role="doc-biblioref">2015</a>)</span> used regularized generalized linear models (sections TODO) to estimate all possible two-way interactions and use a penalization method to determine which should be retained. <span class="citation" data-cites="miller1984selection">Miller (<a href="#ref-miller1984selection" role="doc-biblioref">1984</a>)</span> and <span class="citation" data-cites="fes">Kuhn and Johnson (<a href="#ref-fes" role="doc-biblioref">2019</a>)</span> (<a href="https://bookdown.org/max/FES/approaches-when-complete-enumeration-is-practically-impossible.html#the-feasible-solution-algorithm">Section 7.4.3</a>) describe the feasible solution algorithm, an iterative search method for interaction discovery. Another, which we will now describe in more detail, is Friedman’s H-statistic <span class="citation" data-cites="friedman2008predictive">(<a href="#ref-friedman2008predictive" role="doc-biblioref">Friedman and Popescu 2008</a>)</span>.</p>
<p>We can estimate the joint effect of a set of predictors on the outcome as well as the effect of individual predictors. If we thought that only main effects and two-factor interactions were possible, we could factor out the individual effects from the joint effect. The leftover predictive ability would then be due to interactions. Consider the linear model in <a href="#eq-two-way-int" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-two-way-int</span></a>. The joint effect would include all possible model terms associated with a predictor. We can also create main effects too:</p>
<p><span class="math display">\[\begin{align}
f(x_1, x_2) &amp;= \beta_{1}x_1 + \beta_{2}x_2 + \beta_{3}x_{1}x_{2} \notag \\
f(x_1) &amp;= \beta_{1}x_1  \notag \\
f(x_2) &amp;= \beta_{2}x_2 \notag
\end{align}\]</span></p>
<p>To isolate the potential interaction effect:</p>
<p><span id="eq-int-isolate"><span class="math display">\[
f(x_1\times x_2) = f(x_1, x_2) - f(x_1) - f(x_2) = \beta_{3}x_{1}x_{2}
\tag{8.2}\]</span></span></p>
<p>This shows that, for this situation, we can isolate the effect of an interaction by removing any other systematic effects in the data<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. <a href="#eq-int-isolate" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-int-isolate</span></a> is based on a simple parametric linear model. For models with more complex prediction equations, we can’t analytically pick out which model parameters should be used to investigate potential interactions.</p>
<p>However, for any model, the joint and marginal effects can be quantified using <em>partial dependence profiles</em> (PDP) (<span class="citation" data-cites="molnar2020interpretable">Molnar (<a href="#ref-molnar2020interpretable" role="doc-biblioref">2020</a>)</span>, <a href="https://christophm.github.io/interpretable-ml-book/pdp.html">Section 8.1</a>). First, we determine a sequence of values covering the observed range of the predictor(s) of interest. Then we randomly sample a data point, perhaps from the training set, and over-ride the value of the predictor of interest with values from the grid. This produces a prediction profile over the grid. We can repeat this process many times to approximate <span class="math inline">\(f(\cdot)\)</span> by averaging the multiple prediction values for each grid point.</p>
<p><a href="#fig-ensemble-pdp" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ensemble-pdp</span></a> visualizes the PDP data derived from using the ensembles of regression trees for <span class="math inline">\(f(hour)\)</span>, <span class="math inline">\(f(day)\)</span>, and <span class="math inline">\(f(hour, day)\)</span>. The first panel shows the random realizations of the relationship between delivery time and order hour from 1,000 randomly sampled rows of the training set. The results are unsurprising; we can see a similarity between these results and the initial visualizations in <a href="#fig-delivery-predictors" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-delivery-predictors</span></a>. The single trend line in panel (b) is the average of these profiles for each value of the predictor (delivery hour). There appears to be, on average, a nonlinear effect of the delivery hour. The day of the week is an informative predictor and the <em>joint effect</em> profile in panel (d) shows that its effect induces different patterns in the delivery hour. If this were not the case, the patterns in panels (b) and (c) would, when combined, approximate the pattern in panel (d).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ensemble-pdp" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ensemble-pdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-ensemble-pdp-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ensemble-pdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.6: Partial dependence profiles for the food delivery hour and day, derived for the tree-based ensemble. Panel (a) shows the individual grid predictions from 1,000 randomly sampled points in the training set. The other panels show the average trends.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Friedman’s H-statistic can quantify the effect of interaction terms using partial dependence profiles. When investigating two-factor interactions between predictors <span class="math inline">\(j\)</span> and <span class="math inline">\(j'\)</span>, the statistic is</p>
<p><span class="math display">\[
H^2_{jj'}=\frac{\sum_\limits{i=1}^B\left[\hat{f}(x_{ij},x_{ij'})-\hat{f}(x_{ij})-\hat{f}(x_{ij'})\right]}{ \sum_\limits{i=1}^B\hat{f}^2(x_{ij}, x_{ij'})}
\]</span></p>
<p>where the <span class="math inline">\(\hat{f}(x_{ij})\)</span> term represents the partial dependence profile for predictor <span class="math inline">\(j\)</span> for sample point <span class="math inline">\(i\)</span> along the grid for that predictor and <span class="math inline">\(B\)</span> is the number of training set samples. The denominator captures the total joint effect of predictors <span class="math inline">\(j\)</span> and <span class="math inline">\(j'\)</span> so that <span class="math inline">\(H^2\)</span> can be interpreted as the fraction of the joint effect explained by the potential interaction.</p>
<p>For a set of <span class="math inline">\(p\)</span> predictors, we could compute all <span class="math inline">\(p(p-1)/2\)</span> pairwise interactions and rank potential interactions by their statistic values. This can become computationally intractable at some point. One potential shortcut suggested by the heredity principle is to quantify the importance of the <span class="math inline">\(p\)</span> predictors and look at all pairwise combinations of the <span class="math inline">\(p^*\)</span> most important values. For <span class="math inline">\(p^* = 10\)</span>, <a href="#fig-hstats-interaction" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-hstats-interaction</span></a> shows the top five interactions detected by this procedure. We’ve already visually identified the hour <span class="math inline">\(\times\)</span> day interaction, so seeing this in the rankings is comforting. However, another large interaction effect corresponds to the variables for items #1 and #10. Discovering this led us to visually confirm the effect back in <a href="#fig-delivery-items" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-delivery-items</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-hstats-interaction" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hstats-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-hstats-interaction-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hstats-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.7: <span class="math inline">\(H^2\)</span> statistics for the top ten pairwise interaction terms.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note that the <em>overall</em> importance of the features are being quantified. From this analysis alone, we are not informed that the relationships between the hour and distance predictors and the outcome are nonlinear. It also doesn’t give any sense of the direction of the interaction (e.g., synergistic or antagonistic). We suggest using this tool early in the exploratory data analysis process to help focus visualizations on specific variables to understand how they relate to the outcome and other predictors.</p>
<p>There is a version of the statistic that can compute how much a specific predictor is interacting with <em>any other</em> predictor. It is also possible to compute a statistical test to understand if the H statistic is different than zero. In our opinion, it is more helpful to use these values as diagnostics rather than the significant/insignificant thinking that often accompanies formal statistical hypothesis testing results.</p>
<p><span class="citation" data-cites="inglis2022visualizing">Inglis, Parnell, and Hurley (<a href="#ref-inglis2022visualizing" role="doc-biblioref">2022</a>)</span> discuss weak spots in the usage of the H-statistic, notably that correlated predictors can cause abnormally large values. This is an issue inherited from the use of partial dependence profiles <span class="citation" data-cites="apley2020">(<a href="#ref-apley2020" role="doc-biblioref">Apley and Zhu 2020</a>)</span>.</p>
<p>Alternatively, <span class="citation" data-cites="greenwell2018simple">Greenwell, Boehmke, and McCarthy (<a href="#ref-greenwell2018simple" role="doc-biblioref">2018</a>)</span> measures the potential for variables to interact by assessing the “flatness” over regions of the partial dependence profile. The importance of a predictor, or a pair of predictors, is determined by computing the standard deviation of the flatness scores over regions. For example, <a href="#fig-delivery-items" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-delivery-items</span></a> shows a flat profile for the item 1 <span class="math inline">\(\times\)</span> item 9 interaction while the item 1 <span class="math inline">\(\times\)</span> item 10 interaction has different ranges of means across values of item 1. <a href="#fig-flat-interaction" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-flat-interaction</span></a> shows ten interactions with the largest flatness importance statistics. As with the <span class="math inline">\(H^2\)</span> results, the hour-by-day interaction. The two top ten lists have 6 other interactions in common.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-flat-interaction" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flat-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-flat-interaction-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flat-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.8: Importance statistics for the top ten pairwise interaction terms.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Other alternatives can be found in <span class="citation" data-cites="hooker2004discovering">Hooker (<a href="#ref-hooker2004discovering" role="doc-biblioref">2004</a>)</span>, <span class="citation" data-cites="herbinger2022repid">Herbinger, Bischl, and Casalicchio (<a href="#ref-herbinger2022repid" role="doc-biblioref">2022</a>)</span>, and <span class="citation" data-cites="oh2022predictive">Oh (<a href="#ref-oh2022predictive" role="doc-biblioref">2022</a>)</span>.</p>
<p>The two PDP approaches we have described apply to any model capable of estimating interactions. There are also model-specific procedures for describing which joint effects are driving the model (if any). One method useful for tree-based models is understanding if two predictors are used in consecutive splits in the data. For example, <a href="#fig-reg-tree" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-reg-tree</span></a> showed a shallow regression tree for the delivery time data. The terminal nodes had splits with orderings <span class="math inline">\(\text{hour}\rightarrow\text{hour}\)</span> and <span class="math inline">\(\text{hour}\rightarrow\text{day} \rightarrow\text{distance}\)</span> (twice). This implies that these three predictors have a higher potential to be involved in interactions with one another.</p>
<p><span class="citation" data-cites="kapelner2013bartmachine">Kapelner and Bleich (<a href="#ref-kapelner2013bartmachine" role="doc-biblioref">2013</a>)</span> describe an algorithm that counts how many times pairs of variables are used in consecutive splits in a tree. They demonstrate this process with Bayesian Adaptive Regression Trees (BART, <a href="#sec-bart-cls" class="quarto-xref"><span class="quarto-unresolved-ref">sec-bart-cls</span></a>) which creates an ensemble of fairly shallow trees. <a href="#fig-bart-interaction" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bart-interaction</span></a> shows the top ten pairwise interactions produced using this technique. Note that the BART implementation split categorical predictors via single categories. For the delivery data, this means that the tree would split on a specific day of the week (.i.e., Friday or not) instead of splitting all of the categories at once. This makes a comparison between the other two methods difficult. However, it can be seen that the hour <span class="math inline">\(\times\)</span> day interaction has shown to be very important in all three methods.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bart-interaction" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bart-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-bart-interaction-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bart-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.9: BART model statistics identifying interactions by how often they occur in the tree rules.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Caution should be exercised with this method. First, some tree ensembles randomly sample a subset of predictors for each split (commonly known as <span class="math inline">\(m_{try}\)</span>). For example, if <span class="math inline">\(m_{try} = 4\)</span>, a random set of four predictors are the only ones considered for that split point. This deliberately coerces non-informative splits; we are not using the best possible predictors in a split. This can dilute the effectiveness of counting predictors in successive splits. Second, as discussed in <a href="#sec-cls-trees" class="quarto-xref"><span class="quarto-unresolved-ref">sec-cls-trees</span></a>, many tree-based splitting procedures disadvantage predictors with fewer unique values. For example, in the delivery data, the distance and hour predictors are more likely to be chosen for splitting than the item predictors <em>even if they are equally informative</em>. There are splitting procedures that correct for this bias, but users should be aware of the potential impartiality.</p>
<p>The H-statistic and its alternatives can be an incredibly valuable tool for learning about the data early on as well as suggesting new features that should be included in the model. They are, however, computationally expensive.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/interactions-nonlinear.html#sec-interactions-detection"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
</section>
<section id="sec-polynomials" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="sec-polynomials"><span class="header-section-number">8.2</span> Polynomial Basis Expansions</h2>
<p>In the food delivery data, we have thoroughly demonstrated that there are quantitative predictors that have nonlinear relationships with the outcome. In <a href="#sec-model-development-whole-game" class="quarto-xref"><span class="quarto-unresolved-ref">sec-model-development-whole-game</span></a>, two of the three models for these data could naturally estimate nonlinear trends. Linear regression, however, could not. To fix this, the hour predictor spawned multiple features called spline terms and adding these to the model enables it to fit nonlinear patterns. This approach is called a <em>basis expansion</em>. In this section, we’ll consider (global) polynomial basis expansions.</p>
<p>The most traditional basis function is the polynomial expansion. If we start with a simple linear regression model with a single predictor:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i
\]</span></p>
<p>the expansion would add additional terms with values of the predictor exponentiated to the <span class="math inline">\(p^{th}\)</span> power<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. For a cubic polynomial model:</p>
<p><span id="eq-poly-linear-reg"><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{i} + \beta_2 x_{i}^2 + \beta_3 x_{i}^3 + \epsilon_i
\tag{8.3}\]</span></span></p>
<p>This is a <em>linear model</em> in the sense that it is linear in the statistical parameters (the <span class="math inline">\(\beta\)</span> values), so we can use standard parameter estimation procedures (e.g., least squares).</p>
<p><a href="#fig-global-polynomial" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-global-polynomial</span></a> shows data from <span class="citation" data-cites="bralower1997mid">Bralower et al. (<a href="#ref-bralower1997mid" role="doc-biblioref">1997</a>)</span> which was collected to understand the relationship between the age of a fossil and the ratio of two radioactive isotopes. The data set contains 106 data points for these two variables. The relationship is highly nonlinear, with several regions where the data have significant convex or concave patterns.</p>
<p>For simplicity, a fit from a cubic model <a href="#eq-poly-linear-reg" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-poly-linear-reg</span></a> is shown as the default. While the pattern is nonlinear, the line does not conform well to the data. The lines above the plot show the three basis functions that, for <span class="math inline">\(p=3\)</span>, correspond to linear (<span class="math inline">\(x\)</span>), quadratic (<span class="math inline">\(x^2\)</span>), and cubic (<span class="math inline">\(x^3\)</span>) terms. The effects of these terms are blended using their estimated slopes (<span class="math inline">\(\hat{\beta}_j\)</span>) to produce the fitted line. Notice that the cubic fit is very similar to the cubic basis shown at the top of the figure. The estimated coefficient for the cubic term is much larger than the linear or quadratic coefficients</p>
<div id="fig-global-polynomial" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-global-polynomial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| label: shiny-global-polynomial
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(patchwork)
library(dplyr)
library(tidyr)
library(ggplot2)
library(splines2)
library(bslib)
library(viridis)
library(aspline)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
# source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-polynomial.R")

# Requires the sources in shiny-setup.R

data(fossil)

ui &lt;- page_fillable(
  # theme = grid_theme,
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-3, 6, -3)),
    sliderInput(
      "global_deg",
      label = "Polynomial Degree",
      min = 1L,
      max = 20L,
      step = 1L,
      value = 3L,
      ticks = TRUE
    ) # sliderInput
  ), # layout_columns
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-1, 10, -1)),
    as_fill_carrier(plotOutput('global'))
  )
)

server &lt;- function(input, output, session) {
  maybe_lm &lt;- function(x) {
    try(lm(y ~ poly(x, input$piecewise_deg), data = x), silent = TRUE)
  }
  
  names_zero_padded &lt;- function(num, prefix = "x", call = rlang::caller_env()) {
    rlang:::check_number_whole(num, min = 1, call = call)
    ind &lt;- format(seq_len(num))
    ind &lt;- gsub(" ", "0", ind)
    paste0(prefix, ind)
  }
  
  expansion_to_tibble &lt;- function(x, original, prefix = "term ") {
    cls &lt;- class(x)[1]
    nms &lt;- names_zero_padded(ncol(x), prefix)
    colnames(x) &lt;- nms
    x &lt;- as_tibble(x)
    x$variable &lt;- original
    res &lt;- tidyr::pivot_longer(x, cols = c(-variable))
    if (cls != "poly") {
      res &lt;- res[res$value &gt; .Machine$double.eps, ]
    }
    res
  }
  
  mult_poly &lt;- function(dat, degree = 4) {
    rng &lt;- extendrange(dat$x, f = .025)
    grid &lt;- seq(rng[1], rng[2], length.out = 1000)
    grid_df &lt;- tibble(x = grid)
    feat &lt;- poly(grid_df$x, degree)
    res &lt;- expansion_to_tibble(feat, grid_df$x)
    
    # make some random names so that we can plot the features with distinct colors
    rand_names &lt;- lapply(
      1:degree,
      function(x) paste0(sample(letters)[1:10], collapse = "")
    )
    rand_names &lt;- unlist(rand_names)
    rand_names &lt;- tibble(name = unique(res$name), name2 = rand_names)
    res &lt;- dplyr::inner_join(res, rand_names, by = dplyr::join_by(name)) %&gt;%
      dplyr::select(-name) %&gt;%
      dplyr::rename(name = name2)
    res
  }
  
  col_rect &lt;- ggplot2::element_rect(fill = "#fcfefe", colour = "#fcfefe")
  theme_light_bl &lt;- function() {
    ggplot2::theme(
      panel.background = col_rect,
      panel.border = col_rect,
      legend.background = col_rect,
      legend.key = col_rect,
      plot.background = col_rect
    )
  }
  
  # ------------------------------------------------------------------------------
  
  spline_example &lt;- tibble(x = fossil$age, y = fossil$strontium.ratio)
  rng &lt;- extendrange(fossil$age, f = .025)
  grid &lt;- seq(rng[1], rng[2], length.out = 1000)
  grid_df &lt;- tibble(x = grid)
  alphas &lt;- 1 / 4
  line_wd &lt;- 1.0
  
  base_p &lt;- spline_example %&gt;%
    ggplot(aes(x = x, y = y)) +
    geom_point(alpha = 3 / 4, pch = 1, cex = 3) +
    labs(x = "Age", y = "Isotope Ratio") +
    lims(x = rng) +
    theme_bw()
  
  output$global &lt;- renderPlot({
    poly_fit &lt;- lm(y ~ poly(x, input$global_deg), data = spline_example)
    poly_pred &lt;- predict(
      poly_fit,
      grid_df,
      interval = "confidence",
      level = .90
    ) %&gt;%
      bind_cols(grid_df)
    
    global_p &lt;- base_p
    
    if (input$global_deg &gt; 0) {
      global_p &lt;- global_p +
        geom_ribbon(
          data = poly_pred,
          aes(y = NULL, ymin = lwr, ymax = upr),
          alpha = 1 / 15
        ) +
        geom_line(
          data = poly_pred,
          aes(y = fit),
          col = "black",
          linewidth = line_wd
        ) +
        theme(
          plot.margin = margin(t = -20, r = 0, b = 0, l = 0),
          panel.background = col_rect,
          plot.background = col_rect,
          legend.background = col_rect,
          legend.key = col_rect
        )
      
      feature_p &lt;- poly(grid_df$x, input$global_deg) %&gt;%
        expansion_to_tibble(grid_df$x) %&gt;%
        ggplot(aes(variable, y = value, group = name, col = name)) +
        geom_line(show.legend = FALSE) + # , linewidth = 1, alpha = 1 / 2
        theme_void() +
        theme(
          plot.margin = margin(t = 0, r = 0, b = -20, l = 0),
          panel.background = col_rect,
          plot.background = col_rect,
          legend.background = col_rect,
          legend.key = col_rect
        ) +
        scale_color_viridis(discrete = TRUE, option = "turbo")
      
      p &lt;- (feature_p / global_p) + plot_layout(heights = c(1.5, 4))
    }
    
    print(p)
  })
}

app &lt;- shinyApp(ui, server)

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-global-polynomial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.10: A global polynomial approach to modeling data from <span class="citation" data-cites="bralower1997mid">Bralower et al. (<a href="#ref-bralower1997mid" role="doc-biblioref">1997</a>)</span> along with a representation of the features (above the data plot). The black dashed line is the true function and the shaded region is the 90% confidence intervals around the mean.
</figcaption>
</figure>
</div>
<p>We might improve the fit by increasing the model complexity, i.e., adding additional polynomial terms. An eight-degree polynomial seems to fit better, especially in the middle of the data where the pattern is most dynamic. However, there are two issues.</p>
<p>First, we can see from the 90% confidence bands around the line that the variance can be very large, especially at the ends. The fitted line and confidence bands go slightly beyond the range of the data (i.e., extrapolation). Polynomial functions become erratic when they are not close to observed data points. This becomes increasingly pathological as the polynomial degree increases. For example, in <a href="#fig-global-polynomial" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-global-polynomial</span></a>, choosing degrees greater than 14 will show extreme uncertainty in the variance of the mean fit (the solid line). This is a good example of the variance-bias tradeoff discussed in <a href="#sec-variance-bias" class="quarto-xref"><span class="quarto-unresolved-ref">sec-variance-bias</span></a> below.</p>
<p>Decreasing the polynomial degree will result in less complex patterns that tend to underfit the data. Increasing the degree will result in the fitted line being closer to the data, but, as evidenced by the confidence interval in the shaded region, the uncertainty in the model explodes (especially near or outside of the range range). This is due to severe overfitting. Eventually, you can increase the degree until the curve passes through every data point. However, the fit for any other values will be wildly inaccurate and unstable.</p>
<p>Second, the trend is concave for age values greater than 121 when it probably should be flat. The issue here is that a <em>global polynomial</em> pattern is stipulated. The effect of each model term is the same across the entire data range<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. We increased the polynomial degree to eight to improve the fit via additional nonlinearity. Unfortunately, this means that some parts of the data range will be <em>too nonlinear</em> than required, resulting in poor fit.</p>
<p>A global polynomial is often insufficient because the nonlinear relationships are different in different data sections. There may be steep increases in one area and gradual increases in others. Rather than using a global polynomial, what if we used different basis expansions in regions with more consistent trends rather than a global polynomial?</p>
<p><a href="#fig-piecewise-polynomials" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-piecewise-polynomials</span></a> shows a crude approach where three different regions have different polynomial fits with the same degree. The pre-set degree and data ranges are probably as good as can be (after much manual fiddling) but the approach isn’t terribly effective. The curves do not connect. This discontinuity is visually discordant and most likely inconsistent with the true, underlying pattern we are attempting to estimate. Also, there are large jumps in uncertainty when predicting values at the extremes of each region’s range.</p>
<div id="fig-piecewise-polynomials" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-piecewise-polynomials-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| label: shiny-piecewise-polynomials
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true

library(shiny)
library(patchwork)
library(dplyr)
library(tidyr)
library(ggplot2)
library(splines2)
library(bslib)
library(viridis)
library(aspline)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-piecewise.R")

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-piecewise-polynomials-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.11: A piecewise polynomial approach to model the data from <a href="#fig-global-polynomial" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-global-polynomial</span></a>.
</figcaption>
</figure>
</div>
<p>It turns out that we can make sure that the different fitted curves can connect at the ends and that they do that smoothly. This is done via a technique called splines which yield better results and are more mathematically elegant.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/interactions-nonlinear.html#sec-polynomials"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-splines" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="sec-splines"><span class="header-section-number">8.3</span> Spline Functions</h2>
<p>Let’s assume we use piecewise cubic polynomials in the regional curve fits. To make the curves connect, we can add additional features that isolate local areas of <span class="math inline">\(x\)</span> via a new feature column such as <span class="math inline">\(h(x - \xi)^3\)</span> where</p>
<p><span class="math display">\[
h(u)  =
\begin{cases} x &amp; \text{if $u &gt; 0$,}
\\
0 &amp; \text{if $u \le 0$.}
\end{cases}
\]</span></p>
<p>and <span class="math inline">\(\xi\)</span> defines one of the boundary points.</p>
<p>This zeros out parts of the predictor’s range that are greater than <span class="math inline">\(\xi\)</span>. A smooth, continuous model would include a global polynomial expansion (i.e., <span class="math inline">\(x\)</span>, <span class="math inline">\(x^2\)</span>, and <span class="math inline">\(x^3\)</span>) and these partial features for each value that defines the regions (these breakpoints, denoted with <span class="math inline">\(\xi\)</span>, are called <em>knots</em><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>). For example, if a model has three regions, the basis expansion has 3 + 2 = 5 feature columns. For the fossil data, a three region model with knots at <span class="math inline">\(\xi_1 = 107\)</span> and <span class="math inline">\(\xi_2 = 114\)</span> would have model terms</p>
<p><span id="eq-simple-spline"><span class="math display">\[
y_i = \beta_0 + \beta_1x_i + \beta_2x^2 + \beta_3x^3 + \beta_4h(x_i - 107)^3 + \beta_5h(x_i - 114)^3 + \epsilon_i
\tag{8.4}\]</span></span></p>
<p>The model fit, with the default interior knots, shown in <a href="#fig-simple-spline" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-simple-spline</span></a> looks acceptable. As with global polynomials, there is a significant increase in the confidence interval width as the model begins to extrapolate slightly beyond the data used to fit the model.</p>
<div id="fig-simple-spline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-simple-spline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| label: shiny-simple-spline
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(patchwork)
library(dplyr)
library(tidyr)
library(ggplot2)
library(bslib)
library(aspline)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-spline-manual-knots.R")

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simple-spline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.12: A simple cubic spline function with three regions. By default, the regions are partitions at age values of 107 and 114. The fitted curve has the same form as <a href="#eq-simple-spline" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-simple-spline</span></a>. The two curves above the data plot indicate the range and values of the partial featured truncated using the function <span class="math inline">\(h(\cdot)\)</span>.
</figcaption>
</figure>
</div>
<p>Let’s consider how each model parameter affects the predictor’s space by labeling the regions A, B, and C (from left to right). For region A, the model is a standard cubic function (using <span class="math inline">\(\beta_0\)</span> though <span class="math inline">\(\beta_3\)</span>). Region B adds the effect of the first partial function by including <span class="math inline">\(\beta_4\)</span>. Finally, region C is a function of all five model parameters. This representation of a spline is called a truncated power series and is mostly used to demonstrate how spline functions can flexibly model local trends.</p>
<div class="note-box">
<p>The word “spline” is used in many different contexts. It’s confusing, but we’ll try to clear it up.</p>
<p>We’ll define a “spline” or “spline function” as a collection of features that represent continuous and smooth piece-wise polynomial features of a single-number predictor (with some specific properties).</p>
<p>“Regression splines” usually describe the use of splines of one or more predictors in a model fit such as ordinary least squares <span class="citation" data-cites="wood2006generalized arnold2019computational">(<a href="#ref-wood2006generalized" role="doc-biblioref">Wood 2006</a>; <a href="#ref-arnold2019computational" role="doc-biblioref">Arnold, Kane, and Lewis 2019</a>)</span>.</p>
<p>The term “smoothing splines” refers to a procedure that encapsulates both a spline function as well as an estimation method to fit it to the data (usually via regularization). There is typically a knot for each unique data point in <span class="math inline">\(x\)</span> <span class="citation" data-cites="Wang2011">(<a href="#ref-Wang2011" role="doc-biblioref">Wang 2011</a>)</span>.</p>
<p>Also, there are many types of spline functions; we’ll focus on one but describe a few others at the end of this section.</p>
</div>
<p>A different method for representing spline basis expansions is the <em>B-spline</em> <span class="citation" data-cites="deboor2003practical">(<a href="#ref-deboor2003practical" role="doc-biblioref">De Boor 2003</a>)</span>. The terms in the model are parameterized in a completely different manner. Instead of the regions using increasingly more model parameters to define the regional fit, B-spline terms have more localized features such that only a few are “active” (i.e., non-zero) at a particular point <span class="math inline">\(x_0\)</span>. This can be seen in <a href="#fig-bases" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bases</span></a> which contains the basis function values for the truncated power series and B-spines</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bases" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bases-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-bases-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bases-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.13: Two representations of a cubic spline for the fossil data with knots at 107 and 114. The top panel illustrates the model terms defined by <a href="#eq-simple-spline" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-simple-spline</span></a>. The bottom panel shows the B-spline representation. All lines have been scaled to the same y-axis range for easy visualization.
</figcaption>
</figure>
</div>
</div>
</div>
<p>B-spline parameterizations are used primarily for their numerical properties and, for this reason, we’ll use them to visualize different types of splines.</p>
<p>Before considering another type of spline function, let’s look at one practical aspect of splines: choosing the knots.</p>
<section id="sec-define-knots" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="sec-define-knots"><span class="header-section-number">8.3.1</span> Defining the Knots</h3>
<p>How do we choose the knots? We can select them manually, and some authors advocate for this point of view. As stated by <span class="citation" data-cites="breiman1988monotone">Breiman (<a href="#ref-breiman1988monotone" role="doc-biblioref">1988</a>)</span>:</p>
<blockquote class="blockquote">
<p>My impression, after much experimentation, is the same - that few knots suffice providing that they are in the right place.</p>
</blockquote>
<p>A good example is seen in <a href="#fig-simple-spline" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-simple-spline</span></a> with the fossil data. Using the knot positioning widget at the top, we can find good <em>and</em> bad choices for <span class="math inline">\(\xi_1\)</span> and <span class="math inline">\(\xi_2\)</span>; knot placement can matter. However, hand-curating each feature becomes practically infeasible as the number of knots/features increases. Otherwise, they can be set algorithmically.</p>
<p>There are two main choices for automatic knot selection. The first uses a sequence of equally-spaced values encompassing the predictor space. The second is to estimate percentiles of the predictor data so that regions have about the same number of values they capture. For example, a fourth degree of freedom spline would have three split points within the data arranged at the 25th, 50th, and 75th percentiles (with the minimum and maximum values bracketing the outer regions).</p>
<p>It would be unwise to place knots as evenly spaced values between the minimum and maximum values. Without taking the distribution of the predictor into account, it is possible that the region between some knots might not contain any training data. Since there are usually several overlapping spline features for a given predictor value, we can still estimate the model despite empty regions (but it should be avoided).</p>
<p>Many, but not all, splines specify how complex the fit should be in the area between the knots. However, cubic splines are a common choice because they allow for greater flexibility than linear or quadratic fits, but are not overly flexible, which could lead to over-fitting. Increasing the polynomial degree beyond three has more disadvantages than advantages. Recall that, in <a href="#fig-global-polynomial" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-global-polynomial</span></a>, variance exploded at the ends of the predictor distribution when the polynomial degree was very large.</p>
</section>
<section id="sec-natural-splines" class="level3" data-number="8.3.2">
<h3 data-number="8.3.2" class="anchored" data-anchor-id="sec-natural-splines"><span class="header-section-number">8.3.2</span> Natural Cubic Splines</h3>
<p>The type of spline that we suggest using by default is the <em>natural cubic spline</em><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> <span class="citation" data-cites="stone1985additive arnold2019computational gauthier2020cubic">(<a href="#ref-stone1985additive" role="doc-biblioref">Stone and Koo 1985</a>; <a href="#ref-arnold2019computational" role="doc-biblioref">Arnold, Kane, and Lewis 2019</a>; <a href="#ref-gauthier2020cubic" role="doc-biblioref">Gauthier, Wu, and Gooley 2020</a>)</span>. It uses cubic fits in the interior regions and linear fits in regions on either end of the predictor distribution. For a simple two-knot model similar to <a href="#eq-simple-spline" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-simple-spline</span></a>, there are only three slope parameters (the global quadratic and cubic terms are not used):</p>
<p><span id="eq-natural-cubic-spline"><span class="math display">\[
y_i = \beta_0 + \beta_1x_i + \beta_2h(x_i - \xi_1)^3 + \beta_3h(x_i - \xi_2)^3 + \epsilon_i
\tag{8.5}\]</span></span></p>
<p>For the fossil data, <a href="#fig-natural-cubic-spline" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-natural-cubic-spline</span></a> shows how this model performs with different numbers of knots chosen using quantiles. The 90% confidence bands show an increase in uncertainty when extrapolating, but the increase is orders of magnitude smaller than a global polynomial with the same number of model terms.</p>
<div id="fig-natural-cubic-spline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-natural-cubic-spline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| label: shiny-natural-cubic-spline
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true

library(shiny)
library(patchwork)
library(dplyr)
library(tidyr)
library(ggplot2)
library(splines2)
library(bslib)
library(viridis)
library(aspline)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-spline-natural.R")

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-natural-cubic-spline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.14: A natural cubic spline fit to the data previously shown in <a href="#fig-simple-spline" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-simple-spline</span></a>. Good fits occur when approximately nine degrees of freedom are used. The top curves are the B-spline representation of the model terms.
</figcaption>
</figure>
</div>
<div class="important-box">
<p>Given the simplicity of this spline function, the fixed polynomial degree, and its stability in the tails of the predictor’s distribution, we suggest choosing natural cubic splines for most problems.</p>
</div>
<p>Splines can be configured in different ways. Two details to specify are how many regions should be separately modeled and how they are apportioned. The number of regions is related to how many features are used in the basis expansion. As the number of regions increases, so does the ability of the spline to adapt to whatever trend might be in the data. However, the risk of overfitting the model to individual data points increases as the flexibility increases.</p>
<p>For this reason, we often tune the amount of complexity that the spline will accommodate. Generally, since the number of regions is related to the number of features, we’ll refer to the complexity of the basis function via the number of degrees of freedom afforded by the spline. We don’t necessarily know how many degrees of freedom to use. There are two ways to determine this. First, we can treat the complexity of the spline as a tuning parameter and optimize it with the tuning methods mentioned in Chapters <a href="#sec-grid-search" class="quarto-xref"><span class="quarto-unresolved-ref ref-noprefix">sec-grid-search</span></a> and <a href="#sec-iterative-search" class="quarto-xref"><span class="quarto-unresolved-ref ref-noprefix">sec-iterative-search</span></a>. Another option is to over-specify the number of degrees of freedom and let specialized training methods solve the potential issue of over-fitting. From <span class="citation" data-cites="wood2006generalized">Wood (<a href="#ref-wood2006generalized" role="doc-biblioref">2006</a>)</span>:</p>
<blockquote class="blockquote">
<p>An alternative to controlling smoothness by altering the basis dimension, is to keep the basis dimension fixed, at a size a little larger than it is believed it could reasonably be necessary, but to control the model’s smoothness by adding a “wiggliness” penalty to the least squares fitting objective.</p>
</blockquote>
<p>This approach is advantageous when there are separate basis expansions for multiple predictors. The overall smoothness can be estimated along with the parameter estimates in the model. The process could be used with general linear models (Chapters <a href="#sec-ols" class="quarto-xref"><span class="quarto-unresolved-ref ref-noprefix">sec-ols</span></a> and <a href="#sec-ordinary-logistic-regression" class="quarto-xref"><span class="quarto-unresolved-ref ref-noprefix">sec-ordinary-logistic-regression</span></a>) or other parametric linear models.</p>
<p>Splines are extremely useful and are especially handy when we want to encourage a simple model (such as linear regression) to approximate the predictive performance of a much more complex black-box model (e.g., a neural network or tree ensemble). We’ll also see splines and spline-like features used within different modeling techniques, such as generalized additive models (Sections <a href="#sec-reg-gam" class="quarto-xref"><span class="quarto-unresolved-ref ref-noprefix">sec-reg-gam</span></a> and <a href="#sec-cls-gam" class="quarto-xref"><span class="quarto-unresolved-ref ref-noprefix">sec-cls-gam</span></a>), multivariate adaptive regression splines (<a href="#sec-mars" class="quarto-xref"><span class="quarto-unresolved-ref">sec-mars</span></a>), and a few others.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/interactions-nonlinear.html#sec-splines"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
</section>
<section id="sec-variance-bias" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="sec-variance-bias"><span class="header-section-number">8.4</span> Sidebar: The Variance-Bias Tradeoff</h2>
<p>Our discussion of basis expansions presents an excellent time for a segue to discuss an essential idea in statistics and modeling: the variance-bias tradeoff. This idea will be relevant in upcoming sections for resampling and specific models. At the end of this section, we’ll also connect it to <a href="#sec-effect-encodings" class="quarto-xref"><span class="quarto-unresolved-ref">sec-effect-encodings</span></a>.</p>
<p>What does variance mean in terms of a machine learning model? In this context, it would quantify how much the fitted model changes if we slightly change the data. For example, would the curve change much if we repeat the data collection that produces the values in <a href="#sec-polynomials" class="quarto-xref"><span class="quarto-unresolved-ref">sec-polynomials</span></a> and fit the sample model (for a fixed sample size and polynomial degree)? We can also look at the <em>variance of prediction</em>: for a specific new data point <span class="math inline">\(x_0\)</span>, how much intrinsic uncertainty is there? Similar to the discussion regarding extrapolation of global polynomials, we might want to compare how much the uncertainty changes as we move outside the training set’s range.</p>
<p>Bias is the difference between some estimate, like an average or a model prediction, and its true value. The true value is not the same as the data we collect; it is the unknowable theoretical value. For this reason, bias is often difficult to compute directly. For machine learning, the most pertinent idea of bias relates to how well a model can conform to the patterns we see in the data (hoping that the observed data are a good representation of the true values). Back in <a href="#fig-global-polynomial" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-global-polynomial</span></a>, we saw that a linear model didn’t fit the data well. This is due to simple linear regression being a high-bias model because, without additional feature engineering, it cannot replicate nonlinear trends. Adding polynomial or spline terms decreased the model’s bias since it was more flexible.</p>
<div class="important-box">
<p>We can think of bias in terms of the question: “How close are we <em>aiming</em> to the center of the target?” and variance as: “How much do our results vary when shooting at the target?”</p>
</div>
<p>The concepts of variance and bias are paired because they are often at odds with one another. For models that predict, we’d like a <em>low variance</em>, <em>low bias</em> model. In many cases, that can be exceedingly difficult to achieve. We can often lower bias by adding model complexity. However, increased complexity usually comes at the expense of stability (i.e., high variance). We will often be in the position of trying to find an acceptable compromise between the two. This was discussed with global polynomials; linear models were bad for our data, but adding too many polynomial terms adds unnecessary complexity and an explosion of variance (reflected in the confidence bands).</p>
<p>This leads us to the mean squared error (MSE). We’ve seen the root mean squared error already where we used it as a measure of accuracy for regression models. More generally, we can write it in terms of some unknown parameter <span class="math inline">\(\theta\)</span> and some estimate <span class="math inline">\(\hat{\theta}\)</span> based on statistical estimation from data: <span class="math display">\[MSE = E\left[(\theta - \hat{\theta})^2\right]\]</span></p>
<p>It turns out that the MSE is a combination of model variance and (squared) bias (<span class="math inline">\(\theta - \hat{\theta}\)</span>):</p>
<p><span class="math display">\[MSE = E\left[(\theta - \hat{\theta})^2\right] = Var[\hat{\theta}] + (\theta - \hat{\theta})^2 + \sigma^2\]</span></p>
<p>where <span class="math inline">\(\sigma^2\)</span> represents some unknown amount of “irreducible noise.” Because of this, MSE offers a statistic to minimize that accounts for both properties. This can offer a compromise between the two.</p>
<p>To illustrate this, we simulated a simple nonlinear model:</p>
<p><span class="math display">\[y_i = x_i^3 + 2\exp\left[-6(x_i - 0.3)^2\right] + \epsilon_i\]</span></p>
<p>where the error terms are <span class="math inline">\(\epsilon_i \sim N(0, 0.1)\)</span> and the predictor values were uniformly spaced across [-1.2, 1.2]. Since we know the true values, we can compute the model bias.</p>
<p>To create a simulated data set, 31 samples were generated. Of these, 30 were used for the training set and one was reserved for the estimating the variance and bias. The training set values were roughly equally spaced across the range of the predictor. One simulated data set is shown in <a href="#fig-nonlinear-sim" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-nonlinear-sim</span></a>, along with the true underlying pattern.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nonlinear-sim" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonlinear-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-nonlinear-sim-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonlinear-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.15: A simulated data set with a nonlinear function. The test set point is the location where our simulation estimates the bias and variance.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<pre><code>#&gt; Warning: executing %dopar% sequentially: no parallel backend registered</code></pre>
</div>
<p>A linear model was estimated using ordinary least squares and a polynomial expansion for each simulated data set. The test set values were predicted, and the bias, variance, and root mean squared error statistics were computed. This was repeated 1,000 times for polynomial degrees ranging from one to twenty.</p>
<p><a href="#fig-sim-results" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-sim-results</span></a> shows the average statistic values across model complexity (i.e., polynomial degree). A linear model performs poorly for the bias due to underfitting (as expected). In panel (a), adding more nonlinearity results in a substantial decrease in bias because the model fit is closer to the true equation. This improvement plateaus at a sixth-degree polynomial and stays low (nearly zero). The variance begins low; even though the linear model is ineffective, it is stable. Once additional terms are added, the variance of the model steadily increases then explodes around a 20<sup>th</sup> degree polynomial<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. This shows the tradeoff; as the model becomes more complex, it fits the data better but eventually becomes unstable.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-sim-results" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sim-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-sim-results-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sim-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.16: Simulation results showing variances, squared biases, and mean squared errors.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The mean squared error in panel (b) shows the combined effect of variance and bias. It shows a steep decline as we add more terms, then plateaus between seven and fifteenth-degree models. After that, the rapidly escalating variance dominates the MSE as it rapidly increases.</p>
<p>The variance-bias tradeoff is the idea that we can exploit one for the other. Let’s go back to the basic sample mean statistic. If the data being averaged are normally distributed, the simple average is an <em>unbiased estimator</em>: it is always “aiming” for the true theoretical value. That’s a great property to have. The issue is that many unbiased estimators can have very high variance. In some cases, a slight increase in bias might result in a drastic decrease in variability. The variance-bias tradeoff helps us when one of those two quantities is more important.</p>
<p><a href="#fig-tradeoff" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tradeoff</span></a> shows a simple example. Suppose we have some alternative method for estimating the mean of a group that adds some bias while reducing variance. That tradeoff might be worthwhile if some bias will greatly reduce the variance. In our figure, suppose the true population mean is zero. The green curve represents the sample mean for a fixed sample size. It is centered at zero but has significant uncertainty. The other curve might be an alternative estimator that produces a slightly pessimistic estimate (its mean is slightly smaller than zero) but has 3-fold smaller variation. When estimating the location of the mean of the population, the biased estimate will have a smaller confidence interval for a given sample size than the unbiased estimate. While the biased estimate may slightly miss the target, the window of the location will be much smaller than the unbiased estimate. This may be a good idea depending on how the estimate will be used <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-tradeoff" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tradeoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-tradeoff-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tradeoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.17: A simple example of the variance-bias tradeoff.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The categorical encoding approach shown in <a href="#eq-effect-posterior" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-effect-posterior</span></a> from <a href="#sec-effect-encodings" class="quarto-xref"><span class="quarto-unresolved-ref">sec-effect-encodings</span></a> is another example. Recall that <span class="math inline">\(\bar{y}_j\)</span> was the average daily rate for agent <span class="math inline">\(j\)</span>. That is an unbiased estimator but has high variance, especially for agents with few bookings. The more complex estimator <span class="math inline">\(\hat{y}_j\)</span> in <a href="#eq-effect-posterior" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-effect-posterior</span></a> is better because it is more reliable. That reliability is bought by biasing the estimator towards the overall mean (<span class="math inline">\(\mu_0\)</span>).</p>
<p>As mentioned, we’ll return to this topic several times in upcoming sections.</p>
</section>
<section id="discretization" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="discretization"><span class="header-section-number">8.5</span> Discretization</h2>
<p>Discretization<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> is the process of converting quantitative data into a set of qualitative groups (a.k.a “bins”). The model uses these values instead of the original predictor column (perhaps requiring an additional step to convert them into binary indicator columns). Sadly, <a href="#fig-wall-of-pie" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-wall-of-pie</span></a> illustrates numerous analyses that we have witnessed. This example uses the food delivery data and breaks the order hour and distance predictors into six and three groups, respectively. It also converts the delivery time outcome into three groups. This visualization, colloquially known as the “Wall of Pie,” tries to explain how the two predictors affect the outcome categories, often with substantial subjectivity.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-wall-of-pie" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wall-of-pie-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-wall-of-pie-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wall-of-pie-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.18: An unfortunate visualization of two predictors and the outcome in the food delivery data. The colors of the pie chart reflect the binned delivery times at cut points of 20 and 30 minutes with lighter blue indicating earlier times.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Our general advice, described in more detail in <span class="citation" data-cites="fes">Kuhn and Johnson (<a href="#ref-fes" role="doc-biblioref">2019</a>)</span>, is that the first inclination should never be to engineer continuous predictors via discretization<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. Other tools, such as splines, are both theoretically and practically superior to converting quantitative data to qualitative data. In addition, if a predictor can be split into regions that are predictive of the response, then methods that use recursive partitioning will be less arbitrary and more effective.</p>
<p>The literature supporting this is extensive, such as: <span class="citation" data-cites="Cohen1983mn">Cohen (<a href="#ref-Cohen1983mn" role="doc-biblioref">1983</a>)</span>, <span class="citation" data-cites="Altman1991ro">Altman (<a href="#ref-Altman1991ro" role="doc-biblioref">1991</a>)</span>, <span class="citation" data-cites="Maxwell1993ig">Maxwell and Delaney (<a href="#ref-Maxwell1993ig" role="doc-biblioref">1993</a>)</span>, <span class="citation" data-cites="Altman1994oa">Altman et al. (<a href="#ref-Altman1994oa" role="doc-biblioref">1994</a>)</span>, <span class="citation" data-cites="Buettner1997bt">Buettner, Garbe, and Guggenmoos-Holzmann (<a href="#ref-Buettner1997bt" role="doc-biblioref">1997</a>)</span>, <span class="citation" data-cites="Altman1998vs">Altman (<a href="#ref-Altman1998vs" role="doc-biblioref">1998</a>)</span>, <span class="citation" data-cites="Taylor2002jj">Taylor and Yu (<a href="#ref-Taylor2002jj" role="doc-biblioref">2002</a>)</span>, <span class="citation" data-cites="MacCallum2002ox">MacCallum et al. (<a href="#ref-MacCallum2002ox" role="doc-biblioref">2002</a>)</span>, <span class="citation" data-cites="Irwin2003mp">Irwin and McClelland (<a href="#ref-Irwin2003mp" role="doc-biblioref">2003</a>)</span>, <span class="citation" data-cites="Owen2005do">Owen and Froman (<a href="#ref-Owen2005do" role="doc-biblioref">2005</a>)</span>, <span class="citation" data-cites="Altman2006gn">Altman and Royston (<a href="#ref-Altman2006gn" role="doc-biblioref">2006</a>)</span>, <span class="citation" data-cites="Royston2006md">Royston, Altman, and Sauerbrei (<a href="#ref-Royston2006md" role="doc-biblioref">2006</a>)</span>, <span class="citation" data-cites="VanWalraven2008ne">van Walraven and Hart (<a href="#ref-VanWalraven2008ne" role="doc-biblioref">2008</a>)</span>, <span class="citation" data-cites="Fedorov2009jy">Fedorov, Mannino, and Zhang (<a href="#ref-Fedorov2009jy" role="doc-biblioref">2009</a>)</span>, <span class="citation" data-cites="Naggara2011xu">Naggara et al. (<a href="#ref-Naggara2011xu" role="doc-biblioref">2011</a>)</span>, <span class="citation" data-cites="Bennette2012ua">Bennette and Vickers (<a href="#ref-Bennette2012ua" role="doc-biblioref">2012</a>)</span>, <span class="citation" data-cites="Kuss2013zi">Kuss (<a href="#ref-Kuss2013zi" role="doc-biblioref">2013</a>)</span>, <span class="citation" data-cites="Kenny2013cf">Kenny and Montanari (<a href="#ref-Kenny2013cf" role="doc-biblioref">2013</a>)</span>, <span class="citation" data-cites="BarnwellMenard2015xa">Barnwell-Menard, Li, and Cohen (<a href="#ref-BarnwellMenard2015xa" role="doc-biblioref">2015</a>)</span>, <span class="citation" data-cites="Fernandes2019na">Fernandes et al. (<a href="#ref-Fernandes2019na" role="doc-biblioref">2019</a>)</span>, as well as the references shown in <span class="citation" data-cites="harrell2015regression">Harrell (<a href="#ref-harrell2015regression" role="doc-biblioref">2015</a>)</span>. These articles identify the main problems of discretization as follows:</p>
<ul>
<li>Arbitrary (non-methodological) choice of breaks for binning can lead to significant bias.</li>
<li>The predictor suffers a significant loss of information, making it less effective. Moreover, there is reduced statistical power to detect differences between groups when they exist.</li>
<li>The number of features are increased, thus exacerbating the challenge of feature selection.</li>
<li>Correlations between predictors are inflated due to the unrealistic reduction in the variance of predictors.</li>
</ul>
<p><span class="citation" data-cites="pettersson2016quantitative">Pettersson et al. (<a href="#ref-pettersson2016quantitative" role="doc-biblioref">2016</a>)</span> shows differences in analyses with and without discretization. Their Fig. 1 shows a common binning analysis: a continuous outcome and one or more predictors are converted to qualitative formats and a grid of pie charts is created. Inferences are made from this visualization. One main problem is related to uncertainty. The noise in the continuous data is squashed so that any visual signal that is seen appears more factual than it is in reality<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>. Also, the pie charts do not show measures of uncertainty; how do we know when two pie charts are “significantly different”?</p>
<p>Alternatively, Figs. 4 and 5 of their paper shows the results of a logistic regression model where all predictors were left as-is and splines were used to model the probability of the outcome. This has a much simpler interpretation and confidence bands give the reader a sense that the differences are real.</p>
<p>While it is not advisable to discretize most predictors, there are some cases when discretization can be helpful. As a counter-example, one type of measurement that is often appropriate to discretize is date. For example, <span class="citation" data-cites="fes">Kuhn and Johnson (<a href="#ref-fes" role="doc-biblioref">2019</a>)</span> show a data set where daily ridership data was collected for the Chicago elevated train system. The primary trend in the data was whether or not the day was a weekday. Ridership was significantly higher when people commute to work. A simple indicator for Saturday/Sunday (as well as major holiday indicators) was the driving force behind many regression models on those data. In this case, making qualitative versions of the date was rational, non-arbitrary, and driven by data analysis.</p>
<p>Note that several models, such as classification/regression trees and multivariate adaptive regression splines, estimate cut points in the model-building process. The difference between these methodologies and manual binning is that the models use all the predictors to derive bins based on a single objective (such as maximizing accuracy). They evaluate many variables simultaneously and are usually based on statistically sound methodologies.</p>
<p>If it is the last resort, how should one go about discretizing predictors? First, topic specific expertise of the problem can be used to create appropriate categories when categories are <em>truly</em> merited as in the example of creating an indicator for weekend day in the Chicago ridership data. Second, and most important, <em>any</em> methodology should be well validated using data that were not used to build the model (or choose the cut points for binning). To convert data to a qualitative format, there are both supervised and unsupervised methods.</p>
<p>The most reliable unsupervised approach is to choose the number of new features and use an appropriate number of percentiles to bin the data. For example, if four new features are required, the 0, 25%, 50%, 75%, and 100% quantiles would be used. This ensures that each resulting bin contains about the same number of samples from the training set.</p>
<p>If you noticed that this is basically the same approach suggested for choosing spline knots in the discussion earlier in this chapter, you are correct. This process is very similar to using a zero-order polynomial spline, the minor difference being the placement of the knots. A zero-order model is a simple constant value, usually estimated by the mean. This is theoretically interesting but also enables users to contrast discretization <em>directly</em> with traditional spline basis expansions. For example, if a B-spline was used, the modeler could tune over the number of model terms (i.e., the number of knots) and the spline polynomial degree. If binning is the superior approach, the tuning process would select that approach as optimal. In other words, we can let the data decide if discretization is a good idea<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
<p>A supervised approach would, given a specific number of new features to create, determine the breakpoints by optimizing a performance measure (e.g., RMSE, classification accuracy, etc.). A good example is a tree-based model (very similar to the process shown in <a href="#fig-collapse" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-collapse</span></a>). After fitting a single tree or, better yet, an ensemble of trees, the split values in the trees can be used as the breakpoints.</p>
<p>Let’s again use the fossil data from <span class="citation" data-cites="bralower1997mid">Bralower et al. (<a href="#ref-bralower1997mid" role="doc-biblioref">1997</a>)</span> illustrated in previous section on basis functions. We can fit a linear regression with qualitative terms for the age derived using:</p>
<ul>
<li>An unsupervised approach using percentiles at cut-points.</li>
<li>A supervised approach where a regression tree model is used to set the breakpoints for the bins.</li>
</ul>
<p>In each case, the number of new features requires tuning. Using the basic grid search tools described in <a href="#sec-grid" class="quarto-xref"><span class="quarto-unresolved-ref">sec-grid</span></a>, the number of required terms was set for each method (ranging from 2 to 10 terms) by minimizing the RMSE from a simple linear regression model. The results are that both approaches required the same number of new features and produced about the same level of performance; the unsupervised approach required 6 breaks to achieve an RMSE of 0.0000355 and the supervised model has an RMSE of 0.0000302 with 6 cut points. <a href="#fig-fossil-bins" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-fossil-bins</span></a> shows the fitted model using the unsupervised terms.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fossil-bins" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fossil-bins-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="interactions-nonlinear_files/figure-html/fig-fossil-bins-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fossil-bins-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.19: The estimated relationship between fossil age and the isotope ratio previously shown in <a href="#fig-piecewise-polynomials" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-piecewise-polynomials</span></a>, now using discretization methods.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The results are remarkably similar to one another. The blocky nature of the fitted trend reflects that, within each bin, a simple mean is used to estimate the sale price.</p>
<p>Again, we want to emphasize that arbitrary or subjective discretization is almost always suboptimal.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/interactions-nonlinear.html#discretization"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="chapter-references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="chapter-references">Chapter References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Altman1991ro" class="csl-entry" role="listitem">
Altman, D G. 1991. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Categorising+continuous+variables&amp;as_ylo=1991&amp;as_yhi=1991&amp;btnG=">Categorising Continuous Variables</a>.”</span> <em>British Journal of Cancer</em> 64 (5): 975.
</div>
<div id="ref-Altman1998vs" class="csl-entry" role="listitem">
Altman, D G. 1998. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Suboptimal+analysis+using+optimal+cutpoints&amp;as_ylo=1998&amp;as_yhi=1998&amp;btnG=">Suboptimal Analysis Using ’Optimal’ Cutpoints</a>.”</span> <em>British Journal of Cancer</em> 78 (4): 556–57.
</div>
<div id="ref-Altman1994oa" class="csl-entry" role="listitem">
Altman, D G, B Lausen, W Sauerbrei, and M Schumacher. 1994. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Dangers+of+using+optimal+cutpoints+in+the+evaluation+of+prognostic+factors&amp;as_ylo=1994&amp;as_yhi=1994&amp;btnG=">Dangers of Using "Optimal" Cutpoints in the Evaluation of Prognostic Factors</a>.”</span> <em>Journal of the National Cancer Institute</em> 86 (11): 829.
</div>
<div id="ref-Altman2006gn" class="csl-entry" role="listitem">
Altman, D G, and P Royston. 2006. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+cost+of+dichotomising+continuous+variables&amp;as_ylo=2006&amp;as_yhi=2006&amp;btnG=">The Cost of Dichotomising Continuous Variables</a>.”</span> <em>BMJ</em> 332 (7549): 1080.
</div>
<div id="ref-altorki2021neoadjuvant" class="csl-entry" role="listitem">
Altorki, Nasser K, Timothy E McGraw, Alain C Borczuk, Ashish Saxena, Jeffrey L Port, Brendon M Stiles, Benjamin E Lee, et al. 2021. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Neoadjuvant+durvalumab+with+or+without+stereotactic+body+radiotherapy+in+patients+with+early+stage+non+small+cell+lung+cancer+a+single+centre+randomised+phase+2+trial&amp;as_ylo=2021&amp;as_yhi=2021&amp;btnG=">Neoadjuvant Durvalumab with or Without Stereotactic Body Radiotherapy in Patients with Early-Stage Non-Small-Cell Lung Cancer: A Single-Centre, Randomised Phase 2 Trial</a>.”</span> <em>The Lancet Oncology</em> 22 (6): 824–35.
</div>
<div id="ref-apley2020" class="csl-entry" role="listitem">
Apley, D, and J Zhu. 2020. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=+Visualizing+the+Effects+of+Predictor+Variables+in+Black+Box+Supervised+Learning+Models+&amp;as_ylo=2020&amp;as_yhi=2020&amp;btnG=">Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models</a>.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 82 (4): 1059–86.
</div>
<div id="ref-arnold2019computational" class="csl-entry" role="listitem">
Arnold, T, M Kane, and B Lewis. 2019. <em>A Computational Approach to Statistical Learning</em>. Chapman; Hall/CRC.
</div>
<div id="ref-BarnwellMenard2015xa" class="csl-entry" role="listitem">
Barnwell-Menard, JL, Q Li, and A Cohen. 2015. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Effects+of+categorization+method+regression+type+and+variable+distribution+on+the+inflation+of+Type+I+error+rate+when+categorizing+a+confounding+variable&amp;as_ylo=2015&amp;as_yhi=2015&amp;btnG=">Effects of Categorization Method, Regression Type, and Variable Distribution on the Inflation of <span>Type-I</span> Error Rate When Categorizing a Confounding Variable</a>.”</span> <em>Statistics in Medicine</em> 34 (6): 936–49.
</div>
<div id="ref-Bennette2012ua" class="csl-entry" role="listitem">
Bennette, C, and A Vickers. 2012. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Against+quantiles+C+ategorization+of+continuous+variables+in+epidemiologic+research+and+its+discontents&amp;as_ylo=2012&amp;as_yhi=2012&amp;btnG=">Against Quantiles: <span>C</span>ategorization of Continuous Variables in Epidemiologic Research, and Its Discontents</a>.”</span> <em>BMC Medical Research Methodology</em> 12: 21.
</div>
<div id="ref-bralower1997mid" class="csl-entry" role="listitem">
Bralower, T, P Fullagar, C Paull, G Dwyer, and R Leckie. 1997. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Mid+Cretaceous+strontium+isotope+stratigraphy+of+deep+sea+sections&amp;as_ylo=1997&amp;as_yhi=1997&amp;btnG=">Mid-Cretaceous Strontium-Isotope Stratigraphy of Deep-Sea Sections</a>.”</span> <em>Geological Society of America Bulletin</em> 109 (11): 1421–42.
</div>
<div id="ref-breiman1988monotone" class="csl-entry" role="listitem">
Breiman, L. 1988. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Monotone+regression+splines+in+action+Comment+&amp;as_ylo=1988&amp;as_yhi=1988&amp;btnG=">Monotone Regression Splines in Action (Comment)</a>.”</span> <em>Statistical Science</em> 3 (4): 442–45.
</div>
<div id="ref-Buettner1997bt" class="csl-entry" role="listitem">
Buettner, P, C Garbe, and I Guggenmoos-Holzmann. 1997. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Problems+in+defining+cutoff+points+of+continuous+prognostic+factors+E+xample+of+tumor+thickness+in+primary+cutaneous+melanoma&amp;as_ylo=1997&amp;as_yhi=1997&amp;btnG=">Problems in Defining Cutoff Points of Continuous Prognostic Factors: <span>E</span>xample of Tumor Thickness in Primary Cutaneous Melanoma</a>.”</span> <em>Journal of Clinical Epidemiology</em> 50 (11): 1201–10.
</div>
<div id="ref-chen2008support" class="csl-entry" role="listitem">
Chen, SH, J Sun, L Dimitrov, A Turner, T Adams, D Meyers, BL Chang, et al. 2008. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+support+vector+machine+approach+for+detecting+gene+gene+interaction&amp;as_ylo=2008&amp;as_yhi=2008&amp;btnG=">A Support Vector Machine Approach for Detecting Gene-Gene Interaction</a>.”</span> <em>Genetic Epidemiology</em> 32 (2): 152–67.
</div>
<div id="ref-chipman1996bayesian" class="csl-entry" role="listitem">
Chipman, H. 1996. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Bayesian+variable+selection+with+related+predictors&amp;as_ylo=1996&amp;as_yhi=1996&amp;btnG=">Bayesian Variable Selection with Related Predictors</a>.”</span> <em>Canadian Journal of Statistics</em> 24 (1): 17–36.
</div>
<div id="ref-Cohen1983mn" class="csl-entry" role="listitem">
Cohen, J. 1983. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+Cost+of+dichotomization&amp;as_ylo=1983&amp;as_yhi=1983&amp;btnG=">The Cost of Dichotomization</a>.”</span> <em>Applied Psychological Measurement</em> 7 (3): 249–53.
</div>
<div id="ref-deboor2003practical" class="csl-entry" role="listitem">
De Boor, C. 2003. <em>A Practical Guide to Splines</em>. Springer-Verlag.
</div>
<div id="ref-elith2008working" class="csl-entry" role="listitem">
Elith, J, J Leathwick, and T Hastie. 2008. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+working+guide+to+boosted+regression+trees&amp;as_ylo=2008&amp;as_yhi=2008&amp;btnG=">A Working Guide to Boosted Regression Trees</a>.”</span> <em>Journal of Animal Ecology</em> 77 (4): 802–13.
</div>
<div id="ref-Fedorov2009jy" class="csl-entry" role="listitem">
Fedorov, V, F Mannino, and R Zhang. 2009. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Consequences+of+dichotomization&amp;as_ylo=2009&amp;as_yhi=2009&amp;btnG=">Consequences of Dichotomization</a>.”</span> <em>Pharmaceutical Statistics</em> 8 (1): 50–61.
</div>
<div id="ref-Fernandes2019na" class="csl-entry" role="listitem">
Fernandes, A, C Malaquias, D Figueiredo, E da Rocha, and R Lins. 2019. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Why+quantitative+variables+should+not+be+recoded+as+categorical&amp;as_ylo=2019&amp;as_yhi=2019&amp;btnG=">Why Quantitative Variables Should Not Be Recoded as Categorical</a>.”</span> <em>Journal of Applied Mathematics and Physics</em> 7 (7): 1519–30.
</div>
<div id="ref-friedman2008predictive" class="csl-entry" role="listitem">
Friedman, J, and B Popescu. 2008. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Predictive+learning+via+rule+ensembles&amp;as_ylo=2008&amp;as_yhi=2008&amp;btnG=">Predictive Learning via Rule Ensembles</a>.”</span> <em>The Annals of Applied Statistics</em> 2 (3): 916–54.
</div>
<div id="ref-garcia2009evaluating" class="csl-entry" role="listitem">
García-Magariños, M, I López-de-Ullibarri, R Cao, and A Salas. 2009. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Evaluating+the+Ability+of+Tree+Based+Methods+and+Logistic+Regression+for+the+Detection+of+SNP+SNP+Interaction&amp;as_ylo=2009&amp;as_yhi=2009&amp;btnG=">Evaluating the Ability of Tree-Based Methods and Logistic Regression for the Detection of <span>SNP-SNP</span> Interaction</a>.”</span> <em>Annals of Human Genetics</em> 73 (3): 360–69.
</div>
<div id="ref-gauthier2020cubic" class="csl-entry" role="listitem">
Gauthier, J, QV Wu, and TA Gooley. 2020. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Cubic+splines+to+model+relationships+between+continuous+variables+and+outcomes+a+guide+for+clinicians&amp;as_ylo=2020&amp;as_yhi=2020&amp;btnG=">Cubic Splines to Model Relationships Between Continuous Variables and Outcomes: A Guide for Clinicians</a>.”</span> <em>Bone Marrow Transplantation</em> 55 (4): 675–80.
</div>
<div id="ref-greenwell2018simple" class="csl-entry" role="listitem">
Greenwell, B, B Boehmke, and A J McCarthy. 2018. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+simple+and+effective+model+based+variable+importance+measure&amp;as_ylo=2018&amp;as_yhi=2018&amp;btnG=">A Simple and Effective Model-Based Variable Importance Measure</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-hamada1992analysis" class="csl-entry" role="listitem">
Hamada, M, and CF Wu. 1992. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Analysis+of+designed+experiments+with+complex+aliasing&amp;as_ylo=1992&amp;as_yhi=1992&amp;btnG=">Analysis of Designed Experiments with Complex Aliasing</a>.”</span> <em>Journal of Quality Technology</em> 24 (3): 130–37.
</div>
<div id="ref-harre1988regression" class="csl-entry" role="listitem">
Harrel, F, K Lee, and B Pollock. 1988. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Regression+models+in+clinical+studies+Determining+relationships+between+predictors+and+response&amp;as_ylo=1988&amp;as_yhi=1988&amp;btnG=">Regression Models in Clinical Studies: Determining Relationships Between Predictors and Response</a>.”</span> <em>JNCI: Journal of the National Cancer Institute</em> 80 (15): 1198–1202.
</div>
<div id="ref-harrell2015regression" class="csl-entry" role="listitem">
Harrell, F. 2015. <em>Regression Modeling Strategies</em>. Springer.
</div>
<div id="ref-herbinger2022repid" class="csl-entry" role="listitem">
Herbinger, J, B Bischl, and G Casalicchio. 2022. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=+REPID+Regional+Effect+Plots+with+implicit+Interaction+Detection&amp;as_ylo=2022&amp;as_yhi=2022&amp;btnG="><span>REPID</span>: Regional Effect Plots with Implicit Interaction Detection</a>.”</span> In <em>International Conference on Artificial Intelligence and Statistics</em>, 10209–33.
</div>
<div id="ref-hooker2004discovering" class="csl-entry" role="listitem">
Hooker, G. 2004. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Discovering+additive+structure+in+black+box+functions&amp;as_ylo=2004&amp;as_yhi=2004&amp;btnG=">Discovering Additive Structure in Black Box Functions</a>.”</span> In <em>Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 575–80.
</div>
<div id="ref-inglis2022visualizing" class="csl-entry" role="listitem">
Inglis, A, A Parnell, and C Hurley. 2022. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Visualizing+variable+importance+and+variable+interaction+effects+in+machine+learning+models&amp;as_ylo=2022&amp;as_yhi=2022&amp;btnG=">Visualizing Variable Importance and Variable Interaction Effects in Machine Learning Models</a>.”</span> <em>Journal of Computational and Graphical Statistics</em> 31 (3): 766–78.
</div>
<div id="ref-Irwin2003mp" class="csl-entry" role="listitem">
Irwin, J R, and G H McClelland. 2003. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Negative+consequences+of+dichotomizing+continuous+predictor+variables&amp;as_ylo=2003&amp;as_yhi=2003&amp;btnG=">Negative Consequences of Dichotomizing Continuous Predictor Variables</a>.”</span> <em>Journal of Marketing Research</em> 40 (3): 366–71.
</div>
<div id="ref-kapelner2013bartmachine" class="csl-entry" role="listitem">
Kapelner, A, and J Bleich. 2013. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=+bartMachine+Machine+learning+with+bayesian+additive+regression+trees&amp;as_ylo=2013&amp;as_yhi=2013&amp;btnG="><span class="nocase">bartMachine</span>: Machine Learning with Bayesian Additive Regression Trees</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-Kenny2013cf" class="csl-entry" role="listitem">
Kenny, P W, and C A Montanari. 2013. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Inflation+of+correlation+in+the+pursuit+of+drug+likeness&amp;as_ylo=2013&amp;as_yhi=2013&amp;btnG=">Inflation of Correlation in the Pursuit of Drug-Likeness</a>.”</span> <em>Journal of Computer-Aided Molecular Design</em> 27 (1): 1–13.
</div>
<div id="ref-fes" class="csl-entry" role="listitem">
Kuhn, M, and K Johnson. 2019. <em><span><a href="https://bookdown.org/max/FES">Feature Engineering and Selection</a></span>: <span class="nocase">A Practical Approach for Predictive Models</span></em>. CRC Press.
</div>
<div id="ref-Kuss2013zi" class="csl-entry" role="listitem">
Kuss, O. 2013. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+danger+of+dichotomizing+continuous+variables+A+visualization&amp;as_ylo=2013&amp;as_yhi=2013&amp;btnG=">The Danger of Dichotomizing Continuous Variables: A Visualization</a>.”</span> <em>Teaching Statistics</em> 35 (2): 78–79.
</div>
<div id="ref-lampa2014identification" class="csl-entry" role="listitem">
Lampa, E, L Lind, P Lind, and A Bornefalk-Hermansson. 2014. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+identification+of+complex+interactions+in+epidemiology+and+toxicology+A+simulation+study+of+boosted+regression+trees&amp;as_ylo=2014&amp;as_yhi=2014&amp;btnG=">The Identification of Complex Interactions in Epidemiology and Toxicology: A Simulation Study of Boosted Regression Trees</a>.”</span> <em>Environmental Health</em> 13 (1): 57.
</div>
<div id="ref-lim2015learning" class="csl-entry" role="listitem">
Lim, M, and T Hastie. 2015. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Learning+interactions+via+hierarchical+group+lasso+regularization&amp;as_ylo=2015&amp;as_yhi=2015&amp;btnG=">Learning Interactions via Hierarchical Group-Lasso Regularization</a>.”</span> <em>Journal of Computational and Graphical Statistics</em> 24 (3): 627–54.
</div>
<div id="ref-MacCallum2002ox" class="csl-entry" role="listitem">
MacCallum, R C, S Zhang, K J Preacher, and D Rucker. 2002. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=On+the+practice+of+dichotomization+of+quantitative+variables&amp;as_ylo=2002&amp;as_yhi=2002&amp;btnG=">On the Practice of Dichotomization of Quantitative Variables</a>.”</span> <em>Psychological Methods</em> 7 (1): 19–40.
</div>
<div id="ref-Maxwell1993ig" class="csl-entry" role="listitem">
Maxwell, S, and H Delaney. 1993. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Bivariate+median+splits+and+spurious+statistical+significance&amp;as_ylo=1993&amp;as_yhi=1993&amp;btnG=">Bivariate Median Splits and Spurious Statistical Significance</a>.”</span> <em>Psychological Bulletin</em> 113 (1): 181–90.
</div>
<div id="ref-miller1984selection" class="csl-entry" role="listitem">
Miller, A. 1984. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Selection+of+subsets+of+regression+variables&amp;as_ylo=1984&amp;as_yhi=1984&amp;btnG=">Selection of Subsets of Regression Variables</a>.”</span> <em>Journal of the Royal Statistical Society. Series A (General)</em>, 389–425.
</div>
<div id="ref-mokhtari2017combination" class="csl-entry" role="listitem">
Mokhtari, Reza Bayat, Tina S Homayouni, Narges Baluch, Evgeniya Morgatskaya, Sushil Kumar, Bikul Das, and Herman Yeger. 2017. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Combination+therapy+in+combating+cancer&amp;as_ylo=2017&amp;as_yhi=2017&amp;btnG=">Combination Therapy in Combating Cancer</a>.”</span> <em>Oncotarget</em> 8 (23): 38022–43.
</div>
<div id="ref-molnar2020interpretable" class="csl-entry" role="listitem">
Molnar, C. 2020. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>. Lulu.com.
</div>
<div id="ref-Naggara2011xu" class="csl-entry" role="listitem">
Naggara, O, J Raymond, F Guilbert, D Roy, A Weill, and D G Altman. 2011. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Analysis+by+categorizing+or+dichotomizing+continuous+variables+is+inadvisable+an+example+from+the+natural+history+of+unruptured+aneurysms&amp;as_ylo=2011&amp;as_yhi=2011&amp;btnG=">Analysis by Categorizing or Dichotomizing Continuous Variables Is Inadvisable: An Example from the Natural History of Unruptured Aneurysms</a>.”</span> <em>AJNR. American Journal of Neuroradiology</em> 32 (3): 437–40.
</div>
<div id="ref-oh2022predictive" class="csl-entry" role="listitem">
Oh, S. 2022. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Predictive+case+based+feature+importance+and+interaction&amp;as_ylo=2022&amp;as_yhi=2022&amp;btnG=">Predictive Case-Based Feature Importance and Interaction</a>.”</span> <em>Information Sciences</em> 593: 155–76.
</div>
<div id="ref-Owen2005do" class="csl-entry" role="listitem">
Owen, S, and R Froman. 2005. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Why+carve+up+your+continuous+data+&amp;as_ylo=2005&amp;as_yhi=2005&amp;btnG=">Why Carve up Your Continuous Data?</a>”</span> <em>Research in Nursing and Health</em> 28 (6): 496–503.
</div>
<div id="ref-pettersson2016quantitative" class="csl-entry" role="listitem">
Pettersson, M, XiXnjun Hou, M Kuhn, T T Wager, G W Kauffman, and P R Verhoest. 2016. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Quantitative+assessment+of+the+impact+of+fluorine+substitution+on+P+glycoprotein+P+gp+mediated+efflux+permeability+lipophilicity+and+metabolic+stability&amp;as_ylo=2016&amp;as_yhi=2016&amp;btnG=">Quantitative Assessment of the Impact of Fluorine Substitution on <span>P-</span>Glycoprotein <span class="nocase">(P-gp)</span> Mediated Efflux, Permeability, Lipophilicity, and Metabolic Stability</a>.”</span> <em>Journal of Medicinal Chemistry</em> 59 (11): 5284–96.
</div>
<div id="ref-NEJMoa2306434" class="csl-entry" role="listitem">
Planchard, D, P Jänne, Y Cheng, J Yang, N Yanagitani, SW Kim, S Sugawara, et al. 2023. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Osimertinib+with+or+without+Chemotherapy+in+EGFR+Mutated+Advanced+NSCLC&amp;as_ylo=2023&amp;as_yhi=2023&amp;btnG=">Osimertinib with or Without Chemotherapy in EGFR-Mutated Advanced NSCLC</a>.”</span> <em>New England Journal of Medicine</em> 389 (21): 1935–48.
</div>
<div id="ref-Royston2006md" class="csl-entry" role="listitem">
Royston, P, D G Altman, and W Sauerbrei. 2006. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Dichotomizing+continuous+predictors+in+multiple+regression+a+bad+idea&amp;as_ylo=2006&amp;as_yhi=2006&amp;btnG=">Dichotomizing Continuous Predictors in Multiple Regression: A Bad Idea</a>.”</span> <em>Statistics in Medicine</em> 25 (1): 127–41.
</div>
<div id="ref-singh2017suppressive" class="csl-entry" role="listitem">
Singh, Nina, and Pamela J Yeh. 2017. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Suppressive+drug+combinations+and+their+potential+to+combat+antibiotic+resistance&amp;as_ylo=2017&amp;as_yhi=2017&amp;btnG=">Suppressive Drug Combinations and Their Potential to Combat Antibiotic Resistance</a>.”</span> <em>The Journal of Antibiotics</em> 70 (11): 1033–42.
</div>
<div id="ref-stone1985additive" class="csl-entry" role="listitem">
Stone, C, and CY Koo. 1985. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Additive+splines+in+statistics&amp;as_ylo=1985&amp;as_yhi=1985&amp;btnG=">Additive Splines in Statistics</a>.”</span> <em>Proceedings of the American Statistical Association</em> 45: 48.
</div>
<div id="ref-Taylor2002jj" class="csl-entry" role="listitem">
Taylor, J M G, and M Yu. 2002. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Bias+and+efficiency+Loss+due+to+categorizing+an+explanatory+variable&amp;as_ylo=2002&amp;as_yhi=2002&amp;btnG=">Bias and Efficiency Loss Due to Categorizing an Explanatory Variable</a>.”</span> <em>Journal of Multivariate Analysis</em> 83 (1): 248–63.
</div>
<div id="ref-VanWalraven2008ne" class="csl-entry" role="listitem">
van Walraven, C, and R Hart. 2008. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Leave+em+alone+W+hy+continuous+variables+should+be+analyzed+as+such&amp;as_ylo=2008&amp;as_yhi=2008&amp;btnG=">Leave ’Em Alone - <span>W</span>hy Continuous Variables Should Be Analyzed as Such</a>.”</span> <em>Neuroepidemiology</em> 30 (3): 138–39.
</div>
<div id="ref-Wang2011" class="csl-entry" role="listitem">
Wang, Y. 2011. <em>Smoothing Splines: <span>M</span>ethods and Applications</em>. CRC Press.
</div>
<div id="ref-wood2006generalized" class="csl-entry" role="listitem">
Wood, S. 2006. <em>Generalized Additive Models: An Introduction with <span>R</span></em>. Chapman; Hall/CRC.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Alternatively, PCA after skewness-correcting transformations may be another good option.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>These were computed using the bootstrap, as in <a href="#sec-eda-whole-game" class="quarto-xref"><span class="quarto-unresolved-ref">sec-eda-whole-game</span></a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>It so happens that the Gaussian likelihood function is equivalent to the sums of squared errors (SSE). That, in turn, is equivalent to the RMSE. That simplification does not automatically occur for other probability distributions.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>This approach works because the model in <a href="#eq-two-way-int" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-two-way-int</span></a> is <em>capable</em> of estimating the interaction. There are many models that do not have the ability to measure interaction effects, and, for this case, it would be impossible to isolate the interaction term(s). However, tree-based ensembles are good at estimating interactions, as are other complex black-box models such as neural networks and support vector machines. The tools described below only work with “interaction capable” models.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>In actuality, a more computationally preferred method is to use <em>orthogonal polynomials</em> which rescale the variables before we exponentiate them. We’ll denote these modified versions of the predictor in equation <a href="#eq-poly-linear-reg" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-poly-linear-reg</span></a> as just <span class="math inline">\(x\)</span> to reduce mathematical clutter. We’ve seen these before in <a href="#fig-ordered" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ordered</span></a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This can be seen in the basis functions above the plot: each line covers the entire range of the data.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Actually, they are the <em>interior knots</em>. The full set of knots includes the minimum and maximum values of <span class="math inline">\(x\)</span> in the training set.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Also called a <em>restricted cubic spline</em> in <span class="citation" data-cites="harre1988regression">Harrel, Lee, and Pollock (<a href="#ref-harre1988regression" role="doc-biblioref">1988</a>)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>In this particular case, the variance becomes very large since the number of parameters is nearly the same as the number of training set points (30). This makes the underlying mathematical operation (matrix inversion) numerically unstable. Even so, it is the result of excessive model complexity.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>This specific example will be referenced again when discussing resampling in <a href="#sec-resampling" class="quarto-xref"><span class="quarto-unresolved-ref">sec-resampling</span></a>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Also known as binning or dichotomization.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>In the immortal words<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> of Lucy D’Agostino McGowan and Nick Strayer: “Live free or dichotomize.”<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p><span class="citation" data-cites="Kenny2013cf">Kenny and Montanari (<a href="#ref-Kenny2013cf" role="doc-biblioref">2013</a>)</span> does an excellent job illustrating this issue.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>But is most likely <strong>not</strong> a good idea.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/aml4td\.org");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/embeddings.html" class="pagination-link" aria-label="Embeddings">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Embeddings</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/overfitting.html" class="pagination-link" aria-label="Overfitting">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Overfitting</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js" type="text/javascript"></script>
<script type="text/javascript">
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>




</body></html>