---
knitr:
  opts_chunk:
    cache.path: "../_cache/categorical-predictors/"
---

# Working with Categorical Predictors {#sec-categorical-predictors}

```{r}
#| label: categorical-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(embed)
library(textrecipes)
library(scales)
library(gt)
library(patchwork)

# ------------------------------------------------------------------------------
# Set a few options

tidymodels_prefer()
set_options()
theme_set(theme_transparent())
```

```{r}
#| label: hotel-rate-setup
#| include: false

source("../R/setup_hotel_rates.R")

# ------------------------------------------------------------------------------
# Functions used to create the book content

clean_customer_name <- function(x, digits = 5, round = TRUE) {
  colnames(x) <- gsub("customer_type", "",   colnames(x))
  if (round) {
    x <- purrr::map_dfc(x, ~ sprintf("%.3.1f", .x))
  }
  tibble::as_tibble(format(as.data.frame(x), digits = digits))
}

mean_adr <- 
  hotel_rate_train %>% 
  summarize(avg_price_per_room = mean(avg_price_per_room), .by = c(customer_type)) %>% 
  arrange(customer_type) %>% 
  mutate(rounded = round(avg_price_per_room, 1))

format_encoding <- function(x, ...) {

  design <- x$x %>%
    tibble::as_tibble() %>%
    distinct() %>% 
    clean_customer_name(..., round = FALSE) %>%
    bind_cols(mean_adr %>% select(ADR = avg_price_per_room))
  
  est <- x %>%
    tidy() %>%
    select(term, estimate) %>%
    pivot_wider(names_from = term, values_from = estimate) %>%
    clean_customer_name() %>%
    bind_cols(ADR = NA_real_)

  table_dat <- 
    bind_rows(design, est) %>%
    mutate(info = c(gsub("_", " ", levels(x$model$customer_type)), "estimate (€):")) %>%
    relocate(info, ADR) 
  
  table_dat %>%
    gt() %>%
    sub_missing(ADR, missing_text = " ") %>%
    cols_label(info = "") %>%
    fmt_number(ADR, decimals = 1) %>% 
    cols_align(
      align = "right",
      columns = everything()
    ) %>%
    tab_spanner(label = "Model Terms", columns = c(-info, -ADR)) %>%
    tab_style(
      style = cell_borders(sides = c("top", "bottom"),  weight = px(2)),
      locations = cells_body(rows = nrow(table_dat))
    )
}

overall_mean <- mean(hotel_rate_train$avg_price_per_room)
levels_only <- function(var, lvl, ordinal = FALSE, sep = "_")  lvl
```

This chapter will discuss tools for handling predictor variables that are not measured on a strict numeric scale. For example, the building type and neighborhood variables are qualitative in the Ames housing data. The possible values of qualitative predictors are often called categories or levels (which will be used interchangeably here).  

Why would we need to modify this type of predictor? First, most mathematical models require information to be numbers. We'll need a method to convert qualitative data effectively to some quantitative format. This is an encoding problem; we are finding another representation of our predictor data. There are various tools for converting qualitative data, ranging from simple indicator variables (a.k.a. dummy variables) to more statistically sophisticated tools. These are the focus of this chapter. 

Second, there are other circumstances where simplifying or transforming categorical predictors might be advantageous. These are usually related to exceptional situations, such as when there are many possible values (i.e., high cardinality) or infrequently occurring values. Techniques to address these issues are also discussed. 

As implied above, some modeling techniques can take the categorical predictors in their natural form. These include tree- or rule-based models, naive Bayes models, and others. In this book's third and fourth parts, we'll discuss how certain models naturally handle categorical variables in their respective sections.

To begin, the primary data set used in this chapter is introduced.

## Example Data: Daily Hotel Rates {#sec-hotel-rates}

@ANTONIO201941 describe a data set with booking records from two Portuguese hotels. There are numerous variables in the data related to the

 - Reservation: room type, date, meal plan, etc., 
 - Booking method: agent, distribution channel, etc., 
 - Customer: type, number of children staying, country of origin, etc. 

In addition, there are several qualitative variables: customer type, market segment, room type, etc. 

We'll analyze these data in various ways (especially in @sec-reg-linear to @sec-reg-summary where they are used as a case study). Our goal is to predict the average daily rate (ADR, i.e., the average cost in euros) from one of the two hotels in the original data. 

```{r}
#| label: date-ranges
#| echo: false
hotel_rate_train_range <- range(hotel_rate_train$arrival_date)
hotel_rate_test_range <- range(hotel_rate_test$arrival_date)
```

There were `r format(nrow(hotel_rates), big.mark = ",")` bookings for the _resort hotel_. An initial two-way split was used: 

- The training set contains `r format(nrow(hotel_rate_train), big.mark = ",")` (75%) bookings whose arrivals are between `r hotel_rate_train_range[1]` and `r hotel_rate_train_range[2]`. 
- The remaining 25% were used as the test set with `r format(nrow(hotel_rate_test), big.mark = ",")` bookings (between `r hotel_rate_test_range[1]` to `r hotel_rate_test_range[2]`). 

This chapter will use these data to demonstrate how qualitative data, such as the booking agent's name or company, can be converted into quantitative information that many models require.

## Simple Indicator Variables {#sec-indicators}

As a simple example, consider the customer type predictor with categories: "contract", "group", "transient", and "transient (party)". What would we do if a model required these data to be numeric? There is no natural ordering to these values, so giving the models integer values (e.g., contract = 1.0, group = 2.0, etc.) would be erroneous since that implies a scale for these values. Instead, it is common to create multiple binary columns (numeric with zeros or ones) representing the occurrence of specific values. These are called **indicator columns** or sometimes **dummy variables**.  

@tbl-indicators shows how this works for the customer type. The rows depict the possible values in the data, while the columns are the resulting features used in place of the original column. This table uses the most common indicator encoding method called _reference cell_ parameterization (also called a _treatment contrast_). First, a category is chosen as the reference value. In  @tbl-indicators, the first alpha-numeric value is used (`"contract"`), but this is an arbitrary choice. After this, we create separate columns for all possible values except for the reference value. Each of these columns has a value of one when the data matches the column for that value (and is zero otherwise). 

```{r}
#| label: tbl-indicators
#| tbl-cap: "Indicator columns produced from a categorical column using a reference cell parameterization."

customer_types <- 
  tibble(customer_type = unique(hotel_rate_train$customer_type)) 

customer_types %>% 
  recipe() %>%
  step_dummy(customer_type, keep_original_cols = TRUE, naming = levels_only) %>%
  prep() %>%
  bake(new_data = NULL) %>% 
  arrange(customer_type) %>% 
  mutate(customer_type = gsub("_", " ", customer_type)) %>% 
  rename_all(~ gsub("_", " ", .x)) %>% 
  gt() %>% 
  tab_spanner(label = "Indicator Columns", columns = c(-`customer type`)) %>% 
  tab_options(table.width = pct(50))
```

The rationale for excluding one column is that you can infer the reference value if you know the values of all of the existing indicator columns^[In other words, we know that the vector of indicators (0, 0, 0) must represent the contract customers.]. Including all possible indicator columns embeds a redundancy in the data.  As we will see shortly, data that contain this type of redundancy pose problems for some models like linear regression.

Before moving on, let's examine how indicator columns shown in @tbl-indicators affect an example data analysis. Suppose that we are modeling a numeric outcome using a linear regression model (discussed in @sec-reg-linear) of the form:

$$ 
y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_4x_{i4} + \epsilon_i
$$ {#eq-linear-reg-again}

where the $\beta$ values are the parameters to be estimated and, in this application, the $x_{ij}$ are the indicator columns shown in @tbl-indicators. The intercept term ($\beta_0$) corresponds to a column of all ones.  It is almost always the case that we include the intercept term in the model. 

```{r}
#| label: customer-type-tables
#| include: false
#| label: agent-data

ref_cell_mod <-
  lm(
    avg_price_per_room ~ customer_type,
    data = hotel_rate_train %>% arrange(customer_type),
    x = TRUE
  )
hot_mod <-
  lm(
    avg_price_per_room ~ customer_type + 0,
    data = hotel_rate_train %>% arrange(customer_type),
    x = TRUE
  )
```

@tbl-ref-cell-effects shows the encoding in @tbl-indicators and adds columns for a numeric outcome and the intercept term. The outcome column shows the average daily rate (in €). Using a standard estimation procedure (called ordinary least squares), the bottom row of @tbl-ref-cell-effects shows the `r ncol(ref_cell_mod$x)` parameter estimates. Since all of the indicators for the contract customer row are zero, the intercept column estimates the mean value for that level ($\widehat{\beta}_0$ = `r round(coef(ref_cell_mod)[["(Intercept)"]], 1)`). The variable $x_{i1}$ only has an indicator for the "group" customers. Hence, its estimate corresponds to the difference in the average group outcome values (`r mean_adr$rounded[mean_adr$customer_type == "group"]`) minus the effect of the reference cell: `r mean_adr$rounded[mean_adr$customer_type == "contract"]` - `r mean_adr$rounded[mean_adr$customer_type == "group"]`. From this, the resulting estimate ($\widehat{\beta}_1$ = `r round(coef(ref_cell_mod)[["customer_typegroup"]], 2)`) is the effect of the group customers above and beyond the impact of the contract customers. The parameter estimates for the other possible values follow analogous interpretations.

```{r}
#| label: tbl-ref-cell-effects
#| tbl-cap: "An example of linear regression parameter estimates corresponding to a reference cell parameterization."

format_encoding(ref_cell_mod) %>% 
  tab_options(table.width = pct(66))
```

Another popular method for making indicator variables is called **one-hot encoding** (also known as a cell means encoding). This technique, shown in @tbl-one-hot, makes indicators for all possible levels of the predictor and does _not_ show an intercept column (for reasons described shortly). In this model parameterization, indicators are specific to each value in the data, and the linear regression estimates are the average response values for each customer type.  

```{r}
#| label: tbl-one-hot
#| tbl-cap: "One-hot encoded indicator variables from a categorical column of data."
format_encoding(hot_mod) %>% 
  tab_options(table.width = pct(69))
```

One-hot encodings are often used in nonlinear models, especially in neural networks and tree-based models. Indicators are not generally required for the latter but can be used^[This is discussed in greater detail for one of the case studies in @sec-reg-summary.]. 

One issue with creating indicator variables from categorical predictors is the potential for **linear dependencies**. This occurs when two or more sets of predictors have a mathematically linear relationship with one another. For example, suppose that each agent only worked with a single customer type. If we were to create indicator variables for the agent names and the customer type, there would be a redundancy: if we know the agent, we know the customer type. Mathematically, the row-wise sum of the agent name indicators would equal the row-wise sum of the position indicators. For some types of predictive models, including linear or logistic regression, the linear dependencies would cause the mathematical estimation of the model parameters to fail.

An example of this is the one-hot encoding method shown in @tbl-one-hot. If we were to include an intercept term with a one-hot encoding, the row-wise sum of the indicator columns equals the intercept column. The implication is that one of the model terms is not "estimable." For this reason, this encoding method is mainly used with models that do not use specific linear algebra operations (e.g., matrix inversion) to estimate parameters. Some tools can identify linear combinations [@estimable], and from this information, the smallest possible set of columns can be found to delete and thereby eliminate the linear dependencies. 

Note that linear dependencies are exact linear relationships between predictors. There are other circumstances where two or more predictors are highly correlated but not perfectly related. This situation, called **multicollinearity**, is discussed throughout the book but mostly in @sec-colinearity.  Multicollinearity can be a problem for many models and needs to be resolved prior to modeling. 

In addition to reference cell and one-hot encodings, there are various tools for creating indicator columns. Some, called _contrast methods_, are associated with more niche strategies, and many produce fractional values instead of binary indicators. For example, Helmert contrasts [@ruberg] are configured so that each category is compared to the average of the preceding categories. No encoding method is uniformly better than the others; the choice is often driven by how the user will interpret the parameter estimates. 

The situation becomes slightly more complex when a model contains multiple categorical predictors. The reference value combines levels across predictors for reference value parameterizations. For example, if a model included the customer's country of residence, the reference would include that factor. Suppose Canadians were the reference nationality.  Then the overall reference cell would be Canadian contract customers. 

Note that these encoding techniques are basically look-up tables that map individual categories known in the training set to specific numeric values. As discussed in the next section, mapping categories can be problematic when novel values are seen in new data (such as the test set). 

## Novel Categories {#sec-novel-categories}

When creating indicator variables, there is the assumption that the categories in the training data reflect all possible categories. This may not be the case. For example, the agents observed in our data are not the only agents that will be booking rooms in the future. After we create a model with a specific set of agents, what happens when we predict future data that involves new agents? 

One approach we could take is to prospectively create a new value for the predictor (let's give it a value of `"new"`). When a future sample has an agent that was not in the original data, we could assign this sample to the general category so that the model can anticipate this new level. In general, this won't help since the model estimation process will never have any "new" values on hand when the model is fit. For some models, this is innocuous and has no real harm (but offers no benefit either). 

Let's revisit linear regression results from the reference cell parameterization again. Suppose there is a new customer type of "influencer." Our reference cell encoding method could add an indicator column for that customer type, but since no influencers currently exist in the training data, it will be a column filled with zeros. We can call this a **zero-variance predictor** since it has a single unique value. It is entirely non-informative. For linear regression, this column of zeros creates a linear dependency with the intercept column because all rows have the same numeric value. Most regression software would detect zero-variance columns and remove them before the model is fit. 

Since there would be no influencer column in the data, all of the indicator columns would be zero when an influencer was observed in new data being predicted. However, from @tbl-ref-cell-effects we know that such a pattern would map to the reference level (which is the `"contract"` customer in our data). The new levels would be mapped to the reference level for this parameterization. This would occur more by happenstance and would not be a deliberate strategy. 

Traditional indicator encoding methods should be avoided if new predictor categories are anticipated. Instead, the techniques used in the next section (effect encodings or feature hashing) will likely be more effective at dealing with this situation. 

## Predictors with Many Values {#sec-high-cardinality}

Some predictors have high cardinality; they can take many possible values relative to the number of data points. For example, in the entirety of the hotel rate data, there are `r length(unique(hotel_rate_train$agent))` unique agents (see @fig-agents). Should we create so many indicator variables for this predictor? In the data, `r hotel_rate_train %>% count(agent) %>% filter(n <= 5) %>% nrow()`  agents had fewer than six bookings. This means there will be many sparse columns with almost all zero entries. Additionally, `r sum(!(unique(hotel_rate_train$agent) %in% unique(hotel_rate_test$agent)))` of the agents have no data in the training set; they only occur in the test set. Their indicator columns would contain all zeros. 

```{r}
#| label: fig-agents
#| fig-cap: "The frequency distribution of agents in the training set. "
#| fig-width: 7
#| fig-height: 3
#| out-width: 70%
hotel_rate_train %>% 
  count(agent, name = "bookings") %>% 
  mutate(
    agent = factor(agent),
    agent = reorder(agent, bookings)
  ) %>% 
  ggplot(aes(bookings)) + 
  geom_histogram(bins = 40, col = "white") + 
  scale_x_log10() + 
  labs(x = "Number of Bookings per Agent")
```

This section will consider several methods for effectively representing high cardinality predictors. The first two techniques are unsupervised (i.e., they do not use the outcome), while the others are supervised. @Cerda2022 surveys different methods for dealing with these types of predictors. 

### "Othering" {#sec-other}

```{r}
#| label: other-comps
#| include: false

freq_cut <- 1 / 20
low_nums <- 
  hotel_rate_train %>% 
  count(agent) %>% 
  mutate(pct = n / nrow(hotel_rate_train) * 100) %>% 
  filter(pct < freq_cut) %>% 
  nrow()
```

It is possible to isolate infrequently occurring categories into a new "other" category. For example, there are `r low_nums` agents with fewer bookings than `r freq_cut`{{< pct >}} of the training set. It may make sense to group these agents into their own categories as long as the predictive ability of the variable is maintained. There is no magic number for the training set frequency cutoff that decides which categories to pool. This pre-processing value requires optimization since each data set will have its own nuances with infrequent categories.

### Feature Hashing {#sec-feature-hashing}

Feature hashing [@weinberger2009feature] is a technique where the user specifies how many indicator columns should be used to represent the data. Using cryptographic principles, an algorithm assigns each row to an indicator column. This assignment only uses the value of the category in the calculations and, unlike the traditional methods discussed in @sec-indicators, does not require a dictionary that maps pre-defined categories to indicator columns. 

Suppose our agent column had a value of `"nobody owens"`. The algorithm translates this value to a fixed-length string using a _hash function_. Hash functions are designed to be non-random and, effectively, irreversible. Once the original value is converted to the _hash value_, it is very difficult to reverse engineer. Also, the frequency distributions of the hash values are designed to appear uniform in distribution, as seen below.  

The hashed value of the predictor's level can be converted to an integer, and, using modular arithmetic, the integer is mapped to a feature column. Suppose that we create 256 hashing features and `nobody owens`  maps to an integer of 123,456. To assign this agent to one of 256 feature columns, we compute `123456 mod 256` = `r 123456 %% 256`. For this row of data, we put a value of one in column `r 123456 %% 256`. 

Presumably, the number of possible categories is larger than the number of indicator columns we request. This means multiple predictor values will map to the same feature column. In cryptography, this is called a _collision_, the same as _aliasing_ in statistical experimental design [@BHH]. There is a trade-off between the number of feature columns and the number of collisions. As the number of columns increases, the probability of collisions decreases. However, the time required to train a model increases as the number of columns increases. Also, it is entirely possible that there are predictor columns where _no_ corresponding categories, yielding all zeros. 

To reduce collisions, the hashing function can produce integers that can be both positive and negative. When assigning the integer to a feature column, we can assign a value of -1 when the hashed integer value is negative. In this way, columns can have values of -1, 0, or 1. This means fewer collisions.

@tbl-feature-hash shows the results for several agents^[The original data uses codes to anonymize the agent names. The names shown here are randomly fabricated. The same is true of the company names.] when ten signed feature hash columns are requested. The last row of the table shows the rate of non-zero values in each column. The average density is close to 10{{< pct >}}, demonstrating that the hash function produces frequencies that emulate a uniform distribution. 

```{r}
#| label: hotel-rate-hash-calcs
#| warning: false
#| error: false
hash_10 <-
  hotel_rate_train %>%
  distinct(agent) %>%
  recipe(~ agent) %>%
  step_dummy_hash(agent, keep_original_cols = TRUE, num_terms = 10) %>%
  prep() %>%
  bake(new_data = NULL) %>%
  rename_at(vars(-agent), ~ gsub("dummyhash_agent_", "", .x)) %>%
  relocate(agent) %>%
  arrange(agent)

hash_256 <-
  hotel_rate_train %>%
  distinct(agent) %>%
  recipe(~ agent) %>%
  step_dummy_hash(agent, keep_original_cols = TRUE, num_terms = 2^8) %>%
  prep() %>%
  bake(new_data = NULL) %>%
  rename_at(vars(-agent), ~ gsub("dummyhash_agent_", "hash ", .x)) %>%
  relocate(agent) %>%
  arrange(agent)
```

```{r}
#| label: tbl-feature-hash
#| tbl-cap: "Signed indicators for agent via feature hashing."

remake_name <- function(x) {
  x <- gsub("_", " ", x)
  stringr::str_to_title(x)
}

hash_summary <-
    hash_10 %>%
      summarise(
        across(c(-agent), 
               ~ sprintf("%.3.1f", mean(abs(.x), na.rm = TRUE) * 100))
      ) %>%
      mutate(agent = c("density (%)"), .before = 1) %>% 
  as.data.frame() %>% 
  format() %>% 
  as.data.frame()

rownames(hash_summary) <- " "

hash_10_chr <- 
  hash_10 %>%
  mutate(agent = gsub("_", " ", agent)) %>% 
  as.data.frame() %>% 
  format() %>% 
  as.data.frame()

hash_top <- 
  hash_10_chr %>% 
  slice_head(n = 4)

hash_middle <- 
  hash_10_chr %>% 
  slice(1) %>% 
  mutate(across(everything(), ~ ":"))

rownames(hash_middle) <- ":"

hash_bottom <- 
  hash_10_chr %>% 
  slice_tail(n = 4)

bind_rows(hash_top, hash_middle, hash_bottom, hash_summary) %>%
  rename(Agent = agent) %>%
  mutate(Agent = remake_name(Agent)) %>% 
  gt() %>% 
  tab_spanner(label = "Features from Hashing", columns = c(-Agent)) %>% 
  cols_label() %>%
  # opt_row_striping() %>%
  tab_style(style = cell_borders(sides = c("top", "bottom"),  weight = px(2)),
            locations = cells_body(rows = 10)) %>%
  tab_style(style = cell_fill(color = "white"),
            locations = cells_body(rows = 10)) %>% 
  cols_align(
    align = "right",
    columns = everything()
  ) %>% 
  tab_options(table.width = pct(70))

```

The main downside of this method is that the use of hash values makes it impossible to explain the model. If the tenth feature column is critical, we can't explain why this is the case for new data (since the hash function is practically non-reversible and may include collisions).  This may be fine if the primary objective is prediction rather than interpretation.  When the goal is to optimize predictive performance, then the number of hashing columns to use can be included as a tuning parameter.  The model tuning process can then determine an optimal value of the number of hashing columns.

One unanticipated side effect of using feature hashing is that it _might_ remove linear dependencies with other columns. This is because hashing distributes the predictor levels to hashing columns roughly uniformly. There is a high probability that this eliminates any existing relationship between other predictor values. 

### Effect Encodings {#sec-effect-encodings}

We can also use a different supervised tool called _effect encoding_^[Also called 
_target encoding_ or _likelihood encoding_.] [@MicciBarreca2001; @pargent2022regularized]. Recall the discussion of one-hot encodings for indicator variables. @tbl-one-hot showed that the model terms for such an encoding estimate the cell means for each category (when the outcome is numeric). For example, the effect on the outcome for contract customers in that table was `r round(coef(hot_mod)["customer_typecontract"], 1)`€.  

Effect encodings use a simple, separate model to estimate the effect of each predictor category on the outcome and then use that value in place of the original data. For the example in @tbl-one-hot, the rows corresponding to the group customers would be replaced with `r round(coef(hot_mod)["customer_typegroup"], 1)`, transient customers with `r round(coef(hot_mod)["customer_typetransient"], 1)`, and so on. Instead of making multiple binary columns out of the categorical predictor, a single numeric column would take its place. 

```{r}
#| label: hotel-rate-effect-encode-calcs
agent_stats <-
  hotel_rate_train %>%
  group_by(agent) %>%
  summarize(
    mean_raw = mean(avg_price_per_room),
    n = n(),
    .groups = "drop"
  )

encoded_results <-
  hotel_rate_train %>%
  mutate(agent0 = agent) %>%
  recipe(avg_price_per_room ~ agent + agent0) %>%
  step_lencode_mixed(agent0, outcome = vars(avg_price_per_room)) %>%
  prep() %>%
  bake(new_data = NULL, agent0, agent) %>%
  rename(regularized = agent0) %>%
  distinct() %>%
  full_join(agent_stats, by = "agent") %>%
  arrange(agent)

example_1 <- 
  encoded_results %>% 
  filter(n == 1) %>% 
  slice_max(mean_raw, n = 1) %>% 
  mutate(diff = abs(mean_raw - regularized)) 

example_2 <- 
  encoded_results %>% 
  mutate(diff = abs(mean_raw - regularized)) %>% 
  filter(n  >= 150) %>% 
  slice_max(mean_raw, n = 1) %>% 
  mutate(agent = remake_name(agent))

mixed_mod <- lme4::lmer(avg_price_per_room ~ 1 + (1 | agent), data = hotel_rate_train)
pop_beta <- lme4::fixef(mixed_mod) 
blups <- 
  mixed_mod %>% 
  coef() %>% 
  pluck("agent") %>% 
  setNames("blup") %>% 
  mutate(
    blup = blup,
    centered = blup - pop_beta
    )

blup_plot <- 
  blups %>% 
  ggplot(aes(x = centered)) +
  geom_histogram(col = "white", bins = 20) + 
  geom_rug(alpha = 1 / 2) + 
  labs(x = ~ paste(hat(beta)[0][j])) +
  ggtitle("(a) Random Intercept Estimates") 
```

This may seem somewhat problematic though; we are using the training data to estimate **the effect of a predictor**, putting the resulting effects back into the data, and using our primary model to (re)estimate the effect of the predictor. Since the same data are used, is this a circular argument? Also, there should be a worry about data quality. As previously mentioned, the number of bookings per agent can vary greatly. For example: 

* Agent `r remake_name(example_1$agent[1])` had a single booking with an associated ADR of `r round(example_1$mean_raw, 1)`€.  Using this single data point as the effect estimate is unsatisfactory since we suspect that the estimated ADR would not stay the same if the number of total bookings were larger^[This demonstrates the concept of _overfitting_, discussed in @sec-overfitting and subsequent chapters.]. 

* Agent `r example_2$agent[1]` had `r format(example_2$n, big.mark = ",")` bookings with a simple mean ADR estimate of  `r round(example_2$mean_raw, 1)`€. We would judge the quality of this agent's data to be better than cases with a handful of bookings. 

How well can we estimate an effect if there are only one or two bookings for an agent in the data set?

Both of these concerns can be addressed by _how_ we estimate the effect of the predictor. Recall that @tbl-one-hot showed how simple linear regression was used to calculate the mean for each customer type. The way that the indicator variables impact the parameter estimates is such that the rows for each customer type are the only data used to estimate that customer type's effect. For example, the contract data do not directly affect the parameter estimate for the transient customers, and so on. This is often called a _no pooling_ method of estimating parameters. In this context, "pooling" means combining data across multiple categories to estimate parameters.  

We could compute the naive ADR estimate using sample means and produce a similar table. However, we can create better mean estimates for each agent by considering the data from _all_ agents using a technique called _partial pooling_ [@gelman1995bayesian; @BayesRules,  chap. 15]. 

::: {.mathy-box}
In the following two sub-sections, we'll describe this technique in detail for cases where the outcome is a numeric value assumed to be normally distributed or or non-Gaussian. These sections will rely more on probability theory and statistical details than most others.
:::

#### Numeric Outcomes {-}

For our ADR analysis, the outcome is a quantitative value. A linear regression model (see @sec-linear-regression) is the most commonly used approach to model this type of outcome. Linear regression models the outcome as a function of a slope and intercept model. For our example, where we only consider estimating the effect of the agents using indicator columns, the model is:

$$
y_{ij} =\beta_0 + \beta_1x_{i1} + \ldots + \beta_{`r nrow(blups) -1`}x_{i `r nrow(blups) -1`} + \epsilon_{ij} = \mu_i  + \epsilon_{ij}
$$

where $y_{ij}$ is the outcome (in this case, the ADR of booking $i$ by agent $j$),  $x_{ij}$ is an indicator for reservation $i$ from agent $j$,  $\beta_0$ is the estimated outcome of the reference cell, and we assume $\epsilon_i \sim N(0, \sigma^2_\epsilon)$. This leads to the assumption that $y_{ij} \sim N(\mu_i, \sigma^2_\epsilon)$. 
 The $\hat{\beta}_{j}$ parameters are the effect of the agents above and beyond the reference cell effect. The index $j$ is from 1 to `r nrow(blups) -1` since there were `r nrow(blups)` agents in the training set with at least one booking. 

Again, this approach is called _no pooling_ since it only uses each agent's data to estimate their effect. It treats the agent as a known set of values and cannot generalize to agents outside of those in the training set^[In the language of mixed effects models, the agent is a _fixed effect_ in this parameterization.].

_Partial pooling_ takes the viewpoint that the agents in the training set are a sample of a _population_ of agents. From this, we might posit a probability distribution should be assigned to the agent-to-agent effects. Since the range of the $\beta$ parameters is the real line, we might propose a _random intercept model_: 

$$
y_{ij} = { \color{darkseagreen} \underbrace{\color{black} (\beta_0 + \beta_{0j})}_{\text{intercept(s)}} }  + \epsilon_{ij} = \mu_j + \epsilon_{ij}
$$  {#eq-effect-random-int}

where the $\beta_{0j}$ are per-agent differences from the population effect ($\beta_{0}$) ($j = 1 \ldots `r nrow(blups)`$ this time). Note that the model has no $x_{ij}$ terms. This is simply a notation difference between the models since most methods that estimate this model do not treat that variable as a traditional regression covariate. 

Several techniques can fit this model. For a _linear mixed model_ approach to fitting a random intercept model [@demidenko2013mixed;@laird1982random], we assume that the model parameters have normal distributions with $\beta_{0} \sim N(\beta, \sigma^2)$  and $\beta_{0j} \sim N(0, \sigma_0^2)$ and that residuals ($\epsilon_{ij}$) follow a Gaussian distribution. When used in Bayesian analysis^[There is a more general discussion of Bayesian methods in @sec-bayes.], such distributions on parameters are referred to as _prior distributions_. We'll stick with this terminology for Bayesian and non-Bayesian techniques. There are many strategies for estimating this model type, including maximum likelihood,  restricted maximum likelihood, or Bayesian methods. See @stroup2012generalized or @gelman1995bayesian for more specifics.

However, we might gain some intuition if we pause and consider a less complex version of this model. For simplicity, we'll also make the completely unrealistic assumptions that all agents had the same number of bookings ($n_j = n$) and that we know that $\sigma^2_\epsilon = 1$. From this, we can also say that sample means of the outcome data for each category are also normal with $\bar{y}_{j} \sim N(\mu_i, 1/n)$. These assumptions greatly simplify the math required to produce a formula for the estimates of the per-category means:

$$
\hat{y}_j = \mu_0 + (\bar{y}_j - \mu_0) \frac{\sigma^2_0}{\frac{1}{n} + \sigma^2_0}
$$ {#eq-effect-posterior}

There are a few aspects of this equation^[@eq-effect-posterior is related to the well-known James-Stein estimator [@EfronHastie2016a, chap. 7].] to note. First, the effect for a specific category is estimated by a combination of the sample mean for each category ($\bar{y}_j$) as well as our prior mean ($\mu_0$). The difference demonstrates the possible shrinkage towards the prior mean.  

Also, the shrinkage coefficient depends on $\sigma^2_0$ and the amount of data in the category. Suppose we choose a non-informative prior by making it extremely wide^[Making the range of the prior exceedingly wide indicates that almost any value is possible.] via $\sigma^2_0 = 100$. No matter how much data are observed in the category, the fraction will be close to 1, and our partial pooling estimate is essentially $\bar{y}_j$. If we had a highly opinionated prior^[In other words, a very narrow distribution. There are other ways to create opinionated priors though. For instance, a skewed prior distribution for one of the $\beta$ regression slopes would be considered opinionated.], $\sigma^2_0 = 1$, the weighting factor becomes $n / (n + 1)$. For $n = 1$, the partial pooling estimate has an even share of the sample mean and the prior mean. The effect of the prior mean decreases as $n$ increases. 

These example computations assumed that each category has the same number of rows in the training set (i.e., $n_j = n$). In practice, this is unrealistic. However, the insights into how the sample size changes the amount of shrinkage demonstrate the previous idea of how "poor data" can affect the estimates with partial pooling. 

While @eq-effect-posterior helps illustrate how partial pooling works, we should understand that the specific results hinge on knowing $\sigma^2_\epsilon = 1$. We would seldom make such a critical assumption in practice. 

We'll use the non-Bayesian mixed model approach to estimate effects for our data since it is more computationally efficient. Using the training set data, maximum likelihood estimation produced $\hat{\beta}_0$ = `r signif(pop_beta, 3)`. @fig-effect-encoding(a) shows the distribution of the `r nrow(blups)` random intercept estimates $\hat{\beta}_{0j}$. The estimated standard deviation of this distribution was $\hat{\sigma_0}$ =  `r round(sqrt(as.data.frame(lme4::VarCorr(mixed_mod))$vcov[1]), 1)`€. Even though we assume that the prior distribution of the individual effects was symmetric, the histogram of the estimates shows a lack of symmetry. This may be due to a few outliers or indicate that our prior should not be symmetric. 

```{r}
#| label: fig-effect-encoding
#| fig-cap: "The effect encoding results for the hotel rate data. Panel (a) shows the distribution of the estimated per-agent effects. Panel (b) compares the two estimation methods for all unique data patterns in the training set. "
#| fig-width: 10
#| fig-height: 5
#| out-width: 90%
#| warning: false
agent_means <- 
  encoded_results %>%
  distinct(mean_raw, regularized, n) %>%
  filter(!is.na(mean_raw)) %>% 
  ggplot(aes(x = mean_raw, y = regularized)) +
  geom_abline(col = "green", alpha = 1/4, linewidth = 2) +
  geom_vline(xintercept = overall_mean, lty = 3) +
  geom_point(aes(size = n), alpha = 1/5)  +
  scale_size(range = c(1, 8)) +
  labs(x = "No Pooling", y = "Partial Pooling") +
  ggtitle("(b) Mean ADR Estimates")

(blup_plot + theme(plot.margin = unit(c(0,20,0,0), "pt"))) + 
  (agent_means + theme(plot.margin = unit(c(0,0,0,20), "pt")))
```

The benefit of using this model, called a hierarchical model, is that it views the per-agent effects as a collection of values from a distribution. The mathematical consequence of this formulation is that the resulting parameter estimates use the data from all agents to estimate each agent’s estimates (hence the use of the phrase "partial pooling"). The intuition for that statistical method applies here: the estimates for agents with "poor data quality" have their results shrunken towards the overall population mean across all agents. Agents with "better" data are shrunken less during estimation. What does "poor data quality" mean? It primarily means high variance, which is usually a function of sample size and some baseline, the irreducible noise level in the system. 

Recall the previous discussion regarding agents `r remake_name(example_1$agent[1])` and `r remake_name(example_2$agent[1])`. Using the partial pooling approach: 

* Agent `r remake_name(example_1$agent[1])`: partial pooling shrinks the single value (`r round(example_1$mean_raw, 1)`€) back towards the estimated population mean (`r signif(pop_beta, 3)`€) to produce a more modest estimated effect of `r round(example_1$regularized, 1)`€. Because there is a single value for this agent, the prior distribution greatly influences the final estimate. 

* Agent `r example_2$agent[1]`: partial pooling only slightly adjusts the original sample mean (`r round(example_2$mean_raw, 1)`€) to the final estimate (`r round(example_2$regularized, 1)`€) since there are more data available. The prior has almost no influence on this estimate since the agent had `r format(example_2$n, big.mark = ",")` bookings in the training set. 

The amount of shrinkage was driven mainly by the number of bookings per agent. @fig-effect-encoding(b) shows the collection of effect estimates for each unique data pattern in the training set. The dotted line indicates the overall ADR sample mean, and the size of the points indicates the total number of bookings by an agent. The distribution of the shrunken effects (_y_-axis) is significantly attenuated compared to the no pooling effects estimates (_x_-axis). However, note that the points with large numbers of bookings line up along the diagonal green line (i.e., minimal shrinkage to the center); regardless of the value of their agent-specific ADR estimate ($\bar{y}_j$).

To reiterate how these values are used for pre-processing this type of predictor, @tbl-effect-estimates shows the linear mixed model analysis results. The numeric column is our primary model's data for representing the agent names. This avoids creating a large number of indicator variables for this predictor. 

```{r}
#| label: tbl-effect-estimates
#| tbl-cap: "Examples of the numeric values that are used in place of each agent's data when effect encodings are used."
effect_chr <- 
  encoded_results %>%
  select(agent, regularized) %>% 
  mutate(
    agent = gsub("_", " ", agent),
    regularized = sprintf("%4.1f", regularized)
  ) %>% 
  as.data.frame() %>% 
  format(digits = 3) %>% 
  as.data.frame()

effect_top <- 
  effect_chr %>% 
  slice_head(n = 3)

effect_middle <- 
  effect_chr %>% 
  slice(1) %>% 
  mutate(across(everything(), ~ "  :  "))

effect_bottom <- 
  effect_chr %>% 
  slice_tail(n = 3)

bind_rows(effect_top, effect_middle, effect_bottom) %>%
  select(Agent = agent, `Effect Estimate (€)` = regularized) %>%
  mutate(Agent = remake_name(Agent)) %>% 
  gt() %>% 
  cols_align(
    align = "right",
    columns = Agent
  ) %>% 
  tab_options(table.width = pct(40))
```

As an alternative to generalized linear mixed models to estimate the effects, a more general approach uses _Bayesian analysis_ [@kruschke2014doing; @Rethinking]. Here, we give all of the model parameters prior probability distributions, which reflect our prior knowledge of the parameters' shape and range. "Priors" can be pretty vague or highly opinionated, depending on what we feel we know about the situation^[The linear mixed model approach mentioned above is often considered as an _empirical Bayes_ approach with more restrictive options for the prior distributions. For empirical Bayes, one might say that you make your priors for your regression parameters any distribution you want, as long as they are Gaussians.]. For example, to continue with the random intercept formulation, we might specify the following:

\begin{align}
\beta_0 &\sim N(0, \sigma_0^2) \notag \\
\beta_{0j} &\sim t(2) \notag \\
\sigma_0 &\sim U(0, \infty)
\end{align}

The uniform prior in the standard deviation is incredibly vague and is considered a non-informative prior. Conversely, the _t_ distribution (with two degrees of freedom) for the random effects reflects that we might expect that the distribution is symmetric but has heavy tails (e.g., potential outliers such as those seen in @fig-effect-encoding(a)). Priors can be vague or highly opinionated. 

When estimating the Bayesian model, the prior distributions are combined with the observed data to produce a _posterior distribution_ for each parameter. Like the previous mixed-model technique, the posterior mimics the prior when there is very little (or noisy) data. The posterior distribution favors the observed results when high-quality data are abundant.  Using Bayesian methods to estimate the effects enables much more versatility in specifying the model but is often more computationally intensive. 

#### Non-Guassian Outcomes {-}

In many of these cases, effect encodings can be computed using Bayesian methods or via the _generalized linear mixed model_ approach [@bolker2009generalized;  @stroup2012generalized] approach fitting a random intercept model. 

The latter model has a similar right-hand side as @eq-effect-random-int. For example, when the outcome ($y_{ij}$) is a count, the standard Poisson model would be 

$$
\log(\mu_{ij}) = \beta_0 + \beta_{0j}
$$ 
where $\mu_{ij}$ is the theoretical effect in the original units. 

Suppose the data are binomial proportions or Bernoulli (i.e., binary 0/1 values). The canonical effects model is: 

$$
\log\left(\frac{\pi_{ij}}{1-\pi_{ij}}\right) = \beta_0 + \beta_{0j}
$$
with $\pi_{ij}$ being the parameter value for the effects. This is a basic logistic regression formulation of the problem.

Similar Bayesian approaches can be used. However, a completely different technique for binary outcomes is to assume again that the original data are binomial with rate parameter $\pi$ but, instead of the logistic regression formulation, assume that the actual rates come from a Beta distribution with parameters $\alpha$ and $\gamma$. With the Beta distribution as the prior, users can specify distributions with different shapes, such as uniform, bell-shaped, or "U" shaped, depending on the values chosen for $\alpha$ and $\gamma$. See [Chapter 3](https://www.bayesrulesbook.com/chapter-3.html) of @BayesRules. The Bayesian approach can be very flexible. 

#### Consequences of using Effect Encodings {-}

As with feature hashing, effect encoding methods can naturally handle novel categories in future data. Without knowing anything about the new category, the most logical choice for this method is to assign the overall population effect (e.g., the mode of the posterior distribution) as the value. 

As with feature hashing, effect encodings can eliminate linear dependencies between sets of indicators. Since the categorical data are being replaced with numeric summaries of the outcome data, it may be likely that there is no longer a redundancy in the data. Indicator columns reflect the _identities_ of the predictor categories, while the effect encoding column measures the _effectiveness_ of the categories. For example, a hypothetical scenario was mentioned where the agent was aliased with the customer type, yielding a linear dependency. Once the agent name column is converted to effect estimates, there would no longer be a relationship between the two predictors. 

Lastly, applying effect encodings requires a robust data usage strategy. The risk of embedding over-fitted statistics into the model could cause problems that may not be observed until a new data set is predicted (see @sec-overfitting). Using resampling methods or a validation set (@sec-resampling) is critical for using this tool. Also, when there is a temporal aspect to the data, we should try to ensure that historical data are not being predicted with future data (i.e., information leakage). For example, when an agent has multiple bookings, we kept the oldest data for an agent in the training set and newer bookings in the test set. Simple random sampling may result in the opposite occurring.

### Supervised Combining of Categories  {#sec-combining-categories}

Another approach for dealing with predictors with large numbers of categories is to collapse them into smaller groups that similarly affect the outcome data. For example, a cluster of agents may have similar performance characteristics (i.e., mean ADRs). If we can derive clusters of categories from the training set, a smaller group of cluster-level indicators might be a better approach than many indicator columns.

This approach closely mirrors the splitting process in tree-based models (discussed in Sections [-@sec-cart-cls] and [-@sec-cart-reg]), and we can use these models to find suitable groupings of predictor values. In summary, the categories are ordered by their estimated effects and then partitioned into two groups by optimizing some performance measures. For classification models, this is usually a measure of the purity of the classes in the resulting partition^[The notion of "purity" can mean a very tight distribution of numerical data (for regression models) or when the frequency distribution of qualitative has a very high probability for one category.]. A measure of error is used for numeric outcomes, such as RMSE. Once the initial partition is made, the process repeats recursively within each division until no more splits can be made, perhaps due to insufficient data. 

```{r}
#| label: collapse

collapse_rec <- 
  recipe(avg_price_per_room ~ ., data = hotel_rate_train) %>% 
  # Note: make sure that the parameters here are the same as those listed 
  # in R/rpart_progress.R
  step_collapse_cart(agent, outcome = vars(avg_price_per_room)) %>% 
  prep()

groupings <- 
  tidy(collapse_rec, number = 1) %>% 
  select(agent = old, group = new)  %>% 
  mutate(group = gsub("agent_", "", group))

groupings_stats <- 
  hotel_rate_train %>%
  full_join(groupings, by = "agent") %>% 
  group_nest(group) %>% 
    mutate(
      stats = map(data, ~ tidy(t.test(.x$avg_price_per_room))),
      std_dev = map_dbl(data, ~ sd(.x$avg_price_per_room)),
      num_agents = map_int(data, ~ length(unique(.x$agent))),
      n = map_int(data, nrow)
    ) %>% 
    select(group, n, std_dev, num_agents, stats) %>% 
    unnest(cols = stats) %>% 
    select(group, n, num_agents, mean = estimate, std_dev, lower = conf.low, 
           upper = conf.high) %>%
  mutate(
    group = as.factor(group),
    xlo = as.numeric(group) - 0.4, 
    xhi = as.numeric(group) + 0.4
  )

agent_stats <-
  hotel_rate_train %>%
  group_by(agent) %>%
  summarize(
    mean = mean(avg_price_per_room),
    `# bookings` = n(),
    .groups = "drop"
  ) 

agent_plot_data <- 
  agent_stats  %>%
  full_join(groupings, by = "agent") 
```

For the agent data, @fig-collapse shows this process. The `r length(unique(groupings$agent))` agent are categorized into `r length(unique(groupings$group))` groups. The top panel shows the ordered mean ADR values for each agent. The vertical lines indicate how the different agents are grouped at each step of the splitting process. The bottom panel shows a scaled estimate of the RMSE that uses the group-level mean ADR to predict the individual outcome results. 

```{r}
#| label: fig-collapse
#| out-width: 60%
#| fig-cap: "The progression of using a tree-based method to collapse predictor categories into smaller sets."
#| message: false
#| warning: false
knitr::include_graphics("../premade/anime_tree_collapse.gif")
```

For these data, there is no rational grouping or clustering of the agents (regarding their ADR) for the tree-based model to discover. The process produces increasingly granular groups, and the stopping point is somewhat arbitrary. For these reasons, it is probably not the best approach for this predictor in this particular data set. 

## Encodings for Ordinal Predictors {#sec-ordinal-encodings}

In some cases, a categorical predictor will have levels with an inherent order. For example, in the Ames data, some house amenities were ranked in terms of quality with values '`excellent`', '`fair`', '`good`', '`typical`', and '`poor`'. These values do not imply a specific numerical translation (e.g., '`excellent`' = 2 times '`poor`'). For this reason, there are different options for converting them to numeric features. 

First, the analysis could treat them as unordered categories and allow the model to try to determine how the values relate to the outcome. For example, if there is the belief that, as the quality of the heating system improves, the house price should increase. Using unordered categories would be agnostic to this assumption and the model would estimate whatever trend it saw in the data.   This approach would essentially be treating the quality variable as a categorical variable with 5 levels.

Another choice is to use domain-specific knowledge to imply relationships between levels. For example, the quality values could be replaced with integers one through five. This does bake the assumed relationship into the features, and that is advantageous when the assumed pattern is correct. 

There are also contrast methods, similar to the one discussed in @sec-indicators, that can automatically create potential patterns using polynomial expansions (more on these in @sec-polynomials). For our quality example, there are five possible levels. Mathematically, we can create up to the number of unique levels (minus one) polynomial function.  In this example, linear, quadratic, cubic, and quartic ($x^4$) polynomial functions can be used. @fig-ordered shows these patterns using orthogonal polynomial values^[These features have characteristics that their values sum to zero and there is zero correlation between features.]. 

```{r}
#| label: fig-ordered
#| echo: false
#| out-width: 70%
#| fig-width: 7
#| fig-height: 4.5
#| fig-cap: Polynomial contrasts for an ordered categorical predictor with five possible levels. 

qual_vals <- c('excellent', 'fair', 'good', 'typical', 'poor')
quality <- data.frame(quality = ordered(qual_vals, levels = qual_vals))

model.matrix(~ quality, quality) %>% 
  as.data.frame() %>% 
  select(-1) %>% 
  setNames(paste("Feature", 1:4)) %>% 
  bind_cols(quality) %>% 
  mutate(ordering = as.numeric(quality)) %>% 
  pivot_longer(cols = c(starts_with("Feature")), names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x = quality, y = value)) + 
  geom_point(aes(col = quality), cex = 2.5, show.legend = FALSE) + 
  geom_line(aes(x = ordering), alpha = 1 / 3) + 
  facet_wrap(~ variable) +
  scale_color_brewer(palette = "Set2") +
  labs(y = NULL)
```

In many applications, higher-degree polynomials (> 2) are unlikely to be effective predictors. For example, the cubic pattern in @fig-ordered ("Feature 3") implies that houses in Ames with '`excellent`' quality heating systems would have lower sale prices than those with '`fair`' or '`poor`' systems.  Utilizing higher order polynomials usually requires some specific knowledge about the problem at hand.

## Chapter References {.unnumbered}
