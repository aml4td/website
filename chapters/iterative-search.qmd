---
knitr:
  opts_chunk:
    cache.path: "../_cache/iterative/"
---

# Iterative Search {#sec-iterative-search}

```{r}
#| label: iterative-setup
#| include: false
source("../R/_common.R")

# ------------------------------------------------------------------------------

library(kableExtra)
library(tidymodels)
library(finetune)
library(patchwork)
library(future)
library(bestNormalize)
library(kableExtra)
library(GA)

# ------------------------------------------------------------------------------
# Set Options

plan("multisession")
tidymodels_prefer()
theme_set(theme_transparent())
set_options()

num_workers <- parallel::detectCores()
```

Grid search is a static procedure; we predetermine which candidates will be evaluated before beginning. How can we adaptively optimize tuning parameters in a sequential manner? Perhaps more importantly, _when_ is this a good approach? 

Previously, we’ve seen that a plateau of good performance in the parameter space can be  possible. This is often the case but will not always be true. If the region of optimal performance is small, we must create a large space-filling design to find it. When there are many tuning parameters, the computational expense can be unreasonable. An even worse situation is one where the previously described speed-up techniques (such as the submodel trick from @sec-submodels) are not applicable. Racing can be very efficient, but if the training set is very large, it may be that a validation set is more appropriate than multiple resamples; racing cannot be used in this situation. Finally, there are practical considerations when the data set is very large. Most parallel processing techniques require the data to be in memory for each parallel worker, rendering that technique moot. 

Generally, we are not often constrained by these issues _for models used for tabular data_. However, there are a few models that might be better optimized sequentially. The first two that come to mind are large neural networks^[Sections [-@sec-cls-svm] and [-@sec-reg-svm]]. and support vector machines^[Sections [-@sec-cls-nnet] and [-@sec-reg-nnet]] (SVMs). For the former, as the number of layers increases, so does the number of tuning parameters. Neural networks also have many important tuning parameters associated with the model’s training, such as the learning rate, regularization parameters, etc. These models also require more preprocessing than others; they are suspectable to uninformative predictors, missing data, and collinearity. Depending on the data set, there can be many pre-model tuning parameters. 

Support vector machines are models with fewer tuning parameters than neural networks but with similar preprocessing requirements. Unfortunately, the tuning parameter space can have large areas of poor performance with "islands" where the model works well. The location of these can change from data set to data set. We’ll see an example of this shortly in a small, two-parameter example. 

These two models, in particular, are more likely to benefit from an optimization method that chooses candidates as the process evolves. 

In theory, any iterative optimization procedures can be used. In general, gradients, such as steepest descent or Newton’s method, are the most commonly used technique for nonlinear optimization. These tools are suboptimal when it comes to parameter tuning but play important parts in other areas of machine learning and, for this reason, they will be discussed later. 

Derivative-free techniques can be helpful when tuning parameters, and there are many that can be used for nonlinear optimizations. Traditional examples are genetic algorithms, simulated annealing, particle swarm optimization, etc. We’ll consider the first two of these in Sections [-@sec-sim-anneal] and [-@sec-genetic-algo]. Currently, the most well-known iterative tool for tuning models is Bayesian optimization. This will be examined in @sec-bayes-opt. 

To get started, let's revisit a data set. 

## Example: Predicting Barley Amounts using Support Vector Machines {#sec-barley-svm}

```{r}
#| label: data-import
#| include: false
source("../R/setup_chemometrics.R")
```

We’ll return to the data previously seen in @sec-barley where we want to predict the percentage of barley oil in a mixture. Recall that the predictors are extremely correlated with one another. In @sec-embeddings, we considered embedding methods like PCA to preprocess the data and these models were able to achieve RMSE values of about 6% (shown in @fig-barley-linear-bakeoff). 

In this chapter, we’ll model these data in two different scenarios. First, we’ll use them as a "toy problem" where only two support vector machine tuning parameters are optimized. This is a little unrealistic, but it allows us to visualize how iterative search methods work in a 2D space. Second, in @sec-bayes-opt-nnet, we’ll get serious and optimize a larger group of parameters for neural networks simultaneously with three parameters of a specialized signal processing technique. 

Let's start with the toy example that uses an SVM regression model. This highly flexible nonlinear model can represent the predictor data in higher dimensions using a _kernel transformation_. This function combines numeric predictor vectors from two data points using a dot product. There are many different types of kernel functions, but for a _polynomial_ kernel, it is: 

$$
k(\boldsymbol{x}_1, \boldsymbol{x}_2) = (a\boldsymbol{x}_1'\boldsymbol{x}_2 + b)^q
$$ {#eq-kernel-poly}

where $a$ is called the scaling factor, $b$ is a constant offset value, and $q$ is the polynomial degree. The dot product of predictor vectors ($\boldsymbol{x}_i$) measures both angle and distance. Note that for the kernel function to work in an appropriate way, the two vectors must have elements with consistent units (i.e., they have been standardized). 

The kernel function operates much like a polynomial basis expansion; it projects a data point into a much larger, nonlinear space. The idea is that a more complex representation of the predictors might enable the model to make better predictions. 

For our toy example, we'll take the `r ncol(barley_train) - 1` predictors, project them to a smaller space of 10 principal components, then standardize those features to have the same mean and standard deviation. A quartic polynomial is used with zero offset. The scale parameter will be tuned. It helps define how much influence the dot product has in the polynomial expansion. 

The most commonly tuned SVM parameter is independent of the different kernel functions: the _cost value_. This dictates how much a poorly predicted model should penalize the performance metric (when evaluated on the training set). Higher cost values will encourage the SVM to become increasingly more complex. We've previously seen the effect of modulating the SVM cost value in @fig-two-class-overfit. For small costs, the SVM classification model did not try too hard to classify samples correctly. When increased to an extreme, it significantly overfit the training set points.

Just as in @sec-embeddings, the RMSE will be computed from a validation set and these statistics will be used to guide our efforts. 

Let's define a wide  space for our two tuning parameters: cost will vary from 2<sup>-10</sup> to 2<sup>10</sup> and the scale factor^[Why do these two parameters use different bases? It’s mostly a convention. Both parameters have valid values that can range across many orders of magnitude. The change in cost tends to occur more slowly than the scaling factor, so a smaller base of 2 is often used.] is allowed to range from 10<sup>-10</sup> to 10<sup>-0.1</sup>. @fig-svm-grid visualizes the RMSE across these ranges^[These are not simulated data, so this surface is an approximation of the true RMSE via the validation set using a very large regular grid. The RMSE results are estimates and would change if we used different random numbers for data splitting.] where darker colors indicate smaller RMSE values. The lower left diagonal area is a virtual "dead zone" with very large RMSE results that don't appear to change much. There is also a diagonal wedge of good performance (symbolized by the darker colors). The figure shows the location of the smallest RMSE value and a diagonal ridge of parameter combinations with nearly equal results. Any points on this line would produce good models (at least for this toy example). Note that there is a small locale of excessively large RMSE values in the upper right. Increasing both tuning parameters to their limit is a bad idea.  

```{r}
#| label: svm-spec
#| include: false
rec <-
  recipe(barley ~ ., data = barley_train) %>%
  step_orderNorm(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = tune()) %>%
  step_normalize(all_predictors())

svm_spec <-
  svm_poly(cost = tune(), degree = tune(), scale_factor = tune(), margin = tune()) %>%
  set_mode("regression")

svm_wflow <- workflow(rec, svm_spec)

svm_param <-
  svm_wflow %>%
  extract_parameter_set_dials() %>%
  update(
    num_comp = num_comp(c(1, 200)),
    cost = cost(c(-10, 10)),
    degree = degree_int(c(1, 8)),
    scale_factor = scale_factor(c(-10, -1/10))
  )

reg_mtr <- metric_set(rmse)
```

```{r}
#| label: fig-svm-grid
#| echo: false
#| fig-width: 5
#| fig-height: 5
#| out-width: 50%
#| fig-cap: "A visualization of model performance when only the SVM cost and scaling factor parameters are optimized. It shows the combination with the smallest RMSE and a ridge of candidate values with nearly equivalent performance."

load("../RData/two_param_iter_large.RData")
x_rng <- 10^extendrange(c(-10, -1/10))
y_rng <- 2^extendrange(c(-10, 10))

log10_labs <- trans_format("log10", math_format(10^.x, function(x) format(x, digits = 3)))
log2_labs <- trans_format("log2", math_format(2^.x, function(x) format(x, digits = 3)))

num_cuts <- 50
rd_or <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "OrRd")))(num_cuts)

regular_mtr %>%
  mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts))) %>% 
  ggplot(aes(scale_factor, cost)) +
  geom_tile(aes(fill = RMSE)) +
  geom_point(data = regular_mtr %>% slice_min(mean), col = "grey", cex = 3) + 
  geom_line(
    data = regular_mtr %>% slice_min(mean, n = 18),
    stat = "smooth",
    col = "grey",
    method = lm,
    se = FALSE,
    formula = y ~ x,
    alpha = 1,
    linewidth = 1
  ) +
  scale_x_log10(limits = x_rng,
                labels = log10_labs,
                expand = expansion(add = c(-1 / 5, -1 / 5))) +
  scale_y_continuous(limits = y_rng, trans = "log2", labels = log2_labs,
                     expand = expansion(add = c(-1/2, -1/2))) +
  scale_fill_manual(values = rd_or) +  
  labs(x = "Scaling Factor", y = "Cost") +
  coord_fixed(ratio = 1/2) +
  theme_bw() +
  theme(legend.position = "none")
```

The next section will discuss gradient descent optimization and why it isn’t a good choice for model tuning. The two subsequent sections will discuss how two traditional global search methods, simulated annealing and genetic algorithms, can be used to search the parameter space. After this, the focus is on Bayesian optimization. Finally, we end with an extended analysis of the barley prediction problem. 

## Sidebar: Gradient-Based Optimization

To formalize the idea of optimization, we have some objective function that we are trying to optimize. This can also be called a loss function (specifically to be minimized). In either case, this value is signified by $f()$. The parameters that modify $f()$ are denoted as $\theta$, which is $p \times 1$ in dimension and is assumed to be real numbers. We'll also assume that $f()$ is smooth and generally differentiable and, without losing generality, assume that smaller values are better. 

For derivatives, we’ll denote the first derivative ($f'(\theta)$), for simplicity, as $g(\theta)$. The matrix of second deriviatives, called the Hessian matrix, is symbolized as $H(\theta)$. 

If we start with an initial guess, $\theta_0$, and compute the gradient at this point, yielding a $p$ dimensional directional vector. To get to our next parameter value, basic gradient descent uses the update:

$$
\theta_{i+1} = \theta_i - \alpha\:g(\theta_i)
$$

The value $\alpha$ defines how far we should travel in our new direction. This can be a constant value^[We’ve seen $\alpha$ before when it was called the learning rate (and will revisit it later in this chapter).] or a secondary procedure, called a line search, can be used to increase the value of $\alpha$ until the objective function worsens.

We proceed to iterate this process until some measure of convergence is achieved. For example, the optimization could be halted if the objective function does not improve more than a very small value. 

As a simple demonstration, @fig-grad-descent shows $f(\theta) = \theta\: cos(\theta/2)$ for values of $\theta$ between $\pm 10.0$. We want to minimize this function. There is a _global_ minimum at about $\theta \approx `r round(optimize(function(x) x * cos(.5 * x) , c(-6, 10))$minimum, 2)`$. Notice that there are _local_ minima at $\theta = -10.0$ and $\theta \approx `r round(optimize(function(x) x * cos(.5 * x) , c(-6, 0))$minimum, 2)`$. These are false solutions where some search procedures might become trapped. 


:::: {.columns}

::: {.column width="5%"}
:::

::: {.column width="90%"}

::: {#fig-grad-descent}

::: {.figure-content}

```{shinylive-r}
#| label: fig-grad-descent
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)

# source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

theme_light_bl<- function(...) {
  
  ret <- ggplot2::theme_bw(...)
  
  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect
  
  ret$legend.position <- "top"
  
  ret
}

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(xs = c(3, 3, 3, 3), sm = 4),
    column(
      width = 12,
      sliderInput(
        "start",
        label = "Starting Value",
        min = -6,
        max = 10,
        value = 3,
        step = 1
      )
    ),
    column(
      width = 12,
      numericInput(
        "rate",
        label = "Leaning Rate",
        min = 0,
        max = 3,
        value = 2,
        step = 0.1
      ),
      checkboxInput(
        "decay",
        label = "Decay?",
        value = FALSE
      )
    ),
    column(
      width = 12,
      sliderInput(
        "iter",
        label = "Iteration",
        min = 0L,
        max = 50L,
        step = 1L,
        value = 0L,
        width = "100%"
      )
    )
  ),
  as_fill_carrier(plotOutput("iterations"))
)

server <- function(input, output) {
  library(ggplot2)
  library(dplyr)
  
  cyclic <- function(epoch, initial = 0.001, largest = 0.1, step_size = 5) {
    if (largest < initial) {
      tmp <- initial
      largest <- initial
      initial <- tmp
    } else if (largest == initial) {
      initial <- initial / 10
    }
    cycle <- floor( 1 + (epoch / 2 / step_size) )
    x <- abs( ( epoch / step_size ) - ( 2 * cycle) + 1 )
    initial + ( largest - initial ) * max( 0, 1 - x)
  }
  decay_time <- function(epoch, initial = 0.1, decay = 1) {
    initial / (1 + decay * epoch)
  }  
  
  fn <- function(x) x * cos(.5 * x) 
  # D(quote(x * cos(.5 * x)), "x")
  deriv_1 <- function(x) cos(0.5 * x) - x * (sin(0.5 * x) * 0.5)
  
  # ------------------------------------------------------------------------------

  max_iter <- 50
  mx <- max_iter + 1
  
  x_seq <- seq(-10, 10, length.out = 1000)
  y_seq <- fn(x_seq)

  # ------------------------------------------------------------------------------
  
  p_base <- 
    dplyr::tibble(x = x_seq, y = y_seq) %>% 
    ggplot(aes(x, y)) +
    geom_line(linewidth = 1, alpha = 1 / 4) +
    labs(x = "Parameter", y = "Objective Function") +
    theme_bw()
  
  output$iterations <-
    renderPlot({
      x_cur <- input$start
      res <- dplyr::tibble(x = rep(NA, mx), y = rep(NA, mx), iter = 0:max_iter)
      res$x[1] <- x_cur
      res$y[1] <- fn(x_cur)
      
      if (input$iter >= 1) {
        for(i in 1:input$iter) {
          
          if (input$decay) {
            # rate <- cyclic(i, initial = input$rate / 2, largest = input$rate)
            rate <- decay_time(i - 1, initial = input$rate, decay = 1)
          } else {
            rate <- input$rate
          }
          
          x_new <- x_cur - rate * deriv_1(x_cur)
          y_new <- fn(x_new)
          res$x[i+1] <- x_new
          res$y[i+1] <- y_new
          x_cur <- x_new
        }
        deriv_val <- deriv_1(x_cur) # for printing
      }
      # start 3, lr 2 is good example
      p <- 
        p_base +
        geom_point(data = res %>% filter(x >= -10 & x <= 10),
                   aes(col = y), 
                   show.legend = FALSE, cex = 2, 
                   alpha = 3 / 4) +
        geom_vline(xintercept = x_cur, col = "black", lty = 3) +
        geom_rug(data = res %>% filter(x >= -10 & x <= 10), 
                 aes(x = x, y = NULL, col = y),
                 alpha = 1 / 2,
                 show.legend = FALSE) + 
        scale_color_viridis_c(
          option = "plasma",
          breaks = 0:max_iter,
          limits = extendrange(y_seq)
        )
      
      if (input$iter > 0) {
        lbl <- paste("gradient:", signif(deriv_val, digits = 3))
        if (input$decay) {
          lbl <- paste0(lbl, ", learning rate:", signif(rate, digits = 3))
        }
      } else {
        lbl <- "initial guess"
      }
      p <- p + labs(title = lbl) 
      
      print(p)
      
    }, res = 100)
}

app <- shinyApp(ui, server)
app
```

:::

An example of simple gradient descent for the function $f(\theta) = \theta\: cos(\theta/2)$.

:::

:::

::: {.column width="5%"}
:::

:::: 

The figure enables different values of the starting value, the value of $\alpha$, and how many iterations to used. Consider a few configurations:

- Starting at $\theta = 3$, a learning rate of $\alpha = 2.0$ is inappropriate. The search initially moves towards the global minimum but then reverses course _past_ the best value and becomes trapped around one of the local optima. 

- If we keep $\theta = 3$ and decrease learning rate to $\alpha = 1.0$, we quickly find the best result. 

- However, if we tried decreasing the learning rate too low, say $\alpha = 0.01$, the optimization moves too slowly. 

Clearly, the learning rate is important.  The starting value also matters; values less than 2.0 never appear to reach the optimum. 

This gradient descent algorithm outlined above is extremely basic. There are far more complex versions, the most well-known of which is the Newton-Raphson method (a.k.a. Newton’s Method) that incorporates second derivatives ($H(\theta)$) in the updating formula^[However, computing the Hessian matrix increases the computational cost by about 3-fold.]. 

We often know _when_ gradient-based optimization will work well. If we know the equation for the objective function, we can determine its properties, such as whether it is a convex function, and theory can tell us if a global optimum can be found with these tools. For example, squared error loss $f(\theta) = (y-\hat{y})^2$, is convex when  the relationship for the predicted value $\hat{y}$ is a well-behaved function of $\theta$ (such as in linear regression). 

What's not to like? There are additional considerations when it comes to optimizations for predictive models. First, since data are not deterministic, the loss function is not only a random variable but can be excessively noisy. This noise can have a detrimental effect on how well the optimization proceeds. 

Second, our objective function is usually a performance metric (such as RMSE) and not all metrics are mathematically well-behaved (or smooth or convex). In the case of deep neural networks, the model equations can lead to a non-convex optimization even when the objective function itself is simple (e.g., squared error loss).  In this case, we must worry that our optimized parameters correspond to a local optimum, not a global one. This can occur because traditional gradient methods are _greedy_; they proceed in a direction that is always optimal for the current estimates of $\theta$. This makes a lot of sense and is mostly effective. However, with complex or non-standard objective functions, the optimization can get stuck in a local optimum.

There are additional complications related to model tuning. In this case, the $\theta$ values are the tuning parameters. These might not be real numbers. For example, the number of spline terms is a whole number, and the updating equation might yield a fractional value for the number of terms. Still other parameters are qualitative in nature, such as the type of activation function in a neural network. 

During model tuning, we know not to repredict the training set (due to overfitting). Procedures such as resampling make the evaluation of $f()$ computationally expensive since multiple models should be trained. Unless we use symbolic gradient equations, the numerical process of approximating the gradient vector $g(\theta)$ can require a large number of function evaluations. 

Later, we’ll discuss *stochastic gradient descent* (SGD). This variation is often used to train very complex networks when there is a large amount of training data. SGD is a way to approximate the gradient vector using a small subset of the data at a time. While this does cause variation in the direction of descent, this variation often has the beneficial side effect of helping to escape from local minima. Also, when there are large amounts of training data, SGD can be much faster than using the entire dataset.

Until then, the next three chapters will describe different stochastic optimization methods that are well-suited for parameter tuning since they can sidestep some of the issues described above. 

## Simulated Annealing  {#sec-sim-anneal}

```{r}
#| label: sa-example-calcs
#| echo: false
load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_sa.RData"))

sa_history <- 
  sa_history %>% 
  mutate(
    log_cost = signif(log2(cost), digits = 3),
    log_scale = signif(log10(scale_factor), digits = 3),
    RMSE = sprintf("%3.2f", mean)
  )

best_iters <- sa_history$.iter[sa_history$results == "new best"]
restart_iters <- sa_history$.iter[sa_history$results == "restart from best"]
  
best_initial <- 
  sa_history %>% 
  filter(.iter == 0) %>% 
  slice_min(mean)

best_overall <- 
  sa_history %>% 
  slice_min(mean)

iter_1 <- 
  sa_history %>% 
  filter(.iter == 1) 

sa_0 <- cli::format_inline("({best_initial$log_cost}, {best_initial$log_scale})")
sa_1 <- cli::format_inline("({iter_1$log_cost}, {iter_1$log_scale})")
```

Non-greedy search methods are not constrained to always proceed in the absolute best direction (as defined by the gradient). One such method is _simulated annealing_ (SA). It is a controlled random search that moves in random directions but with some amount of control over the path. It can also incorporate restarts if the search moves into clearly poor regions.

Given a starting value, SA creates a random perturbation of the current candidate solution, often residing within some local neighborhood of the current value.  The objective function is evaluated and compared to the previous value. If the new candidate produces an improvement, the process continues. If it is not, there are two options: 

 - We can accept the current value as "suboptimal" and base our next perturbation on it, or
 - discard the current solution as if it did not occur. The next candidate point is a perturbation of the last "acceptable" point. 

For our SVM example, suppose we start with a candidate where ($log_2 (cost)$, $log_{10} (scale)$) was (-10, -0.1) and had an associated RMSE value of 6.10%. A perturbation of this values is created, say (-7.11, -0.225). Suppose that the corresponding RMSE was measured at 5.97%. Since the new candidate has a better performance metric it is automatically accepted and is used to create the next parameter. 

However, suppose that our new RMSE was 7.00%, meaning that the new candidate did worse than our initial one.  

Simulated annealing generates a probability threshold for accepting the worse solution. Most often, the probability depends on two quantities: 

* The difference in prior and current estimates of the objective function. If the new candidate is nearly as good as the current one, the probability of acceptance should be larger than that of a severely unsatisfactory candidate. 
* The probability of acceptance should decrease over search iterations. An exponentially decaying function, called a "cooling schedule," often used.

An often used equation for the probability, assuming that smaller values are better, is 

$$
Pr[accept] = \exp\Bigl[-i\bigl(f(\theta_{i}) - f(\theta_{i-1})\bigr)\Bigr]
$$

where $i$ is the iteration number. To compare 6.1% versus 7.0%, the acceptance probability is `r round(exp(-(7-6.10)) * 100, 1)`%. To make the determination, a random uniform number is generated and is used to decide. If this difference were to occur at a later iteration, say $i = 5$, the probability would drop to `r round(exp(-5*(7-6.10)) * 100, 1)`%.

One small matter is related to the scale of the objective function. The difference in the exponent is very sensitive to scale. For example, instead of percentages, if we were to use the proportions 0.61 and 0.70, the probability would change from `r round(exp(-(7-6.10)) * 100, 1)`% to `r round(exp(-(.7-.610)) * 100, 1)`%. One way to mitigate this issue is to use a normalized difference by dividing the raw difference by the previous objective function (i.e., (0.70-0.61) / 0.70). This is the approach used in the SA analyses here. 

This process is continued until either a pre-defined limit is reached or if there is no improvement by a certain number of iterations. The best result found at any time in the optimization is used as the final value; there is no real notion of "convergance" for this method. Also, as previously mentioned, a restart rule can stop simulated annealing from delving too far into suboptimal regions if no best results have occurred within some period of time. When the processes is restarted, can be continue from either the last best candidate point from previous iterations or at a random point in the parameter space. 

How should the candidates be perturbed from iteration to iteration? When the tuning parameters are all numeric, we can create a random distance and angle from the current values. A similar process can be used for integers by "flooring" them to the nearest whole number. For qualitative tuning parameters, a random subset of parameters is chosen to change to a different value chosen at random. The amount of change should be large enough to search the parameters space and potentially get out of a local optimum.

It is also important to turn the parameters in their potentially transformed space. This ensures that the entire range of the parameter values is treated equally. For example, we’ve been working with the values on their log (base 2) scale for the SVM cost parameter. When perturbing the values, we make sure to change them in the log space. This is true of the other search methods described in this chapter. 

To illustrate, a very small initial space-filling design with three candidates was generated and evaluated. The results are in @tbl-svm-initial. To start the SA search^[These values will be also used as the initial substrate for Bayesian optimization.], we will start with the candidate with the smallest RMSE and proceed for 50 iterations without a rule for early stopping. A restart to the last known best results was enforced after eight suboptimal iterations. 

:::: {.columns}

::: {.column width="25%"}
:::

::: {.column width="50%"}

::: {#tbl-svm-initial}


```{r}
#| label: tbl-svm-initial
#| echo: false
sa_history %>% 
  filter(.iter == 0) %>% 
  arrange(mean) %>% 
  select(RMSE, log_cost, log_scale) %>% 
  kable(
    digits = 1,
    col.names  = c("RMSE (%)", "Cost (log-2)", "Scale (log-10)")
  ) %>% 
  kable_styling(bootstrap_options = "responsive", full_width = FALSE)

load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_initial.RData"))
```

The initial set of candidates used for iterative search with the toy example from @fig-svm-grid. One candidate does poorly while the other two have relatively similar results. 

:::

:::

::: {.column width="25%"}
:::

:::: 
 
@fig-sa-example contains an animation of the results of the SA search. In the figure, the initial points are represented by open circles, and a grey diagonal line reminds readers that a ridge of values exists that corresponds to the best RMSE results. 

:::: {.columns}

::: {.column width="15%"}
:::

::: {.column width="70%"}

::: {#fig-sa-example}

::: {.figure-content}

```{shinylive-r}
#| label: fig-sa-example
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(purrr)
library(scales)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")

# TODO add these apps to their own R files on GH
ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  sliderInput(
    "iter",
    label = "Iteration",
    min = 1L,
    max = 50L,
    step = 1L,
    value = 1L,
    width = "100%"
  ),
  as_fill_carrier(plotOutput("path")),
  renderText("notes")
)

server <- function(input, output) {
  
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_sa.RData"))
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_large.RData"))
  
  num_cuts <- 50
  rd_or <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "OrRd")))(num_cuts)
  
  # ------------------------------------------------------------------------------
  
  x_rng <- 10^extendrange(c(-10, -1/10))
  y_rng <- 2^extendrange(c(-10, 10))
  
  log10_labs <- trans_format("log10", math_format(10^.x, function(x) format(x, digits = 3)))
  log2_labs <- trans_format("log2", math_format(2^.x, function(x) format(x, digits = 3)))

  # ------------------------------------------------------------------------------
  
  sa_history <- 
    sa_history %>% 
    mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts)))
  
  # TODO pre-compute these
  sa_init <- 
    sa_history %>% 
    filter(.iter == 0)
  best_init <- 
    sa_init %>% 
    slice_min(mean) %>% 
    select(.iter, cost, scale_factor, RMSE)
  poor_init <- 
    anti_join(sa_init, best_init, by = c(".iter", "cost", "scale_factor")) %>% 
    select(.iter, cost, scale_factor)
  
  initial_plot <- 
    regular_mtr %>%
    mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts))) %>% 
    ggplot(aes(scale_factor, cost, col = RMSE)) +
    geom_point(data = sa_init, cex = 3, pch = 1, col = "black") +
    geom_line(
      data = regular_mtr %>% slice_min(mean, n = 18),
      stat = "smooth",
      col = "black",
      method = lm,
      se = FALSE,
      formula = y ~ x,
      alpha = 1 / 8,
      linewidth = 2
    ) + 
    scale_x_log10(limits = x_rng,
                  labels = log10_labs,
                  expand = expansion(add = c(-1 / 5, -1 / 5))) +
    scale_y_continuous(limits = y_rng, trans = "log2", labels = log2_labs,
                       expand = expansion(add = c(-1/2, -1/2))) +
    scale_color_manual(values = rd_or, drop = FALSE) +
    coord_fixed(ratio = 1/2) +
    theme_bw() +
    theme(legend.position = "none")
  
  # ------------------------------------------------------------------------------
  
  output$path <-
    renderPlot({
      
      current_path <- 
        paths[[input$iter]] %>% 
        mutate(mean = RMSE,
               RMSE = cut(RMSE, breaks = seq(5, 31, length = num_cuts)))
      last_best <- 
        iter_best[[input$iter]] %>% 
        mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts)))
      
      last_best_iter <-
        last_best %>% 
        pluck(".iter")

      lab <- iter_label[[input$iter]]
      
      current_plot <- 
        initial_plot +
        geom_path(data = current_path, col = "black", alpha = 1 / 5) +
        geom_point(
          data = current_path %>% filter(.iter > 0),
          aes(scale_factor, cost, col = RMSE),
          cex = 2
        ) +
        geom_point(data = last_best, cex = 4, aes(col = RMSE), pch = 8,
                   show.legend = FALSE) +
        labs(x = "Scaling Factor", y = "Cost", title = lab) 
        
      print(current_plot)
      
    }, res = 100)
  
  output$notes <-
    renderText({
      current_path <- 
        resolve_sa_path(sa_history, input$iter) %>% 
        anti_join(poor_init, by = c(".iter", "cost", "scale_factor"))
      describe_result(current_path)
    })
}

app <- shinyApp(ui, server)
```

:::

An example of how simulated annealing can investigate the tuning parameter space. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.

:::

:::

::: {.column width="15%"}
:::

:::: 

During the search, there were `r cli::format_inline("{length(best_iters)} iteration{?s}")` where a new global best result was discovered (iterations `r cli::format_inline("{best_iters}")`). There were also `r cli::format_inline("{length(restart_iters)} restart{?s}")`  restarts at iterations `r cli::format_inline("{restart_iters}")`. In the end, the best results occurred with a cost value of 2<sup>`r best_overall$log_cost`</sup> and a scale factor of 10<sup>`r round(best_overall$log_scale, 2)`</sup>. The corresponding validation set RMSE was `r round(best_overall$mean, 2)`%. With this random seed, the search gets near the ridge of best performance but only lingers there for short times. It does spend a fair amount of time meandering in regions of poor performance. 

Simulated annealing has some attractive qualities. First, the process of creating a new candidate works for any type of parameter (e.g., real, integer, or qualitative). This isn’t the case for the two other iterative methods we’ll discuss. Also, the perturbation process is very fast; there is very little computational overhead between acquiring the next objective function value. Also, since we are effectively simulating new candidate sets, we can potentially apply constraints to parameters or groups of parameters. For example, if a tuning parameter could only be odd integers, this would not be a significant problem for simulated annealing. 

There are a few downsides to this method. Compared to the other search methods, SA makes small incremental changes. If we start far away from the optimum, many iterations might be required to reach it.  One way to mitigate this issue is to do a small space-filling design and start from the best point (as we did). In fact, applying SA search after a grid search (perhaps using racing) can be a good way to verify that the grid search was effective. 

Another disadvantage is that a single candidate is processed at a time. If the training set size is not excessive, we could parallel process the multiple candidates simultaneously. We could make a batch of perturbations and pick the best value to keep or apply the probabilistic process of accepting a poor value. 
This optimization took `r prettyunits::pretty_sec((initial_time[3] + sa_time[3]) / nrow(sa_history))` per candidate to execute. 

## Genetic Algorithms  {#sec-genetic-algo}

Genetic algorithms (GAs) are an optimization method that mimics the process of evolution via natural selection. Our application isn't well suited to GAs; a standard GA search will use hundreds to millions of objective function evaluations. We will use a small search due to practical considerations. Despite this, genetic algorithms can often find a solution near the optimal value fairly quickly and, as previously stated, there is often a region of acceptable candidates. Finally, GAs are very unlikely to become stuck in a locally optimal space. 

Instead of search iterations, genetic algorithms are counted in _generations_. A generation is a group of candidate values that are evaluated at the same time (as a batch)^[Think of the candidates within a generation as a population of people.]. Once their corresponding performance metrics are computed, a small set of the best candidates is selected and are used to create the next generation via _reproduction_. Reproduction would entail combining a pair of candidates by switching information^[Analogous to chromosomes.], and then random mutations can be applied. Once the next generation is created, the process continues for some pre-defined time limit or maximum number of generations. 

How, exactly, does this work? The first step is to pick a numerical representation for each candidate. Let’s consider methods for our main types of data. 
 
First, consider candidates that are real numbers. One option is to keep them as-is (i.e., floating point values). To use them to reproduce, we can make two children from a pair of well-performing candidates via a linear combination. For candidate vectors $\theta_j$, we can use:

$$
\begin{align}
\theta^{\:kid}_1 &= \alpha \theta^{\:par}_1 + (1 - \alpha) \theta^{\:par}_2 \notag \\
\theta^{\:kid}_2 &= (1-\alpha) \theta^{\:par}_1 + \alpha \theta^{\:par}_2 \notag \\
\end{align}
$$
where $\alpha$ is a random standard uniform number. Notice that the two children's candidate values will always be in-between the values of their parents. 

For mutation, a candidate’s values are either locally perturbed or simulated with rate $\pi_m$. For example, we might mutate the log cost value by simulating a random number across the range that defines its search space. 

In the early days of genetic algorithms, binary encodings of numbers were suggested. In this case, each candidate value is encoded as a set of binary integers. For example, consider a log<sub>2</sub> cost value of -2.34. To convert this to binary, we multiply it by 100 (assuming a limit of two decimal places) to convert it to an integer. If we use 8 binary digits (a.k.a. "bits") to represent 234, we get ``r GA::decimal2binary(234) %>% paste(collapse = "")``. If the candidates could have both positive and negative numbers, we can add an extra bit at the start that is 1 when the value is positive, yielding:  ``r GA::decimal2binary(234) %>% c(0, .) %>% paste(collapse = "")``. 

The method of _cross-over_ was often used for the reproduction of binary representations. For a single cross-over, a random location between digits was created for each candidate value, and the binary digits were swapped. For example, a representation with bits might have parents `ABCDE` and `VWXYZ.`  If they were crossed over between the second and third elements, the children would be `ABXYZ` and `VWCDE`. There are reproduction methods that use multiple cross-over points to create a more granular sharing of information. 

One reason that binary representations are no longer favored for numbers is positional bias. If the first bits (`A` and `V`) capture the sign of the value, the sign is more likely to follow the initial bits than the later bits to an offspring. 

For mutating a binary representation, each child’s bit would be flipped at a rate of $\pi_m$. 

After these reproduction and mutation, the values are decoded into real numbers. 

For integers, the same approach can be used as real numbers, but after the usual operations, the decimal values are coerced to integers via rounding. If a binary encoding is used, the same process can be used for integers as real numbers; they are all just bits to the encoding process. 

For qualitative tuning parameters, one (inelegant) approach is to encode them into values on the real line or as integers. For example,  for a parameter with values "red,", "blue," and "green", we could map them to bins of `[0, 1/3)`, `[1/3, 2/3)`, and `[2/3, 1]`. From here, we reproduce and mutate them as described above, then convert them back to their non-numeric categories. This is more palatable when there is a natural ordering of the values but is otherwise a workable but unfortunate approach. Mutation is simple though; at rate $\pi_m$, a value is flipped to a random selection of the possible values.

Now that we know how to make new candidates, we can form new generations. To configure the generations, we generally start by choosing the size. Normally, the minimum population size within a generation is at least 50 candidates. This may be infeasible depending on the computational cost of the model and the characteristics of the training set, such as size. We can use fewer points in the generation, but the risk is that we will never sample any acceptable results. In that case, combining the best results only results in more mediocre candidates. Increasing the mutation rate may help avoid this problem. For the first iteration, random sampling of the space is often used. Unsurprisingly, we strongly recommend that a space-filling design is a better tool to create the initial candidate set. 

Finally, there is the choice of which candidates to reproduce at each generation. There are myriad techniques for selecting which candidates are used to make the next generation of candidates. The simplest is to pick two parents by randomly selecting them with probabilities that are proportional to their performance (i.e., the best candidates are chosen most often). There is also the idea of _elitism_ in selection. Depending on the size of the generation, we could retain a few of the best-performing candidates from the previous generation. The performance values of these candidates would not have to be recomputed (saving time), and their information is likely to persist in a few good candidates for the next generation.


```{r}
#| label: ga-descr
#| echo: false

load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_ga.RData"))

ga_history <- ga_history %>% filter(generation <= 7)
  
best_by_gen <- 
  ga_history %>% 
  slice_min(fitness, by = c(generation))

is_same_as_gen_1 <- best_by_gen$fitness == best_by_gen$fitness[1]
same_as_gen_1 <- sum(is_same_as_gen_1) - 1

other_best <- round(unique(best_by_gen$fitness[!is_same_as_gen_1]), 3)
other_best <- paste0(other_best, "%")
```

To illustrate genetic algorithms in two dimensions, a population size of 10 was used and 5 generations were created for the toy example. These are not good default values but were instead chosen to be at parity with the previous SA search and the optimization method used in the next section. The values of the two tuning parameters were kept as floating point values. The parental selection was conducted using sampling weights proportional to the RMSE values (smaller being better), a mutation rate of 10%, and a single candidate was retained at each generation via elitism.  The computations within a generation where computed in parallel. In parallel, this optimization took `r prettyunits::pretty_sec(ga_time[3] / nrow(ga_history))` per candidate (on average).  @fig-ga-example illustrates the results of the search.   


:::: {.columns}

::: {.column width="15%"}
:::

::: {.column width="70%"}

::: {#fig-ga-example}

::: {.figure-content}

```{shinylive-r}
#| label: fig-ga-example
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(purrr)
library(scales)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  sliderInput(
    "gen",
    label = "Generation",
    min = 1L,
    max = 7L,
    step = 1L,
    value = 1L,
    width = "100%"
  ),
  as_fill_carrier(plotOutput("generations"))
)

server <- function(input, output) {
  
  num_cuts <- 50
  rd_or <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "OrRd")))(num_cuts)
  
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_ga.RData"))
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_large.RData"))

  ga_history$mean <- ga_history$fitness
  ga_history <- 
    ga_history %>% 
    mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts)))

  # ------------------------------------------------------------------------------
  
  x_rng <- 10^extendrange(c(-10, -1/10))
  y_rng <- 2^extendrange(c(-10, 10))
  
  log10_labs <- trans_format("log10", math_format(10^.x, function(x) format(x, digits = 3)))
  log2_labs <- trans_format("log2", math_format(2^.x, function(x) format(x, digits = 3)))
  
  # ------------------------------------------------------------------------------
  
  output$generations <-
    renderPlot({
      
      last_best <- 
        ga_history %>%
        filter(generation <= input$gen) %>% 
        slice_min(mean)
      
      current_gen <- ga_history %>% filter(generation == input$gen) 
      
      p <- 
        ga_history %>%
        ggplot(aes(scale_factor, cost)) +
        geom_line(
          data = regular_mtr %>% slice_min(mean, n = 18),
          stat = "smooth",
          col = "black",
          method = lm,
          se = FALSE,
          formula = y ~ x,
          alpha = 1 / 8,
          linewidth = 2
        ) +
        geom_point(data = current_gen, aes(col = RMSE), alpha = 2/4, cex = 2, 
                   show.legend = FALSE) +
        geom_point(data = last_best, cex = 4, aes(col = RMSE), pch = 8,
                   show.legend = FALSE) +
        labs(x = "Scaling Factor", y = "Cost") +
        scale_x_log10(limits = x_rng,
                      labels = log10_labs,
                      expand = expansion(add = c(-1 / 5, -1 / 5))) +
        scale_y_continuous(limits = y_rng, trans = "log2", labels = log2_labs,
                           expand = expansion(add = c(-1/2, -1/2))) +
        scale_color_manual(values = rd_or, drop = FALSE) +
        coord_fixed(ratio = 1/2) +
        theme_bw() +
        theme(legend.location = "none")
      
      print(p)
      
    }, res = 100)
}

app <- shinyApp(ui, server)
```

:::

Several generations of a genetic algorithm that search tuning parameter space. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.

:::

:::

::: {.column width="15%"}
:::

:::: 

A space-filling design was used to ensure that the initial population was diverse. Fortuitously, one design point was very close to the ridge, with an RMSE of `r round(best_by_gen$fitness[1], 2)`%. This candidate continued to be the best for `r cli::format_inline("{same_as_gen_1} generation{?s}")`.  After this, there `r cli::format_inline("{?was/were} {length(other_best)} generations with better candidate{?s}")` (with almost identical performance): `r cli::format_inline("{other_best}")`. We can see that, after three generations, the search is concentrated around the ridge of optimal performance. In later generations, some candidates have outliers in one dimension; this is the effect of mutation during reproduction. 

This small toy example is somewhat problematic for demonstrating genetic algorithms. The example’s low dimensionality and the relatively low computational expense per candidate might deceive one into believing that genetic algorithms could be the answer to most of our problems.  Genetic algorithms are versatile and powerful tools for global optimization. Like simulated annealing, there is very little overhead between generations. However, in most real applications, there are more than two parameter dimensions. This means that larger populations are required and, most likely, many more generations. If the data size is not massive, we've seen that parallel processing can help. On the whole, GAs would probably not be our first choice for tuning models.

## Bayesian Optimization {#sec-bayes-opt}

Bayesian optimization (BO) is a search procedure that uses an overarching statistical model to predict the next set of candidate values, given past performance. It is currently the most used iterative search routine for optimizing models. 

It is often tied to a particular Bayesian predictive model: the Gaussian process (GP) regression model. However, before diving into the details of that model, we can discuss what a Bayesian model is and why it is required for this optimization process. 

### What Makes a Model Bayesian?

```{r}
#| label: include-forested
#| include: false
load("../RData/forested_data.RData")
```

Let’s look at a very simple example. For the forestry data last seen in @sec-spatial-resampling, the outcome is categorical with values “yes” and “no” for the question “Is this location forested?” We might use the training data set to estimate the rate ($\pi$) that the event occurs. The simplest statistic is the sample proportion: the number of events divided by the number of total data points: ($\hat{\pi}$ = `r signif(mean(forested_train$class == "Yes"), 3)`). 

To estimate this rate using maximum likelihood estimation, we treat the data as fixed and would probably assume that the data follow a binomial distribution with theoretical probability $\pi$. From there we can solve equations that find values of $\pi$ that correspond to the largest likelihood value. It turns out that the sample proportion is also the maximum likelihood estimate. From this, the distribution is fully specified and we can compute quantities like p-values, confidence intervals etc. 

Once we solve the equations, there are single "best" values for the parameters. These are the maximum likelihood estimates and with them comes a full specification of the sampling distributions of the model parameters. With the sampling distributions, tools like hypothesis testing. standard errors, and confidence intervals can be used to help describe the data or answer specific questions.


### Why are Bayesian models good for optimization?

As previously said, the Bayesian model will predict which candidate vector should be sampled next. The outcome used in the model is our performance metric of interest. For the barley data, we’ve used RMSE as our primary metric. A Bayesian model could take a new, hypothetical candidate value and predict the expected RMSE. 

```{r}
#| label: fig-two-candidates-dist
#| include: false

set.seed(28383)
num_points <- 5000
two_cand_data <- 
  tibble(RMSE = c(rnorm(num_points, 5.75, 1/4), rnorm(num_points, 5.5, 3/4)),
         `Candidate:` = rep(paste("#", 1:2), each = num_points))

two_cand_probs <- 
  two_cand_data %>% 
  mutate(group = `Candidate:`) %>% 
  summarize(prob = round(mean(RMSE >= 6) * 100, 1), .by = c(group))
```

Since the product of a Bayesian model is a posterior (predictive) distribution, we can obtain the distribution of the performance metric for a potential new candidate. However, we often summarize this predictive distribution with the mean and standard deviation. @fig-two-candidates shows two hypothetical candidates and their respective predictive distributions. The first candidate has a slightly better predicted RMSE (on average). The area of the distribution to the right of the vertical line corresponds reflects the probability of being worse than the current solution which, for this candidate, is `r two_cand_probs$prob[two_cand_probs$group == "# 1"]`%.

```{r}
#| label: fig-two-candidates
#| echo: false
#| out-width: 55%
#| fig-width: 6
#| fig-height: 4
#| fig-cap: An example of a choice between two candidates where performance is gauged by RMSE. One candidate has slightly better predicted mean performance and another where the mean RMSE is predicted to be much lower but with high uncertainty. 

two_cand_data %>% 
  ggplot(aes(RMSE, col = `Candidate:`)) + 
  geom_line(stat = "density", adjust = 1.5, trim = TRUE, linewidth = 1) +
  geom_vline(mapping = aes(xintercept = 6), lty = 2) +
  theme(legend.position = "top") +
  annotate("text", x = 6.5, y = 1.25, label = "Current\nbest") + 
  labs(y = "Posterior Density")
```

The second candidate has a mean RMSE that is predicted to have an improvement that is  twice as large as the first but also has considerably large uncertainty. On average, this is a better choice but there is more area to the right of the vertical line. The probability of candidate two having worse results than the current best is `r two_cand_probs$prob[two_cand_probs$group == "# 2"]`%. 

Which choice is better? The first is a safer bet and the second offers higher risk and higher reward. Which candidate to pick is often driven by out state of mind. However, we should definitely consider _why_ the variance is so large for the second candidate. 

We’ve previously discussed the variance-bias tradeoff and demonstrated that large uncertainty in the performance metric can often be explained by _model variance_. Recall that models might vary due to predictive instability caused by a model that is far more complex than is required, such as the polynomial example in @sec-variance-bias. 

There is another reason that the noise in our performance statistic can be large. During tuning, we are focused on exploring the overall parameter space and, as such, we can view this process from a spatial sampling point of view. Back in @sec-spatial-splitting, we described _spatial autocorrelation_, where objects are more similar to closer objects than to objects further away. If our Bayesian model reflects spatial variability, the increased uncertainty can be attributed to how each candidate relates spatially to the existing set. If this were the case for the results in @fig-two-candidates, it implies that the first candidate is closer to the current sample of tuning parameter candidates. Conversely, the large uncertainty for the second candidate suggests that it is placed far away from  existing results. 

This example demonstrates different philosophies for a search: _exploitation_ and _extrapolation_. 

- Exploitation expresses that we would rather keep the search near existing data and capitalize on their results to find a solution closer to the things that we know. 

- Extrapolation is the notion that we should more aggressively explore new spaces to find novel candidates to improve the model. 

The value of a Bayesian model in parameter tuning is that these two concepts can be weighted differently because the Bayesian model can predict the mean and variance of performance for a potential new candidate. 

Let’s assume that our surrogate Bayesian model does significantly account for spatial effects. The next section describes the overall process of how we use it to optimize the model. 

### How does BO work?

The search routine starts with a set of $s_0$ candidates with corresponding performance statistics. Using these data, the metric is used as the model outcome and the candidate values as predictors. This surrogate model is used to make predictions of the mean and standard deviation of the metric at new candidate values. A new objective function, called an _acquisition function_, combines the mean and standard into a single value. The next candidate value to evaluate is the one that optimizes the acquisition function (and then repeat this process). 

One of the most well-known acquisition function is based on the notion of expected improvement. If the current best metric value is denoted as $\hat{Q}_{opt}$, the expected impriovement is the positive part of $\hat{\delta}(\theta) = Q_{opt} - \mu(\theta)$, where $\hat{\mu}(\theta)$ is the mean of the posterior prediction (assuming that smaller metrics are better). REFERNCE found that, probabilisticly, the expected improvement is 

$$
EI(\theta; Q_{opt}) = \hat{\delta}(\theta) \Phi\left(\frac{\hat{\delta}(\theta)}{\hat{\sigma}(\theta)}\right) + \hat{\sigma}(\theta) \phi\left(\frac{\hat{\delta}(\theta)}{\hat{\sigma}(\theta)}\right)
$$

where $\Phi(\cdot)$ is the cumulative standard normal and $\phi(\cdot)$ is the standard normal density.

One of these terms is driven by the mean prediction and the other by the standard deviation of prediction. Expected improvement is a blend of both but, at different stages of the optimization, one may dominate. Imagine a parameter space where most of the regions have been sampled to some degree. If our Bayesian model is driven by spatial variation, most of the remaining space has relatively small values for sigma and the first term will drive the optimization. Conversely, at the initial iterations, the second term will be very large. Unless there is a large mean prediction effect, the spatial aspect of the prediction will dominate (and the focus is more on extrapolation). 

Additionally, the expected improvement can be altered to include a tradeoff factor $\xi$ so that improvement is the positive part of 

$$
\hat{\delta}(\theta) = Q_{opt} - \mu(\theta) + \xi
$$
This effectively inflates the value of the current best and resulting smaller value of $\hat{\delta}(\theta)$ helps emphasize the second term EI term (leading to more extrapolation). $\xi$ could be a single constant or a function that changes the tradeoff value as a function of iterations. 

As with simulated annealing, we can keep sampling until a pre-specified limit on iterations or computational time is reached. An early stopping rule can be helpful where we prematurely end the search if a new best result is not found within a specific number of iterations. 

Let's see how this approach works in a single dimension. @fig-bo-example shows how expected improvement can be used to find the global minimum of $f(\theta) = \theta\: cos(\theta/2)$. The process starts with $s_0 = 2$ candidate points at -8.0 and 3.0. From these two points, the surrogate model is fit and the expected improvement can be computed across the range of values. Notice that the predicted line fails to capture the true trend, which is not unexpected with two data points. For our model, the predicted variance drops to zero near the existing points while the regions in between points has high variance. 

At the first iteration, two areas around $\theta = 2.0$ are advantageous. The region on the left is sampled and, once $f(\theta = 3)$ is evaluated, the next point is closer to the true global optima. The process continues and alternates between sampling near the best results and filling in regions where there are no nearby candidate values. After 15 iterations, the search is very close to the best results. 

::: {#fig-bayes-opt}

::: {.figure-content}

```{shinylive-r}
#| label: fig-bayes-opt
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(bslib)
library(ggplot2)
library(patchwork)
library(dplyr)

load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/bayesian_opt_1d.RData"))

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

theme_light_bl<- function(...) {
  
  ret <- ggplot2::theme_bw(...)
  
  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect
  
  ret$legend.position <- "top"
  
  ret
}

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  sliderInput(
    "iteration",
    label = "Iterations",
    min = 1L,
    max = 15L,
    step = 1L,
    value = 1L,
    width = "100%"
  ),
  as_fill_carrier(plotOutput("iterations"))
)


server <- function(input, output) {
  
  y_rng <- extendrange(true_curve$y)
  p_base <- 
    ggplot(true_curve, aes(x = x)) +
    geom_line(aes(y = y), alpha = 1 / 10, linewidth = 1) +
    labs(x = "Parameter", y = "Objective Function") +
    lims(y = y_rng) +
    theme_bw()
  
  output$iterations <-
    renderPlot({
      
      plot_data <- obs_dat[obs_dat$iteration < input$iteration, ]
      next_data <- obs_dat[obs_dat$iteration == input$iteration, ]
      plot_curve <- gp_pred[[input$iteration]]
      plot_curve$`Std Dev` <- plot_curve$.sd
      
      p <- 
        p_base +
        geom_line(data = plot_curve, aes(y = .mean, col = `Std Dev`), 
                  alpha = 1, linewidth = 1.2) +
        geom_point(data = plot_data, aes(y = y), cex = 2) +
        geom_vline(xintercept = next_data$x, col = "red", lty = 2) +
        scale_colour_viridis_c(option = "mako", direction = -1) +
        theme(
          plot.margin = margin(t = 0.0, r = 5.5, b = 5.5, l = 5.5, unit = "pt")
        )
      
      p_ei <- 
        ggplot(plot_curve, aes(x = x, y = objective)) + 
        geom_line() +
        geom_vline(xintercept = next_data$x, col = "red", lty = 2) + 
        lims(y = 0:1) + 
        labs(y = "Expected Imp.", x = NULL) + 
        theme_bw() +
        theme(
          axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          plot.margin = margin(t = 5.5, r = 5.5, b = 0.0, l = 5.5, unit = "pt")
        ) 
  
      
      print((p_ei / p) + plot_layout(heights = c(2, 3)))
      
    }, res = 100)
}

app <- shinyApp(ui, server)
app
```

:::

An example of Bayesian optimization for the function $f(\theta) = \theta\: cos(\theta/2)$. The top plot shows the expected improvement for the current iteration. The bottom panel shows the true curve in grey and the mean prediction line (colored by the predicted standard deviation). The vertical line indicates the location of the next candidate. 

:::

Numerous other acquisition functions can be used.  For example, instead of expected improvement, can maximize the probability of improvement:

$$
PI(\theta) = \Phi\left(\frac{\tau - \hat{\mu}(\theta)}{\hat{\sigma}(\theta)}\right) 
$$

where $\tau$ is some performance metric goal (e.g., an _R_<sup>2</sup> of 80%). This is generally deprecated in favor of EI for a few reasons. Most importantly, PI does not directly incorporate the current best value. Additionally, the value of $\tau$ is arbitrary and the search can be inconsistent across values of $\tau$ (JONES 2001).

Another acquisition function is based on confidence bounds. For minimizing the performance statistic, the lower bound would be computed:
 
$$
CB(\theta) = \hat{\mu}(\theta) - \kappa\: \hat{\sigma}(\theta)
$$

where $\kappa$ sets the confidence level and often has values that are _much_ smaller than the traditional 0.90 to 0.99 range used for statistical inference. This multiplier also modulates between extrapolation (large $\kappa$) and exploitation (small $\kappa$) for the search. COX and JOHN 1992

Apart from different acquisition functions, many other variations can be used in Bayesian optimization. For example, like simulated annealing, the default is the sample one candidate at a time. This limits the computational efficiency of the search process if we can fit models in parallel. There are alterations where multiple candidates can be _simulated_. TODO MORE HERE

Now that the general optimization process has been outlined, let's focus on the most popular Bayesian model for searchm the Gaussian process (GP) model. 

### Gaussian Process models

GPs are often motivated as special cases of stochastic processes, which are the collection of random variables that are indexed by one or more variables. The most common example of a stochastic process is the stock market. One or more stocks are random variables that, in this case, are indexed by time. A Gaussian process is just a stochastic process whose data are assumed to be multivariate normal. 

Gaussian processes are different from basic multivariable normal distributions. Let's say that we collected the height and weight of 100 students in a specific grade. If we assume height and weight to be multivariate normal, we might that the two-dimensional vector with their data, x, has a population mean vector and a 2$\times$2 covariance matrix, i.e., $\boldsymbol{x} \sim N(\boldsymbol{\mu}, \Sigma)$. In this case, the data are a sample of people at a static time point. 

However, a Gaussian process is a sequence of data, so we have to specify their mean and covariance parameters dynamically; there are mean and covariance _functions_ instead of vectors and matrices. The mean function is often denoted as  $\mu(\boldsymbol{x})$. The most common way of writing the covariance function is $\Sigma(\boldsymbol{x}, \boldsymbol{x}') = \tau^2k(\boldsymbol{x}, \boldsymbol{x}')$ where $k(\cdot, \cdot)$ is a kernel function. 

We've seen kernels before, back in @eq-kernel-poly when they were associated with support vector machine models. For Gaussian processes that define the variance between two data points from the process. Some kernels can be written as a function of the difference $\delta_{ii'} = ||\boldsymbol{x} - \boldsymbol{x}'||$. A common fuction is the squared exponential:

$$
k(\boldsymbol{x}, \boldsymbol{x}') = \exp\left(-\sigma d_{ii'}^2 \right)
$$ {#eq-kernel-sq-expo}

This function shows that as the two vectors become more different from one another (i.e., further away), the exponentiated difference becomes large, indicating high variance (apart form $\tau^2$. For two identical vectors, the variance is $\tau^2$. Now, we can see why the previous section had a strong emphasis on spatial variability for Bayesian optimization. @fig-bayes-opt used this kernel function. 

Note that this covariance function has it's own parameter $\sigma$ (similar to $a$, $b$, and $q$ in @eq-kernel-poly). 

A more general function is the Matern kernel: 

$$
k_{\nu }(\delta_{ii'})=
{\frac {2^{1-\nu }}{\Gamma (\nu )}}{\Bigg (}{\sqrt {2\nu }}{\frac {\delta_{ii'}}{\rho }}{\Bigg )}^{\nu }\mathcal{K}_{\nu }{\Bigg (}{\sqrt {2\nu }}{\frac {\delta_{ii'}}{\rho }}{\Bigg )}
$$ {#eq-kernel-matern}

where $\mathcal{K}(\cdot)$ is the Bessel function, $\Gamma(\cdot)$ is the Gamma function, $\rho$ is a tuning parameter for TODO, and $\nu$ is a tuning paramter that controls smoothness. 

Unlike the tuning parameters for the kernel functions used in SVMs, the Gaussian nature of the probability calculations allows different methods for estimating kernel functions directly; there is no need for yet another resampling procedure to optimize them. 

TODO nuggets

How does this relate to iterative tuning parameter optimization? Suppose that our multivariate random variable is the concatenation of an outcome variable and one or more predictors, our vector of random variable could be thought of as $\boldsymbol{d} = [y, x_1, ..., x_p]$. In a model, we'd like to know what the value of $y$ is for a specific set of $x$ values. Mathematically, this is $Pr[y | \boldsymbol{x}]$ and, since this collection of data is multivariate normal, there are some nice linear algebra equations that can be used to compute this. This ties into Bayesian analysis since Bayes Rule is 

$$
Pr[y | \boldsymbol{x}] = \frac{Pr[y] Pr[\boldsymbol{x} | y]}{Pr[\boldsymbol{x}]}
$$

The model is Bayesian because we have specified the distributions for the outcome and predictors ($Pr[y]$ and $Pr[\boldsymbol{x}]$, respectively) in a circuitous way. 

_In theory_, each of these is easy to compute with a simple multivariate normal. However, we don't the value of $\tau^2$ nor do we know the values of any kernel parameters used by $k(.,.)$.  

Training Gaussian process models can be difficult and the computation cost of training these models increases _cubically_ with the number of data points. In our application, this is the number of tuning parameter candidates with performance statistics. However, after the initial GP model has been trained, there are updating algorithms that can prevent the model from being completely re-estimated from scratch and thus reducing training time. 

TODO qualitative parameters

:::{.callout-note}
In summary, Gaussian process models are nonlinear models that can use tuning parameter values to make predictions on future performance statistics. They can predict the mean and variance of performance, the latter being mostly driven by spatial variability.  
:::

In @fig-bayes-opt we seen how well Bayesian optimization works for a simple one-dimensional function. Let’s apply it in context by using it to optimize our two SVM tuning parameters. 

### A Two Dimensional Illustration

```{r}
#| label: bo-example-calcs
#| echo: false
load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_bo.RData"))

bo_best <- numeric(0)

rmse_best <- min(bo_mtr$mean[bo_mtr$.iter == 0])
for (i in 1:max(bo_mtr$.iter)) {
  new_rmse <- bo_mtr$mean[bo_mtr$.iter == i]
  if (new_rmse < min(rmse_best)) {
    bo_best <- c(bo_best, i)
    rmse_best <- c(rmse_best, new_rmse)
  }
}


bo_best_chr <- cli::format_inline("iterations {bo_best}")
rmse_best_chr <- cli::format_inline("{paste0(signif(rmse_best, 4), '%')}")
```

Returning the the SVM example, we initialize the GP model using the same three points shown in @tbl-svm-initial. We again use the squared exponential kernel in @eq-kernel-sq-expo to measure variability. 

@fig-bo-example shows the results that help us select the first new candidate value. Due to the small number of points used to fit the GP, the EI surface is virtually flat. The flatness of the acquisition function results in a fairly uninformative choice: the first point is very close to the best candidate in the initial set.

Once this new point is sampled, the surrogate GP model produces a clear trend in EI indicating a strong preference for higher cost values (and no effect of the scale parameter). The selected candidate is closer to the ridge of optimal performance. After acquiring this point, EI is focused on the scale parameter value and ignores the cost parameter. By iteration 13, a candidate is chosen that is nearly optimal. 

:::: {.columns}

::: {.column width="15%"}
:::

::: {.column width="70%"}

::: {#fig-bo-example}

::: {.figure-content}

```{shinylive-r}
#| label: fig-bo-example
#| viewerHeight: 630
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(purrr)
library(scales)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  sliderInput(
    "iter",
    label = "Iteration",
    min = 1L,
    max = 50L,
    step = 1L,
    value = 1L,
    width = "100%"
  ),
  radioButtons(
    inputId = "objective",
    label = "Objective Function",
    inline = TRUE,
    choices = list("Exp. Improivement" = "objective", "Mean" = ".mean", 
                   "Std. Dev" = ".sd")
  ),
  as_fill_carrier(plotOutput("path"))
)

server <- function(input, output) {
  
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_bo.RData"))
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_large.RData"))
  
  # ------------------------------------------------------------------------------
  
  x_rng <- 10^extendrange(c(-10, -1/10))
  y_rng <- 2^extendrange(c(-10, 10))
  
  log10_labs <- trans_format("log10", math_format(10^.x, function(x) format(x, digits = 3)))
  log2_labs <- trans_format("log2", math_format(2^.x, function(x) format(x, digits = 3)))
  
  # ------------------------------------------------------------------------------
  
  best_init <- 
    bo_mtr %>% 
    filter(.iter == 0) %>% 
    slice_min(mean) %>% 
    select(cost, scale_factor, .iter)
  
  initial_plot <- 
    regular_mtr %>%
    ggplot(aes(scale_factor, cost)) +
    geom_point(data = init_grid, cex = 3, pch = 1) +
    scale_x_log10(limits = x_rng,
                  labels = log10_labs,
                  expand = expansion(add = c(-1 / 5, -1 / 5))) +
    scale_y_continuous(limits = y_rng, trans = "log2", labels = log2_labs,
                       expand = expansion(add = c(-1/2, -1/2))) +
    theme_bw() +
    labs(x = "Scaling Factor", y = "Cost") +
    coord_fixed(ratio = 1/2)
  
  bo_tile <- tidyr::pivot_longer(
    bo_tile,
    cols = c(.mean, .sd, objective),
    names_to = "metric",
    values_to = "value"
  )
  
  # ------------------------------------------------------------------------------
  
  output$path <-
    renderPlot({
      
      tile <- bo_tile %>% filter(.iter == input$iter & metric == input$objective)
      last_best <- bo_mtr %>% filter(.iter < input$iter) %>% slice_min(mean)
      path <- 
        bo_mtr %>% 
        filter(.iter <= input$iter & .iter > 0) %>% 
        select(cost, scale_factor, .iter) %>% 
        bind_rows(best_init) %>% 
        arrange(.iter)

      # TODO convert to discrete colors via cut
      rc_pal <- switch(
        input$objective,
        "objective" = "Greens",
        ".mean" = "OrRd",
        ".sd" = "Blues"
      )
      
      hist_alpha <- 
        case_when(
          input$iter < 5 ~ 1/2, 
          input$iter >= 5 & input$iter < 10 ~ 1/3,  
          input$iter >= 10 & input$iter < 20 ~ 1/4, 
          input$iter >= 20 & input$iter < 30 ~ 1/5, 
          TRUE ~ 1 / 10)
      hist_alpha <- 2 * hist_alpha
      
      col_dir <- if_else(input$objective == ".mean", -1, 1)
      col_tr <- "identity"

      current_plot <- 
        initial_plot + 
        geom_tile(data = tile, aes(fill = value), alpha = 1 / 2) +
        geom_line(
          data = regular_mtr %>% slice_min(mean, n = 18),
          stat = "smooth",
          col = "black",
          method = lm,
          se = FALSE,
          formula = y ~ x,
          alpha = 1 / 4,
          linewidth = 2
        ) +
        geom_path(data = path, col = "black", alpha = hist_alpha, lty = 3) +
        geom_point(data = new_bo_points %>% filter(iter == input$iter), 
                   col = "black", pch = 16, cex = 4) +
        geom_point(data = last_best, cex = 4, col = "black", pch = 8) + 
        scale_fill_distiller(palette = rc_pal, type = "seq", 
                             direction = col_dir, transform = col_tr) +
        theme(legend.title = element_blank(), legend.text = element_blank())
      print(current_plot)
      
    }, res = 100)

}

app <- shinyApp(ui, server)
```

:::

An illustration of Bayesian optimization. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.

:::

:::

::: {.column width="15%"}
:::

:::: 


The complete search produced new optimal values at iterations `r bo_best_chr`. The decrease in RMSE shows few meaningful improvements among negligible decreases: `r rmse_best_chr`. 

The cost of the Gaussian process fit does become more expensive as new candidates are sampled, making the process somewhat less efficient than the previous approaches. The time to compute each candidate was `r prettyunits::pretty_sec((initial_time[3] + bo_time[3])/nrow(bo_mtr))`.

Nonetheless, the search process was able to effectively tune the SVM model and find very good parameters. 

### Advantages and Disadvantages

SFD does a pretty good job but does measure the extremes of the parameter space; if expensive, use more efficient parameters and let BO sample where it thinks that it should. 

one at a time but there are ways of expanding that. 


## Example: Tuning a Neural Network {#sec-bayes-opt-nnet}

We’ll return to the data previously seen in @sec-barley where we want to predict the percentage of barley oil in a mixture. Recall that the predictors are extremely correlated with one another. In @sec-embeddings, we considered embedding methods like PCA to preprocess the data. 


### Savitzky-Golay Signal Processing {#sec-savitzky-golay}

In this chapter, we’ll take a more sophisticated approach to preprocessing these data using signal processing techniques, specifically the **Savitzky-Golay** method. This technique incorporates two main goals: to smooth the data from location to location (i.e., across the x-axis in @fig-barley-data(b)) and to difference the data. Smoothing attempts to reduce measurement noise without diminishing the data patterns that could predict the outcome. Differencing can remove the correlation between locations and might also mitigate other issues, such as systematic background noise. 

For smoothing, the procedure uses a moving window across a sample’s location and, within each window, uses a polynomial basis expansion to produce a smoother representation of the data. The window width and the polynomial degree are both tuning parameters. Note that the polynomial degree cannot be larger than the window width. 

Differencing has a single tuning parameter: the difference order. A zero-order difference leaves the data as-is (After smoothing). A first-order difference is the smoothed value minus the smoothed value for the next location (and so on). 

@fig-savitzky-golay shows an example of a small set of locations. The top row shows the effect of smoothing when differencing is not used. For a linear smoother (i.e., a polynomial degree of one), the process can over-smooth the data such that the nonlinear portions of the raw data lose significant amounts of information. This issue goes away as the polynomial order is increased. However, the higher degree polynomial nearly interpolates between data points and may not be smoothing the data. 

The bottom panel shows the data when a first-order difference is used. The pattern is completely different since the data now represent the rate of change in the outcome rather than the absolute measurement values. Again, larger window sizes oversmooth the data, and the highest degree of polynomial provides minimal smoothing. 

We’ll want to optimize these preprocessing parameters simultaneously with the parameters for the supervised model. 

```{r}
#| label: fig-savitzky-golay
#| echo: false
#| fig-width: 8.5
#| fig-height: 6
#| out-width: 100%
#| warning: false
#| fig-cap: "Demonstrations of the Savitzky-Golay preprocessing method for a small region for one specific sample."

load("../RData/savitzky_golay_demo.RData")

smoothed_p <-
  smoothed_plot_data %>%
  mutate(.location = .location + (`window size`/2)) %>%
  ggplot(aes(.location, .measure)) +
  geom_point(data = raw_plot_data, cex = 1/2) +
  geom_path(aes(col = window), alpha = 3 / 4) +
  facet_wrap(~ `polynomial degree`, labeller = label_both) +
  lims(y = c(0.595, 0.606), x = c(505, 544)) +
  labs(y = "Original Value", x = NULL) +
  theme(legend.position = "top")

diff_p <- diff_plot_data %>%
  mutate(.location = .location + (`window size`/2)) %>%
  ggplot(aes(.location, .measure)) +
  geom_hline(yintercept = 0, lty = 2, alpha = 1 / 2) +
  geom_point(data = raw_diff_plot_data, cex = 1/2) +
  geom_path(aes(col = window), alpha = 3 / 4) +
  facet_wrap(~ `polynomial degree`, labeller = label_both) +
  lims(y = c(-0.0012, 0.0014), x = c(505, 544)) +
  labs(y = "1st Difference", x = "Wavelength (nm)") +
  theme(legend.position = "none")

smoothed_p / diff_p
```

### Initial Candidate Set

```{r}
#| label: collect-iterative
#| include: false
load("../RData/barley_iterative.RData")

iters <- max(mlp_sfd_bo_ci$.iter)
rmse_rng <- extendrange(c(mlp_sfd_bo_ci$.lower, mlp_sfd_bo_ci$.upper, 
                          mlp_sfd_sa_ci$.lower, mlp_sfd_sa_ci$.upper))

mlp_sfd_bo_best_ci <- 
  mlp_sfd_bo_ci %>% 
  filter(.iter == max(mlp_sfd_bo_best))

mlp_sfd_bo_ci$is_best <- "no"
mlp_sfd_bo_ci$is_best[mlp_sfd_bo_ci$.iter %in% mlp_sfd_bo_best] <- "yes"

mlp_sfd_sa_best_ci <- 
  mlp_sfd_sa_ci %>% 
  filter(.iter == max(mlp_sfd_sa_best))

mlp_sfd_sa_ci$is_best <- "no"
mlp_sfd_sa_ci$is_best[mlp_sfd_sa_ci$.iter %in% mlp_sfd_sa_best] <- "yes"

mthds <- c("Simulated Annealing", "Bayesian Optimization")
restarts <- result_history$.iter[result_history$results == "restart from best"]
restarts <- tibble(Method = factor("Simulated Annealing", levels = mthds), 
                   .iter = restarts)
```

::: {#tbl-nnet-initial}


```{r}
#| label: tbl-nnet-initial
#| echo: false
mlp_sfd_bo_ci %>% 
  filter(.iter == 0) %>% 
  mutate(
    `window size` = 2 * window_side + 1,
    mixture = mixture * 100,
    `penalty (log-10)` = log10(penalty),
    `learn rate (log-10)` = log10(learn_rate),
    activation = gsub("_", " ", activation),
    rate_schedule = gsub("(decay_time)|(decay_expo)", "decay", rate_schedule)
    ) %>% 
  arrange(.estimate) %>% 
  select(`RMSE (%)` = .estimate, `window size`, differentiation_order, degree,
         hidden_units, `penalty (log-10)`, activation, `learn rate (log-10)`,
         rate_schedule, stop_iter, mixture) %>% 
  kable(
    digits = 2,
    col.names  = c(
     "RMSE (%)", "Window", "Diff. Order", "Degree", "Units", "Penalty (log-10)", 
     "Activation", "Learn Rate (log-10)", "Rate Schedule", "Stop Iter.", 
     "L1 Mixture"
    )
  ) %>% 
  kable_styling(bootstrap_options = "responsive", full_width = FALSE)
```

The initial set of candidates used for iterative search methods.
 
:::

### Iterative Search


```{r}
#| label: fig-nnet-sg-iterations
#| echo: false
#| fig-width: 6
#| fig-height: 5
#| out-width: 70%
#| fig-cap: "Validation set RMSE results for the optimization. The validation set RMSE is shown on the y-axis with 90% confidence intervals. Orange points indicate iterations where a new optimal value was found and vertical lines indicate where the algorithm restarted from the previous best candidate."
#| warning: false


mlp_sfd_sa_ci %>% 
  mutate(Method = "Simulated Annealing") %>% 
  bind_rows(
    mlp_sfd_bo_ci %>% mutate(Method = "Bayesian Optimization")
  ) %>% 
  mutate(Method = factor(Method, levels = mthds)) %>% 
  filter(.iter > 0) %>% 
  ggplot(aes(.iter, col = is_best)) + 
  geom_point(aes(y = .estimate), show.legend = FALSE, cex = 1 / 2) +
  geom_errorbar(
    aes(ymin = .lower, ymax = .upper), 
    width = 1/2, 
    show.legend = FALSE
  ) +
  geom_vline(data = restarts, aes(xintercept = .iter), col = "darkred", lty = 3) +
  facet_wrap(~ Method, ncol = 1) +
  labs(x = "Iteration", y = "RMSE") +
  scale_color_manual(values = c("black", "#FF7F00"))

```

On average, simulated annealing took `r prettyunits::pretty_sec(mlp_sa_time[3] / iters)`  for each candidate and BO used `r prettyunits::pretty_sec(mlp_bo_time[3] / iters)` (a `r round(mlp_bo_time[3]/mlp_sa_time[3], 1)`-fold difference). 


Simulated annealing arrived at the following candidate set (at iteration `r mlp_sfd_sa_best`): 


The corresponding validation set RMSE was `r round(mlp_sfd_sa_best_ci$.estimate, 2)`% with 90% confidence interval of (`r round(mlp_sfd_sa_best_ci$.lower, 2)`%, `r round(mlp_sfd_sa_best_ci$.upper, 2)`%).


:::: {.columns}

::: {.column width="25%"}
:::

::: {.column width="50%"}

::: {#tbl-nnet-sg-best}


```{r}
#| label: tbl-nnet-sg-best
#| echo: false
iter_rows <- 
  c("Iterations", "Differentiation Order", "Polynomial Degree", "Window Size",
    "Hidden Units", "Activation", "Penalty (log10)", "L1 Proportion", 
    "Learning Rate (log10)", "Rate Schedule", "Stopping Rule", "RMSE", 
    "Conf. Int."
    )

bind_rows(mlp_sfd_sa_best_ci, mlp_sfd_bo_best_ci) %>% 
  mutate(
    `Penalty (log10)` = log10(penalty),
    `Learning Rate (log10)` = log10(learn_rate),
    `Conf. Int.` = map2_chr(.lower, .upper, 
                       ~ cli::format_inline("({round(.x, 2)}%, {round(.y, 2)}%)")),
    RMSE = map_chr(.estimate, ~ cli::format_inline("{round(.x, 2)}%")),
    `Window Size` = 1 + 2 * window_side,
    across(where(is.numeric), ~ format(.x, digits = 3))
  ) %>% 
  select(-.metric, -.estimator, -.config, -penalty, -learn_rate, 
         -.lower, -.upper, -.estimate, -window_side) %>% 
  rename(Iterations = .iter, `Hidden Units` = hidden_units, Activation = activation,
         `Stopping Rule` = stop_iter, `L1 Proportion` = mixture, 
         `Rate Schedule` = rate_schedule, `Polynomial Degree` = degree,
         `Differentiation Order` = differentiation_order) %>% 
  pivot_longer(cols = c(-method), names_to = "Parameter", values_to = "Value") %>% 
  pivot_wider(id_cols = c(Parameter), names_from = "method", values_from = "Value") %>% 
  mutate(
    Parameter = factor(Parameter, levels = iter_rows)
  ) %>% 
  arrange(Parameter) %>% 
  kable(
    col.names  = c(" ", "Simulated Annealing", "Bayesian Optimzation")
  ) %>% 
  kable_styling(bootstrap_options = "responsive", full_width = FALSE)

```

Results of the two iterative searches for a neural network using Savitzky-Golay preprocessing. 

:::

:::

::: {.column width="25%"}
:::

:::: 


## Chapter References {.unnumbered}

