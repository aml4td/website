---
knitr:
  opts_chunk:
    cache.path: "../_cache/iterative/"
---

# Iterative Search {#sec-iterative-search}

```{r}
#| label: iterative-setup
#| include: false
source("../R/_common.R")

# ------------------------------------------------------------------------------

library(kableExtra)
library(tidymodels)
library(finetune)
library(patchwork)
library(future)
library(bestNormalize)
library(kableExtra)

# ------------------------------------------------------------------------------
# Set Options

plan("multisession")
tidymodels_prefer()
theme_set(theme_transparent())
set_options()

num_workers <- parallel::detectCores()
```

Grid search is a static procedure; we predetermine the candidates to be evaluated before beginning. How can we adaptively optimize tuning parameters in a sequential manner? Perhaps more importantly, _when_ is this a good approach? 

We’ve seen previously that a plateau of good performance in the parameter space is possible. This is often the case but will not always be true. If the optimal performance range is small, we must create a large space-filling design to find it. When there are many tuning parameters, the computational expense can be unreasonable. An even worse situation is one where the previously described speed-up techniques (such as the submodel trick from @sec-submodels) are not applicable. Racing can be very efficient, but if the training set is very large, it may be that a validation set is more appropriate than multiple resamples; racing cannot be used in this situation. Finally, there are practical considerations when the data set is very large. Most parallel processing techniques require the data to be in memory for each parallel worker, rendering that technique moot. 

Generally, we are not often constrained by these issues _for models used for tabular data_. However, there are a few models that might be better optimized sequentially. The first two that come to mind are large neural networks^[Sections [-@sec-cls-svm] and [-@sec-reg-svm]]. and support vector machines^[Sections [-@sec-cls-nnet] and [-@sec-reg-nnet]] (SVMs). For the former, as the number of layers increases, so does the number of tuning parameters. Neural networks also have many important tuning parameters associated with the model’s training, such as the learning rate, regularization parameters, etc. These models also require more preprocessing than others; they are suspectable to uninformative predictors, missing data, and collinearity. Depending on the data set, there can be many pre-model tuning parameters. 

Support vector machines are models with fewer parameters than neural networks but with similar pre-processing requirements. Unfortunately, the tuning parameter space can have large areas of poor performance with "islands" where the model works well. The location of these can change from data set to data set. We’ll see an example of this shortly in a small, two-parameter example. 

These two models, in particular, are more likely to benefit from an optimization method that chooses candidates as the process evolves. 

In theory, any iterative optimization procedures can be used. In general, gradients, such as steepest descent or Newton’s method, are the most commonly used technique for nonlinear optimization. These tools are suboptimal when it comes to parameter tuning but play important parts in other areas of machine learning and will be discussed later. For that reason, let’s take a segue and discuss them. 


Derivative-free techniques can be helpful here, and there are many for nonlinear optimizations. Traditional examples are genetic algorithms, simulated annealing, particle swarm optimization, etc. We’ll consider the first two of these in Sections [-@sec-sim-anneal] and [-@sec-genetic-algo]. Currently, the most well-known iterative tool for optimizing models is Bayesian optimization. This will be examined in @sec-bayes-opt. 

## Example: Predicting Barley Amounts using Support Vector Machines {#sec-barley-svm}

```{r}
#| label: data-import
#| include: false
source("../R/setup_chemometrics.R")
```

We’ll return to the data previously seen in @sec-barley where we want to predict the percentage of barley oil in a mixture. Recall that the predictors are extremely correlated with one another. In @sec-embeddings, we considered embedding methods like PCA to preprocess the data and these models were able to achieve RMSE values of about 6% (shown in @fig-barley-linear-bakeoff). 

In this chapter, we’ll predict these data in two different scenarios. First, we’ll use them as a “toy problem" where only two support vector machine tuning parameters are optimized. This is a little unrealistic, but it allows us to observe how iterative search methods work in a 2D space. Second, in @sec-bayes-opt-nnet, we’ll get serious and optimize a larger group of parameters for neural networks simultaneously with three parameters of a specialized signal processing technique. 

Let's start with the toy example that uses an SVM regression model. This highly flexible nonlinear model can represent the predictor data in higher dimensions using a _kernel transformation_. This function combines numeric predictor vectors from two data points using a dot product. There are many different types of kernel functions, but for a _polynomial_ kernel, it is: 

$$
k(\boldsymbol{x}_1, \boldsymbol{x}_2) = (a\boldsymbol{x}_1'\boldsymbol{x}_2 + b)^p
$$ {#eq-kernel-poly}

where $a$ is called the scaling factor, $b$ is a constant offset value, and $p$ is the polynomial degree. The dot product measures both angle and distance. Note that for the kernel function to be valid, the two vectors must have elements with consistent units (i.e., they have been standardized). 

The kernel function operates much like a polynomial basis expansion; it projects a data point into a much larger, nonlinear space. The idea is that a more complex representation of the predictors might enable the model to make better predictions. 

For our toy example, we'll take the `r ncol(barley_train) - 1` predictors, project them to a smaller space of 10 principal components, then standardize those features to have the same mean and standard deviation. A quartic polynomial is used with zero offset. The scale parameter will be tuned. It helps define how much influence the dot product has in the polynomial expansion. 

The most commonly tuned SVM parameter is independent of the different kernel functions: the _cost value_. This dictates how much a poorly predicted model should penalize the performance metric (when evaluated on the training set). Higher cost values will encourage the SVM to become increasingly more complex. We've previously seen the effect of modulating the cost value in @fig-two-class-overfit. For small costs, the SVM classification model did not try too hard to classify samples correctly. When increased to an extreme, it significantly overfit the training set points.

As before, the RMSE will be used with the validation set for external validation of models. Let's define a wide  space for our two tuning parameters: cost will vary from 2<sup>-10</sup> to 2<sup>10</sup> and the scale factor^[Why are the two parameters based on different bases? It’s mostly a convention. Both parameters have valid values that can range across many orders of magnitude. The change in cost tends to occur more slowly than the scaling factor, so a smaller base of 2 is often used.] is allowed to range from 10<sup>-10</sup> to 10<sup>-0.1</sup>. @fig-svm-grid visualizes the RMSE across these ranges^[These are not simulated data, so this surface is an approximation of the true RMSE via the validation set using a very large regular grid. The RMSE results are estimates and would change if we used different random numbers for data splitting.]. The lower left diagonal area is a virtual "dead zone" with very large RMSE values that don't appear to change much. There is also a diagonal wedge of good performance. The figure shows the location of the smallest RMSE value and a diagonal ridge of values with nearly equal results. Any points on this line would produce good models (at least for this toy example). Note that there is a small locale of excessively large RMSE values in the upper right. Going too far in this direction is a bad idea.  

```{r}
#| label: svm-spec
#| include: false
rec <-
  recipe(barley ~ ., data = barley_train) %>%
  step_orderNorm(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = tune()) %>%
  step_normalize(all_predictors())

svm_spec <-
  svm_poly(cost = tune(), degree = tune(), scale_factor = tune(), margin = tune()) %>%
  set_mode("regression")

svm_wflow <- workflow(rec, svm_spec)

svm_param <-
  svm_wflow %>%
  extract_parameter_set_dials() %>%
  update(
    num_comp = num_comp(c(1, 200)),
    cost = cost(c(-10, 10)),
    degree = degree_int(c(1, 8)),
    scale_factor = scale_factor(c(-10, -1/10))
  )

reg_mtr <- metric_set(rmse)
```

```{r}
#| label: fig-svm-grid
#| echo: false
#| fig-width: 5
#| fig-height: 5
#| out-width: 50%
#| fig-cap: "A visualization of model performance when only the SVM cost and scaling factor parameters are optimized. It shows the combination with the smallest RMSE and a ridge of candidate values with nearly equivalent performance."

load("../RData/two_param_iter_large.RData")
x_rng <- 10^extendrange(c(-10, -1/10))
y_rng <- 2^extendrange(c(-10, 10))

log10_labs <- trans_format("log10", math_format(10^.x, function(x) format(x, digits = 3)))
log2_labs <- trans_format("log2", math_format(2^.x, function(x) format(x, digits = 3)))

num_cuts <- 50
rd_or <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "OrRd")))(num_cuts)

regular_mtr %>%
  mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts))) %>% 
  ggplot(aes(scale_factor, cost)) +
  geom_tile(aes(fill = RMSE)) +
  geom_point(data = regular_mtr %>% slice_min(mean), col = "yellow", cex = 2) + 
  geom_line(
    data = regular_mtr %>% slice_min(mean, n = 18),
    stat = "smooth",
    col = "yellow",
    method = lm,
    se = FALSE,
    formula = y ~ x,
    alpha = 1
  ) +
  scale_x_log10(limits = x_rng,
                labels = log10_labs,
                expand = expansion(add = c(-1 / 5, -1 / 5))) +
  scale_y_continuous(limits = y_rng, trans = "log2", labels = log2_labs,
                     expand = expansion(add = c(-1/2, -1/2))) +
  scale_fill_manual(values = rd_or) +  
  labs(x = "Scaling Factor", y = "Cost") +
  coord_fixed(ratio = 1/2) +
  theme_bw() +
  theme(legend.position = "none")
```

The next two sections will discuss how two traditional global search methods, simulated annealing and genetic algorithms, can be used to search the parameter space. After this, the focus is on Bayesian optimization. 


## Sidebar: Gradient-Based Optimization

To formalize the idea of optimization, we have some objective functions that we are trying to optimize. This can also be called a loss function (specifically to be minimized). In either case, this value is signified by $f()$. The parameters that modify $f()$ are denoted as $\theta$, which is $p \times 1$ in dimension and is assumed to be real numbers. We'll also assume that $f()$ is smooth and generally differentiable and, without losing generality, assume that smaller values are better. 

For derivatives, we’ll denote the first derivative ($f'(\theta)$), for simplicity, as $g(\theta)$.

If we start as some point $\theta_0$, we compute the gradient at that point and use it as a $p$ dimensional directional vector. To get to our next parameter value, basic gradient descent uses the update 

$$
\theta_{i+1} = \theta_i - \alpha\:g(\theta_i)
$$

The value $\alpha$ defines how far we should travel in our new direction. This can be a constant value or a secondary procedure could be used called a line search that increases the value of $\alpha$ until the objective function worsens^[We’ve seen $\alpha$ before when it was called the learning rate (and will revisit it later in this chapter).].

We proceed to iterate this process until some measure of convergence is achieved. For example, the optimization could be halted if the objective function does not improve more than a very small value. 


:::: {.columns}

::: {.column width="5%"}
:::

::: {.column width="90%"}

::: {#fig-grad-descent}

::: {.figure-content}

```{shinylive-r}
#| label: fig-grad-descent
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)

# source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

theme_light_bl<- function(...) {
  
  ret <- ggplot2::theme_bw(...)
  
  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect
  
  ret$legend.position <- "top"
  
  ret
}

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(xs = c(3, 3, 3, 3), sm = 4),
    column(
      width = 12,
      sliderInput(
        "start",
        label = "Starting Value",
        min = -6,
        max = 10,
        value = 3,
        step = 1
      )
    ),
    column(
      width = 12,
      numericInput(
        "rate",
        label = "Leaning Rate",
        min = 0,
        max = 3,
        value = 2,
        step = 0.1
      ),
      checkboxInput(
        "decay",
        label = "Decay?",
        value = FALSE
      )
    ),
    column(
      width = 12,
      sliderInput(
        "iter",
        label = "Iteration",
        min = 0L,
        max = 50L,
        step = 1L,
        value = 0L,
        width = "100%"
      )
    )
  ),
  as_fill_carrier(plotOutput("iterations"))
)

server <- function(input, output) {
  library(ggplot2)
  library(dplyr)
  
  cyclic <- function(epoch, initial = 0.001, largest = 0.1, step_size = 5) {
    if (largest < initial) {
      tmp <- initial
      largest <- initial
      initial <- tmp
    } else if (largest == initial) {
      initial <- initial / 10
    }
    cycle <- floor( 1 + (epoch / 2 / step_size) )
    x <- abs( ( epoch / step_size ) - ( 2 * cycle) + 1 )
    initial + ( largest - initial ) * max( 0, 1 - x)
  }
  decay_time <- function(epoch, initial = 0.1, decay = 1) {
    initial / (1 + decay * epoch)
  }  
  
  fn <- function(x) x * cos(.5 * x) 
  # D(quote(x * cos(.5 * x)), "x")
  deriv_1 <- function(x) cos(0.5 * x) - x * (sin(0.5 * x) * 0.5)
  
  # ------------------------------------------------------------------------------

  max_iter <- 50
  mx <- max_iter + 1
  
  x_seq <- seq(-10, 10, length.out = 1000)
  y_seq <- fn(x_seq)

  # ------------------------------------------------------------------------------
  
  p_base <- 
    dplyr::tibble(x = x_seq, y = y_seq) %>% 
    ggplot(aes(x, y)) +
    geom_line(linewidth = 1, alpha = 1 / 4) +
    labs(x = "Parameter", y = "Objective Function") +
    theme_bw()
  
  output$iterations <-
    renderPlot({
      x_cur <- input$start
      res <- dplyr::tibble(x = rep(NA, mx), y = rep(NA, mx), iter = 0:max_iter)
      res$x[1] <- x_cur
      res$y[1] <- fn(x_cur)
      
      if (input$iter >= 1) {
        for(i in 1:input$iter) {
          
          if (input$decay) {
            # rate <- cyclic(i, initial = input$rate / 2, largest = input$rate)
            rate <- decay_time(i - 1, initial = input$rate, decay = 1)
          } else {
            rate <- input$rate
          }
          
          x_new <- x_cur - rate * deriv_1(x_cur)
          y_new <- fn(x_new)
          res$x[i+1] <- x_new
          res$y[i+1] <- y_new
          x_cur <- x_new
        }
        deriv_val <- deriv_1(x_cur) # for printing
      }
      # start 3, lr 2 is good example
      p <- 
        p_base +
        geom_point(data = res %>% filter(x >= -10 & x <= 10),
                   aes(col = y), 
                   show.legend = FALSE, cex = 2, 
                   alpha = 3 / 4) +
        geom_vline(xintercept = x_cur, col = "black", lty = 3) +
        geom_rug(data = res %>% filter(x >= -10 & x <= 10), 
                 aes(x = x, y = NULL, col = y),
                 alpha = 1 / 2,
                 show.legend = FALSE) + 
        scale_color_viridis_c(
          option = "plasma",
          breaks = 0:max_iter,
          limits = extendrange(y_seq)
        )
      
      if (input$iter > 0) {
        lbl <- paste("gradient:", signif(deriv_val, digits = 3))
        if (input$decay) {
          lbl <- paste0(lbl, ", learning rate:", signif(rate, digits = 3))
        }
      } else {
        lbl <- "initial guess"
      }
      p <- p + labs(title = lbl) 
      
      print(p)
      
    }, res = 100)
}

app <- shinyApp(ui, server)
app
```

:::

An example of simple gradient descent for the function $f(x) = x\: cos(x/2)$.

:::

:::

::: {.column width="5%"}
:::

:::: 




This gradient descent algorithm is extremely basic. There are far more complex versions, the most well-known of which is the Newton-Raphson method (a.k.a. Newton’s Method) that incorporates second derivatives in the updating formula. 

We often know when gradient-based optimization will work well. If we know the equation for the objective function, we can determine its properties, such as whether it is a convex function, and theory can tell us if a global optimum can be found with these tools. For example, squared error loss $f(\theta) = (y-\hat{y})^2$, is convex when  the relationship for the predicted value $\hat{y}$ is a well-behaved function of estimates $\hat{\theta}$ (such as in linear regression). 

What's not to like? There are additional considerations when it comes to optimizations for predictive models. First, since data are not deterministic, the loss function is not only a random variable but can be excessively noisy. This noise can have a detrimental effect on how well the optimization proceeds. 

Second, our objective function is usually a performance metric (such as RMSE) and not all metrics are mathematically well-behaved (or smooth or convex). In the case of deep neural networks, the model equations can lead to a non-convex optimization even when the objective function itself is simple (e.g., squared error loss).  In this case, we must worry that our optimized parameters correspond to a local optimum, not a global one. This can occur because traditional gradient methods are _greedy_; they proceed in a direction that is always optimal for the current estimates of $\theta$. This makes a lot of sense and is mostly effective. However, with complex or non-standard objective functions, the optimization can get stuck in a local optimum.

There are additional complications related to model tuning. In this case, the $\theta$ values are the tuning parameters. These might not be real numbers. For example, the number of spline terms is a whole number, and the updating equation might yield a fractional value for the number of terms. Still other parameters are qualitative in nature, such as the type of activation function in a neural network. 

During model tuning, we know not to repredict the training set (due to overfitting). Procedures such as resampling make the evaluation of $f()$ computationally expensive since multiple models should be trained. Unless we use symbolic gradient equations, the numerical process of approximating the gradient vector $g(\theta)$ can require a large number of function evaluations. 

Later, we’ll discuss stochastic gradient descent. This variation is often used to train very complex networks when there is a large amount of training data. 

Until then, the next three chapters will describe different stochastic optimization methods that are well-suited for parameter tuning since they can sidestep some of the issues described above. 

## Simulated Annealing  {#sec-sim-anneal}

```{r}
#| label: sa-example-calcs
#| echo: false
load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_sa.RData"))

sa_history <- 
  sa_history %>% 
  mutate(
    log_cost = signif(log2(cost), digits = 3),
    log_scale = signif(log10(scale_factor), digits = 3),
    RMSE = sprintf("%3.2f", mean)
  )

best_iters <- which(sa_history$results == "new best")
restart_iters <- which(sa_history$results == "restart from best")
  
best_initial <- 
  sa_history %>% 
  filter(.iter == 0) %>% 
  slice_min(mean)

best_overall <- 
  sa_history %>% 
  slice_min(mean)

iter_1 <- 
  sa_history %>% 
  filter(.iter == 1) 

sa_0 <- cli::format_inline("({best_initial$log_cost}, {best_initial$log_scale})")
sa_1 <- cli::format_inline("({iter_1$log_cost}, {iter_1$log_scale})")
```

Non-greedy search methods are not constrained to always go in the absolute best direction. One such method is simulated annealing_ (SA). It is a controlled random search that moves in random directions but with some amount of control over the path. It can also incorporate restarts, where we return to the last known best value and go in a different direction. 

Given a starting value, SA chooses a random permutation of the current solution, often residing within some local neighborhood.  The objective function is evaluated here and compared to the previous value. If it is an improvement, the process continues. If it is not, there are two options: 

 - We can accept the current value as “suboptimal" and base our next permutation on it, or
 - discard the current solution as if it did not occur. The next candidate point is a permutation of the last “acceptable" point. 

For our SVM example, suppose we start with a candidate where ($log_2 (cost)$, $log_{10} (scale)$) was (-10, -0.1) and had an associated RMSE value of 6.10%. A permutation of this values is created, say (-7.11, -0.225) and its RMSE was measured to be 5.97%. This is smaller and we would repeat the process to create and evaluate a third point.

However, suppose that our new RMSE was 7.00%, meaning that the new candidate did worse than our initial one.  

Using the prior and current estimates of the objective function, simulated annealing uses their difference to compute the probability of accepting a suboptimal candidate. First, the probability depends on the difference. Candidates slightly worse than the current values should have a higher acceptance probability than candidates with very poor results. Second, regardless of the difference in values, the probability should decrease as we continue the optimization. The canonical format of this probability, assuming that smaller values are better, is 

$$
Pr[accept] = \exp\left[-i\bigl(f(\theta_{i}) - f(\theta_{i-1})\bigr)\right]
$$
where $i$ is the iteration number. To compare 6.1% versus 7.0%, the acceptance probability is `r round(exp(-(7-6.10)) * 100, 1)`%. To make the determination, a random uniform number is generated and is used to decide. If this difference were to occur at a later iteration, say $i = 5$, the probability would drop to `r round(exp(-5*(7-6.10)) * 100, 1)`%.

One small matter is related to the scale of the objective function. The difference in the exponent is very sensitive to scale. For example, if we were to use the proportions 0.61 and 0.70, the probability would change from `r round(exp(-(7-6.10)) * 100, 1)`% to `r round(exp(-(.7-.610)) * 100, 1)`%. One way to mitigate this issue is to use a normalized difference by dividing the raw difference by the previous objective function (i.e., (0.70-0.61) / 0.70). This is the approach used in the SA analyses here. 

TODO how do we make candidates

This process is continued until either a pre-defined limit is reached or if there is no improvement by a certain number of iterations. The best result found at any time in the optimization is used as the final value. Also, as previously mentioned, a restart rule can stop simulated annealing from delving too far into suboptimal regions if no best results have occurred within some period of time. 

To illustrate, a very small initial space-filling design with three candidates was generated and evaluated. The results are in @tbl-svm-initial^[These values will be also used as the initial substrate for Bayesian optimization.]. To start the SA search, we will start with the candidate with the smallest RMSE and proceed for 50 iterations without a rule for early stopping. A restart to the last known best results was enforced after eight suboptimal iterations. 

:::: {.columns}

::: {.column width="25%"}
:::

::: {.column width="50%"}

::: {#tbl-svm-initial}


```{r}
#| label: tbl-svm-initial
#| echo: false
sa_history %>% 
  filter(.iter == 0) %>% 
  arrange(mean) %>% 
  select(RMSE, log_cost, log_scale) %>% 
  kable(
    digits = 1,
    col.names  = c("RMSE (%)", "Cost (log-2)", "Scale (log-10)")
  ) %>% 
  kable_styling(bootstrap_options = "responsive", full_width = FALSE)
```

The initial set of candidates used for iterative search with the toy example from @fig-svm-grid. One candidate does poorly while the other two have relatively similar results. 

:::

:::

::: {.column width="25%"}
:::

:::: 
 
@fig-sa-example contains an animation of the results of the SA search. In the figure, the initial points are represented by open circles, and a grey diagonal line reminds readers that a ridge of values exists that corresponds to the best RMSE results. 

:::: {.columns}

::: {.column width="15%"}
:::

::: {.column width="70%"}

::: {#fig-sa-example}

::: {.figure-content}

```{shinylive-r}
#| label: fig-sa-example
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(purrr)
library(scales)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")

# TODO add these apps to their own R files on GH
ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  sliderInput(
    "iter",
    label = "Iteration",
    min = 1L,
    max = 50L,
    step = 1L,
    value = 1L,
    width = "100%"
  ),
  as_fill_carrier(plotOutput("path")),
  renderText("notes")
)

server <- function(input, output) {
  
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_sa.RData"))
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_large.RData"))
  
  num_cuts <- 50
  rd_or <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "OrRd")))(num_cuts)
  
  # ------------------------------------------------------------------------------
  
  x_rng <- 10^extendrange(c(-10, -1/10))
  y_rng <- 2^extendrange(c(-10, 10))
  
  log10_labs <- trans_format("log10", math_format(10^.x, function(x) format(x, digits = 3)))
  log2_labs <- trans_format("log2", math_format(2^.x, function(x) format(x, digits = 3)))

  # ------------------------------------------------------------------------------
  
  sa_history <- 
    sa_history %>% 
    mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts)))
  
  # TODO pre-compute these
  sa_init <- 
    sa_history %>% 
    filter(.iter == 0)
  best_init <- 
    sa_init %>% 
    slice_min(mean) %>% 
    select(.iter, cost, scale_factor, RMSE)
  poor_init <- 
    anti_join(sa_init, best_init, by = c(".iter", "cost", "scale_factor")) %>% 
    select(.iter, cost, scale_factor)
  
  initial_plot <- 
    regular_mtr %>%
    mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts))) %>% 
    ggplot(aes(scale_factor, cost, col = RMSE)) +
    geom_point(data = sa_init, cex = 3, pch = 1, col = "black") +
    geom_line(
      data = regular_mtr %>% slice_min(mean, n = 18),
      stat = "smooth",
      col = "black",
      method = lm,
      se = FALSE,
      formula = y ~ x,
      alpha = 1 / 8,
      linewidth = 2
    ) + 
    scale_x_log10(limits = x_rng,
                  labels = log10_labs,
                  expand = expansion(add = c(-1 / 5, -1 / 5))) +
    scale_y_continuous(limits = y_rng, trans = "log2", labels = log2_labs,
                       expand = expansion(add = c(-1/2, -1/2))) +
    scale_color_manual(values = rd_or, drop = FALSE) +
    coord_fixed(ratio = 1/2) +
    theme_bw() +
    theme(legend.position = "none")
  
  # ------------------------------------------------------------------------------
  
  output$path <-
    renderPlot({
      
      current_path <- 
        paths[[input$iter]] %>% 
        mutate(mean = RMSE,
               RMSE = cut(RMSE, breaks = seq(5, 31, length = num_cuts)))
      last_best <- 
        iter_best[[input$iter]] %>% 
        mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts)))
      
      last_best_iter <-
        last_best %>% 
        pluck(".iter")

      lab <- iter_label[[input$iter]]
      
      current_plot <- 
        initial_plot +
        geom_path(data = current_path, col = "black", alpha = 1 / 5) +
        geom_point(
          data = current_path %>% filter(.iter > 0),
          aes(scale_factor, cost, col = RMSE),
          cex = 2
        ) +
        geom_point(data = last_best, cex = 4, col = "black", pch = 8) +
        labs(x = "Scaling Factor", y = "Cost", title = lab) 
        
      print(current_plot)
      
    }, res = 100)
  
  output$notes <-
    renderText({
      current_path <- 
        resolve_sa_path(sa_history, input$iter) %>% 
        anti_join(poor_init, by = c(".iter", "cost", "scale_factor"))
      describe_result(current_path)
    })
}

app <- shinyApp(ui, server)
```

:::

An example of how simulated annealing can investigate the tuning parameter space. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.

:::

:::

::: {.column width="15%"}
:::

:::: 

During the search, there were `r cli::format_inline("{length(best_iters)} iteration{?s}")` where a new global best result was discovered (iterations `r cli::format_inline("{best_iters}")`). There were also `r cli::format_inline("{length(restart_iters)} restart{?s}")`  restarts at iterations `r cli::format_inline("{restart_iters}")`. In the end, the best results occurred with a cost value of 2<sup>`r best_overall$log_cost`</sup> and a scale factor of 10<sup>`r round(best_overall$log_scale, 2)`</sup>. The corresponding validation set RMSE was `r round(best_overall$mean, 2)`%.

good things about sa: any data type, no extra evaluations, not likely to get stuck locally

bad things: 1 sample at a time, better for fine tuning, limited parallelism



## Genetic Algorithms  {#sec-genetic-algo}


:::: {.columns}

::: {.column width="15%"}
:::

::: {.column width="70%"}

::: {#fig-ga-example}

::: {.figure-content}

```{shinylive-r}
#| label: fig-ga-example
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(purrr)
library(scales)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  sliderInput(
    "gen",
    label = "Generation",
    min = 1L,
    max = 10L,
    step = 1L,
    value = 1L,
    width = "100%"
  ),
  as_fill_carrier(plotOutput("generations"))
)

server <- function(input, output) {
  
  num_cuts <- 50
  rd_or <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "OrRd")))(num_cuts)
  
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_ga.RData"))
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_large.RData"))

  ga_history$mean <- ga_history$fitness
  ga_history <- 
    ga_history %>% 
    mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts)))

  # ------------------------------------------------------------------------------
  
  x_rng <- 10^extendrange(c(-10, -1/10))
  y_rng <- 2^extendrange(c(-10, 10))
  
  log10_labs <- trans_format("log10", math_format(10^.x, function(x) format(x, digits = 3)))
  log2_labs <- trans_format("log2", math_format(2^.x, function(x) format(x, digits = 3)))
  
  # ------------------------------------------------------------------------------
  
  output$generations <-
    renderPlot({
      
      last_best <- 
        ga_history %>%
        filter(generation <= input$gen) %>% 
        slice_min(mean)
      
      current_gen <- ga_history %>% filter(generation == input$gen) 
      
      p <- 
        ga_history %>%
        ggplot(aes(scale_factor, cost)) +
        geom_line(
          data = regular_mtr %>% slice_min(mean, n = 18),
          stat = "smooth",
          col = "black",
          method = lm,
          se = FALSE,
          formula = y ~ x,
          alpha = 1 / 8,
          linewidth = 2
        ) +
        geom_point(data = current_gen, aes(col = RMSE), alpha = 2/4, cex = 2, 
                   show.legend = FALSE) +
        geom_point(data = last_best, cex = 4, col = "black", pch = 8) +
        labs(x = "Scaling Factor", y = "Cost") +
        scale_x_log10(limits = x_rng,
                      labels = log10_labs,
                      expand = expansion(add = c(-1 / 5, -1 / 5))) +
        scale_y_continuous(limits = y_rng, trans = "log2", labels = log2_labs,
                           expand = expansion(add = c(-1/2, -1/2))) +
        scale_color_manual(values = rd_or, drop = FALSE) +
        coord_fixed(ratio = 1/2) +
        theme_bw() +
        theme(legend.location = "none")
      
      print(p)
      
    }, res = 100)
}

app <- shinyApp(ui, server)
```

:::

Several generations of a genetic algorithm that search tuning parameter space. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.

:::

:::

::: {.column width="15%"}
:::

:::: 




## Bayesian Optimization {#sec-bayes-opt}

:::: {.columns}

::: {.column width="15%"}
:::

::: {.column width="70%"}

::: {#fig-bo-example}

::: {.figure-content}

```{shinylive-r}
#| label: fig-bo-example
#| viewerHeight: 630
#| viewerWidth: "100%"
#| standalone: true
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(purrr)
library(scales)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  sliderInput(
    "iter",
    label = "Iteration",
    min = 1L,
    max = 50L,
    step = 1L,
    value = 1L,
    width = "100%"
  ),
  radioButtons(
    inputId = "objective",
    label = "Objective Function",
    inline = TRUE,
    choices = list("Exp. Improivement" = "objective", "Mean" = ".mean", 
                   "Std. Dev" = ".sd")
  ),
  as_fill_carrier(plotOutput("path"))
)

server <- function(input, output) {
  
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_bo.RData"))
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_large.RData"))
  
  # ------------------------------------------------------------------------------
  
  x_rng <- 10^extendrange(c(-10, -1/10))
  y_rng <- 2^extendrange(c(-10, 10))
  
  log10_labs <- trans_format("log10", math_format(10^.x, function(x) format(x, digits = 3)))
  log2_labs <- trans_format("log2", math_format(2^.x, function(x) format(x, digits = 3)))
  
  # ------------------------------------------------------------------------------
  
  best_init <- 
    bo_mtr %>% 
    filter(.iter == 0) %>% 
    slice_min(mean) %>% 
    select(cost, scale_factor, .iter)
  
  initial_plot <- 
    regular_mtr %>%
    ggplot(aes(scale_factor, cost)) +
    geom_point(data = init_grid, cex = 3, pch = 1) +
    scale_x_log10(limits = x_rng,
                  labels = log10_labs,
                  expand = expansion(add = c(-1 / 5, -1 / 5))) +
    scale_y_continuous(limits = y_rng, trans = "log2", labels = log2_labs,
                       expand = expansion(add = c(-1/2, -1/2))) +
    theme_bw() +
    labs(x = "Scaling Factor", y = "Cost") +
    coord_fixed(ratio = 1/2)
  
  bo_tile <- tidyr::pivot_longer(
    bo_tile,
    cols = c(.mean, .sd, objective),
    names_to = "metric",
    values_to = "value"
  )
  
  # ------------------------------------------------------------------------------
  
  output$path <-
    renderPlot({
      
      tile <- bo_tile %>% filter(.iter == input$iter & metric == input$objective)
      last_best <- bo_mtr %>% filter(.iter < input$iter) %>% slice_min(mean)
      path <- 
        bo_mtr %>% 
        filter(.iter <= input$iter & .iter > 0) %>% 
        select(cost, scale_factor, .iter) %>% 
        bind_rows(best_init) %>% 
        arrange(.iter)

      # TODO convert to discrete colors via cut
      rc_pal <- switch(
        input$objective,
        "objective" = "Greens",
        ".mean" = "OrRd",
        ".sd" = "Blues"
      )
      
      hist_alpha <- 
        case_when(
          input$iter < 5 ~ 1/2, 
          input$iter >= 5 & input$iter < 10 ~ 1/3,  
          input$iter >= 10 & input$iter < 20 ~ 1/4, 
          input$iter >= 20 & input$iter < 30 ~ 1/5, 
          TRUE ~ 1 / 10)
      hist_alpha <- 2 * hist_alpha
      
      col_dir <- if_else(input$objective == ".mean", -1, 1)
      col_tr <- "identity"

      current_plot <- 
        initial_plot + 
        geom_tile(data = tile, aes(fill = value), alpha = 1 / 2) +
        geom_line(
          data = regular_mtr %>% slice_min(mean, n = 18),
          stat = "smooth",
          col = "black",
          method = lm,
          se = FALSE,
          formula = y ~ x,
          alpha = 1 / 4,
          linewidth = 2
        ) +
        geom_path(data = path, col = "black", alpha = hist_alpha, lty = 3) +
        geom_point(data = new_bo_points %>% filter(iter == input$iter), 
                   col = "black", pch = 16, cex = 4) +
        geom_point(data = last_best, cex = 4, col = "black", pch = 8) + 
        scale_fill_distiller(palette = rc_pal, type = "seq", 
                             direction = col_dir, transform = col_tr) +
        theme(legend.title = element_blank(), legend.text = element_blank())
      print(current_plot)
      
    }, res = 100)

}

app <- shinyApp(ui, server)
```

:::

An illustration of Bayesian optimization. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.

:::

:::

::: {.column width="15%"}
:::

:::: 


## Example: Tuning a Neural Network {#sec-bayes-opt-nnet}

We’ll return to the data previously seen in @sec-barley where we want to predict the percentage of barley oil in a mixture. Recall that the predictors are extremely correlated with one another. In @sec-embeddings, we considered embedding methods like PCA to preprocess the data. 

In this chapter, we’ll take a more sophisticated approach to preprocessing these data using signal processing techniques, specifically the **Savitzky-Golay** method. This technique incorporates two main goals: to smooth the data from location to location (i.e., across the x-axis in @fig-barley-data(b)) and to difference the data. Smoothing attempts to reduce measurement noise without diminishing the data patterns that could predict the outcome. Differencing can remove the correlation between locations and might also mitigate other issues, such as systematic background noise. 

For smoothing, the procedure uses a moving window across a sample’s location and, within each window, uses a polynomial basis expansion to produce a smoother representation of the data. The window width and the polynomial degree are both tuning parameters. Note that the polynomial degree cannot be larger than the window width. 

Differencing has a single tuning parameter: the difference order. A zero-order difference leaves the data as-is (After smoothing). A first-order difference is the smoothed value minus the smoothed value for the next location (and so on). 

@fig-savitzky-golay shows an example of a small set of locations. The top row shows the effect of smoothing when differencing is not used. For a linear smoother (i.e., a polynomial degree of one), the process can over-smooth the data such that the nonlinear portions of the raw data lose significant amounts of information. This issue goes away as the polynomial order is increased. However, the higher degree polynomial nearly interpolates between data points and may not be smoothing the data. 

The bottom panel shows the data when a first-order difference is used. The pattern is completely different since the data now represent the rate of change in the outcome rather than the absolute measurement values. Again, larger window sizes oversmooth the data, and the highest degree of polynomial provides minimal smoothing. 

We’ll want to optimize these preprocessing parameters simultaneously with the parameters for the supervised model. 

```{r}
#| label: fig-savitzky-golay
#| echo: false
#| fig-width: 8.5
#| fig-height: 6
#| out-width: 100%
#| warning: false
#| fig-cap: "Demonstrations of the Savitzky-Golay preprodcessing method for a small region for one specific sample."

load("../RData/savitzky_golay_demo.RData")

smoothed_p <-
  smoothed_plot_data %>%
  mutate(.location = .location + (`window size`/2)) %>%
  ggplot(aes(.location, .measure)) +
  geom_point(data = raw_plot_data, cex = 1/2) +
  geom_path(aes(col = window), alpha = 3 / 4) +
  facet_wrap(~ `polynomial degree`, labeller = label_both) +
  lims(y = c(0.595, 0.606), x = c(505, 544)) +
  labs(y = "Original Value", x = NULL) +
  theme(legend.position = "top")

diff_p <- diff_plot_data %>%
  mutate(.location = .location + (`window size`/2)) %>%
  ggplot(aes(.location, .measure)) +
  geom_hline(yintercept = 0, lty = 2, alpha = 1 / 2) +
  geom_point(data = raw_diff_plot_data, cex = 1/2) +
  geom_path(aes(col = window), alpha = 3 / 4) +
  facet_wrap(~ `polynomial degree`, labeller = label_both) +
  lims(y = c(-0.0012, 0.0014), x = c(505, 544)) +
  labs(y = "1st Difference", x = "Wavelength (nm)") +
  theme(legend.position = "none")

smoothed_p / diff_p
```


```{r}
#| label: collect-iterative
#| include: false
load("../RData/barley_iterative.RData")

iters <- max(mlp_sfd_bo_ci$.iter)
rmse_rng <- extendrange(c(mlp_sfd_bo_ci$.lower, mlp_sfd_bo_ci$.upper, 
                          mlp_sfd_sa_ci$.lower, mlp_sfd_sa_ci$.upper))

mlp_sfd_bo_best_ci <- 
  mlp_sfd_bo_ci %>% 
  filter(.iter == max(mlp_sfd_bo_best))

mlp_sfd_bo_ci$is_best <- "no"
mlp_sfd_bo_ci$is_best[mlp_sfd_bo_ci$.iter %in% mlp_sfd_bo_best] <- "yes"

mlp_sfd_sa_best_ci <- 
  mlp_sfd_sa_ci %>% 
  filter(.iter == max(mlp_sfd_sa_best))

mlp_sfd_sa_ci$is_best <- "no"
mlp_sfd_sa_ci$is_best[mlp_sfd_sa_ci$.iter %in% mlp_sfd_sa_best] <- "yes"

restarts <- result_history$.iter[result_history$results == "restart from best"]
```

::: {#tbl-nnet-initial}


```{r}
#| label: tbl-nnet-initial
#| echo: false
mlp_sfd_bo_ci %>% 
  filter(.iter == 0) %>% 
  mutate(
    `window size` = 2 * window_side + 1,
    mixture = mixture * 100,
    `penalty (log-10)` = log10(penalty),
    `learn rate (log-10)` = log10(learn_rate),
    activation = gsub("_", " ", activation),
    rate_schedule = gsub("(decay_time)|(decay_expo)", "decay", rate_schedule)
    ) %>% 
  arrange(.estimate) %>% 
  select(`RMSE (%)` = .estimate, `window size`, differentiation_order, degree,
         hidden_units, `penalty (log-10)`, activation, `learn rate (log-10)`,
         rate_schedule, stop_iter, mixture) %>% 
  kable(
    digits = 2,
    col.names  = c(
     "RMSE (%)", "Window", "Diff. Order", "Degree", "Units", "Penalty (log-10)", 
     "Activation", "Learn Rate (log-10)", "Rate Schedule", "Stop Iter.", 
     "L1 Mixture"
    )
  ) %>% 
  kable_styling(bootstrap_options = "responsive", full_width = FALSE)
```

The initial set of candidates used for iterative search methods.
 
:::




The best results occurred at iteration `r mlp_sfd_bo_best`: 

 - `r cli::format_inline("{mlp_sfd_bo_best_ci$hidden_units} hidden unit{?s}")`.
 - log<sub>10</sub> penalty  of `r signif(log10(mlp_sfd_bo_best_ci$penalty), 3)`. 
 - log<sub>10</sub> learning rate of `r signif(log10(mlp_sfd_bo_best_ci$learn_rate), 2)`. 
 - a `r mlp_sfd_bo_best_ci$rate_schedule` learning rate schedule.
 - stopping training after `r cli::format_inline("{mlp_sfd_bo_best_ci$stop_iter} iteration{?s}")`. 
 - regularization than is `r round(mlp_sfd_bo_best_ci$mixture * 100, 1)`% L<sub>1</sub>.
 - a difference order of `r mlp_sfd_bo_best_ci$differentiation_order`.
 - a polynomial degree of `r mlp_sfd_bo_best_ci$degree`.
 - a smoothing window size of `r 1 + 2 * mlp_sfd_bo_best_ci$window_side`.
 
The corresponding validation set RMSE was `r round(mlp_sfd_bo_best_ci$.estimate, 2)`% with 90% confidence interval of (`r round(mlp_sfd_bo_best_ci$.lower, 2)`%, `r round(mlp_sfd_bo_best_ci$.upper, 2)`%).

The BO took approximately `r prettyunits::pretty_sec(mlp_bo_time[3] / iters)` per candidate. 


```{r}
#| label: fig-svm-bo-rmse
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| out-width: 70%
#| fig-cap: "The progress of Bayesian optimization. The validation set RMSE is shown on the y-axis with 90% confidence intervals. Orange points indicate iterations where a new optimal value was found. "
#| warning: false

mlp_sfd_bo_ci %>% 
  ggplot(aes(.iter, col = is_best)) + 
  geom_point(aes(y = .estimate), show.legend = FALSE, cex = 1 / 2) +
  geom_errorbar(
    aes(ymin = .lower, ymax = .upper), 
    width = 1/2, 
    show.legend = FALSE
  ) +
  labs(x = "Iteration", y = "RMSE") +
  ylim(rmse_rng) +
  scale_color_manual(values = c("black", "#FF7F00"))
```

Simulated annealing arrived at the following candidate set (at iteration `r mlp_sfd_sa_best`): 

 - `r cli::format_inline("{mlp_sfd_sa_best_ci$hidden_units} hidden unit{?s}")`.
 - log<sub>10</sub> penalty  of `r signif(log10(mlp_sfd_sa_best_ci$penalty), 3)`. 
 - log<sub>10</sub> learning rate of `r signif(log10(mlp_sfd_sa_best_ci$learn_rate), 2)`. 
 - a `r mlp_sfd_sa_best_ci$rate_schedule` learning rate schedule.
 - stopping training after `r cli::format_inline("{mlp_sfd_sa_best_ci$stop_iter} iteration{?s}")`. 
 - regularization than is `r round(mlp_sfd_sa_best_ci$mixture * 100, 1)`% L<sub>1</sub>.
 - a difference order of `r mlp_sfd_sa_best_ci$differentiation_order` 
 - a polynomial degree of `r mlp_sfd_sa_best_ci$degree`.
 - a smoothing window size of `r 1 + 2 * mlp_sfd_sa_best_ci$window_side`.
 
The corresponding validation set RMSE was `r round(mlp_sfd_sa_best_ci$.estimate, 2)`% with 90% confidence interval of (`r round(mlp_sfd_sa_best_ci$.lower, 2)`%, `r round(mlp_sfd_sa_best_ci$.upper, 2)`%).



```{r}
#| label: fig-svm-sa-rmse
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| out-width: 70%
#| fig-cap: "Validation set RMSE results for the optimization via simulated annealing. The validation set RMSE is shown on the y-axis with 90% confidence intervals. Orange points indicate iterations where a new optimal value was found and vertical lines indicate where the algorithm restarted from the previous best candidate."
#| warning: false

mlp_sfd_sa_ci %>% 
  ggplot(aes(.iter, col = is_best)) + 
    geom_vline(xintercept = restarts, col = "darkred", lty = 3) +
  geom_point(aes(y = .estimate), show.legend = FALSE, cex = 1 / 2) +
  geom_errorbar(
    aes(ymin = .lower, ymax = .upper), 
    width = 1/2, 
    show.legend = FALSE
  ) +
  labs(x = "Iteration", y = "RMSE") +
  ylim(rmse_rng) +
  scale_color_manual(values = c("black", "#FF7F00"))
```

Simulated annealing took `r prettyunits::pretty_sec(mlp_sa_time[3] / iters)` per candidate. 


```{r}
#| label: fig-svm-sa-parameters
#| echo: false
#| fig-width: 6
#| fig-height: 5
#| out-width: 80%
#| fig-cap: "Plots for the progress of SA for each tuning parameter."
#| warning: false

mlp_sfd_sa_ci %>% 
  mutate(
    `penalty (log-10)` = log10(penalty),
    `learning rate (log-10)` = log10(learn_rate),
  ) %>% 
  select(-.metric, -.estimator, -.estimate, -.lower, -learn_rate,
         -.upper, -.config, -is_best, -method, -penalty) %>% 
  pivot_longer(
    cols = c(hidden_units, `penalty (log-10)`, `learning rate (log-10)`, 
             stop_iter, mixture, differentiation_order, degree, window_side),
    names_to = "Parameter", 
    values_to = "Value"
  ) %>% 
  ggplot(aes(.iter, Value, col = activation, pch = rate_schedule)) + 
  geom_point() + 
  facet_wrap(~ Parameter, scales = "free_y") +
  theme(legend.position = "top")
```

## Chapter References {.unnumbered}

