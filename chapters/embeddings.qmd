---
knitr:
  opts_chunk:
    cache.path: "../_cache/embeddings/"
---

# Embeddings {#sec-embeddings}

```{r}
#| label: embeddings-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------
library(MASS)
library(tidymodels)
library(embed)
library(bestNormalize)
library(patchwork)
library(scales)
library(modeldatatoo) # remove this later with setup file
library(ggforce)
library(viridis)
library(GGally)

# ------------------------------------------------------------------------------
# set options
tidymodels_prefer()
theme_set(theme_transparent())
set_options()
```

```{r}
#| label: data-import
#| include: false
source("../R/setup_ames.R")
source("../R/setup_chemometrics.R")
```

When there are a multitude of predictors, it might make sense to condense them into a smaller number of artificial features. To be useful, this smaller set should represent what is essential in the original data. This process is often called _feature extraction_, _dimension redunction_, or _manifold learning_. We’ll use a more general term currently en vogue: **embeddings**. While this chapter focuses on feature extraction, embeddings  can be used for other purposes, such as converting non-numeric data (e.g., text) into a more usable numeric format. 

This section will examine two primary classes of embedding methods that can achieve multiple purposes. First, we’ll consider linear methods that take a numeric input matrix $X$ that is $n \times p$ and create a different, probably smaller set of features $X^*$ ($n \times m$)^[With $m <<< p$.] using the transformation $X^* = XQ$. 

After describing linear methods, we will consider a different class of transformations that focuses on the distances between data points called _multidimensional scaling_ (MDS). MDS creates a new set of $m$ features that are not necessarily linear combinations of the original features but often use some of the same math as the linear techniques. 

Before beginning, we’ll introduce another data set that will be used here and in forthcoming chapters.

## Example: Predicting Barley Amounts  {#sec-barley}

@larsen2019deep and @pierna2020applicability describe a data set where laboratory measurements are used to predict what percentage of a liquid was lucerne, soy oil, or barley oil^[Retreived from [`https://chemom2019.sciencesconf.org/resource/page/id/13.html`](https://chemom2019.sciencesconf.org/resource/page/id/13.html)]. An instrument is used to measure how much of particular wavelengths of light are absorbed by the mixture to help determine chemical composition. We will focus on using the lab measurements to predict the percentage of barley oil in the mixture. The distribution of these values is shown in @fig-barley-data(a). 

```{r}
#| label: base-rec
#| echo: false
#| cache: true

barley_base <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  prep()
```

```{r}
#| label: fig-barley-data 
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 6
#| fig-cap: "(a) The distribution of the outcome for the entire data set. The bar colors reflect the percent barley distribution and are used in subsequent sections. (b) Selected training set spectra for four barley samples. Each line represents the set of 550 predictors in the data."

key <- tibble(
  barley_bin = levels(chimiometrie_2019$barley_bin),
  midpoint = barley_breaks[-length(barley_breaks)] + diff(barley_breaks)/2
)

barley_p <- 
  chimiometrie_2019 %>% 
  summarize(
    count = n(),
    median = median(barley),
    .by = barley_bin
    ) %>% 
  full_join(key, by = "barley_bin") %>% 
  ggplot(aes(midpoint / 100, count, fill = median)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_fill_viridis(option = "viridis") +
  labs(x = "Barley", title = "(a)") + 
  scale_x_continuous(labels = scales::label_percent())

set.seed(41) 
spect_p <- 
    chimiometrie_2019 %>% 
    filter(barley_bin %in% c("[0,2]", "(18,20]", "(38,40]", "(52,54]")) %>% 
    slice_sample(n = 1, by = "barley_bin") %>% 
    add_rowindex() %>% 
    pivot_longer(c(starts_with("wv"))) %>% 
    mutate(index = as.numeric(gsub("wvlgth_", "", name))) %>% 
    full_join(wave, by = "index") %>%
    ggplot(aes(wavelength, value, col = barley, group = .row)) + 
    geom_line(show.legend = FALSE, linewidth = 1, alpha = 1) +
    scale_color_viridis(option = "viridis") +
    labs(y = "Measurement", x = "Wavelength (nm)", title = "(b)") 

barley_p / spect_p
```

Note that most of the data have very little barley oil. About `r round(mean(chimiometrie_2019$barley <= 1) * 100, 0)`% of the data are less than 1%, and the median barley oil percentage is `r round(median(chimiometrie_2019$barley), 2)`%.

The `r nrow(wave)` predictors are the light absorbance for sequential values in the light region of interest (believed to be from wavelengths between `r min(wave$wavelength)` and `r max(wave$wavelength)` nm). @fig-barley-data(b) shows a selection of four samples from the data. The darker lines represent samples with lower barley content. 

These predictor values, called _spectra_, have a very high serial correlation between predictors; median correlation between the predictors was `r round(median(wave_corr), 2)`. The high degree of between-predictor correlation can be a major complication for some models and can degrade predictive performance.  Therefore, we need methods that will simultaneously decorrelate predictors while extracting useful predictive information for the outcome. 

Analyses of similar data sets can be found in [Section 9.1](https://bookdown.org/max/FES/illustrative-data-pharmaceutical-manufacturing-monitoring.html) of @fes and @wtf2024.

In the following computations, each predictor was standardized using the orderNorm transformation mentioned earlier (unless otherwise noted). 

The data originated from a modeling competition to find the most accurate model and specific samples were allocated to training and test sets. However, there were no public outcome values for the test set; our analysis will treat the `r format(nrow(chimiometrie_2019), big.mark = ",")` samples in their training set as the overall pool of samples. This is enough data to split into separate training ($n_{tr} =$ `r format(nrow(barley_train), big.mark = ",")`), validation ($n_{val} =$ `r format(nrow(barley_val), big.mark = ",")`), and test sets ($n_{te} =$ `r format(nrow(barley_test), big.mark = ",")`). The allocation of samples to each of the three data sets utilized stratified sampling based on the outcome data. 


## Linear Transformations  {#sec-linear-embed}

The barley data set presents two common challenges for many machine learning techniques:

1. The dimensionality of the data is fairly large (`r ncol(barley_train) - 1` orginal columns). 
2. The features are highly correlated. Techniques that require inverting the covariance matrix of the features, such as linear regression, will become numerically unstable or may not be able to be fit when features are highly correlated. 

What can we do if we desire to use a modeling technique that is adversely affected by either (or both) of these characteristics?  

Dimensionality reduction is one technique that can help with the first issue (an abundance of columns). As we’ll see below, we might be able to extract new features ($X^*$) such that the new features optimally summarize information from the original data ($X$). 

For the problem of highly correlated predictors, some embedding methods can additionally _decorrelated_ the predictors by deriving the embedded features with minimal correlation.  

The three embedding methods we’ll discuss first are linear in a mathematical sense because they transform a table of numeric features into new features that are linear combinations of the original features. These new linear combinations are often called _scores_. This transformation uses the equation.

$$ \underset{n\times m}{X^*} = \underset{n\times p}{X}\ \underset{p\times m}{Q} $$

where $Q$ is a matrix that translates or embeds the original features into a potentially lower dimensional space ($m < p$) without losing much information. It is easy to see why this matrix operation is linear when the elements of the $X$ and $Q$ matrices are expanded.  For example, the first score for the $i^{th}$ sample would be:

$$ 
x^*_{i1} = q_{11} x_{i1} + q_{21}x_{i2} + \ldots +  q_{p1}x_{ip} 
$$ {#eq-pca-linear-combo}

The $q_{ij}$ values are often referred to as the _loadings_ for each predictor in the linear combination. 

In this section, we will review principal components analysis (PCA), independent component analysis (ICA), and partial least squares (PLS), which are fundamental linear embedding techniques that are effective at identifying meaningful embeddings but estimate $A$ in different ways.  PCA and ICA extract embedded features using only information from the original features; they are unsupervised.  Alternatively, PLS adds in information from the original features and the outcome during training; PLS is a supervised technique. 

There are many other linear embedding methods that can be used. For example, non-negative matrix factorization is an embedding method that is useful when the data in $X$ are integer counts or other values that cannot be negative. 

To start, we’ll focus on the most often used embedding method: PCA.

### Principal Component Analysis

```{r}  
#| label: pca-rec
#| echo: false
#| cache: true
#| warning: false

barley_rec <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  prep()

barley_preproc <- bake(barley_rec, barley_train)

barley_linear_trans_base <-
  recipe(barley ~ ., data=barley_preproc) 

barley_pca <-
  barley_rec %>%
  step_pca(all_numeric_predictors(), num_comp = 20, id="PCA")

barley_pca_estimates <- prep(barley_pca, training=barley_preproc)
barley_pca_data <- bake(barley_pca_estimates, barley_preproc)
barley_cumulative_variance <- 
  tidy(barley_pca_estimates, id="PCA", type="variance") %>% # OR number=3.  Do the same for ICA
  dplyr::filter(terms=="cumulative percent variance") %>%
  dplyr::filter(component <= 10) %>%
  mutate(previous_cum_var = dplyr::lag(value, n=1)) %>%
  mutate(previous_cum_var = ifelse(component == 1, 0, previous_cum_var)) %>%
  mutate(pct_total_var = value - previous_cum_var)
```


```{r}  
#| label: pca-rec2
#| echo: false
#| cache: true
#| warning: false

barley_norm_rec <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  prep()

barley_norm_train <- bake(barley_norm_rec, barley_val)

barley_pca <-
  barley_norm_rec %>%
  step_pca(all_numeric_predictors(), num_comp = 4, id = "pca") %>% 
  prep()

barley_pca_val <- bake(barley_pca, barley_val)

barley_cumulative_variance <-
  tidy(barley_pca, id = "pca", type = "variance") %>% 
  dplyr::filter(terms == "cumulative percent variance" & component <= 10) %>%
  mutate(
    previous_cum_var = dplyr::lag(value, n = 1), 
    previous_cum_var = ifelse(component == 1, 0, previous_cum_var), 
    pct_total_var = value - previous_cum_var
  )
```


```{r}  
#| label: ica-rec2
#| echo: false
#| cache: true
#| warning: false

set.seed(227)
barley_ica <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_ica(all_numeric_predictors(), num_comp = 4, id = "ica") %>% 
  prep()

barley_ica_val <- bake(barley_ica, barley_val)
```

```{r}  
#| label: pls-rec2
#| echo: false
#| cache: true
#| warning: false

barley_pls <-
  barley_norm_rec %>%
  step_pls(all_numeric_predictors(), outcome = "barley", num_comp = 4, id = "pls") %>% 
  prep()

barley_pls_val <- bake(barley_pls, barley_val)
```


```{r}
#| label: all-linear-results
#| echo: false
#| warning: false
pivot_me <- function(x, method) {
  x %>% 
    add_rowindex() %>% 
    pivot_longer(cols = c(-barley, -.row), names_to = "label", values_to = "value") %>% 
    mutate(
      component_num = gsub("[a-zA-Z]", "", label),
      component_num = as.numeric(component_num),
      Method = method)
}

all_scores_val <- 
  bind_rows(
    pivot_me(barley_pca_val, "PCA"),
    pivot_me(barley_ica_val, "ICA"),
    pivot_me(barley_pls_val, "PLS")
  ) %>%
  mutate(
    value = ifelse(label == "PLS2", -value, value)
  )

all_loadings <- 
  bind_rows(
    tidy(barley_pca, id = "pca"), 
    tidy(barley_ica, id = "ica"),
    tidy(barley_pls, id = "pls")
  ) %>% 
  mutate(
    index = gsub("wvlgth_", "", terms),
    index = as.integer(index),
    component_number = gsub("[a-zA-Z]", "", component),
    component_number = as.numeric(component_number),
    # make signs more consistent across methods
    value = ifelse(id == "pca" & component_number == 2, -value, value),
  ) %>% 
  dplyr::select(-terms) %>% 
  dplyr::filter(component_number <= 4) %>% 
  mutate(
    component_number = format(component_number),
    id = factor(id, levels = c("pca", "ica", "pls"))
  ) %>% 
  full_join(wave, by = "index")

if ( !file.exists("../RData/barley_linear_embeddings.RData") ) {
  save(all_scores_val, all_loadings, file = "../RData/barley_linear_embeddings.RData")
}
```

Karl Pearson introduced PCA over a century ago, yet it remains a fundamental dimension reduction method [@jolliffe2016principal].  Its relevance stems from the underlying objective of the technique: to find linear combinations of the original predictors that maximize amount of variation in the new features^[As is, this is far too general. We could maximize the variance by making every value of $Q$ equal to $\infty$. PCA, and other methods, impose an implicit constraint to limit the elements of $Q$. In this case, PCA constrains the loading vectors to have unit length.]. From a statistical perspective, variation is synonymous with information.  

Simultaneously, the scores produced by PCA (i.e., $X^*$) are required to be orthogonal to each other. The important side benefit of this technique is that _PCA scores are uncorrelated_. This is very useful for modeling techniques that need the predictors to be relatively uncorrelated: multiple linear regression, neural networks, support vector machines, and others.

In a way, PCA components have an order or hierarchy. The first principal component has a higher variance than any other component. The second component has a higher variance than subsequent components, and so on. We can think of this process as one of deflation; the first component extracts the largest sources of information/variation in the predictors set. The second component extracts as much as possible for whatever is left behind by the first, and so on. For this reason, we can track how much variation in the original data each component accounts for. We often use the phrase that "PCA chases variation."

Suppose we start with $p$ columns^[Assuming that the $p$ columns are linearly independent.] in our data. In that case, we can produce up to $p$ PCA components, and their accumulated variation will eventually add up to the total variation in the original $X$.

While PCA can deliver new features with the desirable characteristics, the technique must be used with care.  Specifically, we must understand that PCA seeks to find embeddings without regard to any further understanding of the original features (such as the response).  Hence, PCA may generate embeddings that are ignorant of the modeling objective.

#### Preparing for PCA {.unnumbered}

Because PCA seeks linear combinations of predictors that maximize variability, it will naturally first be drawn to summarizing predictors with more variation.  If the original predictors are on measurement scales that differ in order of magnitude, then the first components will focus on summarizing the higher magnitude predictors.  This means that the PCA scores will focus on the scales of the measurements (rather than their intrinsic value).  If, by chance, the most relevant feature for predicting the response also has the largest numerical values, the new features created by PCA will retain the essential information for predicting the response.  However, if the most important features are in the smallest units, then the top features created by PCA will be much less effective. 

In most practical machine learning applications, features are on vastly different scales. We suggest using the tools from @sec-common-scale to transform the data so that they have the same units. This prevents PCA from focusing dimension reduction simply on the measurement scale.

Additionally, the distributions of each feature may show considerable skewness or outliers. While there are no specific distributional requirements to use PCA, it is focused on variance and variance calculations often involve squared terms (e.g., $(x_i - \bar{x})^2$). This type of computation can be very sensitive to outliers and skewness; resolving these issues prior to PCA is recommended. See the methods previously described in @sec-skewness.  


#### How does PCA work? {.unnumbered}

Principal component analysis is focused on understanding the relationships between the predictor columns. PCA is only effective when there are correlations between predictors. Otherwise, each new PCA feature would only highlight a single predictor, and you would need the full set of PCA features to approximate the original columns. Conversely, data sets with many between-predictor relationships require very few predictors to represent the source columns. 

The barley data set has abnormally high correlations between each predictor. These tend to be autoregressive; the correlations between predictors at adjacent wavelengths are very high, and the correlation between a pair of predictors diminishes as you move away from their locations on the spectrum. For example, the correlation between the first and second predictor (in their raw form) is essentially `r signif(cor(barley_train[, 2:3])[1,2], 8)` while the corresponding correlation between the first and fiftieth columns is `r round(cor(barley_train[, c(2, 51)])[1,2], 2)`. We'll explore the latter pair of columns below. 

Before proceeding, let's go on a small "side-quest" to talk about the mathematics of PCA. 

::: {.mathy-box}
The remainder of this subsection discussed a mathematical operation called the singular value decomposition (SVD), otherwise known as the spectral decomposition. PCA and many other computations rely on on the SVD. It is a fundamental linear algebra calculation. 

We'll describe it loosely with a focus on how it is used. The concept of eigenvalues will come up again in the next section and in subsequent chapters. 
:::


The SVD process is intended to find a way to _rotate_ the original data in a way that the resulting columns are orthogonal to one another. It takes a matrix as an input and can decompose it into several pieces: 

$$\underset{p\times p}{A} = \underset{p\times p}{Q}\quad\underset{p\times p}{\Lambda}\quad \underset{p\times p}{Q'}$$

The results of this computation are the $p$ eigenvectors $\mathbf{q}_j$ and eigenvalues $\lambda_j$. The eignevectors tell you about the directions in which values of $A$ are moving and the eigenvalues describe the corresponding magnitudes. 

The transformation works with PCA by using the covariance matrix as the input $A$. We are trying to find trends in the relationships between variables and the SVD is designed to translate the original matrix to one that is orthogonal (that is,). This aspect of the SVD is how it connects to PCA. If we want to approximate our original data with new features that are uncorrelated, we need a transformation to orthogonality.

The matrix $Q$ houses the $p$ possible eigenvectors. These are the values by which you multiply the original matrix $A$ to achieve an orthogonal version. For PCA, they are the loading values. The matrix $\Lambda$ is a diagonal matrix whose $p$ values are the eigenvalues. It turns out that the eigenvalues represent the amount of variation (i.e., information) captured by each component. 

If we conduct the SVD (and PCA) on the covariance matrix of the data, we are again subject to issues around potentially different units and scales of the predictors. Previously, we suggested to center and scale the data prior to PCA. The equivalent approach here is to conduct the SVD on the correlation matrix of the data. Similar to our recommendations on standardization, we recommend using the correlation as the input to PCA. 

One interesting side-effect of using the covariance or correlation matrix as the input to PCA is related to missing data (discussed in more detail in @sec-missing-data). The conventional sample covariance (or correlation) matrices can be computed for each matrix element (i.e., without using matrix multiplication). Each covariance or correlation can be computed with whatever rows of the two predictors have pairwise-complete results. This means that we do not have to globally drop specific rows of the data when a small number of columns contain missing data. 


#### A two dimensional example {.unnumbered}

```{r}
#| label: barley-2d-data
#| include: false

load("../RData/barley_2d_pca_.RData")
angles <- min(opt_variances$angle2)
angles <- c(angles, angles + 180)
angles_txt <- paste0(round(angles, 0), "$^{\\circ}$")
angles_txt <- knitr::combine_words(angles_txt)

q_11 <- 
  barley_pca_coefs %>% filter(terms == "wvlgth_001" & component == "PC1") %>%
  pluck("value") %>% round(2)
q_12 <- 
  barley_pca_coefs %>% filter(terms == "wvlgth_050" & component == "PC1") %>%
  pluck("value") %>% round(2)
q_21 <- 
  barley_pca_coefs %>% filter(terms == "wvlgth_001" & component == "PC2") %>%
  pluck("value") %>% round(2)
q_22 <- 
  barley_pca_coefs %>% filter(terms == "wvlgth_050" & component == "PC2") %>%
  pluck("value") %>% round(2)
```

To demonstrate, we chose two predictors in the barley data. They correspond to wavelengths 1,300 and 1,398 shown^[Thse wavelengths correspond to the 1<sup>st</sup> and 50<sup>th</sup> predictors.] in @fig-barley-data(b).  In the data, these two predictors were preprocessed using the ORQ procedure (@sec-skewness), and these processed values have a correlation of `r barley_val_cor`. 

PCA was performed on these data, and the results are animated in @fig-pca-rotation. This visualization shows the original data (colored by their outcome data) and then rotates it around the center position. The PCA transformation corresponds to the rotation where the x-axis has the largest spread. The variance is largest at _two_ angles (`r angles_txt`) since the solution for PCA is _unique up to sign_. Both possible solutions are correct, and we usually pick one. For one solution, the PCA loading coefficients are: 

\begin{align}
x^*_{i1} &= `r q_11` x_{i1} `r q_21` x_{i2} \notag \\
x^*_{i2} &= `r q_21` x_{i1} + `r q_22` x_{i2} \notag
\end{align}

Because of this illustrative example’s low-dimensional aspect, this solution is excessively simple. Ordinarily, the loading values take a great many values.  

```{r}
#| label: fig-pca-rotation 
#| echo: false
#| warning: false
#| out-width: 40%
#| fig-cap: "A demonstration that the PCA transformation is just of rotation of the data. This animation shows the rotation of two of the predictors (those at the first and fiftieth wavelength). The original data and the two optimal rotations that correspond to the principal components."

knitr::include_graphics("../premade/anime_barley_pca.gif")
```

Now that we've seen PCA on a small scale, let's look at an analysis of the full set of predictors 

#### PCA on the barley data  {.unnumbered}

Similar to the previous section, the ORQ procedure was used to standardize the predictors for all of the embedding methods used with the barley data in this chapter. From these, PCA was calculated on the training set. There are `r ncol(barley_train) - 1` feature columns and the same number of possible principal components. The number of new features to retain is a parameter that must be determined.  In practice, the number of new features is often selected based on the percentage of variability that the new features summarize relative to the original features.  @fig-barley-pca-scree displays the new PCA feature number (x-axis) versus the percent of total variability across the original features (commonly called a “scree plot”).  In this graph, three components summarize `r round(barley_cumulative_variance$value[3],1)`% of the total variability.  We often select the smallest number of new features that summarize the greatest variability.  Instead of specifying the number of new features based on the amount of variability summarized, the number of features can be, and _should_ be, a tuning parameter in the model tuning process. 

Note that the first component alone captured `r round(barley_cumulative_variance$value[1],1)`% of the variation in the `r ncol(barley_train) - 1` training data columns. This reflects the extreme correlation seen in the predictors. 
```{r}
#| label: fig-barley-pca-scree
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 6
#| fig-height: 4
#| fig-cap: "A plot of the PCA component number versus the percent of variance explained for the first 10 components of the barley data."

ggplot(barley_cumulative_variance,
       aes(x = component, y = cumsum(pct_total_var))) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = c(1:10)) +
  labs(x = "PCA Component", y = "% Total Variance")
```

@fig-linear-scores shows a scatterplot of the first two principal components colored by the percentage of barley oil.  The figure reveals that the first PCA component delineates the higher barley oil samples from the samples with less oil.  There also aappears to be three different clusters of samples with very low (if any) barley oil. It might be helpful to investigate these samples to see

::: {#fig-linear-scores}

::: {.figure-content}

```{shinylive-r}
#| label: fig-linear-scores
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)
library(dplyr)
library(tidyr)
library(purrr)

# ------------------------------------------------------------------------------

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

# ------------------------------------------------------------------------------

theme_light_bl<- function(...) {
  
  ret <- ggplot2::theme_bw(...)
  
  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect
  
  ret$legend.position <- "top"
  
  ret
}

# ------------------------------------------------------------------------------

ui <- fluidPage(
  theme = grid_theme,
  fluidRow(
    
    column(
      width = 4,
      checkboxInput(
        inputId = "common",
        label = "Standardize Scale",
        FALSE
      )
    ), # common scale
    column(
      width = 4,
      sliderInput(
        inputId = "x_comp",
        label = "X axis component",
        min = 1,
        max = 4,
        value = 1,
        width = "100%",
        step = 1
      )
    ), # x-axis
    
    column(
      width = 4,
      sliderInput(
        inputId = "y_comp",
        label = "Y axis component",
        min = 1,
        max = 4,
        value = 2,
        width = "100%",
        step = 1
      )
    ),
    fluidRow(
      column(
        width = 4,
        radioButtons(
          inputId = "method",
          label = "Embedding",
          choices = list("PCA" = "PC", "ICA" = "IC", "PLS" = "PLS")
        )
      ),
      column(
        width = 6,
        align = "center",
        plotOutput('scores')
      )
    )
  ) # top fluid row
)

server <- function(input, output) {
  load(url("https://raw.githubusercontent.com/aml4td/website/embeddings_kj/RData/barley_linear_embeddings.RData"))
  

  output$scores <-
    renderPlot({
      comp_num <- c(input$x_comp, input$y_comp)
      methods <- paste0(input$method, comp_num)
      
      if (length(unique(methods)) > 1) {
        
        if (input$common) {
          tmp <- all_scores_val %>% dplyr::filter(grepl(input$method, label))
          axis_rng <- range(tmp$value)
        }
        
        dat <- all_scores_val %>% 
          dplyr::filter(label %in% methods) %>% 
          dplyr::select(barley, .row, label, value) %>% 
          tidyr::pivot_wider(id_cols = c(barley, .row), names_from = label, values_from = value) %>% 
          purrr::set_names(c("barley", ".row", "x", "y"))
        
        p <-
          ggplot(dat, aes(x = x, y = y, col = barley)) +
          geom_point(alpha = 1 / 3, cex = 3) +
          scale_color_viridis(option = "viridis") +
          theme_light_bl() +
          labs(x = methods[1], y = methods[2]) +
          guides(col = guide_colourbar(barheight = 0.5))
        
        if (input$common) {
          p <- p + lims(x = axis_rng, y = axis_rng)
        }
        
        print(p)
      }
    })
}

app <- shinyApp(ui = ui, server = server)
```
:::

A visualization of the new features for different linear embedding methods. The data shown are the validation set results.  

:::

@fig-linear-scores shows a scatterplot of the first two principal components colored by the percentage of barley oil.  The figure reveals that the first PCA component delineates the higher barley oil samples from those with less oil.  There also appear to be three different clusters of samples with very low (if any) barley oil. It might be helpful to investigate these samples to ascertain if they are artifacts or caused by some systematic, underlying factors not in the current set of columns. The pairing of the second and fourth components appears to help differentiate lower barely samples from the broader set. The pairing of the third and fourth components doesn’t offer much predictive power on their own.

Note that these trends are difficult to see when the axes are converted to a common scale based on the overall range of points across all components. This reflects @fig-barley-pca-scree, emphasizing that the first component captures the most information about the predictors.

For PCA, it can be very instructive to visualize the loadings for each component. This can help in several different ways. First, it can tell which predictors dominate each specific new PCA component. If a particular component ends up having a strong relationship with the outcome, this can aid our explanation of how the model works. Second, we can examine the magnitude of the predictors to determine some of the relationships between the predictors. If a group of predictors have approximately the same loading value, this implies that they have a common relationship with one another. This, on its own, can be a significant aid when conducting exploratory data analysis on a high-dimensional data set. @fig-linear-loadings shows the relationship between the PCA loadings and each predictor’s position on the spectrum.

::: {#fig-linear-loadings}

::: {.figure-content}

```{shinylive-r}
#| label: fig-linear-loadings
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)
library(dplyr)

# ------------------------------------------------------------------------------

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

# ------------------------------------------------------------------------------

theme_light_bl<- function(...) {
  
  ret <- ggplot2::theme_bw(...)
  
  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect
  
  ret$legend.position <- "top"
  
  ret
}

# ------------------------------------------------------------------------------


ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(xs = c(-2, 4, 4, -2), sm = 4),
    column(
      width = 4,
      checkboxGroupInput(
        inputId = "method",
        label = "Embedding",
        choices = list("PCA" = "pca", "ICA" = "ica", "PLS" = "pls"),
        selected = "pca"
      )
    ),
    column(
      width = 4,
      checkboxGroupInput(
        inputId = "comps",
        label = "Component",
        choices = list("1" = "1", "2" = "2", "3" = "3", "4" = "4"),
        selected = paste0(1:4),
        inline = TRUE
      )
    )
  ),
  as_fill_carrier(plotOutput("loadings"))
)


server <- function(input, output) {
  load(url("https://raw.githubusercontent.com/aml4td/website/embeddings_kj/RData/barley_linear_embeddings.RData"))
  
  
  output$loadings <-
    renderPlot({
      dat <- 
        all_loadings %>% 
        dplyr::filter(id %in% input$method & component_number %in% input$comps)
      p <-
        ggplot(dat, aes(x = wavelength, y = value, col = component_number)) +
        geom_hline(yintercept = 0, lty = 3) +
        geom_line(alpha = 3 / 4, linewidth = 1) +
        labs(y = "Loading Value") +
        scale_color_brewer(palette = "Dark2") +
        theme_light_bl() 
      if ( length(input$method) > 1) {
        p <- p + facet_wrap(~ id, nrow = 1)
      }
      print(p)
    })
}

app <- shinyApp(ui = ui, server = server)
```
:::

The loadings for the first four components of each linear embedding method as a function of wavelength.

:::

For the first component, wavelengths less than about 1,400 nm have a limited impact on the components. After this, each predictor’s impact is about the same. The second PCA component is almost the opposite; it emphasizes the smaller wavelengths while predictors above 1,400 nm fluctuate around zero. The third and fourth components emphasize different areas of the spectrum and are generally different than zero.


#### How to incorporate PCA prior to the model {.unnumbered}

The barley data demonstrates that PCA can effectively capture the training set information in a smaller predictor set when there is a correlation between predictors. The number of components to retain depends on the data. It is a good idea to optimize the number of components to retain via model tuning. 

::: {.note-box}

Note that “dimension reduction” is _not_ the same as feature selection. The PCA components described above are functions of all of the predictors. Even if you only need a handful of PCA components to approximate the training set, the original predictor set is still required when predicting new samples. 
  
:::

Another aspect of using PCA for feature extraction is the scale of the resulting scores. @fig-linear-scores demonstrates that the first component has the largest variance, resulting in a wider range of training set values. For the validation set scores, the variance of PC<sub>1</sub> is `r round(var(barley_pca_val$PC1) / var(barley_pca_val$PC2), 0)`-fold larger than the variance of PC<sub>2</sub>. Even though the scores are unitless, there may be issues for models that expect the predictors to be in the same units, similar to the requirement for PCA itself. In these cases, it may be advisable to standardize the PCA components to have the same range before serving them to the supervised ML model.


#### Non-standard PCA {.unnumbered}

Various methods exist for estimating the PCA loadings. Some are well-suited to different dimensions of the predictor’s data. For example, "small $n$, large $p$" data sizes can make PCA computations difficult. @wu1997kernel and others describe a _kernel_ method more appropriate for very wide data sets. @scholkopf1998nonlinear extended this even further but using nonlinear kernel methods^[We will see similar methods in @sec-svm-cls.]. Still, others use modified objective functions to compensate for some potential deficiencies of the canonical technique.

For example, the standard loading matrix (i.e., the eigenvectors) are _dense_. It is possible that the loadings for some predictors shouldn’t affect specific components. The SVD might estimate these to be close to zero but not exactly zero. Sparse PCA techniques can, through various means, estimate some loading values to be exactly zero, indicating that the predictor has no functional effect on the embedding. 

For example, @shen2008sparse take a penalization approach that puts a high price for loadings to have very large values. This regularization method can shrink the values towards zero and make some absolute zero. This is similar to the regularization approach seen in @sec-effect-encodings and is directly related to techniques discussed in @sec-logistic-reg, @sec-nnet-cls, and @sec-linear-reg. This option can coerce values across the loadings and components to be zero. It is unlikely to force a particular predictor’s values to be zero across all components. In other words, it may not completely erase the effect of a predictor from the embedding altogether. 

Another method is based on a Bayesian approach [@ning2021spike], where the loading values are assumed to come from a mixture of two different distributions. One has a sharp “spike” around zero, and the other is a flat, wide distribution that encompasses a wide range of values (called the “slab”). The method then estimates the parameters to favor one distribution or the other and sets some proportion of the loadings to zero. 

Additionally, there are PCA variants for non-numeric data, such as categorical predictors. Probabilistic PCA [@tipping1999probabilistic;@pmlrvR4schein03a] uses a PCA generalization to reduce the dimensions of qualitative data. 

<br>




```{r}  
#| label: ica-rec
#| echo: false
#| cache: true
#| warning: false
barley_ica <-
  barley_linear_trans_base %>%
  step_ica(all_numeric_predictors(), num_comp = 20, id="ica")

barley_ica_estimates <- prep(barley_ica, training=barley_preproc)
barley_ica_data <- bake(barley_ica_estimates, barley_preproc)

barley_ica_data_for_pairs_plot <-
  barley_ica_data %>%
  dplyr::select(barley, IC01, IC02, IC03, IC04) %>%
  mutate(barley_bin = ifelse(barley < as.numeric(quantile(barley, 0.25)), "Q1",
                             ifelse(barley >= as.numeric(quantile(barley, 0.25)) & barley < as.numeric(quantile(barley, 0.5)), "Q2",
                                    ifelse(barley >= as.numeric(quantile(barley, 0.5)) & barley < as.numeric(quantile(barley, 0.75)), "Q3", "Q4")))) %>%
  mutate(barley_bin = factor(barley_bin, levels=c("Q1", "Q2", "Q3", "Q4"))) %>%
  dplyr::select(-barley)

barley_linear_trans_base <-
  recipe(barley ~ ., data=barley_preproc) 

barley_pls <-
  barley_linear_trans_base %>%
  step_pls(all_numeric_predictors(), outcome="barley", num_comp = 2)

barley_pls_estimates <- prep(barley_pls, training=barley_preproc)
barley_pls_data <- bake(barley_pls_estimates, barley_preproc)
  
barley_pca_data_long <-
  barley_pca_data %>%
  dplyr::select(barley, PC01, PC02) %>%
  rename(Comp1 = PC01,
         Comp2 = PC02) %>%
  pivot_longer(cols=c("Comp1", "Comp2"),  values_to = "score", names_to = "Component") %>%
  mutate(Method = "PCA")

barley_ica_data_long <-
  barley_ica_data %>%
  dplyr::select(barley, IC01, IC02) %>%
  rename(Comp1 = IC01,
         Comp2 = IC02) %>%
  pivot_longer(cols=c("Comp1", "Comp2"),  values_to = "score", names_to = "Component") %>%
  mutate(Method = "ICA")
```

```{r}  
#| label: pls-rec
#| echo: false
#| cache: true
#| warning: false
barley_pls_data_long <-
  barley_pls_data %>%
  rename(Comp1 = PLS1,
         Comp2 = PLS2) %>%
  pivot_longer(cols=c("Comp1", "Comp2"),  values_to = "score", names_to = "Component") %>%
  mutate(Method = "PLS")

for_comparison_figure <-
  bind_rows(barley_pca_data_long, barley_ica_data_long, barley_pls_data_long) %>%
  mutate(Component = factor(Component, levels=c("Comp1", "Comp2")),
         Method = factor(Method, levels=c("PCA", "ICA", "PLS")))

component1_correlations <-
  tibble(Method = c("PCA", "ICA", "PLS"),
         R = c(round(cor(barley_pca_data$barley, barley_pca_data$PC01), 2),
               round(cor(barley_ica_data$barley, barley_ica_data$IC01), 2),
               round(cor(barley_pls_data$barley, barley_pls_data$PLS1), 2)))

barley_pca_data_2_comp <-
  barley_pca_data %>%
  dplyr::select(barley, PC01, PC02) %>%
  rename(Comp1 = PC01,
         Comp2 = PC02) %>%
  mutate(Method = "PCA")

barley_ica_data_2_comp <-
  barley_ica_data %>%
  dplyr::select(barley, IC01, IC02) %>%
  rename(Comp1 = IC01,
         Comp2 = IC02) %>%
  mutate(Method = "ICA")

barley_pls_data_2_comp <-
  barley_pls_data %>%
  rename(Comp1 = PLS1,
         Comp2 = PLS2) %>%
  mutate(Method = "PLS")

two_component_comparison <-
  bind_rows(barley_pca_data_2_comp, barley_ica_data_2_comp, barley_pls_data_2_comp) %>%
  mutate(Method = factor(Method, levels=c("PCA", "ICA", "PLS")))

#Get correlations between first two components for each method
two_component_correlations <-
  two_component_comparison %>%
  group_by(Method) %>%
  summarise(r = cor(Comp1, Comp2)) %>%
  ungroup()

#First PCA, ICA, and PLS component compared to the 4 selected samples
three_wavelengths <-
  barley_train %>%
  dplyr::select(wvlgth_001, wvlgth_250, wvlgth_550, barley)

pca_wavelength <-
  bind_cols(three_wavelengths,
            barley_pca_data %>% dplyr::select(PC01)) %>%
  rename(Comp1 = PC01) %>%
  mutate(Method = "PCA")

ica_wavelength <-
  bind_cols(three_wavelengths,
            barley_ica_data %>% dplyr::select(IC01)) %>%
  rename(Comp1 = IC01) %>%
  mutate(Method = "ICA")

pls_wavelength <-
  bind_cols(three_wavelengths,
            barley_pls_data %>% dplyr::select(PLS1)) %>%
  rename(Comp1 = PLS1) %>%
  mutate(Method = "PLS")

comp1_wavelength_comparison <-
  bind_rows(pca_wavelength, ica_wavelength, pls_wavelength) %>%
  rename(`1300` = wvlgth_001,
         `1900` = wvlgth_250,
         `2398` = wvlgth_550) %>%
  pivot_longer(cols=c(`1300`, `1900`, `2398`), values_to="Measurement", names_to="Wavelength") %>%
  mutate(Wavelength = factor(Wavelength, levels=c("1300", "1900", "2398")),
         Method = factor(Method, levels=c("PCA", "ICA", "PLS")))
```


### Independent Component Analysis

ICA has roots in signal processing, where the observed signal that is measured is a combination of several different input signals.  A common example used to conceptualize the objective of ICA is the microphone problem.  Suppose that a voice recording is taken during a meeting where there is one primary presenter.  During the meeting, someone close to the microphone continues to shuffle papers which is also captured on the recording.  The final recording contains both the primary presenter's voice and the noise of the shuffled papers, which are two independent sources of audio information. 

The objective of independent component analysis (ICA) is to identify linear combinations of the original features that are statistically independent of each other [@nordhausen2018independent].  For the microphone problem, the goal would be to separate the two unique sources of sound in the recording.  The noise source would then be isolated and could be removed.  Because ICA's objective requires statistical independence among components, it will generate a different set of features than PCA.  This enables ICA to be able to model a different set of trends than PCA, which focuses on orthogonality and linear relationships.  Depending on the problem, ICA may generate new features that may be more predictive of the response than PCA.  Unlike PCA, which orders new components based on summarized variance, ICA's components have no natural ordering.  Thhis means that ICA may require more components than PCA in order to find the ones that are related to the outcome.

To compare PCA and ICA, let's return to @fig-barley-component-scatter.  First, both techniques are similar in that the first and second components are uncorrelated.  However, while there are visual patterns between the first two components for PCA, there are no patterns for ICA.  This demonstrates the ICA creates independent components.  Third, while the outcome is related to the first two PCA components, it is not related to the first two ICA components.  For ICA to be used in this problem, more ICA components will need to be extracted to locate those components that are related to the outcome.

```{r}
#| label: fig-barley-ica-scatter 
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 7.5
#| fig-height: 7.5
#| fig-cap: "A pairwise scatterplot of the first four ICA components, colored by the percentage of barley oil."

ggpairs(barley_ica_data_for_pairs_plot, columns=1:4, ggplot2::aes(color=barley_bin)) +
  scale_color_viridis(option = "viridis", discrete=TRUE)
```

@fig-barley-ica-scatter is a scatterplot matrix of the first 4 ICA components colored by the four quartiles of the outcome.  In this figure, the first three ICA components do not distinguish separation in the outcome.  However, the fourth component begins to separate the lower barley oil percentage samples from the higher barley oil samples.  Therefore, at least 4 ICA components would be needed to predict barley oil percentage.  Like PCA, the number of ICA components should be a tuning parameter in the model building process.

In practice, satisfying the statistically independence requirement of ICA can be done several ways.  One way to do this is to maximize the "non-Gaussianity" of the resulting components. There are different ways to measure non-Gaussianity and the _fastICA_ approach uses an information theory called _negentropy_ [@hyvarinen2000independent].  ICA should create components that are dissimilar from PCA unless the predictors demonstrate significant multivariate normality or strictly linear trends. Also, unlike PCA, there is no unique ordering of the components. 

### Partial Least Squares {#numeric-pls}

Both PCA and ICA focus strictly on summarizing information based on the features exclusively.  When the outcome is related to the way that the unsupervised techniques extract the embedded features, then PCA and/or ICA can be effective dimension reduction tools.  However, when the qualities of variance summarization or statistical independence are not related to the outcome, then these techniques may struggle to identify appropriate embedded features that are useful for predicting the outcome.

Partial least squares (PLS) is a dimension reduction technique that identifies embedded features that are optimally linearly related to the outcome [@stone1990continuum].  While the objective of PCA finds linear combinations of the original features that best summarize variability, the objective of PLS is to do the same _and_ to find the linear combinations that are correlated with the outcome.  This process can be thought of as a constrained optimization problem in which PLS is forced to compromise between optimal variability summary of the predictors and optimal prediction of the outcome.  This means that the outcome is used to focus the dimension reduction process such that the new features have the greatest correlation with the outcome.  Because one of the objectives of PLS is to summarize variance among the original features, the features should be pre-processed in the same way as PCA.  

The most common implementation of PLS produces new features that are uncorrelated, which is similar to PCA and is beneficial for many machine learning techniques.  This characteristic can be seen in the right facet of @fig-barley-component-scatter.  Similar to PCA, the first two components are visually related the outcome, but the correlation is `r round(two_component_correlations %>% dplyr::filter(Method=="PLS") %>% dplyr::select(r) %>% pull(), 2)`.  Because PLS utilizes the response to find the embedded features, it generally requires the same number components as PCA or, usually, fewer components to generate a model with similar predictive performance.  This implies that fewer features are necessary when using PLS, thus leading to a smaller input data set to the machine learning models.  As compared to PCA, PLS utilizes the percentage of barley oil to reorient the dimension reduction with higher barley oil samples in the upper right and lower barley oil samples in the lower left.  This reorientation is not surprising since PLS is simultaneously optimizing correlation with the response.  

### Overall Comparisons

```{r}
#| label: fig-barley-method-correlation-comparison
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 4
#| fig-cap: "A scatterplot matrix of the first component of each method versus percentage of barley oil."

ggplot(for_comparison_figure %>% dplyr::filter(Component == "Comp1"), aes(x=score, y=barley, color=barley)) +
  facet_grid(cols=vars(Method), scales="free") +
  geom_point() +
  scale_color_viridis(option = "viridis")
```

Because PLS utilizes the outcome as part of the dimension reduction, its components will have as great or greater correlation with the outcome.  @fig-barley-method-correlation-comparison illustrates the relationship between the first component of each method with the outcome.  The correlation between the first score of PCA, ICA, and PLS with the percentage of barley oil is `r paste0(round(component1_correlations$R,2)[1], ", ", round(component1_correlations$R,2)[2], ", and ", round(component1_correlations$R,2)[3])`.  Notice that PLS has the largest absolute correlation, which means that it has found a linear combination of the original features that is better than PCA or ICA.

```{r}
#| label: fig-barley-method-wavelength-comparison
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 4
#| fig-cap: "A scatterplot matrix of the first component of each method compared to wavelengths 1300, 1900, and 2398, colored by the percentage of barley oil."

ggplot(comp1_wavelength_comparison, aes(x=Comp1, y=Measurement, color=barley)) + 
  facet_grid(rows=vars(Wavelength), cols=vars(Method), scales="free") +
  geom_point() + 
  scale_color_viridis(option = "viridis")
```

As a final comparison of the three linear embedding methods, let's examine how the first components of each method are related to three selected wavelengths at the beginning, middle, and end of the spectrum (@fig-barley-pca-pls-comparison).  The first characteristic that stands out is that PCA and PLS are nearly mirror images for each of the three selected wavelengths.  This is not surprising given the similarities we have seen between these methods for this data set.  The first ICA component does not provide separation for percentage of barley oil, regardless of the selected wavelength.  However, the first PCA and PLS components have a near linear relationship with wavelength 1900 which are also related to the outcome.  For PCA, smaller scores are associated with larger barley oil percentages.  While for PLS the opposite is true.  For PCA and PLS, the correlation between an original feature and a component is called a loading.  Features that load onto the same component are often related in some meaningful way. This information can be used to aid in interpreting the new components and why these are related to the outcome.  In this example, the chemical information contained in wavelength 1900 may be unique to barley oil.

PCA and PLS will be discussed in more detail in Chapter [TODO:  Add reference to this chapter], where these methods are directly used as modeling techniques.

## Multidimensional Scaling {#sec-mds}

Multidimensional scaling [@torgerson1952multidimensional] is a feature extraction tool that creates embeddings that try to preserve the geometric distances between training set points. In other words, the distances between points in the smaller dimensions should be comparable to those in the original dimensions. Since the methods in this section use distances, the predictors should be standardized to equivalent units before the embedding is trained. We also recommend transformations to resolve skewness. 

Take @fig-mds-example(a) as an example. There are ten points in two dimensions (colored by three outcome classes). If we were to project these points down to a single dimension, we'd like points that are close in the original two dimensions to remain close when projected down to a single dimension. Panel (c) shows two such solutions. Each does reasonably well with some exceptions (i.e., points six and nine are too close for non-Metric MDS). 

```{r}
#| label: mds-example-computations
#| include: false

pens <- penguins[complete.cases(penguins),]

n <- 10
set.seed(119)
pen_subset <-
  pens %>%
  sample_n(n) %>%
  select(length = bill_length_mm, depth = bill_depth_mm, species) %>%
  arrange(length, depth) %>%
  mutate(
    sample = row_number(),
    length = as.vector(scale(length)),
    depth = as.vector(scale(depth)))

pen_subset %>%
  ggplot(aes(x = length, y = depth, color = species)) +
  geom_point()

ar <- diff(range(pen_subset$length)) / diff(range(pen_subset$depth))

### Compute distances and KNN

pen_dist <-
  dist(pen_subset[, 1:2]) %>%
  as.matrix()

dist_alt <- pen_dist
diag(dist_alt) <- 10^38

nearest <- vector(mode = "list", length = n)
k <- 2

for (i in 1:n) {
  nearest[[i]] <- order(dist_alt[i,])[1:k]
}

### 

mds_1D <- sammon(pen_dist, k = 1, tol = 0.000001, niter = 1000)$points[, 1]
mds_1D_df <-
  tibble(
    MDS_1 = as.vector(scale(mds_1D)),
    species = pen_subset$species,
    sample = pen_subset$sample,
    method = "Non-Metric MDS"
  )

### 

iso_data <- 
  recipe(species ~ length + depth, data = pen_subset) %>% 
  step_isomap(all_numeric_predictors(), neighbors = 2, num_terms = 1) %>% 
  prep()

isomap_1D_df <-
  tibble(
    MDS_1 = as.vector(scale(iso_data$steps[[1]]$res@data@data[,1])),
    species = pen_subset$species,
    sample = pen_subset$sample,
    method = "Isomap"
  )

one_d_df <- bind_rows(mds_1D_df, isomap_1D_df)
```
```{r}
#| label: fig-mds-example
#| echo: false
#| dev: "ragg_png"
#| out-width: 70%
#| fig-width: 8
#| fig-height: 8
#| fig-cap: "(a) A collection of samples associated with three outcome classes. (b) A diagram of the two nearest neighbors for each data point. (c) A one-dimensional projection using two different methods: non-metric MDS and Isomap."

base_p <- 
  pen_subset %>%
  ggplot(aes(x = length, y = depth)) +
  coord_fixed(ratio = ar) +
  scale_color_brewer(palette = "Dark2")

orig_p <-
  base_p +
  geom_point(
    pch = 16,
    cex = 8,
    fill = "white",
    col = "white",
    show.legend = FALSE
  ) +
  geom_point(
    pch = 1,
    cex = 8,
    aes(col = species),
    fill = "white",
    show.legend = FALSE
  ) +
  geom_text(aes(label = sample, col = species), show.legend = FALSE) +
  labs(title = "(a) Original Data", x = "Predictor A", y = "Predictor B")

### 

knn_p <- base_p

for(i in 1:n) {
  tmp <- pen_subset[nearest[[i]], 1:2] %>% rename_with(~paste0(., "_end"))
  dat <- pen_subset %>% slice(rep(i, 2)) %>% bind_cols(tmp)

  knn_p <-
    knn_p +
    geom_segment(data = dat, aes(xend = length_end, yend = depth_end),
                 col = "grey")
}
knn_p <-
  knn_p +
  geom_point(
    pch = 16,
    cex = 8,
    fill = "white",
    col = "white",
    show.legend = FALSE
  ) +
  geom_point(
    pch = 1,
    cex = 8,
    aes(col = species),
    fill = "white",
    show.legend = FALSE
  ) +
  geom_text(aes(label = sample, col = species), show.legend = FALSE) +
  labs(title = "(b) 2-Nearest Neighbors", x = "Predictor A", y = "Predictor B") 

### 

  one_d_p <-
    one_d_df %>%
    ggplot(aes(x = MDS_1, y = method)) +
    geom_point(
      pch = 16,
      cex = 8,
      fill = "white",
      col = "white",
      show.legend = FALSE
    ) +
    geom_point(
      pch = 1,
      cex = 8,
      aes(col = species),
      fill = "white",
      show.legend = FALSE
    ) +
    geom_text(aes(label = sample, col = species), show.legend = FALSE) +
    labs(title = "(c) 1 Dimensional MDS (two ways)", y = NULL, 
         x = "Embedded Value") +
    scale_color_brewer(palette = "Dark2")
  
### 

layout <- "
AAAAAAAA
AAAAAAAA
AAAAAAAA
AAAAAAAA
AAAAAAAA
AAAAAAAA
#BBBBBB#
#BBBBBB#
"

((orig_p + knn_p) / one_d_p) +
  plot_layout(design = layout)
```

Here we present two MDS methods, but there are many more. @Ghojogh2023 has an excellent review of an assortment of methods and their nuances. 

Some MDS methods compute all pairwise distances between points and use this as the input to the embedding algorithm. This is similar to how PCA can be estimated using the covariance or correlation matrices. One technique, _Non-Metric MDS_ [@kruskal1964multidimensional;@kruskal1964nonmetric;@sammon1969nonlinear], finds embeddings that minimize an objective function called "stress":

$$
\text{Stress} = \sqrt{\frac{\sum\limits^{n_{tr}}_{i = 1}\;\sum\limits^{n_{tr}}_{j = i+1}\left(d(x_i, x_j) - d(x^*_i, x^*_j)\right)^2}{\sum\limits^{n_{tr}}_{i = 1}\;\sum\limits^{n_{tr}}_{j = i+1}d(x_i, x_j)^2}}
$$

The numerator uses the squared difference between the pairwise distances in the original values ($x$) and the smaller embedded dimension ($x^*$). The summations only move along the upper triangle of the distance matrices to reduce redundant computations. @fig-mds-example(c, top row) has the resulting one dimensional projection of our two-dimensional data.

This can be an effective dimension reduction procedure, although there are a few issues. First, the entire matrix of distances is required (with $n_{tr}(n_{tr}-1)/2$ entries). For large training sets, this can be unwieldy and time-consuming.  Second, like PCA, it is a global method that uses all data in the computations.  We might be able to achieve more nuanced embeddings by focusing on local structures.  Finally, it is challenging to apply metric MDS to project new data onto the space in which the original data was projected.

### Isomap  {#sec-isomap}

To start, we'll focus on _Isomap_ [@tenenbaum2000global]. This nonlinear MDS method uses a specialized distance function to find the embedded features. First, the _K_ nearest neighbors are determined for each training set point using standard functions, such as Euclidean distance. @fig-mds-example(b) shows the _K_ = 2 nearest neighbors for our example data. Many nearest-neighbor algorithms can be very computationally efficient and their use eliminates the need to compute all of the pairwise distances. 

The connections between neighbors form a _graph structure_ that defines which data points are closely related to one another. From this, a new metric called _geodesic distance_ can be approximated. For a graph, we can compute the approximate geodesic distance using the shortest path between two points on the graph. With our example data, the Euclidean distance between points four and five is not large. However, its approximate geodesic distance is greater because the shortest path is through points nine, eight, and seven. @Ghojogh2023 use a wonderful analogy: 

> A real-world example is the distance between Toronto and Athens. The Euclidean distance is to dig the Earth from Toronto to reach Athens directly. The geodesic distance is to move from Toronto to Athens on the curvy Earth by the shortest path between two cities. The approximated geodesic distance is to dig the Earth from Toronto to London in the UK, then dig from London to Frankfurt in Germany, then dig from Frankfurt to Rome in Italy, then dig from Rome to Athens.

The Isomap embeddings are a function of the eigenvalues computed on the geodesic distance matrix. The $m$ embedded features are functions of the first $m$ eigenvectors. Although eigenvalues are associated with linear embeddings (e.g., PCA), nonlinear geodesic distance results in a global nonlinear embedding. @fig-mds-example(c, bottom row) shows the 1D results for the example data set. For a new data point, its nearest-neighbors in the training set are determined so that the approximate geodesic distance can be computed. The estimated eigenvectors and eigenvalues are used to project the new point into the embedding space. 

For Isomap, the number of nearest neighbors and the number of embeddings are commonly optimized. @fig-barley-isomap shows a two-dimensional Isomap embedding for the barley data with varying numbers of neighbors. In each configuration, the higher barley values are differentiated from the small (mostly zero) barley samples. There does seem to be two or three clusters of data associated with small outcome values. The new features become more densely packed as the number of neighbors increases.

```{r}
#| label: barley-isomap-computations
#| include: false
#| cache: true


neighbor_grid <- c(5, 20, 50)

isomap_grid <- NULL
for (i in neighbor_grid) {
  set.seed(1)
  tmp_rec <- 
    barley_base %>% 
    step_isomap(all_numeric_predictors(), neighbors = i, num_terms = 2) %>% 
    prep()

  tmp_iso <- 
    bake(tmp_rec, new_data = barley_val) %>% 
    mutate(Neighbors = i)
  
  isomap_grid <- bind_rows(isomap_grid, tmp_iso)
}
```

```{r}
#| label: fig-barley-isomap
#| echo: false
#| dev: png
#| out-width: 80%
#| fig-width: 8
#| fig-height: 3.4
#| fig-cap: "Isomap for the barley data for different numbers of nearest neighbors. The training set was used to fit the model and these results show the projections on the validation set. Lighter colors indicate larger values of the outcome."

isomap_grid %>%
  ggplot(aes(Isomap1, Isomap2, col = barley)) +
  geom_point(alpha = 1 / 3, cex = 2)  +
  facet_wrap(~ Neighbors, nrow = 1, labeller = "label_both") +
  scale_color_viridis(option = "viridis") +
  labs(x = "Isomap Embedding #1", y = "Isomap Embedding #2") +
  guides(col = guide_colourbar(barheight = 0.5))
```

### Laplacian Eigenmaps  {#sec-eigenmaps}

There are many other approaches to preserve local distances. One is _Laplacian eigenmaps_ [@belkin2001laplacian]. Like Isomap, it uses nearest neighbors to define a graph of connected training set points. For each connected point, a weight between graph nodes is computed that becomes smaller as the distance between points in the input space increases. The radial basis kernel (also referred to as the "heat kernel") is a good choice for the weighting function^[A note about some notation... We commonly think of the _norm_ notation as $\|\boldsymbol{x}\|_p = \left(|x_1|^p + |x_2|^p + \ldots + |x_n|^p\right)^{1/p}$. So what does the lack of a subscript in $||\boldsymbol{x}||^2$ mean? The convention is the sum of squares:  $||\boldsymbol{x}||^2 = x_1^2 + x_2^2 + \ldots + x_n^2$.]: 

$$
w_{ij} = \exp\left(\frac{-||\boldsymbol{x}_i - \boldsymbol{x}_j||^2}{\sigma}\right)
$$

where $\sigma$ is a scaling parameter that can be tuned. If two points are not neighbors, or if $i = j$, then $w_{ij} = 0$. Note that the equation above uses Euclidean distance. For the 2-nearest neighbor graph shown in @fig-mds-example(b) and $\sigma = 1 / 2$, the weight matrix is roughly

```{r}
#| label: wt-matrix
#| echo: false
#| results: asis

sigma <- 1 / 2
wt_mat <- exp(-pen_dist/sigma)

adj_mat <- pen_dist * 0

for (i in 1:n) {
  nn <-  nearest[[i]]
  for (j in seq_along(nn)) {
    adj_mat[i, nn[j]] <- 1
    adj_mat[nn[j], i] <- 1
  }
}

wt_mat <- adj_mat * wt_mat

rounded_mat <- round(wt_mat, 1)

chr_wt_mat <- format(rounded_mat)
chr_wt_mat[lower.tri(chr_wt_mat)] <- ""
chr_wt_mat[chr_wt_mat == "0.0"] <- "\\0"
chr_wt_mat[6, 3] <- "sym"

tex_1 <- apply(chr_wt_mat, 1, function(x) paste(x, collapse = " & "))
tex_2 <- paste0(tex_1, collapse = "\\\\\n")
cat("$$\n")
cat("\\newcommand{\\0}{{\\color{lightgray} 0.0}}\n")
cat("W = \\begin{bmatrix}\n")
cat(tex_2, "\n")
cat("\\end{bmatrix}\n$$\n")
```

The use of nearest neighbors means that the matrix can be very sparse and the zero values help define locality for each data point. Recall that samples 2 and 3 are fairly close to one another, while samples 1 and 2 are farther away. The weighting scheme gives the former pair a `r rounded_mat[2,3] / rounded_mat[1,2]`-fold larger weight in the graph than the latter pair. 

Laplacian eigenmaps rely heavily on graph theory. This method computes a _graph Laplacian_ matrix, defined as $L = D - W$ where the matrix $D$ has zero non-diagonal entries and diagonals equal to the sum of the weights for each row. For our example data, the matrix is: 

```{r}
#| label: Laplacian-matrix
#| echo: false
#| results: asis

laplacian <- diag(apply(rounded_mat, 1, sum)) - rounded_mat

rounded_L <- round(laplacian, 1)

chr_L <- format(rounded_L)
chr_L[lower.tri(chr_L)] <- ""
chr_L[chr_L == "0.0" | chr_L == " 0.0"] <- "\\0"
chr_L[6, 3] <- "sym"

tex_1 <- apply(chr_L, 1, function(x) paste(x, collapse = " & "))
tex_2 <- paste0(tex_1, collapse = "\\\\\n")
cat("$$\n")
cat("\\newcommand{\\0}{{\\color{lightgray} 0.0}}\n")
cat("L = \\begin{bmatrix}\n")
cat(tex_2, "\n")
cat("\\end{bmatrix}\n$$\n")
```

The eigenvalues and eigenvectors of this matrix are used as the main ingredients for the embeddings. @Bengio2003advances shows that, since these methods eventually use eigenvalues in the embeddings, they can be easily used to project new data. 

### UMAP {#sec-umap}

The Uniform Manifold Approximation and Projection (UMAP) [@sainburg2020parametric] technique is one of the most popular distance-based methods. Its precursors, stochastic neighbor embedding (SNE) [@hinton2002stochastic] and  Student’s t-distributed stochastic neighbor embedding (t-SNE) [@van2008visualizing], redefined feature extraction, particularly for visualizations. UMAP borrows significantly from Laplacian eigenmaps and t-SNE but has a more theoretically sound motivation. 
 
As with Laplacian eigenmaps, UMAP converts the training data points to a sparse graph structure. Given a set of nearest neighbors, it computes values similar to the previously shown weights ($W$ matrix), which we will think of as the probability that point $j$ is a neighbor of point $i$:  

$$
p_{j|i} = \exp\left(\frac{-\left(||\boldsymbol{x}_i - \boldsymbol{x}_j||^2 - \rho_i\right)}{\sigma_i}\right)
$$

where $\rho_i$ is the distance from $\boldsymbol{x}_i$ to its closest neighbor, and $\sigma_i$ is a scale parameter that now varies with each sample ($i$). To compute $\sigma_i$, we can solve the equation   
 
$$
\sum_{i=1}^K  \exp\left(\frac{-\left(||\boldsymbol{x}_i - \boldsymbol{x}_j||^2 - \rho_i\right)}{\sigma_i}\right) = \log_2(K)
$$

Unlike the previous weighting system, the resulting $n \times n$ matrix may not be symmetric, so the final weights are computed using $p_{ij} = p_{j|i} + p_{i|j} - p_{j|i}p_{i|j}$. 

UMAP performs a similar calculation for the embedded values $x^*$. We'll denote the probability that embedded points $\boldsymbol{x}_i^*$ and $\boldsymbol{x}_j^*$ are connected as $p_{ij}^*$. 

Numerical optimization methods^[Specifically gradient descent with a user-defined learning rate.] used to estimate the $n \times m$ values $x^*_{ij}$. The process is initialized using a very sparse Laplacian eigenmap, the first few PCA components, or random uniform numbers. The objective function is based on cross-entropy and attempts to make the graphs in the input and embedded dimensions as similar as possible by minimizing:   

$$
CE = \sum_{i=1}^{n_{tr}}\sum_{j=i+1}^{n_{tr}} \left[p_{ij}\, \log\frac{p_{ij}}{p_{ij}^*} + (1 - p_{ij})\log\frac{1-p_{ij}}{1-p_{ij}^*}\right]
$$

Unlike the other embedding methods shown in this section, UMAP can also create supervised embeddings so that the resulting features are more predictive of a qualitative or quantitative outcome value. See @sainburg2020parametric.

Besides the number of neighbors and embedding dimensions, several more tuning parameters exist.  The optimization process's number of optimization iterations (i.e., epochs) and the learning rate can significantly affect the final results. A distance-based tuning parameter, often called _min-dist_, specifies how "packed" points should be in the reduced dimensions. Values typically range from zero to one. However, the original authors state:

> We view min-dist as an essentially aesthetic parameter governing the appearance of the embedding, and thus is more important when using UMAP for visualization.

As will be seen below, the initialization scheme is an important tuning parameter. 

For supervised UMAP, there is an additional weighting parameter (between zero and one) that is used to balance the importance of the supervised and unsupervised aspects of the results. 

@fig-umap shows an interactive visualization of how UMAP can change with different tuning parameters. Each combination was trained for 1,000 epochs and used a learning rate of 1.0. For illustrative purposes, the resulting embeddings were scaled to a common range. 

::: {#fig-umap}

::: {.figure-content}

```{shinylive-r}
#| label: fig-umap
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)

# ------------------------------------------------------------------------------

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

# ------------------------------------------------------------------------------

theme_light_bl<- function(...) {

  ret <- ggplot2::theme_bw(...)

  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect

  ret$legend.position <- "top"

  ret
}

# ------------------------------------------------------------------------------

ui <- fluidPage(
  theme = grid_theme,
  fluidRow(

    column(
      width = 4,
      sliderInput(
        inputId = "min_dist",
        label = "Min Distance",
        min = 0.0,
        max = 1.0,
        value = 0.2,
        width = "100%",
        step = 0.2
      )
    ), # min distance
    column(
      width = 4,
      sliderInput(
        inputId = "neighbors",
        label = "Neighbors",
        min = 5,
        max = 45,
        value = 5,
        width = "100%",
        step = 10
      )
    ), # nearest neighbors

    column(
      width = 4,
      sliderInput(
        inputId = "supervised",
        label = "Amount of Supervision",
        min = 0.0,
        max = 0.7,
        value = 0,
        width = "100%",
        step = 0.1
      )
    ),
    fluidRow(
      column(
        width = 4,
        radioButtons(
          inputId = "initial",
          label = "Initialization",
          choices = list("Laplacian Eigenmap" = "spectral", "PCA" = "pca", 
                         "Random" = "random")
        )
      ),
      column(
        width = 6,
        align = "center",
        plotOutput('umap')
      )
    )
  ) # top fluid row
)

server <- function(input, output) {
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/umap_results.RData"))

  output$umap <-
    renderPlot({
      
      dat <-
        umap_results[
          umap_results$neighbors == input$neighbors &
            umap_results$min_dist == input$min_dist &
            umap_results$initial == input$initial &
            # log10(umap_results$learn_rate) == input$learn_rate &
            umap_results$supervised == input$supervised,
        ]

      p <-
        ggplot(dat, aes(UMAP1, UMAP2, col = barley)) +
        geom_point(alpha = 1 / 3, cex = 3) +
        scale_color_viridis(option = "viridis") +
        theme_light_bl() +
        coord_fixed() +
        labs(x = "UMAP Embedding #1", y = "UMAP Embedding #2") +
        guides(col = guide_colourbar(barheight = 0.5))

      print(p)

    })
}

app <- shinyApp(ui = ui, server = server)
```
:::

A visualization of UMAP results for the barley data using different values for several tuning parameters. The points are the validation set values. 

:::

There are a few notable patterns in these results: 

 - The initialization method can heavily impact the patterns in the embeddings. 
 - As with Isomap, there are two or three clusters of data points with small barley values. 
 - When the amount of supervision increases, one or more circular structures form that are associated with small outcome values. 
 - The minimum distance parameter can drastically change the results. 

t-SNE and UMAP have become very popular tools for visualizing complex data. Visually, they often show interesting patterns that linear methods such as PCA cannot. However, they are computationally slow and unstable over different tuning parameter values. Also, it is easy to believe that the UMAP distances between embedding points are important or quantitatively predictive. That is not the case; the distances can be easily manipulated using the tuning parameters (especially the minimum distance). 
 

## Centroid-Based Methods  {#sec-centroids}

prototype-based methods

## Embedding Qualitative Predictors  {#sec-qual-embedding}

## Other Methods

autoencoders? 


## Chapter References {.unnumbered}


