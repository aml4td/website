---
knitr:
  opts_chunk:
    cache.path: "../_cache/embeddings/"
---

# Embeddings {#sec-embeddings}

```{r}
#| label: embeddings-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------
library(MASS)
library(tidymodels)
library(embed)
library(bestNormalize)
library(patchwork)
library(scales)
library(modeldatatoo) # remove this later with setup file
library(viridis)
library(doParallel)
# pamr also called by namespace

# ------------------------------------------------------------------------------
# set options
tidymodels_prefer()
theme_set(theme_transparent())
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
set_options()
```

```{r}
#| label: data-import
#| include: false
source("../R/setup_ames.R")
source("../R/setup_chemometrics.R")
```

When there are a multitude of predictors, it might be advantegous to condense them into a smaller number of artificial features. To be useful, this smaller set should represent what is essential in the original data. This process is often called _feature extraction_, _dimension reduction_, or _manifold learning_. We’ll use a more general term currently en vogue: **embeddings**. While this chapter focuses on feature extraction, embeddings  can be used for other purposes, such as converting non-numeric data (e.g., text) into a more usable numeric format. 

This chapter will examine several primary classes of embedding methods that can achieve multiple purposes. First, we’ll consider linear methods that take a numeric input matrix $\boldsymbol{X}$ that is $n \times p$ and create a different, probably smaller set of features $\boldsymbol{X}^*$ ($n \times m$) using the transformation $\boldsymbol{X}^* = \boldsymbol{X}\boldsymbol{Q}$. We hope that we can find appropriate embeddings so that $m << p$.

After describing linear methods, we will consider a different class of transformations that focuses on the distances between data points called _multidimensional scaling_ (MDS). MDS creates a new set of $m$ features that are not necessarily linear combinations of the original features but often use some of the same math as the linear techniques. 

Finally, some embedding techniques specific to classification are discussed. These are based on _class centroids_. 

Before beginning, we’ll introduce another data set that will be used here and in forthcoming chapters.

## Example: Predicting Barley Amounts  {#sec-barley}

@larsen2019deep and @pierna2020applicability describe a data set where laboratory measurements are used to predict what percentage of a liquid was lucerne, soy oil, or barley oil^[Retreived from [`https://chemom2019.sciencesconf.org/resource/page/id/13.html`](https://chemom2019.sciencesconf.org/resource/page/id/13.html)]. An instrument is used to measure how much of particular wavelengths of light are absorbed by the mixture to help determine chemical composition. We will focus on using the lab measurements to predict the percentage of barley oil in the mixture. The distribution of these values is shown in @fig-barley-data(a). 

```{r}
#| label: base-rec
#| echo: false
#| cache: true

barley_base <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  prep()
```

```{r}
#| label: fig-barley-data 
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 6
#| fig-cap: "(a) The distribution of the outcome for the entire data set. The bar colors reflect the percent barley distribution and are used in subsequent sections. (b) Selected training set spectra for four barley samples. Each line represents the set of 550 predictors in the data."

key <- tibble(
  barley_bin = levels(chimiometrie_2019$barley_bin),
  midpoint = barley_breaks[-length(barley_breaks)] + diff(barley_breaks)/2
)

barley_p <- 
  chimiometrie_2019 %>% 
  summarize(
    count = n(),
    median = median(barley),
    .by = barley_bin
    ) %>% 
  full_join(key, by = "barley_bin") %>% 
  ggplot(aes(midpoint / 100, count, fill = median)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_fill_viridis(option = "viridis") +
  labs(x = "Barley", title = "(a)") + 
  scale_x_continuous(labels = scales::label_percent())

set.seed(41) 
spect_p <- 
    chimiometrie_2019 %>% 
    filter(barley_bin %in% c("[0,2]", "(18,20]", "(38,40]", "(52,54]")) %>% 
    slice_sample(n = 1, by = "barley_bin") %>% 
    add_rowindex() %>% 
    pivot_longer(c(starts_with("wv"))) %>% 
    mutate(index = as.numeric(gsub("wvlgth_", "", name))) %>% 
    full_join(wave, by = "index") %>%
    ggplot(aes(wavelength, value, col = barley, group = .row)) + 
    geom_line(show.legend = FALSE, linewidth = 1, alpha = 1) +
    scale_color_viridis(option = "viridis") +
    labs(y = "Measurement", x = "Wavelength (nm)", title = "(b)") 

barley_p / spect_p
```

Note that most of the data have very little barley oil. About `r round(mean(chimiometrie_2019$barley <= 1) * 100, 0)`% of the data are less than 1%, and the median barley oil percentage is `r round(median(chimiometrie_2019$barley), 2)`%.

The `r nrow(wave)` predictors are the light absorbance for sequential values in the light region of interest (believed to be from wavelengths between `r min(wave$wavelength)` and `r max(wave$wavelength)` nm). @fig-barley-data(b) shows a selection of four samples from the data. The darker lines represent samples with lower barley content. 

These predictor values, called _spectra_, have a very high serial correlation between predictors; median correlation between the predictors was `r round(median(wave_corr), 2)`. The high degree of between-predictor correlation can be a major complication for some models and can degrade predictive performance. Therefore, we need methods that will simultaneously decorrelate predictors while extracting useful predictive information for the outcome. 

Analyses of similar data sets can be found in [Section 9.1](https://bookdown.org/max/FES/illustrative-data-pharmaceutical-manufacturing-monitoring.html) of @fes and @wtf2024.

In the following computations, each predictor was standardized using the orderNorm transformation mentioned earlier (unless otherwise noted). 

The data originated from a modeling competition to find the most accurate model and specific samples were allocated to training and test sets. However, there were no public outcome values for the test set; our analysis will treat the `r format(nrow(chimiometrie_2019), big.mark = ",")` samples in their training set as the overall pool of samples. This is enough data to split into separate training ($n_{tr} =$ `r format(nrow(barley_train), big.mark = ",")`), validation ($n_{val} =$ `r format(nrow(barley_val), big.mark = ",")`), and test sets ($n_{te} =$ `r format(nrow(barley_test), big.mark = ",")`). The allocation of samples to each of the three data sets utilized stratified sampling based on the outcome data. 


## Linear Transformations  {#sec-linear-embed}

The barley data set presents two common challenges for many machine learning techniques:

1. The number of original predictors (`r ncol(barley_train) - 1`) is fairly large . 
2. The features are highly correlated. Techniques that require inverting the covariance matrix of the features, such as linear regression, will become numerically unstable or may not be able to be fit when features are highly correlated. 

What can we do if we desire to use a modeling technique that is adversely affected by either (or both) of these characteristics?  

Dimensionality reduction is one technique that can help with the first issue (an abundance of columns). As we’ll see below, we might be able to extract new features ($X^*$) such that the new features optimally summarize information from the original data ($X$). 

For the problem of highly correlated predictors, some embedding methods can additionally _decorrelated_ the predictors by deriving embedded features with minimal correlation. 

The three embedding methods we’ll discuss first are linear in a mathematical sense because they transform a table of numeric features into new features that are linear combinations of the original features. These new linear combinations are often called _scores_. This transformation uses the equation.

$$ \underset{n\times m}{\boldsymbol{X}^*} = \underset{n\times p}{\boldsymbol{X}}\ \underset{p\times m}{\boldsymbol{Q}} $$

where $\boldsymbol{Q}$ is a matrix that translates or embeds the original features into a potentially lower dimensional space ($m < p$) without losing much information. It is easy to see why this matrix operation is linear when the elements of the $\boldsymbol{X}$ and $\boldsymbol{Q}$ matrices are expanded. For example, the first score for the $i^{th}$ sample would be:

$$ 
x^*_{i1} = q_{11} x_{i1} + q_{21}x_{i2} + \ldots +  q_{p1}x_{ip} 
$$ {#eq-pca-linear-combo}

The $q_{ij}$ values are often referred to as the _loadings_ for each predictor in the linear combination. 

In this section, we will review principal components analysis (PCA), independent component analysis (ICA), and partial least squares (PLS), which are fundamental linear embedding techniques that are effective at identifying meaningful embeddings but estimate $A$ in different ways. PCA and ICA extract embedded features using only information from the original features; they are unsupervised. Conversely, PLS uses the predictors and outcome; it is a supervised technique. 

There are many other linear embedding methods that can be used. For example, non-negative matrix factorization [@lee1999learning;@lee2000algorithms]is an embedding method that is useful when the data in $X$ are integer counts or other values that cannot be negative. 

To start, we’ll focus on the most often used embedding method: PCA.

### Principal Component Analysis

```{r}  
#| label: pca-rec
#| echo: false
#| cache: true
#| warning: false

barley_rec <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  prep()

barley_preproc <- bake(barley_rec, barley_train)

barley_linear_trans_base <-
  recipe(barley ~ ., data=barley_preproc) 

barley_pca <-
  barley_rec %>%
  step_pca(all_numeric_predictors(), num_comp = 20, id="PCA")

barley_pca_estimates <- prep(barley_pca, training=barley_preproc)
barley_pca_data <- bake(barley_pca_estimates, barley_preproc)
barley_cumulative_variance <- 
  tidy(barley_pca_estimates, id="PCA", type="variance") %>% # OR number=3.  Do the same for ICA
  dplyr::filter(terms=="cumulative percent variance") %>%
  dplyr::filter(component <= 10) %>%
  mutate(previous_cum_var = dplyr::lag(value, n=1)) %>%
  mutate(previous_cum_var = ifelse(component == 1, 0, previous_cum_var)) %>%
  mutate(pct_total_var = value - previous_cum_var)
```


```{r}  
#| label: pca-rec2
#| echo: false
#| cache: true
#| warning: false

barley_norm_rec <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  prep()

barley_norm_train <- bake(barley_norm_rec, barley_val)

barley_pca <-
  barley_norm_rec %>%
  step_pca(all_numeric_predictors(), num_comp = 4, id = "pca") %>% 
  prep()

barley_pca_val <- bake(barley_pca, barley_val)

barley_cumulative_variance <-
  tidy(barley_pca, id = "pca", type = "variance") %>% 
  dplyr::filter(terms == "cumulative percent variance" & component <= 10) %>%
  mutate(
    previous_cum_var = dplyr::lag(value, n = 1), 
    previous_cum_var = ifelse(component == 1, 0, previous_cum_var), 
    pct_total_var = value - previous_cum_var
  )
```


```{r}  
#| label: ica-rec2
#| echo: false
#| cache: true
#| warning: false

# For reproducibility, initialize with our own random numbers. 

set.seed(228)
initial_loadings <- matrix(rnorm(4^2), ncol = 4)

barley_ica <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_ica(
    all_numeric_predictors(),
    num_comp = 4,
    id = "ica",
    seed = 538,
    options = list(
      method = "C",
      w.init = initial_loadings,
      maxit = 1000
    )
  ) %>%
  prep()

barley_ica_val <- bake(barley_ica, barley_val)
```

```{r}  
#| label: pls-rec2
#| echo: false
#| cache: true
#| warning: false

barley_pls <-
  barley_norm_rec %>%
  step_pls(all_numeric_predictors(), outcome = "barley", num_comp = 4, id = "pls") %>% 
  prep()

barley_pls_val <- bake(barley_pls, barley_val)
```


```{r}
#| label: all-linear-results
#| echo: false
#| warning: false
pivot_me <- function(x, method) {
  x %>% 
    add_rowindex() %>% 
    pivot_longer(cols = c(-barley, -.row), names_to = "label", values_to = "value") %>% 
    mutate(
      component_num = gsub("[a-zA-Z]", "", label),
      component_num = as.numeric(component_num),
      Method = method)
}

all_scores_val <- 
  bind_rows(
    pivot_me(barley_pca_val, "PCA"),
    pivot_me(barley_ica_val, "ICA"),
    pivot_me(barley_pls_val, "PLS")
  ) %>%
  mutate(
    value = ifelse(label == "PLS1", -value, value)
  )

all_loadings <- 
  bind_rows(
    tidy(barley_pca, id = "pca"), 
    tidy(barley_ica, id = "ica"),
    tidy(barley_pls, id = "pls") %>% 
      # standardize signs
      mutate(value = ifelse(component == "PLS2", -value, value))
  ) %>% 
  mutate(
    index = gsub("wvlgth_", "", terms),
    index = as.integer(index),
    component_number = gsub("[a-zA-Z]", "", component),
    component_number = as.numeric(component_number),
    # make signs more consistent across methods
    value = ifelse(id == "pca" & component_number == 2, -value, value),
  ) %>% 
  dplyr::select(-terms) %>% 
  dplyr::filter(component_number <= 4) %>% 
  mutate(
    component_number = format(component_number),
    id = factor(id, levels = c("pca", "ica", "pls"))
  ) %>% 
  full_join(wave, by = "index")

if ( !file.exists("../RData/barley_linear_embeddings.RData") ) {
  save(all_scores_val, all_loadings, file = "../RData/barley_linear_embeddings.RData")
}
```

Karl Pearson introduced PCA over a century ago, yet it remains a fundamental dimension reduction method [@jolliffe2016principal].  Its relevance stems from the underlying objective of the technique: to find linear combinations of the original predictors that maximize amount of variation in the new features^[As is, this is far too general. We could maximize the variance by making every value of $Q$ equal to $\infty$. PCA, and other methods, impose an implicit constraint to limit the elements of $Q$. In this case, PCA constrains the loading vectors to have unit length.]. 

::: {.note-box}
From a statistical perspective, variation is synonymous with information.  
:::

Simultaneously, the scores produced by PCA (i.e., $X^*$) are required to be orthogonal to each other. The important side benefit of this technique is that _PCA scores are uncorrelated_. This is very useful for modeling techniques that need the predictors to be relatively uncorrelated: multiple linear regression, neural networks, support vector machines, and others.

In a way, PCA components have an order or hierarchy. The first principal component has a higher variance than any other component. The second component has a higher variance than subsequent components, and so on. We can think of this process as one of deflation; the first component extracts the largest sources of information/variation in the predictors set. The second component extracts as much as possible for whatever is left behind by the first, and so on. For this reason, we can track how much variation in the original data each component accounts for. We often use the phrase that "PCA chases variation" to achieve its goals.

Suppose we start with $p$ columns^[Assuming that the $p$ columns are linearly independent.] in our data. In that case, we can produce up to $p$ PCA components, and their accumulated variation will eventually add up to the total variation in the original $X$.

While PCA can deliver new features with the desirable characteristics, the technique must be used with care.  Specifically, we must understand that PCA seeks to find embeddings without regard to any further understanding of the original features (such as the response).  Hence, PCA may generate embeddings that are ignorant of the modeling objective (i.e., they lack predictive information).

#### Preparing for PCA {.unnumbered}

Because PCA seeks linear combinations of predictors that maximize variability, it will naturally first be drawn to summarizing predictors with more variation.  If the original predictors are on measurement scales that differ in magnitude, then the first components will focus on summarizing the higher magnitude predictors.  This means that the PCA scores will focus on the scales of the measurements (rather than their intrinsic value).  If, by chance, the most relevant feature for predicting the response also has the largest numerical values, the new features created by PCA will retain the essential information for predicting the response.  However, if the most important features are in the smallest units, then the top features created by PCA will be much less effective. 

In most practical machine learning applications, features are on vastly different scales. We suggest using the tools from @sec-common-scale to transform the data so that they have the same units. This prevents PCA from focusing dimension reduction simply on the measurement scale.

Additionally, the distributions of each feature may show considerable skewness or outliers. While there are no specific distributional requirements to use PCA, it is focused on variance and variance calculations often involve squared terms (e.g., $(x_i - \bar{x})^2$). This type of computation can be very sensitive to outliers and skewness; resolving these issues prior to PCA is recommended. See the methods previously described in @sec-skewness. When outliers are a specific concern, the spatial-sign transformation might be a good idea [@visuri2000sign;@croux2002sign].


#### How does PCA work? {.unnumbered}

Principal component analysis is focused on understanding the relationships between the predictor columns. PCA is only effective when there are correlations between predictors. Otherwise, each new PCA feature would only highlight a single predictor, and you would need the full set of PCA features to approximate the original columns. Conversely, data sets with a handful of intense between-predictor relationships require very few predictors to represent the source columns. 

The barley data set has abnormally high correlations between each predictor. These tend to be autoregressive; the correlations between predictors at adjacent wavelengths are very high, and the correlation between a pair of predictors diminishes as you move away from their locations on the spectrum. For example, the correlation between the first and second predictor (in their raw form) is essentially `r signif(cor(barley_train[, 2:3])[1,2], 8)` while the corresponding correlation between the first and fiftieth columns is `r round(cor(barley_train[, c(2, 51)])[1,2], 2)`. We'll explore the latter pair of columns below. 

Before proceeding, let's go on a small "side-quest" to talk about the mathematics of PCA. 

::: {.mathy-box}
The remainder of this subsection discussed a mathematical operation called the singular value decomposition (SVD). PCA and many other computations rely on on the SVD. It is a fundamental linear algebra calculation. 

We'll describe it loosely with a focus on how it is used. The concept of eigenvalues will come up again in the next section and in subsequent chapters. @banerjee2014linear and @aggarwal2020linear are thorough but helpful resources for learning more. 
:::


The SVD process is intended to find a way to _rotate_ the original data in a way that the resulting columns are orthogonal to one another. It takes a square matrix as an input and can decompose it into several pieces: 

$$\underset{p\times p}{A} = \underset{p\times p}{Q}\quad\underset{p\times p}{\Lambda}\quad \underset{p\times p}{Q'}$$

The results of this computation are the $p$ eigenvectors $\mathbf{q}_j$ and eigenvalues $\lambda_j$. The eignevectors tell you about the directions in which values of $A$ are moving and the eigenvalues describe the corresponding magnitudes (e.g. how far they go in their corresponding direction). 

The transformation works with PCA by using the covariance matrix as the input $A$. We are trying to find trends in the relationships between variables and the SVD is designed to translate the original matrix to one that is orthogonal (that is, uncorrelated). This aspect of the SVD is how it connects to PCA. If we want to approximate our original data with new features that are uncorrelated, we need a transformation to orthogonality.

The matrix $Q$ houses the $p$ possible eigenvectors. These are the values by which you multiply the original matrix $A$ to achieve an orthogonal version. For PCA, they are the loading values shown in @eq-pca-linear-combo. The matrix $\Lambda$ is a diagonal matrix whose $p$ values are the eigenvalues. It turns out that the eigenvalues represent the amount of variation (i.e., information) captured by each component. 

If we conduct the SVD (and PCA) on the covariance matrix of the data, we are again subject to issues around potentially different units and scales of the predictors. Previously, we suggested to center and scale the data prior to PCA. The equivalent approach here is to conduct the SVD on the correlation matrix of the data. Similar to our recommendations on standardization, we recommend using the correlation as the input to PCA. 

One interesting side-effect of using the covariance or correlation matrix as the input to PCA is related to missing data (discussed in more detail in @sec-missing-data). The conventional sample covariance (or correlation) matrices can be computed for each matrix element (i.e., without using matrix multiplication). Each covariance or correlation can be computed with whatever rows of the two predictors have pairwise-complete results. This means that we do not have to globally drop specific rows of the data when a small number of columns contain missing data. 


#### A two dimensional example {.unnumbered}

```{r}
#| label: barley-2d-data
#| include: false

load("../RData/barley_2d_pca_.RData")
angles <- min(opt_variances$angle2)
angles <- c(angles, angles + 180)
angles_txt <- paste0(round(angles, 0), "$^{\\circ}$")
angles_txt <- knitr::combine_words(angles_txt)

q_11 <- 
  barley_pca_coefs %>% filter(terms == "wvlgth_001" & component == "PC1") %>%
  pluck("value") %>% round(2)
q_12 <- 
  barley_pca_coefs %>% filter(terms == "wvlgth_050" & component == "PC1") %>%
  pluck("value") %>% round(2)
q_21 <- 
  barley_pca_coefs %>% filter(terms == "wvlgth_001" & component == "PC2") %>%
  pluck("value") %>% round(2)
q_22 <- 
  barley_pca_coefs %>% filter(terms == "wvlgth_050" & component == "PC2") %>%
  pluck("value") %>% round(2)
```

To demonstrate, we chose two predictors in the barley data. They correspond to wavelengths^[These wavelengths correspond to the 1<sup>st</sup> and 50<sup>th</sup> predictors.] 1,300 and 1,398 shown in @fig-barley-data(b).  In the data, these two predictors were preprocessed using the ORQ procedure (@sec-skewness). The standardized versions of the predictors have a correlation of `r barley_val_cor` and are shown in @fig-pca-rotation(a). 

PCA was performed on these data, and the results are animated in @fig-pca-rotation(b). This visualization shows the original data (colored by their outcome data) and then rotates it around the center position. The PCA transformation corresponds to the rotation where the x-axis has the largest spread. The variance is largest at _two_ angles (`r angles_txt`) since the solution for PCA is _unique up to sign_. Both possible solutions are correct, and we usually pick one. For one solution, the PCA loading coefficients are: 

\begin{align}
x^*_{i1} &= `r q_11` x_{i1} `r q_21` x_{i2} \notag \\
x^*_{i2} &= `r q_21` x_{i1} + `r q_22` x_{i2} \notag
\end{align}
 
Because of this illustrative example’s low-dimensional aspect, this solution is excessively simple. Ordinarily, the loading values take a great many values.  

::: {#fig-pca-rotation layout-ncol=2}

```{r}
#| label: barley-ord-orig
#| echo: false
#| warning: false
#| fig-height: 4
#| fig-width: 4
#| out-width: 100%
#| dev: png
#| dev-args:
#|   bg: transparent
barley_norm_train %>%
  ggplot(aes(wvlgth_001, wvlgth_050, col = barley)) +
  geom_point(cex = 2, alpha = 1 / 3, show.legend = FALSE) +
  scale_color_viridis(option = "viridis") +
  theme_transparent() +
  labs(x = "Wavelength 1300", y = "Wavelength 1398", title = "(a)") +
  coord_obs_pred()
``` 
```{r}
#| label: pca-rotation-gif
#| echo: false
#| warning: false
#| out-width: 100%

knitr::include_graphics("../premade/anime_barley_pca.gif")
```

A demonstration that PCA is just a rotation of the data. The original predictors (those at the first and fiftieth wavelengths) are in Panel (a). The animation in (b) shows the rotation of two of the predictors, which stops at the two optimal rotations corresponding to the principal components and the original data.

:::


Again,the remarkable characteristic of this specific set of linear combinations is that they are orthogonal (i.e., uncorrelated) with one another.

::: {.note-box}

Using PCA to transform the data into a full set of uncorrelated variables is often called “whitening the data.” It retains all of the original information and is more pliable to many computations. It can be used as a precursor to other algorithms.

:::

Now that we've seen PCA on a small scale, let's look at an analysis of the full set of predictors 

#### PCA on the barley data  {.unnumbered}


We'll repeat the analysis in the preceding section; PCA was calculated on the training set using all predictors. There are `r ncol(barley_train) - 1` feature columns and the same number of possible principal components. The number of new features to retain is a parameter that must be determined.  In practice, the number of new features is often selected based on the percentage of variability that the new features summarize relative to the original features.  @fig-barley-pca-scree displays the new PCA feature number (x-axis) versus the percent of total variability across the original features (commonly called a “scree plot”).  In this graph, three components summarize `r round(barley_cumulative_variance$value[3],1)`% of the total variability.  We often select the smallest number of new features that summarize the greatest variability. Again, this is an unsupervised optimization. To understand how the number of components affects supervised predictors (e.g., RMSE), the number of features can be treated as a tuning parameter   . 

```{r}
#| label: fig-barley-pca-scree
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 3.5
#| fig-cap: "A scree plot of the PCA component number versus the percent of variance explained for the first 10 components of the barley data."

ggplot(barley_cumulative_variance,
       aes(x = component, y = cumsum(pct_total_var))) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = c(1:10)) +
  labs(x = "PCA Component", y = "% Total Variance")
```

Note that the first component alone captured `r round(barley_cumulative_variance$value[1],1)`% of the variation in the `r ncol(barley_train) - 1` training data columns. This reflects the extreme correlation seen in the predictors. 

@fig-linear-scores shows a scatterplot of the principal components colored by the percentage of barley oil.  The figure reveals that the first PCA component delineates the higher barley oil samples from those with less oil.  There also appear to be three different clusters of samples with very low (if any) barley oil. It might be helpful to investigate these samples to ascertain if they are artifacts or caused by some systematic, underlying factors not in the current set of columns. The pairing of the second and fourth components appears to help differentiate lower barely samples from the broader set. The pairing of the third and fourth components doesn’t offer much predictive power on their own.

::: {#fig-linear-scores}

::: {.figure-content}

```{shinylive-r}
#| label: fig-linear-scores
#| out-width: "80%"
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)
library(dplyr)
library(tidyr)
library(purrr)
library(ggforce)

# ------------------------------------------------------------------------------

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

# ------------------------------------------------------------------------------

theme_light_bl<- function(...) {
  
  ret <- ggplot2::theme_bw(...)
  
  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect
  
  ret$legend.position <- "top"
  
  ret
}

# ------------------------------------------------------------------------------

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = TRUE,
    column(
      width = 10,
      radioButtons(
        inputId = "method",
        label = "Embedding",
        choices = list("PCA" = "PC", "ICA" = "IC", "PLS" = "PLS"),
        inline = TRUE
      )
    )
  ),  
  
  as_fill_carrier(plotOutput("scores"))
)

server <- function(input, output) {
  load(url("https://raw.githubusercontent.com/aml4td/website/embeddings_kj/RData/barley_linear_embeddings.RData"))
  
  
  output$scores <-
    renderPlot({
      dat <- 
        all_scores_val %>% 
        dplyr::filter(grepl(input$method, Method)) %>% 
        dplyr::select(-component_num, -Method) %>% 
        tidyr::pivot_wider(id_cols = c(barley, .row), names_from = label, values_from = value)
      
      p <- dat %>% 
        ggplot(aes(x = .panel_x, y = .panel_y)) + 
        geom_point(aes(col = barley), alpha = 1 / 3, cex = 1) + 
        geom_autodensity(alpha = 1 / 2) +
        facet_matrix(vars(c(-barley, -.row)), layer.diag = 2, grid.y.diag = FALSE) +
        scale_color_viridis(option = "viridis") +
        theme_light_bl()

      
      print(p)
      
    })
  
}

app <- shinyApp(ui = ui, server = server)
```
:::

A visualization of the four new features for different linear embedding methods. The data shown are the validation set results.

:::

One note about this visualization. The axis scales are not common across panels (for ease of illustration). If we were to keep a common scale, the later components would appear to have very little effect due to the flatness of the resulting figures.  When looking at the scores, we suggest keeping a common scale, which will most likely be the scale of the first component. This will help avoid over-interpreting patterns in the later components. 

For PCA, it can be very instructive to visualize the loadings for each component. This can help in several different ways. First, it can tell which predictors dominate each specific new PCA component. If a particular component ends up having a strong relationship with the outcome, this can aid our explanation of how the model works. Second, we can examine the magnitude of the predictors to determine some of the relationships between predictors. If a group of predictors have approximately the same loading value, this implies that they have a common relationship with one another. This, on its own, can be a significant aid when conducting exploratory data analysis on a high-dimensional data set. @fig-linear-loadings shows the relationship between the PCA loadings and each predictor’s position on the spectrum.

::: {#fig-linear-loadings}

::: {.figure-content}

```{shinylive-r}
#| label: fig-linear-loadings
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)
library(dplyr)

# ------------------------------------------------------------------------------

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

# ------------------------------------------------------------------------------

theme_light_bl<- function(...) {
  
  ret <- ggplot2::theme_bw(...)
  
  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect
  
  ret$legend.position <- "top"
  
  ret
}

# ------------------------------------------------------------------------------


ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(xs = c(-2, 4, 4, -2), sm = 4),
    column(
      width = 4,
      checkboxGroupInput(
        inputId = "method",
        label = "Embedding",
        choices = list("PCA" = "pca", "ICA" = "ica", "PLS" = "pls"),
        selected = "pca"
      )
    ),
    column(
      width = 4,
      checkboxGroupInput(
        inputId = "comps",
        label = "Component",
        choices = list("1" = "1", "2" = "2", "3" = "3", "4" = "4"),
        selected = paste0(1:4),
        inline = TRUE
      )
    )
  ),
  as_fill_carrier(plotOutput("loadings"))
)


server <- function(input, output) {
  load(url("https://raw.githubusercontent.com/aml4td/website/embeddings_kj/RData/barley_linear_embeddings.RData"))
  
  
  output$loadings <-
    renderPlot({
      dat <- 
        all_loadings %>% 
        dplyr::filter(id %in% input$method & component_number %in% input$comps)
      p <-
        ggplot(dat, aes(x = wavelength, y = value, col = component_number)) +
        geom_hline(yintercept = 0, lty = 3) +
        geom_line(alpha = 3 / 4, linewidth = 1) +
        labs(y = "Loading Value") +
        scale_color_brewer(palette = "Dark2") +
        theme_light_bl() 
      if ( length(input$method) > 1) {
        p <- p + facet_wrap(~ id, nrow = 1)
      }
      print(p)
    })
}

app <- shinyApp(ui = ui, server = server)
```
:::

The loadings for the first four components of each linear embedding method as a function of wavelength.

:::

For the first component, wavelengths at the low ends of the spectrum have a relatively small impact that increases as you get to about 1,400 nm. At this point, the predictors have a constant effect on that component. The second PCA component is almost the opposite; it emphasizes the smaller wavelengths while predictors above 1,400 nm fluctuate around zero. The third and fourth components emphasize different areas of the spectrum and are generally different than zero.

#### How to incorporate PCA into the model {.unnumbered}

The barley data demonstrates that PCA can effectively capture the training set information in a smaller predictor set when there is a correlation between predictors. The number of components to retain depends on the data. It is a good idea to optimize the number of components to retain via model tuning. 

::: {.note-box}

Note that “dimension reduction” is _not_ the same as feature selection. The PCA components described above are functions of all of the predictors. Even if you only need a handful of PCA components to approximate the training set, the original predictor set is still required when predicting new samples. 
  
:::

Another aspect of using PCA for feature extraction is the scale of the resulting scores. @fig-linear-scores demonstrates that the first component has the largest variance, resulting in a wider range of training set values. For the validation set scores, the variance of PC<sub>1</sub> is `r round(var(barley_pca_val$PC1) / var(barley_pca_val$PC2), 0)`-fold larger than the variance of PC<sub>2</sub>. Even though the scores are unitless, there may be issues for models that expect the predictors to have a common scale, similar to the requirement for PCA itself. In these cases, it may be advisable to standardize the PCA components to have the same range before serving them to the supervised ML model.


#### Non-standard PCA {.unnumbered}

Various methods exist for estimating the PCA loadings. Some are well-suited to different dimensions of the predictor’s data. For example, "small $n$, large $p$" data sizes can make PCA computations difficult. @wu1997kernel and others describe a _kernel_ method more appropriate for very wide data sets. @scholkopf1998nonlinear extended this even further but using nonlinear kernel methods^[We will see similar methods in @sec-svm-cls.]. Still, others use modified objective functions to compensate for some potential deficiencies of the canonical technique.

For example, the standard loading matrix (i.e., the eigenvectors) are _dense_. It is possible that the loadings for some predictors shouldn’t affect specific components. The SVD might estimate these to be close to zero but not exactly zero. Sparse PCA techniques can, through various means, estimate some loading values to be exactly zero, indicating that the predictor has no functional effect on the embedding. 

For example, @shen2008sparse take a penalization approach that puts a high price for loadings to have very large values. This regularization method can shrink the values towards zero and make some absolute zero. This is similar to the regularization approach seen in @sec-effect-encodings and is directly related to techniques discussed in @sec-logistic-reg, @sec-nnet-cls, and @sec-linear-reg. This option can coerce values across the loadings to be zero. It is unlikely to force a particular predictor’s values to be zero across all components. In other words, it may not completely erase the effect of a predictor from the embedding altogether. 

Another method is based on a Bayesian approach [@ning2021spike], where the loading values are assumed to come from a mixture of two different distributions. One has a sharp “spike” around zero, and the other is a flat, wide distribution that encompasses a wide range of values (called the “slab”). The method then estimates the parameters to favor one distribution or the other and sets some proportion of the loadings to zero. 

Additionally, there are PCA variants for non-numeric data, such as categorical predictors. Probabilistic PCA [@tipping1999probabilistic;@pmlrvR4schein03a] uses a PCA generalization to reduce the dimensions of qualitative data. 


```{r}  
#| label: ica-rec
#| echo: false
#| cache: true
#| warning: false
barley_ica <-
  barley_linear_trans_base %>%
  step_ica(all_numeric_predictors(), num_comp = 20, id="ica")

barley_ica_estimates <- prep(barley_ica, training=barley_preproc)
barley_ica_data <- bake(barley_ica_estimates, barley_preproc)

barley_ica_data_for_pairs_plot <-
  barley_ica_data %>%
  dplyr::select(barley, IC01, IC02, IC03, IC04) %>%
  mutate(barley_bin = ifelse(barley < as.numeric(quantile(barley, 0.25)), "Q1",
                             ifelse(barley >= as.numeric(quantile(barley, 0.25)) & barley < as.numeric(quantile(barley, 0.5)), "Q2",
                                    ifelse(barley >= as.numeric(quantile(barley, 0.5)) & barley < as.numeric(quantile(barley, 0.75)), "Q3", "Q4")))) %>%
  mutate(barley_bin = factor(barley_bin, levels=c("Q1", "Q2", "Q3", "Q4"))) %>%
  dplyr::select(-barley)

barley_linear_trans_base <-
  recipe(barley ~ ., data=barley_preproc) 

barley_pls <-
  barley_linear_trans_base %>%
  step_pls(all_numeric_predictors(), outcome="barley", num_comp = 2)

barley_pls_estimates <- prep(barley_pls, training=barley_preproc)
barley_pls_data <- bake(barley_pls_estimates, barley_preproc)
  
barley_pca_data_long <-
  barley_pca_data %>%
  dplyr::select(barley, PC01, PC02) %>%
  rename(Comp1 = PC01,
         Comp2 = PC02) %>%
  pivot_longer(cols=c("Comp1", "Comp2"),  values_to = "score", names_to = "Component") %>%
  mutate(Method = "PCA")

barley_ica_data_long <-
  barley_ica_data %>%
  dplyr::select(barley, IC01, IC02) %>%
  rename(Comp1 = IC01,
         Comp2 = IC02) %>%
  pivot_longer(cols=c("Comp1", "Comp2"),  values_to = "score", names_to = "Component") %>%
  mutate(Method = "ICA")
```

```{r}  
#| label: pls-rec
#| echo: false
#| cache: true
#| warning: false
barley_pls_data_long <-
  barley_pls_data %>%
  rename(Comp1 = PLS1,
         Comp2 = PLS2) %>%
  pivot_longer(cols=c("Comp1", "Comp2"),  values_to = "score", names_to = "Component") %>%
  mutate(Method = "PLS")

for_comparison_figure <-
  bind_rows(barley_pca_data_long, barley_ica_data_long, barley_pls_data_long) %>%
  mutate(Component = factor(Component, levels=c("Comp1", "Comp2")),
         Method = factor(Method, levels=c("PCA", "ICA", "PLS")))

component1_correlations <-
  tibble(Method = c("PCA", "ICA", "PLS"),
         R = c(round(cor(barley_pca_data$barley, barley_pca_data$PC01), 2),
               round(cor(barley_ica_data$barley, barley_ica_data$IC01), 2),
               round(cor(barley_pls_data$barley, barley_pls_data$PLS1), 2)))

barley_pca_data_2_comp <-
  barley_pca_data %>%
  dplyr::select(barley, PC01, PC02) %>%
  rename(Comp1 = PC01,
         Comp2 = PC02) %>%
  mutate(Method = "PCA")

barley_ica_data_2_comp <-
  barley_ica_data %>%
  dplyr::select(barley, IC01, IC02) %>%
  rename(Comp1 = IC01,
         Comp2 = IC02) %>%
  mutate(Method = "ICA")

barley_pls_data_2_comp <-
  barley_pls_data %>%
  rename(Comp1 = PLS1,
         Comp2 = PLS2) %>%
  mutate(Method = "PLS")

two_component_comparison <-
  bind_rows(barley_pca_data_2_comp, barley_ica_data_2_comp, barley_pls_data_2_comp) %>%
  mutate(Method = factor(Method, levels=c("PCA", "ICA", "PLS")))

#Get correlations between first two components for each method
two_component_correlations <-
  two_component_comparison %>%
  group_by(Method) %>%
  summarise(r = cor(Comp1, Comp2)) %>%
  ungroup()

#First PCA, ICA, and PLS component compared to the 4 selected samples
three_wavelengths <-
  barley_train %>%
  dplyr::select(wvlgth_001, wvlgth_250, wvlgth_550, barley)

pca_wavelength <-
  bind_cols(three_wavelengths,
            barley_pca_data %>% dplyr::select(PC01)) %>%
  rename(Comp1 = PC01) %>%
  mutate(Method = "PCA")

ica_wavelength <-
  bind_cols(three_wavelengths,
            barley_ica_data %>% dplyr::select(IC01)) %>%
  rename(Comp1 = IC01) %>%
  mutate(Method = "ICA")

pls_wavelength <-
  bind_cols(three_wavelengths,
            barley_pls_data %>% dplyr::select(PLS1)) %>%
  rename(Comp1 = PLS1) %>%
  mutate(Method = "PLS")

comp1_wavelength_comparison <-
  bind_rows(pca_wavelength, ica_wavelength, pls_wavelength) %>%
  rename(`1300` = wvlgth_001,
         `1900` = wvlgth_250,
         `2398` = wvlgth_550) %>%
  pivot_longer(cols=c(`1300`, `1900`, `2398`), values_to="Measurement", names_to="Wavelength") %>%
  mutate(Wavelength = factor(Wavelength, levels=c("1300", "1900", "2398")),
         Method = factor(Method, levels=c("PCA", "ICA", "PLS")))
```


### Independent Component Analysis

ICA has roots in signal processing, where the observed signal that is measured is a combination of several different input signals.  

For example, electroencephalography (EEG) uses a set of electrodes on a patient’s scalp to noninvasively measure the brain’s electrical activity over time. The electrodes are placed on specific parts of the scalp and measure different brain regions (e.g., prefrontal cortex etc.). However, since the electrodes are near others, they can often measure mixtures of multiple underlying signals.

The objective of independent component analysis is to identify linear combinations of the original features that are _statistically independent_ of each other [@nordhausen2018independent;@hyvarinen2000independent]. 

::: {.note-box}

Two variables can be uncorrelated but still have a relationship (most likely nonlinear). For example, suppose we have a variable $z$ that is a sequence of evenly spaced values between -2 and 2.  The correlation between $z$ and $z^2$ is very close to zero but there is a clear and precise relationship between them, therefore the variables are not independent.

:::

In practice, satisfying the statistically independence requirement of ICA can be done several ways.  One approach maximizes the "non-Gaussianity" of the resulting components. As shown by @hyvarinen2000independent, 

> The fundamental restriction in ICA is that the independent components must be nongaussian for ICA to be possible.

There are different ways to measure non-Gaussianity. The _fastICA_ algorithm uses an information theory statistic called _negentropy_ [@hyvarinen2000independent]. Another approach called InfoMax uses neural networks to estimate a different information theory statistic. 

Because ICA's objective requires statistical independence among components, it will generate features different from those of PCA.  Unlike PCA, which orders new components based on summarized variance, ICA's components have no natural ordering.  This means that ICA may require more components than PCA to find the ones related to the outcome. 

The predictors should be centered (and perhaps scaled) before estimating the ICA loadings. Additionally, some ICA algorithms internally add a PCA step to “whiten” the data before computing the ICA loadings (using all possible PCA components). Since independent component analysis thrives on non-Gaussian data, we should avoid transformations that induce a more symmetric distribution. For this reason, we will not use the ORD transformation to preprocess the predictors. Finally, the ICA loadings are often initialized before training using random values. If this randomness is not controlled, different results will likely occur from training run to training run.

```{r}
#| label: ica-text
#| include: false

# Due to the variability of ICA to input changes and random noise, 
# automate the discussion points (to some degree)

ica_marginals <-
  all_scores_val %>%
  filter(Method == "ICA") %>%
  mutate(binned = factor(ifelse(barley > .1, "some", "none"))) %>%
  summarize(
    low = mean(value[binned == "none"]),
    high = mean(value[binned == "some"]),
    .by = c(component_num)
  ) %>%
  mutate(
    difference = abs(high - low),
    num_txt = xfun::numbers_to_words(component_num))

comp_marginals <- ica_marginals$num_txt[ica_marginals$difference > 1 / 4]
num_marginals <- length(comp_marginals)
num_marginals <- xfun::numbers_to_words(num_marginals)
num_marginals <- tools::toTitleCase(num_marginals)

marginal_txt <- cli::pluralize("{num_marginals} components that appear to differentiate levels of barley by themselves (components {comp_marginals})")
```

@fig-linear-scores contains a scatterplot matrix of the first 4 ICA components colored by the outcome. In this figure, `r marginal_txt`. Unsurprisingly, their interaction appears to be the one that visually differentiates the different amounts of barley oil in the validation set. 

The loadings are shown in @fig-linear-loadings and tell an interesting story. The pattern of loadings one and three have similar patterns over the wavelengths. The same situation is the case for components two and four. These two pairs of trends in the loadings are somewhat oppositional to one another. Also, the patterns have little in common with the PCA loadings. For example, the first PCA loading was relatively constant across wavelengths. None of the ICA components show a constant pattern. 

### Partial Least Squares {#numeric-pls}

Both PCA and ICA focus strictly on summarizing information based on the features exclusively. When the outcome is related to the way that the unsupervised techniques extract the embedded features, then PCA and/or ICA can be effective dimension reduction tools. However, when the qualities of variance summarization or statistical independence are not related to the outcome, then these techniques may struggle to identify appropriate embedded features that are useful for predicting the outcome.

Partial least squares (PLS) [@Wold1983] is a dimension reduction technique that identifies embedded features that are optimally linearly related to the outcome. While the objective of PCA is to find linear combinations of the original features that best summarize variability, the objective of PLS is to do the same _and_ to find the linear combinations that are correlated with the outcome [@stone1990continuum]. This process can be thought of as a constrained optimization problem in which PLS is forced to compromise between maximizing information from the predictors and simultaneously maximizing the prediction of the outcome. This means that the outcome is used to focus the dimension reduction process such that the new features have the greatest correlation with the outcome. For more detailed information, we suggest @Geladi1986 and @esposito2013partial. 

Because one of the objectives of PLS is to summarize variance among the original features, the features should be pre-processed in the same way as PCA. 

For the barley data, the first few sets of PLS loads are fairly similar to those generated by PCA. The first three components have almost identical values as their PCA analogs. The fourth component has different loadings, and the correlation between the fourth PCA and PLS scores is `r round(cor(barley_pls_val$PLS4, barley_pca_val$PC4), 2)`. The similarities in the first three scores and loadings between the methods can be seen in @fig-linear-scores and @fig-linear-loadings, respectively. 

If our goal is to maximize predictive performance, fewer features are necessary when using PLS. As we’ll see below, we can sometimes achieve similar performance using PCA or ICA but we will need more features to match what PLS can do with less.

Like principal component analysis, there are many modified versions of partial least squares. For example, @le2008sparse and @le2008sparse describe a sparse estimation routine that can set some loadings equal to absolute zero. Also, @barker2003partial and @Liu2007b adapt the algorithm for classification problems (i.e., qualitative outcomes). 

### Overall Comparisons

```{r}
#| label: pca-bakeoff
#| include: false
#| cache: true

ctrl <- control_grid(save_pred = TRUE)

barley_pca_tune <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>% 
  step_pca(all_numeric_predictors(), num_comp = tune()) %>% 
  step_normalize(all_predictors())

pca_wflow <- workflow(barley_pca_tune, linear_reg())

pca_res <- 
  pca_wflow %>% 
  tune_grid(
    resamples = barley_rs,
    grid = tibble(num_comp = 1:25),
    metrics = metric_set(rmse),
    control = ctrl
  )

set.seed(38)
pca_int <- 
  pca_res %>% 
  int_pctl(times = 2000, alpha = 0.1) %>% 
  mutate(method = "PCA")
```

```{r}
#| label: ica-bakeoff
#| include: false
#| cache: true

barley_ica_tune <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_ica(all_numeric_predictors(), num_comp = tune()) %>% 
  step_normalize(all_predictors())

ica_wflow <- workflow(barley_ica_tune, linear_reg())

ica_res <- 
  ica_wflow %>% 
  tune_grid(
    resamples = barley_rs,
    grid = tibble(num_comp = 1:25),
    metrics = metric_set(rmse),
    control = ctrl
  )

set.seed(38)
ica_int <- 
  ica_res %>% 
  int_pctl(times = 2000, alpha = 0.1) %>% 
  mutate(method = "ICA")
```

```{r}
#| label: pls-bakeoff
#| include: false
#| cache: true

barley_pls_tune <-
  recipe(barley ~ ., data = barley_train) %>%
  step_zv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>% 
  step_pls(all_numeric_predictors(), outcome = "barley", num_comp = tune()) %>% 
  step_normalize(all_predictors())

pls_wflow <- workflow(barley_pls_tune, linear_reg())

pls_res <- 
  pls_wflow %>% 
  tune_grid(
    resamples = barley_rs,
    grid = tibble(num_comp = 1:25),
    metrics = metric_set(rmse),
    control = ctrl
  )

set.seed(38)
pls_int <- 
  pls_res %>% 
  int_pctl(times = 2000, alpha = 0.1) %>% 
  mutate(method = "PLS")
```

How do these three approaches differ in terms of predictive performance? To evaluate this, these embeddings were computed with up to 25 new features and used as the inputs for a linear regression model. After the embeddings and regression parameters were estimated, the validation set RMSE was computed. @fig-barley-linear-bakeoff has the results. The bands around each curve are 90% confidence intervals for the RMSE.

```{r}
#| label: fig-barley-linear-bakeoff
#| echo: false
#| warning: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 4
#| fig-cap: "Results for linear regressions using three different linear embeddings."

bind_rows(pca_int, ica_int, pls_int) %>% 
  mutate(Method = factor(method, levels = c("PCA", "ICA", "PLS"))) %>% 
  ggplot(aes(num_comp, .estimate,)) + 
  geom_line(aes(col = Method)) +
  geom_point(aes(col = Method, pch = Method)) +
  geom_ribbon(aes(ymin = .lower, ymax = .upper, fill = Method), alpha = 1 / 7) +
  labs(y = "RMSE (validation)", x = "Number of Components")
```

Overall, the patterns are similar. By the time 25 components are added, there is parity between the methods. However, PLS appears to be more efficient at finding predictive features since its curve has uniformly smaller RMSE values than the others . This isn’t surprising since it is the only supervised embedding of the three. 

PCA and PLS will be discussed in more detail in @sec-colinearity.

## Multidimensional Scaling {#sec-mds}

Multidimensional scaling [@torgerson1952multidimensional] is a feature extraction tool that creates embeddings that try to preserve the geometric distances between training set points. In other words, the distances between points in the smaller dimensions should be comparable to those in the original dimensions. Since the methods in this section use distances, the predictors should be standardized to equivalent units before the embedding is trained. As with PCA, we also recommend transformations to resolve skewness. 

Take @fig-mds-example(a) as an example. There are ten points in two dimensions (colored by three outcome classes). If we were to project these points down to a single dimension, we'd like points that are close in the original two dimensions to remain close when projected down to a single dimension. Panel (c) shows two such solutions. Each does reasonably well with some exceptions (i.e., points six and nine are too close for non-Metric MDS). 

```{r}
#| label: mds-example-computations
#| include: false

pens <- penguins[complete.cases(penguins),]

n <- 10
set.seed(119)
pen_subset <-
  pens %>%
  sample_n(n) %>%
  select(length = bill_length_mm, depth = bill_depth_mm, 
         class = species) %>%
  arrange(length, depth) %>%
  mutate(
    sample = row_number(),
    length = as.vector(scale(length)),
    depth = as.vector(scale(depth)))

ar <- diff(range(pen_subset$length)) / diff(range(pen_subset$depth))

### Compute distances and KNN

pen_dist <-
  dist(pen_subset[, 1:2]) %>%
  as.matrix()

dist_alt <- pen_dist
diag(dist_alt) <- 10^38

nearest <- vector(mode = "list", length = n)
k <- 2

for (i in 1:n) {
  nearest[[i]] <- order(dist_alt[i,])[1:k]
}

### 

mds_1D <- sammon(pen_dist, k = 1, tol = 0.000001, niter = 1000)$points[, 1]
mds_1D_df <-
  tibble(
    MDS_1 = as.vector(scale(mds_1D)),
    class = pen_subset$class,
    sample = pen_subset$sample,
    method = "Non-Metric MDS"
  )

### 

iso_data <- 
  recipe(class ~ length + depth, data = pen_subset) %>% 
  step_isomap(all_numeric_predictors(), neighbors = 2, num_terms = 1) %>% 
  prep()

isomap_1D_df <-
  tibble(
    MDS_1 = as.vector(scale(iso_data$steps[[1]]$res@data@data[,1])),
    class = pen_subset$class,
    sample = pen_subset$sample,
    method = "Isomap"
  )

one_d_df <- bind_rows(mds_1D_df, isomap_1D_df)
```
```{r}
#| label: fig-mds-example
#| echo: false
#| dev: "ragg_png"
#| out-width: 70%
#| fig-width: 8
#| fig-height: 8
#| fig-cap: "(a) A collection of samples associated with three outcome classes. (b) A diagram of the two nearest neighbors for each data point. (c) A one-dimensional projection using two different methods: non-metric MDS and Isomap."

base_p <- 
  pen_subset %>%
  ggplot(aes(x = length, y = depth)) +
  coord_fixed(ratio = ar) +
  scale_color_brewer(palette = "Dark2")

orig_p <-
  base_p +
  geom_point(
    # pch = 16,
    cex = 8,
    aes(col = class, shape = class),
    show.legend = FALSE
  ) +
  geom_text(aes(label = sample), col = "white", show.legend = FALSE) +
  labs(title = "(a) Original Data", x = "Predictor A", y = "Predictor B")


knn_p <- base_p

for(i in 1:n) {
  tmp <- pen_subset[nearest[[i]], 1:2] %>% rename_with(~paste0(., "_end"))
  dat <- pen_subset %>% slice(rep(i, 2)) %>% bind_cols(tmp)

  knn_p <-
    knn_p +
    geom_segment(data = dat, aes(xend = length_end, yend = depth_end),
                 col = "grey")
}
knn_p <-
  knn_p +
    geom_point(
      # pch = 16,
      cex = 8,
      aes(col = class, shape = class),
      show.legend = FALSE
    ) +
  geom_text(aes(label = sample), col = "white", show.legend = FALSE) +
  labs(title = "(b) 2-Nearest Neighbors", x = "Predictor A", y = "Predictor B") 

### 

  one_d_p <-
    one_d_df %>%
    ggplot(aes(x = MDS_1, y = method)) +
      geom_point(
        # pch = 16,
        cex = 8,
        aes(col = class, shape = class),
        show.legend = FALSE
      ) +
    geom_text(aes(label = sample), col = "white", show.legend = FALSE) +
    labs(title = "(c) 1 Dimensional MDS (two ways)", y = NULL, 
         x = "Embedded Value") +
    scale_color_brewer(palette = "Dark2")
  
### 

layout <- "
AAAAAAAA
AAAAAAAA
AAAAAAAA
AAAAAAAA
AAAAAAAA
AAAAAAAA
#BBBBBB#
#BBBBBB#
"

((orig_p + knn_p) / one_d_p) +
  plot_layout(design = layout)
```

Here we present a few MDS methods, but there are many more. @Ghojogh2023 has an excellent review of an assortment of methods and their nuances. 

Some MDS methods compute all pairwise distances between points and use this as the input to the embedding algorithm. This is similar to how PCA can be estimated using the covariance or correlation matrices. One technique, _Non-Metric MDS_ [@kruskal1964multidimensional;@kruskal1964nonmetric;@sammon1969nonlinear], finds embeddings that minimize an objective function called "stress":

$$
\text{Stress} = \sqrt{\frac{\sum\limits^{n_{tr}}_{i = 1}\;\sum\limits^{n_{tr}}_{j = i+1}\left(d(x_i, x_j) - d(x^*_i, x^*_j)\right)^2}{\sum\limits^{n_{tr}}_{i = 1}\;\sum\limits^{n_{tr}}_{j = i+1}d(x_i, x_j)^2}}
$$

The numerator uses the squared difference between the pairwise distances in the original values ($x$) and the smaller embedded dimension ($x^*$). The summations only move along the upper triangle of the distance matrices to reduce redundant computations. @fig-mds-example(c, top row) has the resulting one dimensional projection of our two-dimensional data.

This can be an effective dimension reduction procedure, although there are a few issues. First, the entire matrix of distances is required (with $n_{tr}(n_{tr}-1)/2$ entries). For large training sets, this can be unwieldy and time-consuming. Second, like PCA, it is a global method that uses all data in the computations. We might be able to achieve more nuanced embeddings by focusing on local structures. Finally, it is challenging to apply metric MDS to project new data onto the space in which the original data was projected.

For these reasons, let's take a look at more modern versions of multidimensional scaling. 

### Isomap  {#sec-isomap}

To start, we'll focus on _Isomap_ [@tenenbaum2000global]. This nonlinear MDS method uses a specialized distance function to find the embedded features. First, the _K_ nearest neighbors are determined for each training set point using standard functions, such as Euclidean distance. @fig-mds-example(b) shows the _K_ = 2 nearest neighbors for our example data. Many nearest-neighbor algorithms can be very computationally efficient and their use eliminates the need to compute all of the pairwise distances. 

The connections between neighbors form a _graph structure_ that qualitatively defines which data points are closely related to one another. From this, a new metric called _geodesic distance_ can be approximated. For a graph, we can compute the approximate geodesic distance using the shortest path between two points on the graph. With our example data, the Euclidean distance between points four and five is not large. However, its approximate geodesic distance is greater because the shortest path is through points nine, eight, and seven. @Ghojogh2023 use a wonderful analogy: 

> A real-world example is the distance between Toronto and Athens. The Euclidean distance is to dig the Earth from Toronto to reach Athens directly. The geodesic distance is to move from Toronto to Athens on the curvy Earth by the shortest path between two cities. The approximated geodesic distance is to dig the Earth from Toronto to London in the UK, then dig from London to Frankfurt in Germany, then dig from Frankfurt to Rome in Italy, then dig from Rome to Athens.

The Isomap embeddings are a function of the eigenvalues computed on the geodesic distance matrix. The $m$ embedded features are functions of the first $m$ eigenvectors. Although eigenvalues are associated with linear embeddings (e.g., PCA), nonlinear geodesic distance results in a local nonlinear embedding. @fig-mds-example(c, bottom row) shows the 1D results for the example data set. For a new data point, its nearest-neighbors in the training set are determined so that the approximate geodesic distance can be computed. The estimated eigenvectors and eigenvalues are used to project the new point into the embedding space. 

For Isomap, the number of nearest neighbors and the number of embeddings are commonly optimized. @fig-barley-isomap shows a two-dimensional Isomap embedding for the barley data with varying numbers of neighbors. In each configuration, the higher barley values are differentiated from the small (mostly zero) barley samples. We again see the two or three clusters of data associated with small outcome values. The new features become more densely packed as the number of neighbors increases.

```{r}
#| label: barley-isomap-computations
#| include: false
#| cache: true


neighbor_grid <- c(5, 20, 50)

isomap_grid <- NULL
for (i in neighbor_grid) {
  set.seed(1)
  tmp_rec <- 
    barley_base %>% 
    step_isomap(all_numeric_predictors(), neighbors = i, num_terms = 2) %>% 
    prep()

  tmp_iso <- 
    bake(tmp_rec, new_data = barley_val) %>% 
    mutate(Neighbors = i)
  
  isomap_grid <- bind_rows(isomap_grid, tmp_iso)
}
```

```{r}
#| label: fig-barley-isomap
#| echo: false
#| dev: png
#| out-width: 80%
#| fig-width: 8
#| fig-height: 3.4
#| fig-cap: "Isomap for the barley data for different numbers of nearest neighbors. The training set was used to fit the model and these results show the projections on the validation set. Lighter colors indicate larger values of the outcome."

isomap_grid %>%
  ggplot(aes(Isomap1, Isomap2, col = barley)) +
  geom_point(alpha = 1 / 3, cex = 2)  +
  facet_wrap(~ Neighbors, nrow = 1, labeller = "label_both") +
  scale_color_viridis(option = "viridis") +
  labs(x = "Isomap Embedding #1", y = "Isomap Embedding #2") +
  guides(col = guide_colourbar(barheight = 0.5))
```

### Laplacian Eigenmaps  {#sec-eigenmaps}

There are many other approaches to preserve local distances. One is _Laplacian eigenmaps_ [@belkin2001laplacian]. Like Isomap, it uses nearest neighbors to define a graph of connected training set points. For each connected point, a weight between graph nodes is computed that becomes smaller as the distance between points in the input space increases. The radial basis kernel (also referred to as the "heat kernel") is a good choice for the weighting function^[A note about some notation... We commonly think of the _norm_ notation as $\|\boldsymbol{x}\|_p = \left(|x_1|^p + |x_2|^p + \ldots + |x_n|^p\right)^{1/p}$. So what does the lack of a subscript in $||\boldsymbol{x}||^2$ mean? The convention is the sum of squares:  $||\boldsymbol{x}||^2 = x_1^2 + x_2^2 + \ldots + x_n^2$.]: 

$$
w_{ij} = \exp\left(\frac{-||\boldsymbol{x}_i - \boldsymbol{x}_j||^2}{\sigma}\right)
$$

where $\sigma$ is a scaling parameter that can be tuned. If two points are not neighbors, or if $i = j$, then $w_{ij} = 0$. Note that the equation above uses Euclidean distance. For the 2-nearest neighbor graph shown in @fig-mds-example(b) and $\sigma = 1 / 2$, the weight matrix is roughly

```{r}
#| label: wt-matrix
#| echo: false
#| results: asis

sigma <- 1 / 2
wt_mat <- exp(-pen_dist/sigma)

adj_mat <- pen_dist * 0

for (i in 1:n) {
  nn <-  nearest[[i]]
  for (j in seq_along(nn)) {
    adj_mat[i, nn[j]] <- 1
    adj_mat[nn[j], i] <- 1
  }
}

wt_mat <- adj_mat * wt_mat

rounded_mat <- round(wt_mat, 1)

chr_wt_mat <- format(rounded_mat)
chr_wt_mat[lower.tri(chr_wt_mat)] <- ""
chr_wt_mat[chr_wt_mat == "0.0"] <- "\\0"
chr_wt_mat[6, 3] <- "sym"

tex_1 <- apply(chr_wt_mat, 1, function(x) paste(x, collapse = " & "))
tex_2 <- paste0(tex_1, collapse = "\\\\\n")
cat("$$\n")
cat("\\newcommand{\\0}{{\\color{lightgray} 0.0}}\n")
cat("\\boldsymbol{W} = \\begin{bmatrix}\n")
cat(tex_2, "\n")
cat("\\end{bmatrix}\n$$\n")
```

The use of nearest neighbors means that the matrix can be very sparse and the zero values help define locality for each data point. Recall that samples 2 and 3 are fairly close to one another, while samples 1 and 2 are farther away. The weighting scheme gives the former pair a `r rounded_mat[2,3] / rounded_mat[1,2]`-fold larger weight in the graph than the latter pair. 

Laplacian eigenmaps rely heavily on graph theory. This method computes a _graph Laplacian_ matrix, defined as $\boldsymbol{L} = \boldsymbol{D} - \boldsymbol{W}$ where the matrix $\boldsymbol{D}$ has zero non-diagonal entries and diagonals equal to the sum of the weights for each row. For our example data: 

```{r}
#| label: Laplacian-matrix
#| echo: false
#| results: asis

laplacian <- diag(apply(rounded_mat, 1, sum)) - rounded_mat

rounded_L <- round(laplacian, 1)

chr_L <- format(rounded_L)
chr_L[lower.tri(chr_L)] <- ""
chr_L[chr_L == "0.0" | chr_L == " 0.0"] <- "\\0"
chr_L[6, 3] <- "sym"

tex_1 <- apply(chr_L, 1, function(x) paste(x, collapse = " & "))
tex_2 <- paste0(tex_1, collapse = "\\\\\n")
cat("$$\n")
cat("\\newcommand{\\0}{{\\color{lightgray} 0.0}}\n")
cat("L = \\begin{bmatrix}\n")
cat(tex_2, "\n")
cat("\\end{bmatrix}\n$$\n")
```

The eigenvalues and eigenvectors of this matrix are used as the main ingredients for the embeddings. @Bengio2003advances shows that, since these methods eventually use eigenvalues in the embeddings, they can be easily used to project new data. 

### UMAP {#sec-umap}

The Uniform Manifold Approximation and Projection (UMAP) [@sainburg2020parametric] technique is one of the most popular distance-based methods. Its precursors, stochastic neighbor embedding (SNE) [@hinton2002stochastic] and  Student’s t-distributed stochastic neighbor embedding (t-SNE) [@van2008visualizing], redefined feature extraction, particularly for visualizations. UMAP borrows significantly from Laplacian eigenmaps and t-SNE but has a more theoretically sound motivation. 
 
As with Laplacian eigenmaps, UMAP converts the training data points to a sparse graph structure. Given a set of _K_ nearest neighbors, it computes values similar to the previously shown weights ($\boldsymbol{W}$ matrix), which we will think of as the probability that point $j$ is a neighbor of point $i$:  

$$
p_{j|i} = \exp\left(\frac{-\left(||\boldsymbol{x}_i - \boldsymbol{x}_j||^2 - \rho_i\right)}{\sigma_i}\right)
$$

where $\rho_i$ is the distance from $\boldsymbol{x}_i$ to its closest neighbor, and $\sigma_i$ is a scale parameter that now varies with each sample ($i$). To compute $\sigma_i$, we can solve the equation   
 
$$
\sum_{i=1}^K  \exp\left(\frac{-\left(||\boldsymbol{x}_i - \boldsymbol{x}_j||^2 - \rho_i\right)}{\sigma_i}\right) = \log_2(K)
$$

Unlike the previous weighting system, the resulting $n \times n$ matrix may not be symmetric, so the final weights are computed using $p_{ij} = p_{j|i} + p_{i|j} - p_{j|i}p_{i|j}$. 

UMAP performs a similar weight calculation for the embedded values $\boldsymbol{x}^*$. We'll denote the probability that embedded points $\boldsymbol{x}_i^*$ and $\boldsymbol{x}_j^*$ are connected as $p_{ij}^*$. We'd like the algorthim to estimate these values such that $p_{ij} \approx p_{ij}^*$. 

Numerical optimization methods^[Specifically gradient descent with a user-defined learning rate.] used to estimate the $n \times m$ values $x^*_{ij}$. The process is initialized using a very sparse Laplacian eigenmap, the first few PCA components, or random uniform numbers. The objective function is based on cross-entropy and attempts to make the graphs in the input and embedded dimensions as similar as possible by minimizing:   

$$
CE = \sum_{i=1}^{n_{tr}}\sum_{j=i+1}^{n_{tr}} \left[p_{ij}\, \log\frac{p_{ij}}{p_{ij}^*} + (1 - p_{ij})\log\frac{1-p_{ij}}{1-p_{ij}^*}\right]
$$

Unlike the other embedding methods shown in this section, UMAP can also create supervised embeddings so that the resulting features are more predictive of a qualitative or quantitative outcome value. See @sainburg2020parametric.

Besides the number of neighbors and embedding dimensions, several more tuning parameters exist. The optimization process's number of optimization iterations (i.e., epochs) and the learning rate can significantly affect the final results. A distance-based tuning parameter, often called _min-dist_, specifies how "packed" points should be in the reduced dimensions. Values typically range from zero to one. However, the original authors state:

> We view min-dist as an essentially aesthetic parameter governing the appearance of the embedding, and thus is more important when using UMAP for visualization.

As will be seen below, the initialization scheme is an important tuning parameter. 

For supervised UMAP, there is an additional weighting parameter (between zero and one) that is used to balance the importance of the supervised and unsupervised aspects of the results. A value of zero specifies a completely unsupervised embedding. 

@fig-umap shows an interactive visualization of how UMAP can change with different tuning parameters. Each combination was trained for 1,000 epochs and used a learning rate of 1.0. For illustrative purposes, the resulting embeddings were scaled to a common range. 

::: {#fig-umap}

::: {.figure-content}

```{shinylive-r}
#| label: fig-umap
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)
library(viridis)

# ------------------------------------------------------------------------------

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

# ------------------------------------------------------------------------------

theme_light_bl<- function(...) {

  ret <- ggplot2::theme_bw(...)

  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect

  ret$legend.position <- "top"

  ret
}

# ------------------------------------------------------------------------------

ui <- fluidPage(
  theme = grid_theme,
  fluidRow(

    column(
      width = 4,
      sliderInput(
        inputId = "min_dist",
        label = "Min Distance",
        min = 0.0,
        max = 1.0,
        value = 0.2,
        width = "100%",
        step = 0.2
      )
    ), # min distance
    column(
      width = 4,
      sliderInput(
        inputId = "neighbors",
        label = "Neighbors",
        min = 5,
        max = 45,
        value = 5,
        width = "100%",
        step = 10
      )
    ), # nearest neighbors

    column(
      width = 4,
      sliderInput(
        inputId = "supervised",
        label = "Amount of Supervision",
        min = 0.0,
        max = 0.7,
        value = 0,
        width = "100%",
        step = 0.1
      )
    ),
    fluidRow(
      column(
        width = 4,
        radioButtons(
          inputId = "initial",
          label = "Initialization",
          choices = list("Laplacian Eigenmap" = "spectral", "PCA" = "pca", 
                         "Random" = "random")
        )
      ),
      column(
        width = 6,
        align = "center",
        plotOutput('umap')
      )
    )
  ) # top fluid row
)

server <- function(input, output) {
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/umap_results.RData"))

  output$umap <-
    renderPlot({
      
      dat <-
        umap_results[
          umap_results$neighbors == input$neighbors &
            umap_results$min_dist == input$min_dist &
            umap_results$initial == input$initial &
            # log10(umap_results$learn_rate) == input$learn_rate &
            umap_results$supervised == input$supervised,
        ]

      p <-
        ggplot(dat, aes(UMAP1, UMAP2, col = barley)) +
        geom_point(alpha = 1 / 3, cex = 3) +
        scale_color_viridis(option = "viridis") +
        theme_light_bl() +
        coord_fixed() +
        labs(x = "UMAP Embedding #1", y = "UMAP Embedding #2") +
        guides(col = guide_colourbar(barheight = 0.5))

      print(p)

    })
}

app <- shinyApp(ui = ui, server = server)
```
:::

A visualization of UMAP results for the barley data using different values for several tuning parameters. The points are the validation set values. 

:::

There are a few notable patterns in these results: 

 - The initialization method can heavily impact the patterns in the embeddings. 
 - As with Isomap, there are two or three clusters of data points with small barley values. 
 - When the amount of supervision increases, one or more circular structures form that are associated with small outcome values. 
 - The minimum distance parameter can drastically change the results. 

t-SNE and UMAP have become very popular tools for visualizing complex data. Visually, they often show interesting patterns that linear methods such as PCA cannot. However, they are computationally slow and unstable over different tuning parameter values. Also, it is easy to believe that the UMAP distances between embedding points are important or quantitatively predictive. That is not the case; the distances can be easily manipulated using the tuning parameters (especially the minimum distance). 
 
## Centroid-Based Methods  {#sec-centroids}

Another common approach to creating features is to calculate distances between landmarks or pairs of predictors. For example, the distance to desirable grade schools (or the university campus) could affect house prices in the Ames housing data. Potential predictors can be created that measure how close each home is to these points of interest. To calculate distance, a simple Euclidean metric can be used. However, for spatial data across longer distances, the Haversine metric [@sinnott1984virtues] is a better alternative because it takes into account the curvature of the earth.

Note that distance-based features are often right skewed.  When a metric produces a right-skewed distribution, the log-transformation often helps improve predictive performance when the predictor is truly informative. 

Distance-based features can also be effective for classification models. A centroid is another name for the multivariate mean of a collection of data. It is possible to compute class-specific centroids using only the data from each class. When a new sample is predicted, this distance to each class centroid can be used as a predictor. These features would be helpful when a model could be better at detecting/emulating linear class boundaries. An example of this would be tree-based models; these models have to work hard to approximate linear trends in the data. Supplementing the data with simple centroid features might improve performance. 

Again, there are several choices for the distance metric.  Mahalanobis distance is a good choice when there is not an overwhelming number of predictors:

$$
D_c(\boldsymbol{x}_0) = (\boldsymbol{x}_0 - \boldsymbol{\bar{x}}_{c})' \boldsymbol{S}^{-1}_c (\boldsymbol{x}_0 - \boldsymbol{\bar{x}}_{c})
$$

where $\boldsymbol{x}_0$ is the new data point being predicted, $\boldsymbol{\bar{x}}$ is a vector of sample means, $\boldsymbol{S}$ is the estimated covariance matrix (the subscript of $c$ denotes the class-specific statistics). This metric requires fewer data points within each class than the number of predictors being used. It also assumes that there are no linear dependencies between the predictors. 

When the model has many features, regularizing the centroid distances can be a good approach. This approach, similar to the tools described in @sec-effect-encodings, will shrink the class-specific centroids towards the overall (class-nonspecific) centroid at different rates. If a predictor does have any discriminative ability in the training set, its contribution to the class-specific centroids can be removed. 

We'll let $x_{ij}$ denote sample $i$ ($i=1\ldots n_{tr}$) for predictor $j$ ($j=1\ldots p$). The approach by @tibshirani2003class estimates the standardized difference between the class-specific centroid and the global centroid using the following:

\begin{align}
\delta_{jc} &= \frac{\bar{x}_{jc} - \bar{x}_j}{w_c s_j} &&\text{ where } \notag \\
\bar{x}_{jc} &= \frac{1}{{n_{tr}^c}}\sum_{i=1}^{{n_{tr}^c}} x_{ij}\, I(y_i = c) && \text{\textcolor{grey}{(class-specific centroid elements)}} \notag \\
\bar{x}_{j} &= \frac{1}{n_{tr}}\sum_{i=1}^{n_{tr}} x_{ij}  && \text{\textcolor{grey}{(global centroid elements)}}\notag \\
w_c &= \sqrt{\frac{1}{{n_{tr}^c}}  - \frac{1}{n_{tr}}}  && \text{\textcolor{grey}{{(weights)}}} \notag \\
s_j &= \frac{1}{n_{tr}-C}\sum_{c=1}^C\sum_{i=1}^{n_{tr}} \left(x_{ij} - \bar{x}_{jc}\right)^2 I(y_i = c)  && \text{\textcolor{grey}{(pooled standard deviation for predictor $j$)}}\notag 
\end{align}

where $n_{tr}^c$ is the number of training set points for class $c$ and $I(x)$ is a function that returns a value of one when $x$ is true. 

To shrink this difference towards zero, a tuning parameter $\lambda$ is used to create a modified version of the each predictors contribution to the difference:

\begin{equation}
\delta^*_{jc} = sign(\delta_{jc})\,h(|\delta_{jc}| - \lambda)
\end{equation}

where $h(x) = x$ when $x > 0$ and zero otherwise. If the difference between the class-specific and global centroid is small (relative to $\lambda$), $\delta^*_{jc} = 0$ and predictor $j$ does not functionally affect the calculations for class $c$. The class-specific shrunken centroid is then

\begin{equation}
\bar{x}^*_{jc}= \bar{x}_j + w_c\, s_j\, \delta^*_{jc}
\end{equation}

New features are added to the model based on the distance between $\boldsymbol{x}_0$ and $\boldsymbol{\bar{x}}^*_{c}$. The amount of shrinkage is best optimized using the tuning methods described in later chapters. There are several variations of this specific procedure. @wangImprovedCentroids describe several different approaches and @efron2009empirical demonstrates the connection to Bayesian methods. 

```{r}
#| label: ncs-comp
#| results: hide

set.seed(92)

dat_1 <- MASS::mvrnorm(n = 200, c(1, -1), matrix(c(1, .5, .5, 1), 2, 2)) 
dat_2 <- MASS::mvrnorm(n = 200, c(1, 3), matrix(c(1, -.75, -.75, 1), 2, 2))
dat_3 <- MASS::mvrnorm(n = 200, c(-1, -1), matrix(c(1, 0, 0, 1), 2, 2))

colnames(dat_1) <- colnames(dat_2) <- colnames(dat_3) <- c("x_1", "x_2")

three_class_dat <- 
  bind_rows(
    dat_1 %>% as_tibble() %>% mutate(class = "class 1"),
    dat_2 %>% as_tibble() %>% mutate(class = "class 2"),
    dat_3 %>% as_tibble() %>% mutate(class = "class 3")
  ) 

dat <-
  list(
    x = t(as.matrix(three_class_dat[, 1:2])),
    y = three_class_dat$class,
    geneid = c("x_1", "x_2")
  )
res <- pamr::pamr.train(dat)
cent_overall <- matrix(res$centroid.overall, ncol = 2)
colnames(cent_overall) <- c("x_1", "x_2")
cent_overall <- as_tibble(cent_overall)
cent_class <- as_tibble(t(res$centroids)) %>% mutate(class = paste("class", 1:3))

shrink <- function(threshold, object) {
  res <- pamr::pamr.predict(object, threshold = threshold, type = "centroid")
  out <- as_tibble(t(res))
  out$class <- colnames(res)
  out$threshold <- threshold
  out
}

cent_shrunk <- map_dfr(res$threshold, shrink, object = res)
```
```{r}
#| label: fig-nsc
#| echo: false
#| out-width: 70%
#| fig-width: 6.25
#| fig-height: 4
#| fig-cap: An example of shrunken, class-specific centroids. Panel (a) shows data where there are three classes. Panel(b) demonstrates how, with increasing regularization, the centorids converge towards the global centroid (in black). 
#| fig-alt: Nearest centroids

three_class_points <- 
  three_class_dat %>% 
  ggplot(aes(x_1, x_2)) + 
  geom_point(
    aes(col = class, pch = class),
    cex = 1,
    alpha = 0.65,
    show.legend = FALSE
  ) +
  labs(x = expression(x[1]), y = expression(x[2])) +
  scale_color_brewer(palette = "Dark2") + 
  scale_shape_manual(values = c(21, 22, 24)) +
  ggtitle("(a)")

three_class_path <- 
  three_class_dat %>% 
  ggplot(aes(x_1, x_2)) + 
  geom_point(aes(col = class, pch = class), alpha = 0.27, pch = 1, cex = 1) +
  geom_point(data = cent_class, aes(col = class, pch = class), cex = 3) +
  geom_path(data = cent_shrunk, aes(col = class), show.legend = FALSE) +
  geom_point(data = cent_overall, col = "black", cex = 3, pch = 16, show.legend = FALSE)  +
  labs(x = expression(x[1]), y = expression(x[2])) +
  scale_color_brewer(palette = "Dark2") + 
  scale_shape_manual(values = c(16, 15, 17)) +
  ggtitle("(b)")


three_class_points + three_class_path + plot_layout(guides = 'collect')
```

@fig-nsc shows the impact of regularization for a simple data set with two predictors and three classes. The data are shown in Panel (a), while Panel (b) displays the raw, class-specific centroids. As regularization is added, the lines indicate the path each takes toward the global centroid (in black). Notice that the centroid for the first class eventually does not use predictor $x_1$; the path becomes completely vertical when that predictor is removed from the calculations. As a counter-example, the centroid for the third class moves in a straight line towards the center. This indicates that both predictors showed a strong signal, and neither was removed as regularization increased.  

Like distance-based methods, predictors based on _data depth_ [@liu1999multivariate; @ghosh2005data;  @mozharovskyi2015classifying] can be helpful for separating classes. The depth of the data was initially defined by @tukey1975mathematics, and it can be roughly understood as the inverse of the distance to a centroid. For example, Mahalanobis depth would be:

$$
Depth_c(\boldsymbol{x}_0) = \left[1 + D_c(\boldsymbol{x}_0)\right]^{-1}
$$

There are more complex depth methods, and some are known for their robustness to outliers. Again, like distances, class-specific depth features can be used as features in a model. 

## Other Methods

There are numerous other methods to create embeddings such as autoencoders[@michelucci2022introduction;@borisov2022deep]. @Boykis_What_are_embeddings_2023 is an excellent survey and discussion of modern embedding methods, highlighting methods for text and deep learning.

## Chapter References {.unnumbered}
