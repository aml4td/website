---
knitr:
  opts_chunk:
    cache.path: "../_cache/transformations/"
---

# Transforming Numeric Predictors {#sec-numeric-predictors}

```{r}
#| label: transformations-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(embed)
library(bestNormalize)
library(patchwork)
library(ggforce)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_transparent())
set_options()
```

```{r}
#| label: ames-split
#| include: false
source("../R/setup_ames.R")
```

As mentioned previously, feature engineering is the process of representing your predictor data so that the model has to do the least amount of work to predict the outcome effectively. There is also the need to properly encode/format the data based on the model's mathematical requirements (i.e., pre-processing). The previous chapter described techniques for categorical data, and in this chapter, we do the same for quantitative predictors. 

We'll begin with operations that only involve one predictor at a time before moving on to group transformations. Later, in @sec-add-remove-features,  procedures on numeric predictors are described that create additional predictor columns from a single column, such as basis function expansions. However, this chapter focuses on transformations that leave the predictors "in place" but altered. 


## When are transformations estimated and applied? 

and using what data

note about not re-estimating; use a single data point and scaling as an example. 

## General Rransformations

Many transformations that involve a single predictor change the distribution of the data. What would a problematic distribution look like? Most predictive models do not place specific parametric assumptions on the predictor variables (e.g., require normality), but some distributions might facilitate better predictive performance than others. 

some based on convention or scientific knowledge. Others like the arc-sin (ref The arcsine is asinine: the analysis of proportions in ecology) or logit?

To start, we'll consier two classes of transformations for individual predictors: those that resolve distributional skewness and those that convert each predictor to a common distribution (or scale). 

After these, an example of a _group_ transformation is described. 

### Resolving skewness

The skew of a distribution can be quantified using the skewness statistic: 

$$\begin{align}
  skewness &= \frac{1}{(n-1)v^{3/2}} \sum_{1=1}^n (x_i-\overline{x})^3 \notag \\
  \text{where}\quad  v &= \frac{1}{(n-1)}\sum_{1=1}^n (x_i-\overline{x})^2 \notag
\end{align}
$$
where values near zero indicate a symmetric distribution, positive values correspond a right skew, and negative values left skew.  For example, @fig-ames-lot-area (panel a) shows the training set distribution of the lot area of houses in Ames. The data are significantly right-skewed (with a skewness value of `r signif(e1071::skewness(ames_train$Lot_Area), 3)`). There are `r sum(ames_train$Lot_Area > 100000)` samples in the training set that sit far beyond the mainstream of the data. 

```{r}
#| label: ames-lot-area-calcs
#| warning: false
lot_area_raw <- 
  ames_train %>% 
  ggplot(aes(Lot_Area)) + 
  geom_histogram(bins = 30, col = "white", fill = "#8E195C", alpha = 1 / 2) +
  geom_rug(alpha = 1 / 2, length = unit(0.04, "npc"), linewidth = 1.2) +
  labs(x = "Lot Area", title = "(a) original")

lot_area_yj_rec  <- 
  recipe(~ Lot_Area, data = ames_train) %>% 
  step_YeoJohnson(Lot_Area) %>% 
  prep()

lot_area_bc_rec  <- 
  recipe(~ Lot_Area, data = ames_train) %>% 
  step_BoxCox(Lot_Area) %>% 
  prep()
    
yj_est <- lot_area_yj_rec %>% tidy(number = 1) %>% pluck("value")
bc_est <- lot_area_bc_rec %>% tidy(number = 1) %>% pluck("value")
bc_skew <- lot_area_bc_rec %>% bake(new_data = NULL) %>% pluck("Lot_Area") %>% e1071::skewness()

lot_area_yj <- 
  lot_area_yj_rec %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(Lot_Area)) +
  geom_rug(alpha = 1 / 2, length = unit(0.04, "npc"), linewidth = 1.2) + 
  geom_histogram(bins = 20, col = "white", fill = "#8E195C", alpha = 1 / 2) +
  labs(x = "Lot Area", title = "(b) Box-Cox/Yeo-Johnson")

lot_area_norm <- 
  recipe(~ Lot_Area, data = ames_train) %>% 
  step_orderNorm(Lot_Area) %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(Lot_Area)) +
  geom_rug(alpha = 1 / 2, length = unit(0.04, "npc"), linewidth = 1.2) + 
  geom_histogram(bins = 20, col = "white", fill = "#8E195C", alpha = 1 / 2) +
  labs(x = "Lot Area", title = "(d) ordered quantile normalization")

lot_area_pctl <- 
  recipe(~ Lot_Area, data = ames_train) %>% 
  step_percentile(Lot_Area) %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(Lot_Area)) +
  geom_rug(alpha = 1 / 2, length = unit(0.04, "npc"), linewidth = 1.2) + 
  geom_histogram(bins = 20, col = "white", fill = "#8E195C", alpha = 1 / 2) +
  labs(x = "Lot Area", title = "(c) percentile")
```

```{r}
#| label: fig-ames-lot-area
#| fig-width: 8
#| fig-height: 5.5
#| out-width: "80%"
#| fig-cap: "Lot area for houses in Ames, IA. The raw data (a) are shown along with transformed versions using the Yeo-Johnson transformations (b), percentile (c), and ordered quantile normalization (d) transformations."
(lot_area_raw  +  lot_area_yj) / (lot_area_pctl + lot_area_norm)
```


One might infer that "samples far beyond the mainstream of the data" is synonymous with the term "outlier"; The Cambridge dictionary defines an outlier as

> a person, thing, or fact that is very different from other people, things, or facts [...]

or 

> a place that is far from the main part of something

These statements imply that outliers belong to a different distribution than the bulk of the data, perhaps due to a typographical error or an incorrect merging of data sources.

The @nist describes them as 

> an observation that lies an abnormal distance from other values in a random sample from a population

In our experience, researchers are quick to label (and discard) extreme data points as outliers. Often, especially when the sample size is not large, these data points are not abnormal but belong to a highly skewed distribution. They are ordinary in a distributional sense. That is the most likely case here; some houses in Ames have very large lot areas, but they certainly fall under the definition of "houses in Ames, Iowa." These values are genuine, just extreme.

This, by itself, is okay. However, suppose that this column is used in a calculation that involves squaring values, such as Euclidean distance or the sample variance. Extreme values in a skewed distribution can influence some predictive models and cause them to place more emphasis on these predictors^[The field of robust techniques is predicated on making statistical calculations insensitive to these types of data points.]. When the predictor is left in its original form, the extreme samples can end up degrading a model's predictive performance.

One way to resolve skewness is to apply a transformation that makes the data more symmetric. There are several methods to do this. The first is to use a standard transformation, such as logarithmic or the square root, the latter being a better choice when the skewness is not drastic, and the data contains zeros. A simple visualization of the data can be enough to make this choice. The problem is when there are many numeric predictors; it may be inefficient to visually inspect each predictor to make a subjective judgment on what if any, transformation function to apply. 

@Box1964p3648 defined a power family of transformations that use a single parameter, $\lambda$, for different methods: 

:::: {.columns}

::: {.column width="10%"}
:::

::: {.column width="40%"}
- no transformation via $\lambda = 1.0$
- square ($x^2$) via $\lambda = 2.0$
- logarithmic ($\log{x}$) via $\lambda = 0.0$
:::

::: {.column width="40%"}
- square root ($\sqrt{x}$) via $\lambda = 0.5$
- inverse square root ($1/\sqrt{x}$) via $\lambda = -0.5$
- inverse ($1/x$) via $\lambda = -1.0$
:::

::: {.column width="10%"}
:::

::::

and others in between. The transformed version of the variable is:

$$
x^* =
\begin{cases} \frac{x^\lambda-1}{\lambda} & \text{if $\lambda \ne 0$,}
\\
log(x) &\text{if $\lambda = 0$.}
\end{cases}
$$

Their paper defines this as a supervised transformation of a non-negative outcome ($y$) in a linear regression model. They find a value of $\lambda$ that minimizes the residual sums of squared errors. In our case, we can co-opt this method to use for unsupervised transformations of non-negative predictors. @yeojohnson extend this method by allowing the data to be negative via a slightly different transformation: 

$$
x^* =
\begin{cases}
\frac{(x + 1)^\lambda-1}{\lambda} & \text{if $\lambda \ne 0$ and $x \ge 0$,} \\
log(x + 1) &\text{if $\lambda = 0$ and $x \ge 0$.} \\
-\frac{(-x + 1)^{2 - \lambda}-1}{2 - \lambda} & \text{if $\lambda \ne 2$ and $x < 0$,} \\
-log(-x + 1) &\text{if $\lambda = 2$ and $x < 0$.} 
\end{cases}
$$

In either case, maximum likelihood is also used to estimate the $\lambda$ parameter. 

In practice, these two transformations might be limited to predictors with acceptable density. For example, the transformation may only be appropriate for a predictor with a few unique values. A threshold of five or so unique values might be a proper rule of thumb (see the discussion in @sec-near-zero-var). On occasion the maximum likelihood estimates of $\lambda$ diverge to huge values; it is also sensible to use values within a suitable range. Also, the estimate will never be absolute zero. Implementations usually apply a log transformation when the $\hat{\lambda}$ is within some range of zero (say between $\pm 0.01$)^[If you've never seen it, the "hat" notation (e.g. $\hat{\lambda}$) indicates an estimate of some unknown parameter.]. 

For the lot area predictor, the Box-Cox and Yeo-Johnson techniques both produce an estimate of $\hat{\lambda} = `r round(yj_est, 3)`$. The results are shown in @fig-ames-lot-area (panel b). There is undoubtedly less right-skew, and the data are more symmetric with a new skewness value of `r signif(bc_skew, 3)` (much closer to zero). However, there are still outlying points.

Skewness can also be resolved using techniques related to distributional percentiles. A percentile is a value with a specific proportion of data below it. For example, for the original lot area data, the 0.1 percentile is `r format(quantile(ames_train$Lot_Area, prob = .1), big.mark = ",")` square feet, which means that 10{{< pct >}} of the training set has lot areas less than `r format(quantile(ames_train$Lot_Area, prob = .1), big.mark = ",")` square feet. The minimum, median, and maximum are the 0.0, 0.5, and 1.0 percentiles, respectively.

Numeric predictors can be converted to their percentiles, and these data, inherently between zero and one, are used in their place. Probability theory tells us that the distribution of the percentiles should resemble a uniform distribution. This results from the transformed version of the lot area shown in @fig-ames-lot-area (panel c). For new data, values beyond the range of the original predictor data can be truncated to values of zero or one, as appropriate.

Additionally, the original predictor data can be coerced to a specific probability distribution. @ORQ define the Ordered Quantile (ORQ) normalization procedure. It estimates a transformation of the data to emulate the true normalizing function where "normalization" literally maps the data to a standard normal distribution. @fig-ames-lot-area (panel d) illustrates the result for the lot area. In this instance, the resulting distribution is precisely what would be seen if the true distribution was Gaussian with zero mean and a standard deviation of one.
 
In @sec-spatial-sign below, another tool for attenuating outliers in _groups_ of predictors is discussed.  
 
### Standardizing to a common scale 

Another goal for transforming individual predictors is to convert them to a common scale. This is a pre-processing requirement for some models. For example, a _K_-nearest neighbors model computes the distances between data points. Suppose Euclidean distance is used with the Ames data. One predictor, the year a house was built, has training set values ranging between `r min(ames_train$Year_Built)` and `r max(ames_train$Year_Built)`. Another, the number of bathrooms, ranges from `r min(ames_train$Baths)` to `r max(ames_train$Baths)`. If these raw data were used to compute the distance, the value would be inappropriately dominated by the year variable simply because its values were large. See TODO appendix for a summary of which models require a common scale.

The previous section discussed two transformations that automatically convert predictors to a common distribution. The percentile transformation generates values roughly uniformly distributed on the `[0, 1]` scale, and the ORQ transformation results in predictors with standard normal distributions. However, two standardization methods are commonly used. 

First is centering and scaling. To convert to a common scale, the mean ($\bar{x}$) and standard deviation ($\hat{s}$) are computed from the training data and the standardized version of the data is $x^* = (x - \bar{x}) / \hat{s}$. The shape of the original distribution is preserved; only the location and scale are modified to be zero and one, respectively.  

In the next chapter, methods are discussed to convert categorical predictors to a numeric format. The standard tool is to create a set of columns consisting of zeros and ones called _indicator_ or _dummy variables_. When centering and scaling, what with these binary features?  These should be treated the same as the dense numeric predictors. The result is that a binary column will still have two unique values, one positive and one negative. The values will depend on the prevalence of the zeros and ones in the training data. While this seems awkward, it is required to ensure each predictor has the same mean and standard deviation. Note that if the predictor set is _only_ scaled, @twosd suggests that the indicator variables be divided by two standard deviations instead of one. 

@fig-standardization(b) shows the results of centering and scaling the gross living area predictor from the Ames data. Note that the shape of the distribution does not change; only the magnitude of the values is different. 

```{r}
#| label: fig-standardization
#| fig-cap: "The original gross living area data and two standardized versions."
#| fig-width: 9
#| fig-height: 3
#| out-width: "92%"
gross_area_raw <- 
  ames_train %>% 
  ggplot(aes(Gr_Liv_Area)) + 
  geom_histogram(bins = 20, col = "white", fill = "#8E195C", alpha = 1 / 2) +
  labs(x = "Gross Living Area", title = "(a) original") +
  geom_rug(alpha = 1 / 2, length = unit(0.02, "npc"))

gross_area_norm <- 
  recipe(~ Gr_Liv_Area, data = ames_train) %>% 
  step_normalize(Gr_Liv_Area) %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(Gr_Liv_Area)) + 
  geom_histogram(bins = 20, col = "white", fill = "#8E195C", alpha = 1 / 2) +
  labs(x = "Gross Living Area", y = "", title = "(b) centered and scaled") +
  geom_rug(alpha = 1 / 2, length = unit(0.02, "npc"))


gross_area_range <- 
  recipe(~ Gr_Liv_Area, data = ames_train) %>% 
  step_range(Gr_Liv_Area) %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(Gr_Liv_Area)) + 
  geom_histogram(bins = 20, col = "white", fill = "#8E195C", alpha = 1 / 2) +
  labs(x = "Gross Living Area", y = "", title = "(c) range scaled") +
  geom_rug(alpha = 1 / 2, length = unit(0.02, "npc"))

gross_area_raw + gross_area_norm + gross_area_range
```

Another common approach is range standardization. Based on the training set, a predictor's minimum and maximum values are computed, and the data are transformed to a `[0, 1]` scale via

$$
x^* = \frac{x - \min(x)}{\max(x) - \min(x)}
$$

When new data are outside the training set range, they can either be clipped to zero/one or allowed to go slightly beyond the intended range. The nice feature of this approach is that the range of the raw numeric predictors matches the range of any indicator variables created from previously categorical predictors. However, this does not imply that the distributional properties are the same (e.g., mean and variance) across predictors. Whether this is an issue depends on the model being used downstream. @fig-standardization(c) shows the result when the gross living predictor is range transformed.  Notice that the shape of the distributions across panels (a), (b), and (c) are the same — only the scale of the x-axis changes.

### Spatial Sign {#sec-spatial-sign}

Some transformations involve multiple predictors. The next section describes a specific class of simultaneous _feature extraction_ transformations. Here, we will focus on the spatial sign transformation [@Serneels]. This method, which requires $p$ standardized predictors as inputs, projects the data points onto a $p$ dimensional unit hypersphere. This makes all of the data points equally distant from the center of the hypersphere, thereby eliminating all potential outliers. The equation is: 

$$
x^*_{ij}=\frac{x_{ij}}{\sum^{P}_{j=1} x_{ij}^2}
$$

Notice that all of the predictors are simultaneously modified and that the calculations occur in a row-wise pattern. Because of this, the individual predictor columns are now combinations of the other columns and now reflect more than the individual contribution of the original predictors. In other words, after this transformation is applied, if any individual predictor is considered important, its significance should be attributed to all of the predictors used in the transformation. 

```{r}
#| label: ames-lot-living-area-calc
two_areas_rec <- 
  recipe(~ Lot_Area + Gr_Liv_Area, data = ames_train) %>% 
  step_mutate(
    location = ifelse(Lot_Area > 30000 | Gr_Liv_Area > 3500, "'outlying'", "mainstream")
  ) %>% 
  prep()

data_cols <- c(rgb(0.27, 0.59, 0.15), rgb(0, 0, 0, 1/5))

two_areas_raw <- 
  two_areas_rec %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(Lot_Area/1000, Gr_Liv_Area)) +
  geom_point(aes(col = location, pch = location, size = location), alpha = 1 / 2) +
  labs(x = "Lot Area (thousands)", y = "Gross Living Area") +
  scale_color_manual(values = data_cols) + 
  scale_size_manual(values = c(3, 1)) +
  coord_fixed(ratio = 1/25)

two_areas_norm <- 
  two_areas_rec %>%
  step_orderNorm(Lot_Area, Gr_Liv_Area) %>%
  prep() %>%
  bake(new_data = NULL) %>%
  ggplot(aes(Lot_Area, Gr_Liv_Area)) +
  geom_point(aes(col = location, pch = location, size = location), alpha = 1 / 2) +
  labs(x = "Lot Area", y = "Gross Living Area") +
  scale_color_manual(values = data_cols) + 
  scale_size_manual(values = c(3, 1)) +
  coord_equal() +
  theme(axis.title.y = element_blank())

two_areas_ss <- 
  two_areas_rec %>% 
  step_normalize(Lot_Area, Gr_Liv_Area) %>% 
  step_spatialsign(Lot_Area, Gr_Liv_Area) %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(Lot_Area, Gr_Liv_Area)) + 
  geom_point(aes(col = location, pch = location, size = location), alpha = 1 / 2) + 
  labs(x = "Lot Area", y = "Gross Living Area") +
  scale_color_manual(values = data_cols) + 
  scale_size_manual(values = c(3, 1 /2)) +
  coord_equal()  +
  theme(axis.title.y = element_blank())
```

@fig-ames-lot-living-area shows predictors from the Ames data. In these data, at least `r sum(bake(two_areas_rec, new_data = NULL)$location == "'outlying'")` samples appear farther away from most of the data in either Lot Area and/or Gross Living Area. Each of these predictors may follow a right-skewed distribution, or there is some other characteristic that is associated with these samples. Regardless, we would like to transform these predictors simultaneously. 

The second panel of the data shows the same predictors _after_ an orderNorm transformation. Note that, after this operation, the outlying values appear less extreme.  


```{r}
#| label: fig-ames-lot-living-area
#| fig-cap: "Lot area (x) versus gross living area (y) in raw format as well as with order-norm and spatial sign transformations."
#| fig-width: 8
#| fig-height: 3
#| out-width: "100%"

two_areas_raw + two_areas_norm + two_areas_ss + 
   plot_layout(guides = "collect") & 
   theme(plot.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = "pt"))
```

The panel on the right shows the data after applying the spatial sign. The data now form a circle centered at (0, 0) where the previously flagged instances are no longer distributionally abnormal. The resulting bivariate distribution is quite jarring when compared to the original. However, these new versions of the predictors can still be important components in a machine-learning model. 

## Feature Extraction and Embeddings


### Linear Projection Methods {#sec-linear-feature-extraction}

spatial sign for robustness


### Nonlinear Techniques  {#sec-nonlinear-feature-extraction}



## Chapter References {.unnumbered}

