---
knitr:
  opts_chunk:
    cache.path: "../_cache/resampling/"
---

# Measuring Performance with Resampling {#sec-resampling}

```{r}
#| label: resampling-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------
# Required packages
library(readr)
library(tidymodels)
library(patchwork)
library(kableExtra)
library(doParallel)
library(leaflet)
library(spatialsample)

# Also requires: 
# for model fitting
# gamair, viridisLite

# ------------------------------------------------------------------------------
# Set options
cl <- makePSOCKcluster(parallel::detectCores(logical = TRUE))
registerDoParallel(cl)
tidymodels_prefer()
theme_set(theme_transparent())
set_options()

# ------------------------------------------------------------------------------
# Load previous data sets

load("../RData/resampling_var_bias.RData")
sel_model <- "logistic"
sel_metric <- "brier_class"

metric_data <-
  bind_rows(
    stats_bootstraps %>% filter(.metric == sel_metric),
    stats_mc_cv %>% filter(.metric == sel_metric),
    stats_v_fold_cv %>% filter(.metric == sel_metric)
  ) %>%
  filter(model == sel_model)


rng_bias <- range(c(metric_data$bias, 0))
rng_std_err <- range(metric_data$std_err, na.rm = TRUE)
rng_std_err[1] <- 0

br_lr <- 
  stats_bootstraps %>% 
  filter(model == "logistic" & .metric == "brier_class" & times == 100)

br_1nn <- 
  stats_bootstraps %>% 
  filter(model == "knn" & .metric == "brier_class" & times == 100)
```

In @sec-external-validation, it was shown that using the same data to estimate and evaluate our model can produce inaccurate estimates of how well it functions. It also described using separate partitions of the data to fit and evaluate the model. 

There are two general approaches for using external data for model evaluation. The first is a validation set, which we’ve seen before in @sec-three-way-split This is a good idea if you have a lot of data on hand. The second approach is to **resample** the training set. Resampling is an iterative approach that reuses the training data multiple times using sound statistical methodology. We’ll discuss both validation sets and resampling in this chapter. 



```{r}
#| label: fig-resampling-scheme
#| echo: false
#| out-width: 80%
#| fig-align: "center" 
#| fig-cap: "A general data usage scheme that includes resampling. The colors denote the data used to train the model (in tan) and the separate data sets for evaluating the model (colored periwinkle)."

knitr::include_graphics("../premade/resampling.svg")
```

@fig-resampling-scheme shows a standard data usage scheme that incorporates resampling. After an initial partition, resampling creates **multiple versions** of the training set. We'll use special terminology^[These names are not universally used. We only invent new terminology to avoid confusion; people often refer to the data in our analysis set as the "training set" because it has the same purpose.] for the data partitions within each resample which serve the same function as the training and test sets:

- The **analysis set** (of size $n_{fit}$) estimates quantities associated with preprocessing, model training, and postprocessing. 
- The **assessment set** ($n_{pred}$) is only used for prediction so that we can compute measures of model effectiveness (e.g., RMSE, classification accuracy, etc).  

Like the training and test sets, the analysis and assessment sets are mutually exclusive data partitions and are different for each of the _B_ iterations of resampling. 

The resampling process fits a model to an analysis set and predicts the corresponding assessment set. One or more performance statistics are calculated from these held-out predictions and saved. This process continues for _B_ iterations, and, in the end, there is a collection of _B_ statistics of efficacy. These are averaged to produce the overall **resampling estimate** of performance. To formalize this, we'll consider a mapping function $M(\mathfrak{D}^{tr}, B)$ that takes the training set as input and can output $B$ sets of analysis and assessment sets. 

@alg-resampling describes this process. 

:::: {#alg-resampling}

::: {.columns}

::: {.column width="5%"}

:::

::: {.column width="65%"}


```pseudocode
#| label: alg-resampling
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
\begin{algorithm}
\begin{algorithmic}
\State $\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$
\State $B$: number of resamples
\State $M(\mathfrak{D}^{tr}, B)$: a mapping function to split $\mathfrak{D}^{tr}$ for each of  $B$ iterations.
\State $f()$: model pipeline 
\Procedure{Resample}{$\mathfrak{D}^{tr}, f, M(\mathfrak{D}^{tr}, B)$}
  \For{$b =1$ \To $B$}
    \State Partition $\mathfrak{D}^{tr}$ into $\{\mathfrak{D}_b^{fit}, \mathfrak{D}_b^{pred}\}$ using $M_b(\mathfrak{D}^{tr}, B)$.
    \State Train model pipeline $f$ on the analysis set to produce $\hat{f}_{b}(\mathfrak{D}_b^{fit})$.
    \State Generate assessment set predictions $\hat{y}_b$ by applying model $\hat{f}_{b}$ to $\mathfrak{D}_b^{pred}$.
    \State Estimate performance statistic $\hat{Q}_{b}$.
  \EndFor 
  \State Compute reampling estimate $\hat{Q} = \sum_{b=1}^B \hat{Q}_{b}$.
  \Return $\hat{Q}$.
\Endprocedure
\end{algorithmic}
\end{algorithm}
```

:::

::: {.column width="15%"}

:::

::: 

Resampling models to estimate performance. 

::::


:::{.callout-note}
In @alg-resampling, $f()$ is the model pipeline described in @sec-model-pipeline. Any estimation methods, before, during, or after the supervised model, are executed on the analysis set _B_ times during resampling.
:::

There are many different resampling methods, such as cross-validation and the bootstrap. They differ in how the analysis and assessment sets are created in line 7 of @alg-resampling. Subsequent sections below discuss a number of mapping functions. 

Two primary aspects of resampling make it an effective tool. First, the separation of data used to create and appraise the model avoids the data reuse problem described in @sec-external-validation. 

Second, using multiple iterations (_B_ > 1) means that you are evaluating your model under slightly different conditions (because the analysis and assessment sets are different). This is a bit like a "multiversal" science fiction story: what would have happened if the situation were slightly different before I fit the model? This lets us directly observe the model variance; how stable is it when the inputs are slightly modified? An unstable model (or one that overfits) will have a high variance.

In this chapter, we'll describe some conceptual aspects of resampling. Then, we’ll define and discuss various methods. Finally, the last section lists frequently asked questions we often hear when teaching these tools. 

Up until @sec-time-series-resampling, we will assume that each row of the data are independent. We’ll see examples of non-independent in the later sections.

## What is Resampling Trying to Do? 

It's important to understand some of the philosophical aspects of resampling. It is a statistical  procedure that tries to estimate some true, unknowable performance value (let's call it $Q$) associated with the same population of data represented by the training set. The quantity $Q$ could be RMSE, the Brier score, or any other performance statistic. 

There have been ongoing efforts to understand what resampling methods do on a theoretical level. @bates2023cross is an interesting work focused on linear regression models with ordinary least squares. Their conclusion regarding resampling estimates is that they

> ... cannot be viewed as estimates of the prediction error of the final model fit on the whole data. Rather, the estimate of prediction error is an estimate of the average prediction error of the final model across other hypothetical data sets from the same distribution.

It is a good idea to think that this will also be true for more complex machine learning models. The important idea is that resampling is measuring performance on training sets similar to ours (and the same size). 

To understand different methods, let’s examine some theoretical properties of resampling estimates that matter to applied data analysis. 

Let’s start with analogy; consider the usual sample mean estimator used in basic statistics ($\bar{x}$). Based on assumptions about the data, we can derive an equation that optimally estimates the true mean value based on some criterion. Often, the statistical theory focuses on our estimators' theoretical mean and variance. For example, if the data are independent and follow the same Gaussian distribution, the sample mean attempts to estimate the actual population mean (i.e., it is an **unbiased** estimator). We can sometimes derive what the estimator's theoretical variance is, too. These properties, or some combination of them, such as mean squared error^[This discussion is very similar to the variance-bias tradeoff discussed in @sec-variance-bias. The context here differs, but the themes are the same.], help guide us to the best estimator. 

The same is true for resampling methods. We can understand how their bias and variances change under different circumstances and choose an appropriate technique. We'll also focus on bias and variance as the main properties of interest. 

First is bias: how accurately does a resampling technique estimate the true population value $Q$? We'd like it to be unbiased, but to our knowledge, that is nearly impossible. However, some things we do know.  @fig-resampling-scheme shows that the training set (of size $n_{tr}$) is split into two partitions of size $n_{fit}$ and $n_{pred}$. It turns out that, as $n_{pred}$ becomes larger, the bias increases in a pessimistic direction^[Meaning that performance looks worse than it should. For example, smaller $R^2$ or inflated Brier score.]. In other words, if our training set has 1,000 samples, a resampling method where the assessment set has $n_{pred} = 100$ data points has a smaller bias than one using $n_{pred} = 500$. Another thing that we know is that increasing the number of resamples (_B_) can't significantly reduce the bias. 

The variance (often called precision) of resampling is also important. We want to get a performance estimate that gives us consistent results if we repeat it. The resampling precision is driven mainly by _B_ and $n_{tr}$. With some resampling techniques, if your results are too noisy, you can resample more and stabilize or reduce the estimated variance (at the cost of increased computational time). We'll examine the bias and precision of different methods as we discuss each method. 

Let's examine a few specific resampling methods, starting with the most simple: a single validation set. 

## Validation Sets {#sec-validation}

We’ve already described a validation set; it is typically created via a three-way initial split^[The split can be made using the same tools already discussed, such as completely random selection, stratified random sampling, etc.] of the data (as shown in @fig-validation). For time series data, it is common to use the most recent data for the test set. Similarly, the validation set would include the most recent data (once the test set partition is created). The training set would include everything else.


```{r}
#| label: fig-validation
#| echo: false
#| out-width: 45%
#| fig-align: "center" 
#| fig-cap: An initial data splitting scheme that incorporates a validation set. 

knitr::include_graphics("../premade/validation.svg")
```

While not precisely the same, a validation set is extremely similar to an approach described below called Monte Carlo cross-validation (MCCV). If we used a single MCCV resample, the results would be effectively the same as those of a validation set; the difference not substantive. 

One aspect of using the validation set is what to do with it after you’ve made your final decision about the model pipeline. Ordinarily, the entire training set is used for the final model fit.

Data can be scarce and, in some cases, an argument can be made that the final model fit could include the training _and_ validation set. Doing this does add some risk; your validation set statistics are no longer _completely valid_ since they measured how well the model works with similar training sets of size $n_{tr}$. If you have an abundance of data, the risk is low, but, at the same time, the model fit won’t change much by adding $n_{val}$ data points. However, if your training data set is not large, adding more data could have a profound impact, and you risk using the wrong model for your data^[Also, if $n_{tr}$ is not “large,” you shouldn’t use a validation set anyway.]. 

## Monte Carlo Cross-Validation {#sec-cv-mc}

Monte Carlo cross-validation emulates the initial train/test partition. For each one of _B_ resamples, it takes a random sample of the training set (say about 75%) to use as the analysis set^[Again, this could be accomplished using any of the splitting techniques described in @sec-data-splitting.]. The remainder is used for model assessment. Each of the _B_ resamples is created independently of the others, so some of the training set points are included in multiple assessment sets. @fig-mccv-scheme shows the results of $M(\mathfrak{D}^{tr}, 3)$ for MCCV with a data set with $n_{tr} = 30$ data points and 80% of the data are allocated to the analysis set. Note that samples 16 and 17 are in multiple assessment sets. 

```{r}
#| label: fig-mccv-scheme
#| echo: false
#| out-width: '65%'
#| fig-cap: "A schematic of three Monte Carlo cross-validation resamples created from an initial pool of 30 data points."
knitr::include_graphics("../premade/mc-cv.svg")
```


How many resamples should we use, and what proportion of the data should be used for the analysis set? These choices are partly driven by computing power and training set size. Clearly, using more resamples means better precision. 

A thought experiment can be useful for any resampling method. Based on the proportion of data going into the assessment set, how confident would you feel in a performance metric being computed for this much data? Suppose that we are computing the RMSE for a regression model. If our assessment set contained $n_{pred} = 10$ (on average), we might think our metric’s mean is excessively noisy.   In that case, we could either increase the proportion held out (better precision, worse bias) or resample more (better precision, same bias, long computational time). 

Each resampling method has different trade-offs between bias and variance. Let’s look at some simulation results to help understand the trade-off for MCCV.

### Another Simulation Study  {.unnumbered}

@sec-complexity described a simulated data set that included 200 training set points. In that section, @fig-two-class-overfit illustrates the effect of overfitting using a particular model (KNN). This chapter will use the same training set but with a simple logistic regression model where a four-degree of freedom spline was used with each of the two predictors. 

Since these are simulated data, an additional, very large data set was simulated and used to approximate the model's true performance, once again evaluated using a Brier score. Our logistic model has a Brier score value of $Q \approx$ `r signif(br_lr$large_sample[1], 3)`, which demonstrates a good fit. Using this estimate, the model bias can be computed by subtracting the resampling estimate from `r signif(br_lr$large_sample[1], 3)`.

The simulation created 500 realizations of this 200 sample training set^[The simulation details, sources, and results can be found at [`https://github.com/topepo/resampling_sim`](https://github.com/topepo/resampling_sim).]. We resampled the logistic model for each and then computed the corresponding Brier score estimate from MCCV. This process was repeated using the different analysis set proportions and values of _B_ shown in @fig-mccv-stats. The left panel shows that the bias^[The percent bias is calculated as $100(Q_{true} - \hat{Q})/Q_{true}$ for metrics where smaller is better.] decreases as the proportion of data in the analysis set becomes closer to the amount in the training set. It is also apparent that the bias is unaffected by the number of resamples (_B_). The panel on the right shows the precision, estimated using the standard error, of the resampled estimates.  This decreases nonlinearly as _B_ increases; the cost-benefit ratio of adding more resamples shows eventual diminishing returns. The amount retained for the analysis set shows that values close to one have higher precision than the others.  Regarding computational costs, the time to resample a model increases with both parameters (amount retained and _B_). 

```{r}
#| label: fig-mccv-stats
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: Variance and bias statistics for simulated data using different configurations for Monte Carlo cross-validation. The range of the y-axes are common across similar plots below for different resampling techniques.  

mc_bias <-
  stats_mc_cv %>%
  filter(model == sel_model & .metric == sel_metric) %>%
  ggplot(aes(times, bias, col = retain, pch = retain)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0, lty = 2) +
  scale_y_continuous(labels = scales::percent, limits = rng_bias) +
  scale_color_brewer(palette = "Set1") +
  labs(y = "Bias", x = "Number of Resamples")

mc_prec <-
  stats_mc_cv %>%
  filter(model == sel_model & .metric == sel_metric) %>%
  ggplot(aes(times, std_err, col = retain, pch = retain)) +
  geom_point() +
  geom_line() +
  lims(y = rng_std_err) +
  geom_hline(yintercept = 0, lty = 2) +
  labs(y = "Std. Error", x = "Number of Resamples") +
  scale_color_brewer(palette = "Set1")

mc_bias + mc_prec +
  plot_layout(guides = 'collect') &
  theme(legend.position = "top")
```

The results of this simulation indicate that, in terms of precision, there is little benefit in including more than 70% of the training set in the assessment set. Also, while more resamples always help, there is incremental benefit in using more than 50 or 60 resamples (for this size training set). Regarding bias, the decrease somewhat slows down near 80% of the training set retained. Holdout proportions of around 75% - 80% might be a reasonable rule of thumb (these are, of course, subjective).

```{r}
#| label: teardown-cluster
#| include: false

stopCluster(cl)
```


## Chapter References {.unnumbered}
