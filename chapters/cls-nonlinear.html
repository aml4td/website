<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>17&nbsp; Complex Nonlinear Boundaries – Applied Machine Learning for Tabular Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/cls-trees.html" rel="next">
<link href="../chapters/cls-linear.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-e0c4a3bc86ed066376318c576df258b1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<meta name="shinylive:serviceworker_dir" content="..">
<script src="../site_libs/quarto-contrib/shinylive-0.10.6/shinylive/load-shinylive-sw.js" type="module"></script>
<script src="../site_libs/quarto-contrib/shinylive-0.10.6/shinylive/run-python-blocks.js" type="module"></script>
<link href="../site_libs/quarto-contrib/shinylive-0.10.6/shinylive/shinylive.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/shinylive-quarto-css/shinylive-quarto.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-7T996NL20Z"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-7T996NL20Z', { 'anonymize_ip': true});
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title"><span id="sec-cls-nonlinear" class="quarto-section-identifier"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Complex Nonlinear Boundaries</span></span></h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../"></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/aml4td/website/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/news.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">News</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/contributing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributing</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/whole-game.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Whole Game</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Preparation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/initial-data-splitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Initial Data Splitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/missing-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Missing Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/numeric-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transforming Numeric Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/categorical-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Working with Categorical Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/interactions-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Interactions and Nonlinear Features</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/overfitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Overfitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/resampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Measuring Performance with Resampling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/grid-search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Grid Search</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/iterative-search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Iterative Search</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/feature-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/comparing-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Comparing Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-metrics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Characterizing Classification Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-linear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Generalized Linear and Additive Classifiers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-nonlinear.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Complex Nonlinear Boundaries</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-trees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Classification using Trees and Rules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-ensembles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Classification Ensembles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-imbalance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Class Imbalances</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-case-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Classification Case Study</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Classification Summary</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Regression</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Characterization</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Finalization</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-nonlinear-da" id="toc-sec-nonlinear-da" class="nav-link active" data-scroll-target="#sec-nonlinear-da"><span class="header-section-number">17.1</span> Nonlinear Discriminants</a>
  <ul class="collapse">
  <li><a href="#sec-naive-bayes" id="toc-sec-naive-bayes" class="nav-link" data-scroll-target="#sec-naive-bayes"><span class="header-section-number">17.1.1</span> Naive Bayes</a></li>
  <li><a href="#sec-fda" id="toc-sec-fda" class="nav-link" data-scroll-target="#sec-fda"><span class="header-section-number">17.1.2</span> Flexible Discriminants</a></li>
  </ul></li>
  <li><a href="#sec-cls-knn" id="toc-sec-cls-knn" class="nav-link" data-scroll-target="#sec-cls-knn"><span class="header-section-number">17.2</span> K-Nearest Neighbors</a></li>
  <li><a href="#sidebar-dot-products" id="toc-sidebar-dot-products" class="nav-link" data-scroll-target="#sidebar-dot-products"><span class="header-section-number">17.3</span> Sidebar: Dot Products</a></li>
  <li><a href="#sec-cls-nnet" id="toc-sec-cls-nnet" class="nav-link" data-scroll-target="#sec-cls-nnet"><span class="header-section-number">17.4</span> Neural Networks via Multilayer Perceptrons</a>
  <ul class="collapse">
  <li><a href="#sec-nnet-activation" id="toc-sec-nnet-activation" class="nav-link" data-scroll-target="#sec-nnet-activation"><span class="header-section-number">17.4.1</span> Activation Functions</a></li>
  <li><a href="#sec-nnet-training" id="toc-sec-nnet-training" class="nav-link" data-scroll-target="#sec-nnet-training"><span class="header-section-number">17.4.2</span> Training and Optimization</a></li>
  <li><a href="#sec-nnet-initialization" id="toc-sec-nnet-initialization" class="nav-link" data-scroll-target="#sec-nnet-initialization"><span class="header-section-number">17.4.3</span> Initialization</a></li>
  <li><a href="#sec-nnet-preprocessing" id="toc-sec-nnet-preprocessing" class="nav-link" data-scroll-target="#sec-nnet-preprocessing"><span class="header-section-number">17.4.4</span> Preprocessing</a></li>
  <li><a href="#sec-nnet-learning-rates" id="toc-sec-nnet-learning-rates" class="nav-link" data-scroll-target="#sec-nnet-learning-rates"><span class="header-section-number">17.4.5</span> Learning Rates</a></li>
  <li><a href="#sec-nnet-regularization" id="toc-sec-nnet-regularization" class="nav-link" data-scroll-target="#sec-nnet-regularization"><span class="header-section-number">17.4.6</span> Regularization</a></li>
  <li><a href="#sec-early-stopping" id="toc-sec-early-stopping" class="nav-link" data-scroll-target="#sec-early-stopping"><span class="header-section-number">17.4.7</span> Early Stopping</a></li>
  <li><a href="#sec-batch-learning" id="toc-sec-batch-learning" class="nav-link" data-scroll-target="#sec-batch-learning"><span class="header-section-number">17.4.8</span> Batch Learning</a></li>
  <li><a href="#sec-gradient-descent" id="toc-sec-gradient-descent" class="nav-link" data-scroll-target="#sec-gradient-descent"><span class="header-section-number">17.4.9</span> Gradient Descent Algorithms</a></li>
  <li><a href="#sec-newtons-method" id="toc-sec-newtons-method" class="nav-link" data-scroll-target="#sec-newtons-method"><span class="header-section-number">17.4.10</span> Newton’s Method</a></li>
  <li><a href="#sec-nnet-forestation" id="toc-sec-nnet-forestation" class="nav-link" data-scroll-target="#sec-nnet-forestation"><span class="header-section-number">17.4.11</span> Forestation Modeling</a></li>
  </ul></li>
  <li><a href="#sec-cls-tab-net" id="toc-sec-cls-tab-net" class="nav-link" data-scroll-target="#sec-cls-tab-net"><span class="header-section-number">17.5</span> Special Tabular Network Models</a></li>
  <li><a href="#sec-cls-svm" id="toc-sec-cls-svm" class="nav-link" data-scroll-target="#sec-cls-svm"><span class="header-section-number">17.6</span> Support Vector Machines</a></li>
  <li><a href="#chapter-references" id="toc-chapter-references" class="nav-link" data-scroll-target="#chapter-references">Chapter References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span id="sec-cls-nonlinear" class="quarto-section-identifier"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Complex Nonlinear Boundaries</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter focuses on several classes of models that naturally produce nonlinear boundaries, such as neural networks, K-nearest neighbors, and support vector machines. A separate category of models, which use tree structures, also has inherently nonlinear classification boundaries. We’ll discuss those models in <a href="cls-trees.html" class="quarto-xref"><span>Chapter 18</span></a> and <a href="cls-ensembles.html" class="quarto-xref"><span>Chapter 19</span></a>.</p>
<p>In this chapter, several recurring motifs are present across different types of models. First, this involves the use of an operation that utilizes the positive part of a function, known as “hinge” or “rectifier” functions. We’ll see these in <a href="#sec-fda" class="quarto-xref"><span>Section 17.1.2</span></a> and <a href="#sec-nnet-activation" class="quarto-xref"><span>Section 17.4.1</span></a>. Another element that will be encountered across different models is the kernel function, previously seen in <a href="iterative-search.html#sec-gp" class="quarto-xref"><span>Section 12.6.2</span></a>. These functions are used to quantify the distance or similarity between data points (or, more specifically, vectors). Both the K-nearest neighbor and the support vector machine (SVM) models rely on these functions. Finally, the dot product between vectors plays a critical role in the important <em>attention</em> mechanism in neural networks (<span class="quarto-unresolved-ref">?sec-attention</span>) as well as in SVM models (<a href="#sec-cls-svm" class="quarto-xref"><span>Section 17.6</span></a>).</p>
<p>This chapter contains <em>two</em> sections on neural networks. The first describes the most basic version of these models, called the <em>multilayer perceptron</em>. The fundamentals of these models are discussed in this section. The second section explores various methodologies to enhance the performance of neural network models for tabular data, similar to their effectiveness in predicting images and text. At the time of this writing, substantial research on this topic is underway in the field. This section will also discuss one of the most effective tools for most deep learning models: attention-based transformers.</p>
<p>To get started, the first chapter summarizes an old, traditional model for classification, linear discriminant analysis (LDA), and shows how it can be effectively modified to produce nonlinear class boundaries.</p>
<section id="sec-nonlinear-da" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="sec-nonlinear-da"><span class="header-section-number">17.1</span> Nonlinear Discriminants</h2>
<p>Discriminant analysis is a classical statistical method for classification. While its prominence in the statistical learning literature has diminished in recent decades, understanding its foundations provides valuable context for several modern and highly effective classification methods. This section focuses on linear discriminant analysis (LDA).</p>
<p>Linear discriminant analysis <span class="citation" data-cites="mclachlan2005discriminant murphy2012machine">(<a href="#ref-mclachlan2005discriminant" role="doc-biblioref">McLachlan 2005</a>; <a href="#ref-murphy2012machine" role="doc-biblioref">Murphy 2012</a>)</span> produces linear classification boundaries and can be motivated via Bayes’ Rule. Let’s say that our training set predictors for class <span class="math inline">\(k\)</span> are contained in the random variable vector <span class="math inline">\(X_k\)</span> with <span class="math inline">\(p\)</span> elements. LDA assumes that the predictor distributions within each class follow a multivariate Gaussian distribution with class-specific mean vectors but a common covariance matrix across all classes. Formally, for class <span class="math inline">\(k\)</span>, the conditional distribution of the predictors given class membership is <span class="math inline">\(X_k \sim N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma})\)</span>.</p>
<p>To classify a new observation with predictor vector <span class="math inline">\(\boldsymbol{x}\)</span>, LDA applies Bayes’ theorem to compute the posterior probability of class membership. Given a prior probability <span class="math inline">\(Pr[Y = k]\)</span> for class <span class="math inline">\(k\)</span>, the posterior probability is:</p>
<p><span id="eq-discrim"><span class="math display">\[
Pr[Y = k |\boldsymbol{x}] =\frac{Pr[Y = k]Pr[\boldsymbol{x}|Y = k]}{\sum\limits_{l=1}^C Pr[Y = l]Pr[\boldsymbol{x}|Y = l]}
\tag{17.1}\]</span></span></p>
<p>This equation is straightforward to calculate. The parameters <span class="math inline">\(\boldsymbol{\mu}_k\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> are estimated from the training data using their maximum likelihood estimates: the class-specific sample mean vectors <span class="math inline">\(\bar{\boldsymbol{x}}_k\)</span> and the pooled sample covariance matrix <span class="math inline">\(\boldsymbol{S}\)</span>, respectively. The class-conditional densities (<span class="math inline">\(Pr[\boldsymbol{x}|Y]\)</span>) are then evaluated using the multivariate normal probability density function, which modern statistical software computes efficiently. In practice, computational efficiency can be improved by calculating only the numerator of <a href="#eq-discrim" class="quarto-xref">Equation&nbsp;<span>17.1</span></a> for each class, then normalizing these values to ensure the posterior probabilities sum to 1. For a new observation <span class="math inline">\(\boldsymbol{x}\)</span>, the class with the maximum posterior probability is assigned as the predicted class.</p>
<p>Alternatively, we can frame the prediction in terms of the discriminant function:</p>
<p><span id="eq-discrim-func"><span class="math display">\[
D_k(\boldsymbol{x}) = \boldsymbol{x}'\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k + \frac{1}{2}\boldsymbol{\mu}_k'\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k + \log\left(Pr[Y = k]\right)
\tag{17.2}\]</span></span></p>
<p>The discriminant values are conceptually similar to the linear predictor in logistic regression, taking values on the real line, with larger values corresponding to higher posterior probabilities. The predicted class is determined by identifying which discriminant function yields the largest value.</p>
<p>A notable advantage of LDA is its explicit modeling of the predictor covariance structure through <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. This characteristic renders the method robust to moderate levels of multicollinearity among predictors. Furthermore, the framework naturally extends to multi-class problems.</p>
<p>However, LDA has important limitations. The method requires that <span class="math inline">\(\boldsymbol{\Sigma}\)</span> be non-singular (invertible). When the training set contains fewer observations than predictors (<span class="math inline">\(n &lt; p\)</span>) or when perfect linear dependencies exist among predictors, the sample covariance matrix becomes singular and the discriminant functions cannot be computed. Additionally, the Gaussian distributional assumption is fundamental to the method’s theoretical justification. While binary indicator variables can technically be included in the model (as they are numeric), their discrete, non-Gaussian nature violates the model assumptions and undermines the probabilistic interpretation of the results. For datasets with categorical predictors or high-dimensional settings where <span class="math inline">\(n &lt; p\)</span>, alternative classification methods may be more appropriate as we will see later.</p>
<p>As presented, LDA is limited as a classification tool. Its assumption of a common covariance matrix across classes restricts it to producing linear decision boundaries. While nonlinear boundaries could be induced through feature engineering (e.g., adding polynomial or interaction terms), such an approach would be better done using logistic regression.</p>
<p>Nevertheless, the discriminant analysis framework has led to extensions that address the various limitations of LDA. Quadratic discriminant analysis (QDA) <span class="citation" data-cites="ghojogh2019linear">(<a href="#ref-ghojogh2019linear" role="doc-biblioref">Ghojogh and Crowley 2019</a>)</span> relaxes the assumption of a common covariance matrix, allowing each class to have its own covariance structure: <span class="math inline">\(X_k \sim N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\)</span>. This modification results in quadratic decision boundaries, providing greater flexibility at the cost of estimating additional parameters. Robust variants of both LDA and QDA have been developed to mitigate the influence of outliers on parameter estimation <span class="citation" data-cites="hubert2024robust">(<a href="#ref-hubert2024robust" role="doc-biblioref">Hubert, Raymaekers, and Rousseeuw 2024</a>)</span>, addressing the well-known sensitivity of maximum likelihood estimates to extreme observations. Additionally, regularized discriminant analysis methods <span class="citation" data-cites="friedman1989regularized pang2009shrinkage">(<a href="#ref-friedman1989regularized" role="doc-biblioref">Friedman 1989</a>; <a href="#ref-pang2009shrinkage" role="doc-biblioref">Pang, Tong, and Zhao 2009</a>)</span> introduce penalty terms that shrink the covariance matrix estimates, improving stability and predictive performance, particularly when sample sizes are modest relative to the number of predictors.</p>
<p>Two extensions of discriminant analysis are practically useful. The first, naive Bayes classification, represents a simplification that assumes conditional independence among predictors within each class. The second, flexible discriminant analysis (FDA), is a generalization of the LDA framework. FDA can accommodate nonlinear decision boundaries, perform implicit feature selection, and incorporate various modeling strategies, making it a considerably more versatile tool for classification problems. These methods are discussed in the following sections.</p>
<section id="sec-naive-bayes" class="level3" data-number="17.1.1">
<h3 data-number="17.1.1" class="anchored" data-anchor-id="sec-naive-bayes"><span class="header-section-number">17.1.1</span> Naive Bayes</h3>
<p>While LDA estimates the class-conditional density <span class="math inline">\(Pr[\boldsymbol{x}|Y = k]\)</span> using a multivariate Gaussian distribution, naive Bayes <span class="citation" data-cites="hand2001idiot">(<a href="#ref-hand2001idiot" role="doc-biblioref">Hand and Yu 2001</a>)</span> adopts an alternative approach by decomposing this joint density into a product of univariate marginal distributions:</p>
<p><span id="eq-ind-probs"><span class="math display">\[
Pr[\boldsymbol{x}|Y = k] = \prod_{j=1}^p Pr[x_j|Y = k]
\tag{17.3}\]</span></span></p>
<p>This formulation assumes conditional independence among all predictors within each class; i.e., the predictors are all statistically independent of one another. This assumption is often violated in practice. Despite this seemingly restrictive assumption, naive Bayes classifiers can occasionally perform surprisingly well in practice.</p>
<p>The primary advantage of the conditional independence assumption is that it relaxes the distributional requirements for individual predictors. Unlike LDA, which requires joint multivariate normality, naive Bayes allows each predictor to follow its own distribution, which need not be Gaussian. Moreover, these distributions do not even need to be fully parametric.</p>
<p>For continuous predictors, kernel density estimation <span class="citation" data-cites="Silverman86 chen2017tutorial">(<a href="#ref-Silverman86" role="doc-biblioref">Silverman 1986</a>; <a href="#ref-chen2017tutorial" role="doc-biblioref">Chen 2017</a>)</span> provides a flexible, nonparametric approach to estimating <span class="math inline">\(Pr[x_j|Y = k]\)</span>. Kernel density estimators are smoothing methods that adaptively characterize the empirical distribution of the data. A kernel function <span class="math inline">\(K(\cdot)\)</span> is a non-negative, symmetric function that integrates to one and serves as a weighting scheme<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Common kernel functions are:</p>
<ul>
<li>The <strong>uniform kernel</strong>, which assigns equal weight to all observations within a fixed range</li>
<li>The <strong>Gaussian kernel</strong>, <span class="math inline">\(K(u) = \frac{1}{\sqrt{2\pi}}\exp(-u^2/2)\)</span>, which assigns weights that decrease exponentially with distance from the point of interest</li>
</ul>
<p>The kernel density estimate at a point <span class="math inline">\(u\)</span> is constructed as:</p>
<p><span class="math display">\[
\hat{f}(u) = \frac{1}{nh}\sum_{i=1}^n K\left(\frac{u - x_i}{h}\right)
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations, <span class="math inline">\(x_i\)</span> are the observed training set data points, and <span class="math inline">\(h\)</span> is the bandwidth parameter that controls the smoothness of the estimate. We’ll come back to the bandwidth parameter shortly.</p>
<p>For example, <a href="#fig-densities" class="quarto-xref">Figure&nbsp;<span>17.1</span></a> displays estimated densities for the minimum vapor pressure variable across three subsets of the data. Kernel density estimates are shown in blue, while fully parametric Gaussian density estimates (assuming <span class="math inline">\(N(\mu, \sigma)\)</span>) are shown in orange. The left-hand panel presents the entire training set, which exhibits modest right skewness. Both estimation approaches provide reasonable approximations for vapor pressure values<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> above 200. However, the Gaussian estimate lacks the flexibility to adequately capture the density in the middle of the range, and both methods perform poorly for values below 200, where the data are sparse.</p>
<p>The middle panel shows the density for non-forested locations. The histogram suggests these data are approximately trimodal with left skewness, and the kernel density estimate successfully captures these multiple modes. In contrast, the Gaussian does not capture the multimodal structure, and relying on this parametric assumption would likely result in underfitting and poor probability estimates for classification.</p>
<p>The right-hand panel displays the density for forested locations, which exhibits right skewness. The parametric Gaussian approach provides a particularly poor fit for the lower half of this distribution, where the bulk of the data reside. The kernel density estimate adapts to the asymmetry and provides a better representation of the empirical distribution.</p>
<p>These examples illustrate the potential advantage of kernel density estimation within the naive Bayes framework: the ability to accommodate non-Gaussian, multimodal, and skewed distributions without requiring distributional transformations or parametric assumptions that may be violated in practice.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-densities" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-densities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-densities-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-densities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.1: Parametric and nonparametric density estimates for the minimum vapor predictor, shown separately for the entire training set and then for each class. The blue line is the nonparametric estimate, and the orange curve is the traditional Gaussian estimate.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The kernel density estimator contains a tuning parameter: the bandwidth <span class="math inline">\(h\)</span>, which controls the degree of smoothing applied to the density estimate. The bandwidth determines the effective width of the kernel’s influence around each observation. Small bandwidth values produce density estimates that closely track the observed data, potentially capturing fine-scale features but risking variability and overfitting. Conversely, large bandwidth values distribute the kernel weight over a broader range, yielding smoother density estimates that may obscure multimodality or other important distributional characteristics.</p>
<p>Several data-driven methods exist for selecting the bandwidth that optimizes specific criteria. Common approaches include minimizing an estimate of the mean integrated squared error or using cross-validation <span class="citation" data-cites="park1990comparison">(<a href="#ref-park1990comparison" role="doc-biblioref">Park and Marron 1990</a>)</span>. These default bandwidth selection methods are designed to produce generally accurate density estimates.</p>
<p>However, the bandwidth that yields the most accurate density estimate may not necessarily produce the best classification performance. The objective in density estimation—minimizing the mean squared error—differs from the classification objective of minimizing prediction error. To address this potential misalignment, the bandwidth can be treated as a tuning parameter in the naive Bayes model. Rather than directly tuning <span class="math inline">\(h\)</span>, it is often more convenient to tune a bandwidth adjustment factor that is a multiplier of the bandwidth. A value greater than 1.0 encourages more smoothing of the data, while a smaller values make very bumpy densities.</p>
<p>Since naive Bayes does not assume much about <span class="math inline">\(Pr[x_j|Y = k]\)</span>, categorical predictors can be readily incorporated into the model. For a categorical predictor, the conditional probabilities are estimated directly from the observed frequencies within each class. <a href="cls-linear.html#fig-counties" class="quarto-xref">Figure&nbsp;<span>16.3</span></a> showed the raw probabilities associated with the forested class (i.e., <span class="math inline">\(Pr[y = forested]\)</span>).</p>
<p>However, sparse data can create computational difficulties. Some counties contain no forested locations, while others have very few locations at all. When a particular predictor-class combination is unobserved in the training data, the estimated probability is zero. Due to the multiplicative structure in <a href="#eq-ind-probs" class="quarto-xref">Equation&nbsp;<span>17.3</span></a>, a single zero probability causes the entire class-conditional probability to equal zero, regardless of the values of other predictors. This results in the class receiving zero posterior probability, which is clearly problematic, especially when it occurs with all classes.</p>
<p>Several smoothing methods address this issue by adjusting probability estimates away from the boundaries of zero and one. One approach employs a Beta-Binomial model, as described in <a href="iterative-search.html#eq-beta-bin-mean" class="quarto-xref">Equation&nbsp;<span>12.4</span></a>, which shrinks extreme estimates toward more moderate values. For example, consider a county with ten locations, none of which are forested. The Beta-Binomial method with <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\beta = 3\)</span> would pull the estimate from 0.0% to 7.1%.</p>
<p>Alternatively, Laplace smoothing <span class="citation" data-cites="Manning2009">(<a href="#ref-Manning2009" role="doc-biblioref">Manning, Raghavan, and Schutze 2009, sec. 11.3.2</a>)</span> provides another approach:</p>
<p><span id="eq-laplace-smooth"><span class="math display">\[
\hat{p}_{k} = \frac {x_k+\alpha }{n_k+\alpha C_{x}}
\tag{17.4}\]</span></span></p>
<p>where <span class="math inline">\(C_{x}\)</span> is the number of levels of the qualitative predictor and <span class="math inline">\(x_k\)</span> and <span class="math inline">\(n_k\)</span> are the number of events and locations, respectively. Here, with <span class="math inline">\(C_x = 2\)</span> and <span class="math inline">\(\alpha = 1\)</span>, the new estimate is 8.3%.</p>
<p>Despite the potentially unrealistic assumption of conditional independence among predictors, naive Bayes classifiers can demonstrate competitive predictive performance with other methods. In addition, naive Bayes models can be computationally efficient since many of the calculations can be done in parallel. Depending on how the conditional densities are computed, they can be easily added to database tables, and the final prediction can be executed using highly efficient SQL.</p>
<p>One potential downside for this model is that there is a tendency for the class probability estimates to be seriously miscalibrated. Since we treat the predictors as independent entities, we end up multiplying many probabilities together. When the predictors <em>are</em> related, this means including redundant terms. When any set of probabilities is multiplied together, there is a tendency for the values to become polar; the products tend to migrate towards zero or one. This may not effect the ability of the model to separate classes, but it will compromise the accuracy of the probability estimates. Many predictions, when incorrect, are <em>confidently</em> incorrect.</p>
<p><a href="#fig-nb-probs" class="quarto-xref">Figure&nbsp;<span>17.2</span></a> illustrates the probability distribution and calibration performance for one model candidate applied to the forestation data. The left panel demonstrates that the predicted probabilities exhibit a bimodal distribution, with the majority of observations concentrated near the boundaries of zero and one, while relatively few observations fall in the intermediate probability range. The right panel presents the corresponding calibration curve, which reveals severe miscalibration. Specifically, observations assigned predicted probabilities near zero exhibit observed event rates of approximately 10%, while those with predicted probabilities approaching one show similarly discordant observed rates. The intermediate probabilities are no better, with observed event rates clustering near 0.5 across multiple bins regardless of predicted probability. The Brier score for these data is poor (0.128) but the area under the ROC curve (0.922) shows good separation of the classes by the model.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nb-probs" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nb-probs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-nb-probs-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nb-probs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.2: An example of how the probability estimates produced by naive Bayes can have poor calibration properties.
</figcaption>
</figure>
</div>
</div>
</div>
<p>To address these calibration issues, an initial feature filter could be helpful. Reducing the number of predictors may improve probability estimates by mitigating potential overfitting or reducing noise from uninformative predictors. For the forestation data, a random forest model was computed (within each resample), and predictors were ranked according to their permutation importance scores. We can choose a proportion of parameters to retain and then fit our naive Bayes models. Proportions of 20%, 40%, …, 100% were used during the tuning process. <a href="#fig-forested-rf-imp" class="quarto-xref">Figure&nbsp;<span>17.3</span></a> displays the resulting importance scores, with error bars representing 90% confidence intervals estimated using the 10 replicate values produced during cross-validation.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-forested-rf-imp" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-forested-rf-imp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-forested-rf-imp-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-forested-rf-imp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.3: Aggregate feature rankings using a random forest importance score on the forestation data. The bands reflect 90% confidence intervals computed using replicates over cross-validation.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The results show that, according to random forest, annual precipitation has, by far, the largest effect on the prediction function. The second tier of importance includes maximum vapor pressure and longitude predictors, which exhibit nearly equivalent importance scores. A moderate cluster of predictors shows intermediate importance values, while several predictors—including county, year, eastness, and westness—yield low importance scores.</p>
<p>It is important to note that variable importance measures are model-specific and should not be interpreted as universal or absolute assessments of predictor relevance<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. In this context, the random forest importance scores serve as a filtering mechanism to identify and remove potentially irrelevant predictors prior to fitting the classification model, rather than as definitive decisions regarding predictor-response relationships.</p>
<p>Additionally, we have summarized the results over resamples for visualization. In practice, we execute the random forest within each resample and then fit the model on that resample’s listing of which predictors are “important”. We <em>do not</em> compute the importance once and use the same ranking across resamples.</p>
<p>For the forestation data, the naive Bayes kernel adjustment parameter was tuned over values ranging from 0.5 to 1.5, while the Laplace smoothing parameter was held constant at <span class="math inline">\(\alpha = 1\)</span>. Beyond feature selection, no additional preprocessing was applied; the categorical counties remained as-is , and no transformations were applied to the data.</p>
<p>The tuning results for Brier score and ROC AUC are presented in <a href="#fig-nb-tune" class="quarto-xref">Figure&nbsp;<span>17.4</span></a>. Both metrics achieved optimal performance when the full predictor set was retained, with substantial performance degradation observed when only 20% of predictors were included. Across all feature selection thresholds, the results favor smaller kernel bandwidths (more wiggly). However, the magnitude of improvement attributable to bandwidth adjustment is modest relative to the effect of feature selection proportion.</p>
<p>The predictions for the <em>best</em> candidate are those shown in <a href="#fig-nb-probs" class="quarto-xref">Figure&nbsp;<span>17.2</span></a>. As noted there, the Brier score was mediocre while the ROC curve results indicate that the model can do a good job at qualitative separating the classes.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nb-tune" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nb-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-nb-tune-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:55.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nb-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.4: Naive Bayes tuning results for the forestation data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>It is also worth noting that, for this model, the Brier and ROC AUC values point to similar findings. Across all of the candidates, the correlation between these metrics was -0.95; they tend to agree very often. The correlation between these metrics and cross-entropy was slightly lower: -0.72 and 0.85 for the Brier score and the ROC AUC, respectively.</p>
<p>Now, let’s turn our attention to the other notable variation of LDA: flexible discriminant analysis.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-nonlinear.html#sec-naive-bayes"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-fda" class="level3" data-number="17.1.2">
<h3 data-number="17.1.2" class="anchored" data-anchor-id="sec-fda"><span class="header-section-number">17.1.2</span> Flexible Discriminants</h3>
<p>Flexible discriminant analysis (FDA) by <span class="citation" data-cites="hastie1994flexible">Hastie, Tibshirani, and Buja (<a href="#ref-hastie1994flexible" role="doc-biblioref">1994</a>)</span> extends LDA to accommodate more complex discriminant functions. The derivation involves substantial linear algebra; you can skip to the “Important” box below for the key concepts.</p>
<p><span class="citation" data-cites="hand1981discrimination">Hand (<a href="#ref-hand1981discrimination" role="doc-biblioref">1981</a>)</span> and <span class="citation" data-cites="breiman1984nonlinear">Breiman and Ihaka (<a href="#ref-breiman1984nonlinear" role="doc-biblioref">1984</a>)</span> demonstrated that the LDA model can be estimated through a series of basic linear regressions. To do this, we first construct an <span class="math inline">\(n_{tr}\times C\)</span> indicator matrix <span class="math inline">\(\boldsymbol{Y}\)</span> where each column contains binary indicators for the corresponding class. Next, create a <span class="math inline">\(C\times (C-1)\)</span> matrix of initial “scores” <span class="math inline">\(\boldsymbol{\Omega}\)</span> using a contrast function (see <a href="categorical-predictors.html#sec-indicators" class="quarto-xref"><span>Section 6.2</span></a>). <span class="citation" data-cites="mda_2024">Hastie and Tibshirani (<a href="#ref-mda_2024" role="doc-biblioref">2024</a>)</span> employ a Helmert-like contrast matrix. For <span class="math inline">\(C=3\)</span> classes, the Helmert contrast is</p>
<p><span class="math display">\[
Helmert =
\begin{pmatrix}
-1 &amp; -1 \\
\phantom{-}  1 &amp; -1 \\
\phantom{-}  0 &amp;\phantom{-}   2 \\
\end{pmatrix}
\]</span></p>
<p>which is then normalized using a function of training set class proportions. For example, with <span class="math inline">\(C = 3\)</span> equally frequent classes, the initial score matrix is:</p>
<p><span class="math display">\[
\boldsymbol{\Omega} =
\begin{pmatrix}
\phantom{-} 1.22 &amp; -0.71 \\
-1.22 &amp; -0.71 \\
\phantom{-} 0.00 &amp; \phantom{-} 1.41 \\
\end{pmatrix}
\]</span></p>
<p>Being a contrast matrix, the column means are zero and have covariances of zero.</p>
<p>The modified response matrix for linear regression is computed as <span class="math inline">\(\boldsymbol{\Omega}^* = \boldsymbol{Y}\boldsymbol{\Omega}\)</span>. Using the predictor matrix <span class="math inline">\(\boldsymbol{X}\)</span> and the columns of <span class="math inline">\(\boldsymbol{\Omega}^*\)</span> as outcomes, multivariate linear regression yields coefficient matrix <span class="math inline">\(\widehat{\boldsymbol{B}}\)</span> and fitted values <span class="math inline">\(\hat{\boldsymbol{\Omega}} = \boldsymbol{X}\hat{\boldsymbol{B}}\)</span>.</p>
<p>An eigendecomposition of <span class="math inline">\(\boldsymbol{\Omega}'\hat{\boldsymbol{\Omega}}\)</span> produces eigenvalues and a matrix of eigenvectors <span class="math inline">\(\boldsymbol{\Phi}\)</span>.</p>
<p>The matrix <span class="math inline">\(\boldsymbol{\Phi}\)</span> scales the linear regression predictions to approximate the discriminant functions in <a href="#eq-discrim-func" class="quarto-xref">Equation&nbsp;<span>17.2</span></a>. For a new sample <span class="math inline">\(\boldsymbol{x}\)</span>, the vector of FDA discriminant functions is:</p>
<p><span id="eq-fda-discrim"><span class="math display">\[
\boldsymbol{D}(\boldsymbol{x}) \approx \hat{\boldsymbol{\Omega}}\Phi = \boldsymbol{x}'\hat{\boldsymbol{B}}\Phi
\tag{17.5}\]</span></span></p>
<p>This <span class="math inline">\((C-1) \times 1\)</span> vector contains entries for all but the reference class<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. Class probabilities are computed from the discriminant functions (typically using the softmax transformation). We compute last probability estimate using <span class="math inline">\(\hat{\pi}_C = 1 - \sum_k^{C-1}\hat{\pi}_k\)</span>.</p>
<div class="important-box">
<p>The magic in <span class="citation" data-cites="hastie1994flexible">Hastie, Tibshirani, and Buja (<a href="#ref-hastie1994flexible" role="doc-biblioref">1994</a>)</span> is that linear regression can be replaced with any regression method while maintaining the discriminant analysis framework. For example, ridge or lasso regression can produce improved predictions that are then converted to class probabilities by rescaling with <span class="math inline">\(\boldsymbol{\Phi}\)</span> as in <a href="#eq-fda-discrim" class="quarto-xref">Equation&nbsp;<span>17.5</span></a>.</p>
</div>
<p><span class="citation" data-cites="hastie1994flexible">Hastie, Tibshirani, and Buja (<a href="#ref-hastie1994flexible" role="doc-biblioref">1994</a>)</span> originally implemented FDA using multivariate adaptive regression splines (MARS) and a penalized spline method called Bruto. In practice, we have found that the best application of FDA uses MARS, which will be discussed further in <span class="quarto-unresolved-ref">?sec-mars</span>.</p>
<p>As we will see in <span class="quarto-unresolved-ref">?sec-reg-nonlinear</span>, MARS is a forward stepwise procedure that sequentially adds pairs of features for each predictor. These features are derived via a <em>hinge function</em>: <span class="math inline">\(h(x) = xI(x &gt; 0)\)</span>. At each iteration, MARS identifies the optimal predictor and split point <span class="math inline">\(a\)</span>, then adds two terms: <span class="math inline">\(h(x - a)\)</span> and <span class="math inline">\(h(a - x)\)</span>. The first term equals zero for <span class="math inline">\(x &lt; a\)</span>, while the second equals zero for <span class="math inline">\(x &gt; a\)</span>. <a href="#fig-fda-hinge" class="quarto-xref">Figure&nbsp;<span>17.5</span></a> illustrates these “left” and “right” hinge functions.</p>
<div id="fig-fda-hinge" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fda-hinge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| label: shiny-fda-hinge
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
#| fig-alt: Examples of a single set of MARS features. 
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(tidyr)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
ui &lt;- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(xs = c(-2, 8, -2), sm = 4),
    sliderInput(
      "beta_1",
      label = "Left slope",
      min = -4.0,
      max = 4.0,
      step = 0.5,
      value = 1
    ),
    sliderInput(
      "beta_2",
      label = "Right slope",
      min = -4.0,
      max = 4.0,
      step = 0.5,
      value = 1
    ),
    sliderInput(
      "split",
      label = "Split",
      min = 0,
      max = 10,
      step = 0.1,
      value = 5
    )
  ),
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(xs = c(-2, 8, -2)),
    as_fill_carrier(plotOutput("plots"))
  )
)

server &lt;- function(input, output) {
  theme_set(theme_bw())
  output$plots &lt;-
    renderPlot(
      {
        dat &lt;- tibble(x = seq(0, 10, by = 0.05))

        h_right &lt;- function(x, a) ifelse(x &gt; a, x - a, 0)
        h_left &lt;- function(x, a) ifelse(x &lt; a, a - x, 0)

        split &lt;- input$split
        beta_0 &lt;- 0
        beta_1 &lt;- input$beta_1
        beta_2 &lt;- input$beta_2

        feat &lt;-
          dat |&gt;
          mutate(
            left = h_left(x, split),
            right = h_right(x, split),
            pred = beta_0 + beta_1 * left + beta_2 * right
          )

        p &lt;-
          feat |&gt;
          pivot_longer(
            cols = c(left, right, pred),
            names_to = "side",
            values_to = "value"
          ) |&gt;
          mutate(
            group = case_when(
              side == "left" ~ "Left Hinge: h(x - a)",
              side == "right" ~ "Right Hinge: h(a - x)",
              TRUE ~ "Regression Equation"
            ),
            group = factor(
              group,
              levels = c(
                "Left Hinge: h(x - a)",
                "Right Hinge: h(a - x)",
                "Regression Equation"
              )
            )
          ) |&gt;
          ggplot(aes(x, value)) +
          geom_line() +
          geom_vline(xintercept = split, lty = 3) +
          facet_wrap(~group, ncol = 1, scale = "free_y") +
          labs(x = "Original Predictor", y = "Feature Value")

        print(p)
      },
      res = 100
    )
}

app &lt;- shinyApp(ui, server)

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fda-hinge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.5: An example of how MARS partitions the predictor space into two distinct regions, each with its own regression line that connects at the split point. For the bottom panel, we assume an intercept of <span class="math inline">\(\beta_0 = 0\)</span> for <a href="#fig-fda-hinge" class="quarto-xref">Figure&nbsp;<span>17.5</span></a>.
</figcaption>
</figure>
</div>
<p>As seen in the figure, the hinge feature <span class="math inline">\(h(x − a)\)</span> isolates the effect of the predictor to values greater than <span class="math inline">\(a\)</span>. Likewise, its companion feature <span class="math inline">\(h(a − x)\)</span> restricts the effect of the predictor only when <span class="math inline">\(x &lt; a\)</span>. If <span class="math inline">\(x\)</span> is the first predictor selected by MARS, these functions create a flexible segmented regression:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 h(x_{i1} - a) + \beta_2 h(a - x_{i1}) + \epsilon_i
\]</span></p>
<p>which is estimated using ordinary linear regression. The figure illustrates the impact of different values of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>.</p>
<p>The “growing” phase of MARS creates new pairs of features by evaluating all prospective pairs, retaining the pair that most improves model fit, and repeating until a user-specified maximum number of terms is reached.</p>
<p>During the training process, MARS may split the same predictor multiple times when the predictor-response relationship exhibits substantial nonlinearity. For categorical predictors with <span class="math inline">\(C\)</span> levels, MARS automatically generates <span class="math inline">\(C\)</span> binary indicators that can then be split. The algorithm can also include a predictor in its original form (without hinge functions) if that representation best improves the model at a given iteration. The maximum number of model terms is an important tuning parameter that controls model complexity.</p>
<p>Thus far, we have described MARS when it is constrained to produce an _additive_model (i.e., where each feature depends on a single predictor). MARS can also generate interaction terms. After selecting an initial pair of hinge functions on one predictor, the algorithm conducts a secondary search over all other predictors (and their candidate split points) to find the combination that most improves the model. This produces four new features, each representing a distinct two-dimensional region of the predictor space. The degree of interaction (typically one or two) is a tuning parameter; higher-order interactions are possible but computationally prohibitive.</p>
<p>Following the growing phase, MARS performs a backward pass that sequentially removes terms using <em>generalized cross-validation</em> (GCV), an efficient approximation to leave-one-out cross-validation that is available when parameters are estimated by ordinary least squares <span class="citation" data-cites="golub1979generalized">(<a href="#ref-golub1979generalized" role="doc-biblioref">Golub, Heath, and Wahba 1979</a>)</span>. GCV quantifies the increase in prediction error when a given term is removed. During pruning, one or both members of a hinge pair may be removed. The final model consists of the retained features, their estimated coefficients, and the <span class="math inline">\(\Phi\)</span> matrix, which together define the prediction function.</p>
<p>FDA-MARS models can produce class boundaries that are segmented or trapezoidal. For example, the “Medium Complexity” and “High Complexity” panels in <a href="cls-linear.html#fig-cls-boundaries" class="quarto-xref">Figure&nbsp;<span>16.1</span></a> were generated using FDA-MARS, with interactions and differing numbers of retained features.</p>
<p>Regarding preprocessing, MARS internally converts categorical predictors to binary indicators, so this transformation should not prior to model fitting. Since MARS estimates feature coefficients using ordinary least squares, standard preprocessing techniques for linear models apply. For instance, transforming predictors to improve symmetry may enhance performance. Missing values must also be imputed prior to modeling.</p>
<p>MARS performs automatic feature selection: if a predictor is never used in a basis function or if all terms containing that predictor are removed during pruning, the final model is independent of that predictor. This property extends to FDA-MARS. As illustrated in <a href="feature-selection.html#fig-irrelevant-predictors" class="quarto-xref">Figure&nbsp;<span>13.1</span></a>, irrelevant predictors can be included during training without adverse effects, eliminating the need for a separate feature selection step. However, including irrelevant features will increase computation time. This may or may not have a practical impact, depending on the data set size and number of irrelevant features.</p>
<p>For using MARS with FDA with our forestation data, we initially generated a total of 200 features (apart from the intercept) during the growing phase. To tune the model, we considered additive and interaction models as well as using forward and backward removal strategies for the pruning phase. The number of retained terms was optimized to be between 2 and 50 final model terms.</p>
<p>We can see how this worked out in <a href="#fig-fda-tune" class="quarto-xref">Figure&nbsp;<span>17.6</span></a>. Performance leveled off around 25-35 terms for each pruning method, with forward selection showing a tiny advantage for these data. Models with interaction effects were uniformly more successful than additive models. An interaction model using forward selection and 30 terms appears to be a reasonable choice. This configuration had an estimated Brier score of 0.083 and an area under the ROC curve of 0.95.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fda-tune" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fda-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-fda-tune-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fda-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.6: FDA-MDA tuning results for the forestation data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Once the FDA-MARS model was fit to the training set, the results was 25 total coefficients. The implementation of MARS checks several stopping conditions for the growing phase based on the coefficient of determination (a.k.a. R<sup>2</sup>), including:</p>
<ul>
<li>Reached a R<sup>2</sup> of 0.999 or more.</li>
<li>No new term increases R<sup>2</sup></li>
<li>Adding a term changes R<sup>2</sup> by less than 0.001.</li>
</ul>
<p>The last condition triggered the training procedure to stop at 25 model terms instead of the 30 that were requested. Of these terms, 6 used a single predictor and 18 were interaction terms. The categorical predictor (county) was used 3 times in both main effects and interactions. To give the reader a sense of what the linear regression model looks like, <a href="#tbl-fda-coefs" class="quarto-xref">Table&nbsp;<span>17.1</span></a> shows the set of coefficients, the model terms, and split points.</p>
<div class="columns">
<div class="column" style="width:10%;">

</div><div class="column" style="width:80%;">
<div id="tbl-fda-coefs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-fda-coefs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="frqnejfhcr" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#frqnejfhcr table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#frqnejfhcr thead, #frqnejfhcr tbody, #frqnejfhcr tfoot, #frqnejfhcr tr, #frqnejfhcr td, #frqnejfhcr th {
  border-style: none;
}

#frqnejfhcr p {
  margin: 0;
  padding: 0;
}

#frqnejfhcr .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#frqnejfhcr .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#frqnejfhcr .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#frqnejfhcr .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#frqnejfhcr .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#frqnejfhcr .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#frqnejfhcr .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#frqnejfhcr .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#frqnejfhcr .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#frqnejfhcr .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#frqnejfhcr .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#frqnejfhcr .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#frqnejfhcr .gt_spanner_row {
  border-bottom-style: hidden;
}

#frqnejfhcr .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#frqnejfhcr .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#frqnejfhcr .gt_from_md > :first-child {
  margin-top: 0;
}

#frqnejfhcr .gt_from_md > :last-child {
  margin-bottom: 0;
}

#frqnejfhcr .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#frqnejfhcr .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#frqnejfhcr .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#frqnejfhcr .gt_row_group_first td {
  border-top-width: 2px;
}

#frqnejfhcr .gt_row_group_first th {
  border-top-width: 2px;
}

#frqnejfhcr .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#frqnejfhcr .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#frqnejfhcr .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#frqnejfhcr .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#frqnejfhcr .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#frqnejfhcr .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#frqnejfhcr .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#frqnejfhcr .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#frqnejfhcr .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#frqnejfhcr .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#frqnejfhcr .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#frqnejfhcr .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#frqnejfhcr .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#frqnejfhcr .gt_left {
  text-align: left;
}

#frqnejfhcr .gt_center {
  text-align: center;
}

#frqnejfhcr .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#frqnejfhcr .gt_font_normal {
  font-weight: normal;
}

#frqnejfhcr .gt_font_bold {
  font-weight: bold;
}

#frqnejfhcr .gt_font_italic {
  font-style: italic;
}

#frqnejfhcr .gt_super {
  font-size: 65%;
}

#frqnejfhcr .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#frqnejfhcr .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#frqnejfhcr .gt_indent_1 {
  text-indent: 5px;
}

#frqnejfhcr .gt_indent_2 {
  text-indent: 10px;
}

#frqnejfhcr .gt_indent_3 {
  text-indent: 15px;
}

#frqnejfhcr .gt_indent_4 {
  text-indent: 20px;
}

#frqnejfhcr .gt_indent_5 {
  text-indent: 25px;
}

#frqnejfhcr .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#frqnejfhcr div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table table table-sm table-striped small" data-quarto-bootstrap="false">
<thead>
<tr class="gt_col_headings header">
<th id="Model-Term" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Model Term</th>
<th id="value" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">value</th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">(Intercept)</td>
<td class="gt_row gt_right" headers="value">−1.04</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(834 - annual precipitation) x h(annual maximum temperature - 10.3)</td>
<td class="gt_row gt_right" headers="value">0.0000138</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(43 - roughness)</td>
<td class="gt_row gt_right" headers="value">0.0173</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(annual precipitation - 650) x h(8.65 - annual maximum temperature)</td>
<td class="gt_row gt_right" headers="value">0.000135</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(annual mean temperature - 7.31) x h(longitude - - 118.905)</td>
<td class="gt_row gt_right" headers="value">0.0876</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(650 - annual precipitation)</td>
<td class="gt_row gt_right" headers="value">0.00478</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(1920 - annual precipitation) x h(annual mean temperature - 7.31)</td>
<td class="gt_row gt_right" headers="value">0.000180</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(650 - annual precipitation) x h(annual mean temperature - 7.95)</td>
<td class="gt_row gt_right" headers="value">−0.00122</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(650 - annual precipitation) x county: ferry</td>
<td class="gt_row gt_right" headers="value">−0.00431</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(650 - annual precipitation) x county: stevens</td>
<td class="gt_row gt_right" headers="value">−0.00409</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(1348 - maximum vapor) x county: whitman</td>
<td class="gt_row gt_right" headers="value">0.00978</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(141 - elevation) x h(annual maximum temperature - 10.3)</td>
<td class="gt_row gt_right" headers="value">0.000645</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(650 - annual precipitation) x h(7.95 - annual mean temperature)</td>
<td class="gt_row gt_right" headers="value">−0.000898</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(annual mean temperature - 7.31) x h( - 118.905 - longitude)</td>
<td class="gt_row gt_right" headers="value">−0.0332</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(year - 2020) x h(43 - roughness)</td>
<td class="gt_row gt_right" headers="value">−0.0158</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(0.3 - dew temperature) x h(annual mean temperature - 7.31)</td>
<td class="gt_row gt_right" headers="value">−0.708</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(43 - roughness) x h(1018 - annual precipitation)</td>
<td class="gt_row gt_right" headers="value">−0.0000238</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(43 - roughness) x h(annual precipitation - 1018)</td>
<td class="gt_row gt_right" headers="value">−0.00000822</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(roughness - 43)</td>
<td class="gt_row gt_right" headers="value">0.00164</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(annual mean temperature - 7.31)</td>
<td class="gt_row gt_right" headers="value">0.151</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(10.3 - annual maximum temperature)</td>
<td class="gt_row gt_right" headers="value">0.0876</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(roughness - 7) x h(annual mean temperature - 7.31)</td>
<td class="gt_row gt_right" headers="value">−0.000739</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(annual precipitation - 650)</td>
<td class="gt_row gt_right" headers="value">0.0000552</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Model Term">h(elevation - 141) x h(annual maximum temperature - 10.3)</td>
<td class="gt_row gt_right" headers="value">0.0000548</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Model Term">h(7 - roughness) x h(annual mean temperature - 7.31)</td>
<td class="gt_row gt_right" headers="value">0.00595</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-fda-coefs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;17.1: Model terms contained used the final FDA-MARS fit from the training set. There were 25 total coefficients, 6 of which used a single predictor, and 18 were interaction terms. The categorical predictor (county) was used in 3 terms.
</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div>
</div>
<p>To better understand what is driving the model, MARS has an <em>internal</em> variable importance metric. The method estimates variable importance based on the improvement in model fit as terms are added. During the forward pass, MARS quantifies the reduction in GCV associated with each new basis function and attributes this improvement to the corresponding predictor(s). These GCV reductions are summed across all terms involving each predictor and scaled to range from 0 to 100, where 0 indicates the predictor does not appear in the final model. For our model, <a href="#fig-forested-fda-imp" class="quarto-xref">Figure&nbsp;<span>17.7</span></a> displays the results. Annual precipitation and annual maximum temperature were the most influential predictors, each receiving an importance score of 100. Terrain roughness, annual mean temperature, and longitude also contributed substantially to the classifier. There were 6 predictors that were not used at all: annual minimum temperature, eastness, january minimum temperature, latitude, minimum vapor, and northness.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-forested-fda-imp" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-forested-fda-imp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-forested-fda-imp-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-forested-fda-imp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.7: MARS feature importance for the final forested model.
</figcaption>
</figure>
</div>
</div>
</div>
<p>FDA via MARS occupies a middle ground between fully interpretable models and black-box methods. While it may not achieve optimal predictive performance, it often approaches it while still being interpretable through visualization, particularly when the model is additive.</p>
<p>For example, suppose we chose an additive model based on the tuning results in <a href="#fig-fda-tune" class="quarto-xref">Figure&nbsp;<span>17.6</span></a>. In an additive model, the functional form relating each predictor to the outcome is independent of the other predictors. Although the values of other predictors shift the range of predicted probabilities, the shape of each predictor’s effect remains constant. This property enables us to construct partial dependence plots showing how predicted class probabilities vary with each predictor while holding others fixed. <a href="#fig-fda-profiles" class="quarto-xref">Figure&nbsp;<span>17.8</span></a> illustrates such profiles from an additive fit, enabling an interpretation of the model’s behavior.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fda-profiles" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fda-profiles-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-fda-profiles-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fda-profiles-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.8: Profile plots showing how each predictor related to the probability of forestation when an additive FDA-MDA is used. The y-axis has no numbers since the probabilities are relative.
</figcaption>
</figure>
</div>
</div>
</div>
<p>For example, we can say that:</p>
<ul>
<li>The probability of forestation has notable jump after the year 2020.</li>
<li>The probability increases sharply when annual precipitation exceeds approximately 500 mm and remains elevated thereafter (holding other predictors constant).</li>
<li>The downward spike in the longitude corresponds to the unforested region east of Seattle.</li>
</ul>
<p>and so on. These patterns illustrate how individual predictors influence the classification boundary in an interpretable manner.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-nonlinear.html#sec-fda"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
</section>
<section id="sec-cls-knn" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="sec-cls-knn"><span class="header-section-number">17.2</span> K-Nearest Neighbors</h2>
<p>K-nearest neighbors (KNN) is predicated on the idea that data points that are close to one another in the predictor space should have outcomes that are also similar. If we are predicting a new sample, we find the <em>K</em> training set points that are “closest” to the new data point and use their outcome values to estimate the prediction. There is essentially no training here; everything happens when the sample is being predicted. KNN falls into a class of techniques called instance-based learning since it stores the training set instances and directly uses these data during prediction.</p>
<p>What does “similar” and/or “close” mean? The operational definition of “proximity” or “closeness” requires specifying either a distance or similarity metric. While Euclidean distance is a common metric, the Minkowski distance provides a more flexible framework. For a new observation <span class="math inline">\(\boldsymbol{u}\)</span> with <span class="math inline">\(p\)</span> predictors, the Minkowski distance is defined as:</p>
<p><span class="math display">\[
d(\boldsymbol{u}, \boldsymbol{x}) ={\biggl (}\sum _{j=1}^{p}|u_j-x_{j}|^{degree}{\biggr )}^{\frac {1}{degree}},
\]</span></p>
<p>where <span class="math inline">\(degree\)</span> is the distance parameter. This formulation yields Euclidean distance when <span class="math inline">\(degree = 2\)</span>, Manhattan distance (also called taxicab or city-block distance) when <span class="math inline">\(degree = 1\)</span>, and intermediate distance metrics for other values of <span class="math inline">\(degree\)</span>.</p>
<p>The choice of distance metric can substantially influence model performance and can be considered a tuning parameter in practice (via the Minkowski degree).</p>
<p>Beyond the Minkowski family, several specialized distance metrics may be appropriate depending on the structure and nature of the predictor space:</p>
<ul>
<li><p>Geodesic distance (discussed in <a href="embeddings.html#sec-isomap" class="quarto-xref"><span>Section 7.3.1</span></a>) is based on a general distance matrix to construct a graph-based representation of inter-point relationships.</p></li>
<li><p>Haversine distance (introduced in <a href="embeddings.html#sec-centroids" class="quarto-xref"><span>Section 7.4</span></a>) accounts for the curvature of the Earth’s surface and is the appropriate metric when predictors represent geographic coordinates (latitude and longitude).</p></li>
<li><p>Mahalanobis distance <span class="citation" data-cites="JohnsonWichern DeMaesschalck20001">(<a href="#ref-DeMaesschalck20001" role="doc-biblioref">De Maesschalck, Jouan-Rimbaud, and Massart 2000</a>; <a href="#ref-JohnsonWichern" role="doc-biblioref">Johnson and Wichern 2007</a>)</span> incorporates the covariance structure among predictors, effectively scaling predictor differences by their variances and correlations. This metric is particularly valuable when predictors exhibit substantial correlation and sufficient data are available to reliably estimate the covariance matrix. The Mahalanobis distance has a direct connection to the mathematics of discriminant analysis.</p></li>
</ul>
<p>These distance-based metrics require numeric predictors. For the Minkowski distance, predictors must be expressed in the same units to prevent variables with larger scales from dominating the distance calculation. This is typically accomplished through centering and scaling (standardization) or other normalization techniques. Additionally, addressing skewness through appropriate transformations (e.g., Box-Cox, Yeo-Johnson, orderNorm, etc.) prior to distance calculation is recommended, as highly skewed distributions can distort relationships and degrade model performance.</p>
<p>For qualitative predictors, one approach is to encode them as indicator (dummy) variables, which should then be preprocessed in the same way as continuous predictors. However, when the predictor set contains a mixture of data types, Gower distance <span class="citation" data-cites="gower">(<a href="#ref-gower" role="doc-biblioref">Gower 1971</a>)</span> provides a unified framework that accommodates different variable types without requiring uniform encoding. Gower distance computes a dissimilarity measure separately for each predictor according to its data type, then aggregates these component distances:</p>
<p><span class="math display">\[
d(\boldsymbol{u}, \boldsymbol{x}) =\frac {1}{p}\sum _{j=1}^{p}d_j(u_j, x_{j}),
\]</span></p>
<p>where <span class="math inline">\(d_j\)</span> denotes the distance function for predictor <span class="math inline">\(j\)</span>. For continuous predictors, the component distance is range-normalized:</p>
<p><span class="math display">\[
d_j(u_j, x_{j})=1-{\frac {|u_{j}-x_{j}|}{R_{j}}}
\]</span></p>
<p>where <span class="math inline">\(R_j\)</span> represents the range of predictor <span class="math inline">\(j\)</span> in the training set. This normalization ensures all continuous predictors contribute equally regardless of their original scales.</p>
<p>For categorical predictors, the component distance is a simple mismatch indicator, <span class="math inline">\(d_j(u_j, x_{j}) = I(u_j = x_j)\)</span>. This binary metric assigns zero distance for matching categories and unit distance otherwise.</p>
<p>For a comprehensive treatment of distance metrics in the context of nearest neighbor methods, see <span class="citation" data-cites="cunningham2021k">Cunningham and Delany (<a href="#ref-cunningham2021k" role="doc-biblioref">2021</a>)</span>.</p>
<p>For classification problems, KNN identifies the <em>K</em> nearest neighbors and estimates class probabilities as the proportion of neighbors belonging to each class:</p>
<p><span class="math display">\[
\widehat{\pi}_{c} = \frac{1}{K}\sum_{k=1}^K y_{kc}
\]</span></p>
<p>where <span class="math inline">\(y_{kc}\)</span> is a binary indicator equal to 1 if neighbor <span class="math inline">\(k\)</span> belongs to class <span class="math inline">\(c\)</span>, and 0 otherwise. For small values of K, these empirical probability estimates can be unreliable, particularly when certain classes are underrepresented among the neighbors. The Laplace correction (previously discussed in <a href="#sec-naive-bayes" class="quarto-xref"><span>Section 17.1.1</span></a>) can be used to regularize the estimate:</p>
<p><span class="math display">\[
\widehat{\pi}_{c} = \frac{1}{K}\sum_{k=1}^K \frac{y_{kc} +\alpha}{K + \alpha C}
\]</span></p>
<p>where <span class="math inline">\(C\)</span> denotes the number of classes and <span class="math inline">\(\alpha \in [0, 2]\)</span> is a smoothing parameter. Setting <span class="math inline">\(\alpha = 0\)</span> yields the unregularized estimate, while <span class="math inline">\(\alpha &gt; 0\)</span> shrinks extreme probabilities toward a uniform distribution.</p>
<p>It is common to fix <em>K</em> and collect all neighbors, regardless of their distances to the query point. Alternatively, one can put a cap on the maximum distance or use a more informative approach of using weighted distances:</p>
<p><span class="math display">\[
\widehat{p}_{c} = \frac{1}{W}\sum_{k=1}^K g(d_k)\;y_{kc}
\]</span></p>
<p>where <span class="math inline">\(g()\)</span> is a kernel function that weights the distances and <span class="math inline">\(W\)</span> is the sum of the <span class="math inline">\(g(d_k)\)</span> <span class="citation" data-cites="dudani1976distance zuo2008kernel Samworth">(<a href="#ref-dudani1976distance" role="doc-biblioref">Dudani 1976</a>; <a href="#ref-zuo2008kernel" role="doc-biblioref">Zuo, Zhang, and Wang 2008</a>; <a href="#ref-Samworth" role="doc-biblioref">Samworth 2012</a>)</span>. <a href="#fig-weight-kernels" class="quarto-xref">Figure&nbsp;<span>17.9</span></a> shows some commonly used functions <span class="citation" data-cites="hechenbichler2004weighted">(<a href="#ref-hechenbichler2004weighted" role="doc-biblioref">Hechenbichler and Schliep 2004</a>)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-weight-kernels" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weight-kernels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-weight-kernels-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weight-kernels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.9: Different kernel functions to downweight far-away neighbors. Values have been rescaled to have the same upper value.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Outside of preprocessing methods, the primary tuning parameters are the number of neighbors, the weighting function, and the distance degree (if using a Minkowski distance). Recall from <a href="feature-selection.html#fig-irrelevant-predictors" class="quarto-xref">Figure&nbsp;<span>13.1</span></a> that KNN models can be harmed when used with irrelevant predictors. As will be seen with these data, tuning an initial feature selection filter can improve model quality.</p>
<p>How well does this technique work for the forestation data? To start, we used preprocessing that included the same random forest importance filter as <a href="#sec-naive-bayes" class="quarto-xref"><span>Section 17.1.1</span></a> (and tune the proportion fo predictors retained). After that, an effect encoding was used for the county column, and then applied an orderNorm transformation to all predictors to standardize them to the same units and potentially remove skewness. The model was tuned over the proportion of features retained (5% to 100%), the number of neighbors (two through fifty), the Minkowski degree parameter (between 0.1 and 2.0), and the kernel functions listed in <a href="#fig-weight-kernels" class="quarto-xref">Figure&nbsp;<span>17.9</span></a>. A total of fifty configurations were evaluated using resampling, and the Brier scores are shown in <a href="#fig-knn-cls-tune" class="quarto-xref">Figure&nbsp;<span>17.10</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-knn-cls-tune" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-knn-cls-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-knn-cls-tune-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knn-cls-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.10: K-nearest neighbors tuning results for the forestation data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Poor results occurred when there were fewer than twenty neighbors, and subsequently, the Brier scores remained consistently low. The distance parameter appeared to favor small values (around 0.5). There are no strong trends in terms of which kernel functions; Gaussian kernels appear to do well overall, but there are configurations that do well using any kernel. Numerically, the best results retained 86.4% of the predictors, used 28 neighbors, a Minkowski degree of 1.65, and a cos kernel function. In the figure above, we can see that there is a small increase in error when the entire predictor set is used. Otherwise, smaller feature sets are underfitting the data. The corresponding Brier score was 0.0778, which is very the smallest Brier score seen so far. The area under the ROC curve was also good, with a resampling estimate of 0.953.</p>
</section>
<section id="sidebar-dot-products" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="sidebar-dot-products"><span class="header-section-number">17.3</span> Sidebar: Dot Products</h2>
<p>Dot products will become important in several subsequent sections of this chapter. While distance metrics (such as those used in KNN) provide one framework for comparing vectors, dot products offer another perspective that captures different geometric properties. Understanding their relationship requires some basic linear algebra and geometry.</p>
<p>Vectors are characterized by two fundamental properties:</p>
<ul>
<li><p>Magnitude (or length) is denoted <span class="math inline">\(||\boldsymbol{a}||\)</span> for a vector <span class="math inline">\(\boldsymbol{a}\)</span> with <span class="math inline">\(p\)</span> elements. The standard measure is the Euclidean norm, computed as the square root of the sum of squared elements: <span class="math inline">\(||\boldsymbol{a}|| = \sqrt{\sum_{i=1}^p a_i^2}\)</span>.</p></li>
<li><p>Direction is defined relative to another reference (say another vector <span class="math inline">\(\boldsymbol{y}\)</span> with the same dimension (<span class="math inline">\(p\)</span>)), typically quantified by the angle <span class="math inline">\(\vartheta\)</span> between two vectors of equal dimension.</p></li>
</ul>
<p>Distance-based methods like KNN depend solely on the magnitude of the difference between vectors and do not consider the angle between them.</p>
<p>The dot product (or inner product) of two <span class="math inline">\(p\)</span>-dimensional vectors is defined as:</p>
<p><span id="eq-dot-prod-sum"><span class="math display">\[
\boldsymbol{a}'\boldsymbol{b} = \sum_{i=1}^p a_ib_i
\tag{17.6}\]</span></span></p>
<p>Note that the dot product does not satisfy the properties of a distance metric since the distance between itself could never be zero unless all of its elements are zero.</p>
<p>Dot products measure both the magnitude of each vector as well as the angle between them. We can rewrite <a href="#eq-dot-prod-sum" class="quarto-xref">Equation&nbsp;<span>17.6</span></a> as</p>
<p><span id="eq-dot-prod-vec"><span class="math display">\[
\boldsymbol{a}'\boldsymbol{b} = ||\boldsymbol{a}||\,||\boldsymbol{b}|| \cos{\vartheta}
\tag{17.7}\]</span></span></p>
<p>This formulation reveals that: The dot product is maximized when vectors are parallel (<span class="math inline">\(\vartheta = 0^{\circ}\)</span>, <span class="math inline">\(\cos\vartheta = 1\)</span>), it equals zero when vectors are orthogonal (<span class="math inline">\(\vartheta = 90^{\circ}\)</span>, <span class="math inline">\(\cos\vartheta = 0\)</span>), and it is negative when vectors point in generally opposite directions (<span class="math inline">\(\vartheta &gt; 90^{\circ}\)</span>). This dual sensitivity to both magnitude and direction distinguishes dot products from distance-based comparisons and makes them particularly useful in certain modeling contexts.</p>
<p>To illustrate, <a href="#fig-dot-products" class="quarto-xref">Figure&nbsp;<span>17.11</span></a> shows the dot products of a vector <span class="math inline">\(\boldsymbol{x} = (2.0, 0.5)\)</span> to a grid of vectors with elements ranging between <span class="math inline">\(\pm 3\)</span>. We can see a diagonal region of light colored values that have dot products close to or at zero. This represents vectors that are orthogonal to <span class="math inline">\(\boldsymbol{x}\)</span>. There is also a dotted line extending to the origin (0.0, 0.0) that coincides with this line.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-dot-products" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dot-products-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-dot-products-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dot-products-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.11: Dot product values of a grid of vectors relative to <span class="math inline">\(\boldsymbol{x} = (2.0, 0.5)\)</span> (the black point). The dashed black line shows the contour of constant dot products and the white dotted line is points to the origin.
</figcaption>
</figure>
</div>
</div>
</div>
<p>What is the dot product of <span class="math inline">\(\boldsymbol{x}\)</span> with itself? That value is <span class="math inline">\(\boldsymbol{x}'\boldsymbol{x} =\)</span> 4.25. The dashed black diagonal line shows which vectors whose dot product is 4.25. These are points that point in the same direction (i.e., <span class="math inline">\(\vartheta = 0^{\circ}\)</span>) and magnitude as our example vector. Note that this line is parallel to the region of zero values, reemphasizing that the zero region is orthogonal to vectors like <span class="math inline">\(\boldsymbol{x}\)</span>.</p>
<p>The vector associated with the largest dot product would move in the same direction as <span class="math inline">\(\boldsymbol{x}\)</span>. Since <span class="math inline">\(||\boldsymbol{x}||\)</span> is fixed and <span class="math inline">\(\cos(\vartheta) = 0^{\circ}\)</span>, the magnitude of the vector would be as large as possible. Conversely, the vector with the smallest dot product will move in the opposite direction and as far as possible. For our <span class="math inline">\(\boldsymbol{x}\)</span>, the dot product is maximized in the upper right corner of <a href="#fig-dot-products" class="quarto-xref">Figure&nbsp;<span>17.11</span></a> and minimized in the lower left corner.</p>
<p>We’ll see the dot product again in <span class="quarto-unresolved-ref">?sec-attention</span> where it is intended to measure the similarity of two embedding vectors within a neural network, and again in <a href="#sec-cls-svm" class="quarto-xref"><span>Section 17.6</span></a> where it is the computational backbone of support vector machines.</p>
</section>
<section id="sec-cls-nnet" class="level2" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="sec-cls-nnet"><span class="header-section-number">17.4</span> Neural Networks via Multilayer Perceptrons</h2>
<p>Neural networks are complex models comprised of multiple hierarchies of <em>latent variables</em> designed to represent different motifs in the data. The model’s architecture consists of a series of <em>layers</em>, each containing a set of units. Neural networks begin with the input layer that contains units representing our predictors. After this are one or more <em>hidden layers</em>, each containing a preset number of hidden units. These are artificial features that are influenced by the layer before. For example, if there were four predictors and the first hidden layer had two hidden units, each hidden unit is defined by an equation that includes each of the predictors. In the broadest sense, it is similar to how, in PCA, each principal component is a linear combination of the predictors. However, the similarities between neural networks and PCA end there.</p>
<p>These connections between units on different layers continue as layers are added. Eventually, the last layer is the <em>output layer</em>. For classification, this usually has as many units as there are class levels in the outcome. <a href="#fig-nnet-cls" class="quarto-xref">Figure&nbsp;<span>17.12</span></a> shows a general architectural diagram for a basic model with a single hidden layer, where the lines indicate that a model parameter exists to connect the nodes.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nnet-cls" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nnet-cls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../premade/mlp.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnet-cls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.12: Architecture of a simple neural network with a single layer of hidden units.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Traditionally, the connections that come into a hidden unit are combined using a linear predictor: each incoming unit has a corresponding slope, called a “weight” in the neural network literature<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. The linear combinations commonly include an intercept term (called a “bias”). These linear predictors are embedded within a nonlinear “activation” function (<span class="math inline">\(\sigma\)</span>) to enable complex patterns. An example of an activation function would be a sigmoidal function, much like the logistic function used in <a href="cls-linear.html#eq-logistic" class="quarto-xref">Equation&nbsp;<span>16.3</span></a>.</p>
<div class="note-box">
<p>In this section we will primarily be concerned with multilayer perceptrons (MLPs), a subset of the broader class of neural networks. MLPs are feed-forward models, meaning each layer only connects to the subsequent layer; there are no feedback loops where future units connect to units in previous layers.</p>
<p>As a counterexample, residual networks <span class="citation" data-cites="he2016deep">(<a href="#ref-he2016deep" role="doc-biblioref">He et al. 2016</a>)</span> are connected to previous layers (a.k.a. skip layers); therefore, they are not multilayer perceptrons. Another popular example is the long-term short memory model for sequential data such as time series <span class="citation" data-cites="hochreiter1997long">(<a href="#ref-hochreiter1997long" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>)</span>.</p>
</div>
<p><br></p>
<p>Let’s walk through the network structure, starting at the input layer. We have predictors <span class="math inline">\(x_1, \ldots x_p\)</span>. These will be mathematically connected to the next layer containing <span class="math inline">\(m\)</span> hidden units <span class="math inline">\(H_\ell\)</span> (<span class="math inline">\(\ell=1, \ldots m\)</span>). First, we create a separate linear predictor for hidden unit <span class="math inline">\(H_l\)</span> denoted as <span class="math inline">\(\beta_{\ell 0} + \beta_{\ell 1} x_{1} + \ldots + \beta_{\ell p} x_{p}\)</span>. We then embed this inside the activation function <span class="math inline">\(\sigma()\)</span> used for this layer so that there is a nonlinear relationship between the predictors and their new representation. Visually<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../premade/nnet_start.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
<p>As we add more layers, each of the first set of hidden units is similarly connected to the hidden units in the next layer, and so on. The layers do not have to use the same activation function. Also, the layers can be specialized, such as the image processing layers shown in <a href="introduction.html#fig-vgg16" class="quarto-xref">Figure&nbsp;<span>1.3</span></a>. For each hidden layer, we have to specify the number of units and the activation function. These, along with the number of layers, are tuning parameters.</p>
<p>Note that the units in the subsequent hidden layer are nested inside linear combinations of the previous hidden layers (plus the input units). These models have sets of nested nonlinear functions and can be very complex, but are mathematically tractable.</p>
<p>For the final layer, the previous set of hidden units is often connected to the <em>output units</em> using a linear activation. For classification, there are often <span class="math inline">\(C\)</span> output units (i.e., one for each class):</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../premade/nnet_end.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
<p>Since linear activation is used for the final layer, the values of the output units (<span class="math inline">\(Y\)</span>) are unlikely to be in the proper units for classification. To remedy this, the raw outputs are normalized to be probability-like via the softmax function:</p>
<p><span class="math display">\[
\pi_{k} = \frac{\exp(Y_{k})}{\sum\limits_{k=1}^C \exp(Y_{k})}
\]</span></p>
<p>In this section, we’ll describe the important elements of multilayer perceptrons, such as the different types of activation functions. We’ll walk through a small but specific model.</p>
<p>In later sections, we’ll also spend a considerable amount of time discussing the various methods for training MLPs. There are also discussions on modern deep learning models that are specifically designed for tabular data. These are discussed below.</p>
<section id="sec-nnet-activation" class="level3" data-number="17.4.1">
<h3 data-number="17.4.1" class="anchored" data-anchor-id="sec-nnet-activation"><span class="header-section-number">17.4.1</span> Activation Functions</h3>
<p>Historically, the most commonly used nonlinear activation functions were sigmoidal. This means that the linear combination of the elements in the previous layer was converted into a “squashed” format, typically bounded (e.g., logistic activation values range between 0 and 1). <a href="#fig-sigmoid-activations" class="quarto-xref">Figure&nbsp;<span>17.13</span></a> shows several such functions. The logistic (<a href="cls-linear.html#eq-logistic" class="quarto-xref">Equation&nbsp;<span>16.3</span></a>) function and hyperbolic tangent are fairly similar in shape, and were typical defaults.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-sigmoid-activations" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sigmoid-activations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-sigmoid-activations-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:65.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sigmoid-activations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.13: Different sigmoidal activation functions.
</figcaption>
</figure>
</div>
</div>
</div>
<p>One issue with bounded sigmoidal shapes, particularly the logistic function, is that they can become problematic when used in conjunction with many hidden layers. Since the logistic function is on [0, 1], repeated multiplication of such values can cause their values to converge to be very close to zero (as seen in <a href="#sec-naive-bayes" class="quarto-xref"><span>Section 17.1.1</span></a>). This is also true of their gradients, leading to the <em>vanishing gradient problem</em>. This is likely not an issue with shallow networks that have a handful of hidden layers.</p>
<p>The tanshrink function diminishes the impact of inputs that are in a neighborhood around zero, where it is sigmoidal. For larger input values, the output is linear in the output (with an offset of <span class="math inline">\(\pm 1\)</span>).</p>
<p>In modern neural networks, the rectified linear unit (ReLU) is the default activation function. As shown in <a href="#fig-relu-activations" class="quarto-xref">Figure&nbsp;<span>17.14</span></a>, the function resembles the hinge functions used by MARS; to the left of zero, the output is zero, and otherwise, it is linear. Much like MARS, this has the effect of isolating some region of the predictor space. However, there are a few key differences. MARS hinge functions affect a single predictor at a time, while neural networks’ activation functions act on the linear combination of parameters. Also, neural networks with multiple layers recursively activate linear combinations inside the prior layers (and so on).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-relu-activations" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-relu-activations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-relu-activations-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-relu-activations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.14: Different ReLU-based activation functions.
</figcaption>
</figure>
</div>
</div>
</div>
<p>There are various modified versions of ReLU that emulate the same form but have continuous derivatives. These are also shown in <a href="#fig-relu-activations" class="quarto-xref">Figure&nbsp;<span>17.14</span></a>, including softplus <span class="citation" data-cites="dugas2000incorporating glorot2011deep">(<a href="#ref-dugas2000incorporating" role="doc-biblioref">Dugas et al. 2000</a>; <a href="#ref-glorot2011deep" role="doc-biblioref">Glorot, Bordes, and Bengio 2011</a>)</span>, gelu <span class="citation" data-cites="hendrycks2016gaussian">(<a href="#ref-hendrycks2016gaussian" role="doc-biblioref">Hendrycks and Gimpel 2016</a>)</span>, silu <span class="citation" data-cites="elfwing2018sigmoid">(<a href="#ref-elfwing2018sigmoid" role="doc-biblioref">Elfwing, Uchibe, and Doya 2018</a>)</span>, elu <span class="citation" data-cites="clevert2015fast">(<a href="#ref-clevert2015fast" role="doc-biblioref">Clevert, Unterthiner, and Hochreiter 2015</a>)</span>, and others. ReLU, and its offspring activation functions, appear to solve the issue of vanishing gradients.</p>
<p>Let’s use an example data set with a simple MLP to demonstrate how ReLU activation works. <a href="#fig-nnet-ex" class="quarto-xref">Figure&nbsp;<span>17.15</span></a> displays the results of a model with a single layer comprised of three hidden units, utilizing ReLU activation. The model converged in 9 epochs and does a good job discriminating between the two classes.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nnet-ex" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nnet-ex-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-nnet-ex-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnet-ex-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.15: An example data set with two predictors and two classes. The black line is the class boundary from an MLP model using three hidden units with ReLU activation.
</figcaption>
</figure>
</div>
</div>
</div>
<p>With two predictors and three hidden units, the first set of coefficients for the model is three sets of linear predictors (i.e., an intercept and two slopes) for each of the three hidden units. The predictors were normalized to have the same mean and variance (zero and one, respectively). The three equations produced during training were:</p>
<div class="cell" data-layout-align="center">
<p><span class="math display">\[\begin{align}H_{1}&amp;= 2.298+1.6\:A-0.3\:B \notag \\
H_{2}&amp;=-0.131-1.2\:A-0.3\:B \notag \\
H_{3}&amp;=-0.064+1.0\:A+2.0\:B \notag\end{align}\]</span></p>
</div>
<p>The intercept estimate for the first hidden unit is significantly larger than those of the other two. That unit also has a large positive slope for predictor A. The second hidden unit emphasizes predictor B, while <span class="math inline">\(H_{i3}\)</span> has strong positive effects for both predictors.</p>
<p>The top of <a href="#fig-nnet-ex-layer-1" class="quarto-xref">Figure&nbsp;<span>17.16</span></a> displays a heatmap illustrating how these three artificial features vary across the two-dimensional predictor space (before activation). The black line segments show the contour corresponding to zero values. The first hidden unit has larger values on the right-hand side of the predictor space, with a slightly diagonal contour for zero. The pattern shown for the second hidden unit is nearly the opposite of the first, although the zero contour cuts more through the center of the space. Finally, the third unit is driven by both predictors with larger values in the upper right region.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nnet-ex-layer-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nnet-ex-layer-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-nnet-ex-layer-1-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnet-ex-layer-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.16: Heatmaps of the three linear predictors and their ReLU values across the predictor space. The black line emphasizes the location where the linear predictor is zero.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The figure’s bottom panels illustrate the effect of the ReLU function, where values less than zero are set to zero. At this point, <span class="math inline">\(H_{i1}\)</span> and <span class="math inline">\(H_{i3}\)</span> have the most substantial effect on the model, but in somewhat overlapping regions.</p>
<p>However, the model estimates additional parameters that will merge these regions using weights that are estimated to be most helpful for predicting the data. Those equations were estimated to be:</p>
<div class="cell" data-layout-align="center">
<p><span class="math display">\[\begin{align}Y_{i1}&amp;= 1.10-1.3H_{i1}+1.2H_{i2}+1.3H_{i3} \notag \\
Y_{i2}&amp;=-0.11+1.1H_{i1}-0.9H_{i2}-2.1H_{i3} \notag\end{align}\]</span></p>
</div>
<p>Note that the coefficients’ signs are opposite for each outcome unit. <a href="#fig-nnet-ex-layer-2" class="quarto-xref">Figure&nbsp;<span>17.17</span></a> illustrates the transition from the hidden layer to the outcome layer. The weighted sum of the three variables at the top produces the regions of strong positive (or negative) activation for the two classes. Once again, the black curve indicates the location of the zero contour.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-nnet-ex-layer-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nnet-ex-layer-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-nnet-ex-layer-2-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nnet-ex-layer-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.17: Heatmaps of the three ReLU units and their output units across the predictor space.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The two output panels at the bottom of the figure show nearly opposite patterns, which is not coincidental. The model converges to this structure, allowing the output units to favor one class or the other. The legend on the bottom right clearly shows inappropriate units; we’d like them to resemble probabilities. As previously mentioned, the softmax function coerces these values to be on [0, 1] and sum to one. This produces the class boundary seen in <a href="#fig-nnet-ex" class="quarto-xref">Figure&nbsp;<span>17.15</span></a>.</p>
<p>As previously stated in <a href="#sec-fda" class="quarto-xref"><span>Section 17.1.2</span></a>, ReLU is not a smooth function, so this demonstration will somewhat differ from the other activation functions; they generally don’t completely isolate regions of the predictor space. However, the overall flow of the network remains the same.</p>
<p>Now that we’ve discussed the architecture of these models and how the layers relate to one another, let’s examine how to estimate model parameters effectively.</p>
</section>
<section id="sec-nnet-training" class="level3" data-number="17.4.2">
<h3 data-number="17.4.2" class="anchored" data-anchor-id="sec-nnet-training"><span class="header-section-number">17.4.2</span> Training and Optimization</h3>
<p>Training neural networks can be challenging. Their model structure is composed of multiple nested nonlinear functions that may not be smooth or continuous. The loss function for classification, commonly cross-entropy, is unlikely to be convex as model complexity increases with the addition of more layers. Consequently, we have no guarantees that a gradient-based optimizer will converge. There is also the possibility of local minima that might trap the training process, saddle points that could lead the search in the wrong direction, and regions of vanishing gradients <span class="citation" data-cites="hochreiter1998vanishing">(<a href="#ref-hochreiter1998vanishing" role="doc-biblioref">Hochreiter 1998</a>)</span>.</p>
<p>First is the challenge of local minima. In some cases, ML models can provide some guarantees regarding the existence of a global minimum loss value<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. This may not be the case for neural networks. Their loss surface may contain multiple valleys where the derivatives of some of the parameters are zero, but a better solution exists in another region of the parameter space. In other words, the model can appear to converge but has suboptimal performance. <span class="citation" data-cites="baldi1989neural">Baldi and Hornik (<a href="#ref-baldi1989neural" role="doc-biblioref">1989</a>)</span> show that, under some conditions, single-layer feedforward networks can have a unique global minimum. It is expected that as more layers and units are added, the probability of finding a unique optimal solution decreases. When “caught” in a local valley, we need our optimization algorithm to be able to escape.</p>
<p>A second related issue is saddle points. These are locations where all of the gradients are zero (i.e., a stationary point), but some directions have increasing loss, and others will produce a decrease. <span class="citation" data-cites="dauphin2014identifying">Dauphin et al. (<a href="#ref-dauphin2014identifying" role="doc-biblioref">2014</a>)</span> postulate that these will exist, and their propensity to occur increases with the number of model parameters. The concern here is that, depending on the type of multidimensional curvature encountered, the optimization algorithm may move in the direction of <em>increasing loss</em> or become stuck.</p>
<p><a href="#fig-stationary" class="quarto-xref">Figure&nbsp;<span>17.18</span></a> has an example with two parameters and a highly nonlinear loss surface. There are four stationary points, the best of which minimizes the loss function at <span class="math inline">\(\boldsymbol{\theta} = [6.8, 0.2]\)</span>. The lower right region has a point associated with maximum loss. The other two locations are saddle points. The point at <span class="math inline">\(\boldsymbol{\theta} = [6.9, -0.3]\)</span> is a good example; moving to the left or right of the point will increase the loss. Moving up or down will decrease the loss (with the upper value being the correct choice in this parameter range). Our optimizer should be able to move in the proper direction or, like simulated annealing, recover and move back to progressively better results.</p>
<div id="fig-stationary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="cls-nonlinear_files/figure-html/stationary-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.18: The loss surface for <span class="math inline">\(\psi(\boldsymbol{\theta}) = \theta_1 cos(0.5\,\theta_1) - 2 \exp(-6(\theta_2 - 0.3)^2) + 0.2\, \theta_1 \theta_2\)</span>. Four stationary points are shown.
</figcaption>
</figure>
</div>
<p>Another possible factor is that many deep learning models are trained on vast amounts of data. This in itself presents constraints on how much data can be held in memory at once, as well as other logistical issues. This may not be an issue for the average application that uses tabular data; however, the problem still persists.</p>
<p>Before proceeding, recall <a href="iterative-search.html#sec-gradient-opt" class="quarto-xref"><span>Section 12.2</span></a> where gradient descent was introduced. There are two commonly used classes of gradient-based optimization:</p>
<ul>
<li><p><em>First-order techniques</em>: <span class="math inline">\(\quad\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \alpha\,g(\boldsymbol{\theta}_i)\)</span>,</p></li>
<li><p><em>Second-order methods</em>: <span class="math inline">\(\quad\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \alpha H(\boldsymbol{\theta_i})^{-1}\,g(\boldsymbol{\theta}_i)\)</span>,</p></li>
</ul>
<p>where <span class="math inline">\(\boldsymbol{\theta}\)</span> are the unknown parameters, <span class="math inline">\(\alpha\)</span> is the learning rate, <span class="math inline">\(g(\boldsymbol{\theta}_i)\)</span> is the gradient vector of first derivatives, and <span class="math inline">\(H(\boldsymbol{\theta_i})\)</span> is the Hessian matrix of second derivatives. The Hessian measures the curvature of the loss function at the current iteration. Pre-multiplying by its inverse rescales the gradient vector, generally making it more effective. We’ll discuss them both in <a href="#sec-gradient-descent" class="quarto-xref"><span>Section 17.4.9</span></a> and <a href="#sec-newtons-method" class="quarto-xref"><span>Section 17.4.10</span></a>. Generally speaking, second-order methods are preferred since they are more accurate <em>when they are computationally feasible</em>. However, first-order search is more commonly used because it is faster to compute and can be significantly more memory-efficient.</p>
<p>To demonstrate, we can use an MLP model for the forestation data that contains one layer of 30 ReLU units and an additional layer of 5 ReLU units, yielding 677 model parameters. An L<sub>2</sub> penalty of 0.15 was set to regularize the cross-entropy loss function. A maximum of 100 epochs was specified, but training was instructed to stop after five consecutive epochs with a loss increase (measured using held-out data). An initial learning rate of <span class="math inline">\(\alpha = 0.1\)</span> was used, but a variable, step-wise rate schedule was implemented to modulate the rate across epochs, as described below.</p>
<p>The model was first trained using basic gradient descent. For illustration, training was executed five times using different initial parameter values. The leftmost panel in <a href="#fig-optimizers" class="quarto-xref">Figure&nbsp;<span>17.19</span></a> shows the results. In each case, the loss initially decreased in the first set of epochs. However, in each case, the model used all 100 epochs. This approach was not very successful. There was little reduction in the loss function, and the terminal phase had a minuscule decrease with no real improvement. This <em>could</em> be due to the search approaching the saddle point. In this case, the gradient becomes very close to zero, and first-order gradient searches have tiny, ineffective updates. We can measure the efficiency of optimizers by computing the time it takes to train a complete iteration. For gradient descent, the time was 0.013s per iteration.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-optimizers" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-optimizers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-optimizers-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-optimizers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.19: The loss function, using held out data, for the same MLP model for four optimization strategies. The lines on each panel represent runs initialized with different random numbers. The profiles are normalized to start from the same initial loss value.
</figcaption>
</figure>
</div>
</div>
</div>
<p>When training neural networks, <em>stochastic</em> gradient descent (SGD) is the standard approach. SGD uses randomly allocated <em>batches</em> training set samples. The gradient search is updated by processing each of these batches until the entire trained set has been used, which signifies the end of that epoch. In other words, a potentially large number of parameter updates are computed based on a small number of training point samples. The batch size used here was 64 data points. As a result, for this training set size, about 76 gradient updates are executed before the model has seen all of the training data.</p>
<p>For stochastic gradient descent, we track the process by <em>epochs</em>. An epoch is finished when the model has been updated using every data point in the training set. For this analysis, this means that there are about 76 parameter updates within an epoch. For regular gradient descent and second-order search methods, we’ll count their progress as <em>iterations</em> since there are no batches.</p>
<p>As shown in <a href="#fig-optimizers" class="quarto-xref">Figure&nbsp;<span>17.19</span></a>, SGD does far better than plain GD and although none stop early, there is a relatively flat trend after about 20 epochs, where the search does not substantially reduce the loss. In terms of time per epoch, SGD takes much longer than GD at 0.163s.</p>
<p>As we’ll see in the section below, many extensions to SGD have made it more effective. One improvement is to use <em>Nesterov momentum</em> <span class="citation" data-cites="sutskever2013importance">(<a href="#ref-sutskever2013importance" role="doc-biblioref">Sutskever et al. 2013</a>)</span>. This initially computes a standard update to the parameters, then approximates the gradient at this position to compute a second prospective step. The actual update that is used is the lookahead position. This technique yields better results than SGD, with several iterations stopping early. However, the training time was longest, using 0.24s per epoch.</p>
<div class="note-box">
<p>The intended takeaways from this exercise are that training these models can be challenging and that the type of parameter optimization can significantly impact model quality.</p>
</div>
<p>Let’s discuss a few operational aspects of gradient-based methods before discussing the first- and second-order search techniques.</p>
</section>
<section id="sec-nnet-initialization" class="level3" data-number="17.4.3">
<h3 data-number="17.4.3" class="anchored" data-anchor-id="sec-nnet-initialization"><span class="header-section-number">17.4.3</span> Initialization</h3>
<p>Network parameter initialization can significantly impact the success of training. Given the size and complexity of these models, it is nearly impossible to analytically derive optimal methodologies for setting initial values. As such, parameters are initialized using random numbers. This means that different initial values will results in different model parameter estimates.</p>
<p>There are a few approaches to do simulate initial values. <span class="citation" data-cites="lecun2002efficient">LeCun et al. (<a href="#ref-lecun2002efficient" role="doc-biblioref">2002</a>)</span> initialization uses a uniform distribution centered around zero with limits defined by the number of connections feeding into the current node (known as the “fan in” and is denoted as <span class="math inline">\(m_{in}\)</span>). They used <span class="math inline">\(U(\pm \sqrt{2/m_{in}})\)</span> as the distribution. There have been a few variations of this:</p>
<ul>
<li>Glorot/Xavier initialization <span class="citation" data-cites="glorot2010understanding">(<a href="#ref-glorot2010understanding" role="doc-biblioref">Glorot and Bengio 2010</a>)</span>: <span class="math inline">\(U(\pm \sqrt{6/(m_{in}+m_{out})})\)</span></li>
<li>He/Kaiming initialization <span class="citation" data-cites="he2015delving">(<a href="#ref-he2015delving" role="doc-biblioref">He et al. 2015</a>)</span>: <span class="math inline">\(N(0, \sqrt{2 / m_{in}})\)</span>.</li>
</ul>
<p>where <span class="math inline">\(m_{out}\)</span> is the number of parameters leaving the node.</p>
<p>One reason that initialization is important is to help mitigate numerical overflow errors. For example, the softmax transformation shown above exponentiates the raw output units (and sums them). If the output units have relatively large values, exponentiating and summing them can exceed allowable tolerances. For example, the output units shown in <a href="#fig-nnet-ex-layer-2" class="quarto-xref">Figure&nbsp;<span>17.17</span></a> have a wide enough range that can be as large as 12.</p>
</section>
<section id="sec-nnet-preprocessing" class="level3" data-number="17.4.4">
<h3 data-number="17.4.4" class="anchored" data-anchor-id="sec-nnet-preprocessing"><span class="header-section-number">17.4.4</span> Preprocessing</h3>
<p>Neural networks require complete, numeric features for computations.</p>
<p>For non-numeric data, the tools described in <a href="categorical-predictors.html" class="quarto-xref"><span>Chapter 6</span></a> can be very effective. Additionally, there are pretrained neural network models that can translate the category string to a word embedding <span class="citation" data-cites="Boykis_What_are_embeddings_2023">(<a href="#ref-Boykis_What_are_embeddings_2023" role="doc-biblioref">Boykis 2023</a>)</span>, such as BERT <span class="citation" data-cites="devlin2019bert">(<a href="#ref-devlin2019bert" role="doc-biblioref">Devlin et al. 2019</a>)</span> or GPT-4 <span class="citation" data-cites="achiam2023gpt">(<a href="#ref-achiam2023gpt" role="doc-biblioref">Achiam et al. 2023</a>)</span>. The potential upside to doing this is that the resulting numeric features may measure some relative context between categorical values. For example, suppose we have a predictor with levels <code>red</code>, <code>darkred</code>, <code>green</code>, <code>chartreuse</code>, and <code>white</code>. Basic encoders, such as binary indicators or hashing features, would not recognize that there are three clusters in these values and that sets such as {<code>red</code>, <code>darkred</code>} and {<code>green</code>, <code>chartreuse</code>} should have values that are “near” one another. LLMs can measure this semantic similarity and potentially generate more meaningful features.</p>
<p>If the non-numeric predictors are not complex, a layer could be added early in the network, enabling the model to derive encodings based on the data when training. <span class="citation" data-cites="guo2016entity">Guo and Berkhahn (<a href="#ref-guo2016entity" role="doc-biblioref">2016</a>)</span> describes a relatively simple architecture for doing this. While LLMs are better at creating embeddings overall, they are unaware of the <em>context</em> in your data. Estimating the parameters associated with a simple embedding layer may be more effective, as it is optimized for your dataset.</p>
<p>For numeric predictors, we suggest avoiding irrelevant predictors and highly collinear feature sets. <a href="feature-selection.html#fig-irrelevant-predictors" class="quarto-xref">Figure&nbsp;<span>13.1</span></a> in <a href="feature-selection.html" class="quarto-xref"><span>Chapter 13</span></a> demonstrated the harm of including predictors unrelated to the outcome in a neural network. Multicollinearity can also affect these models, even if we do not invert the feature matrix or use the Hessian during optimization. Highly redundant predictors can result in the loss function having difficult characteristics and a large number of unnecessary parameters. Using an L<sub>2</sub> penalty or a feature extraction method can be very helpful.</p>
<p>Also, predictors with highly skewed distributions might benefit from a transformation to make their distribution more symmetric. If not, outlying values can exert significant influence in computations, potentially preventing the generation of high-quality parameter estimates.</p>
<p>Finally, since the networks use random initialization, we should ensure that our predictors are standardized to have the same units. If there are only numeric predictors, centering/scaling or orderNorm operations can be very effective<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Using this approach, we must also apply the same standardization to binary indicators created from categorical predictors. Alternatively, we could leave 0/1 indicators as-is, but <em>range</em> scale our predictors to also be between zero and one. This may have a disadvantage; the range will be common across predictors, but the mean and variance can be quite different.</p>
</section>
<section id="sec-nnet-learning-rates" class="level3" data-number="17.4.5">
<h3 data-number="17.4.5" class="anchored" data-anchor-id="sec-nnet-learning-rates"><span class="header-section-number">17.4.5</span> Learning Rates</h3>
<p>The learning rate (<span class="math inline">\(\alpha\)</span>) can profoundly affect model quality, perhaps more than the structural tuning parameters that define complexity (e.g., number of hidden units). A high learning range (<span class="math inline">\(\approx\)</span> 0.1) will help converge towards effective parameter estimates, but once there, it can cause the model to consistently overshoot the best location. One approach to modulating the rate is to use a scheduler <span class="citation" data-cites="lu2022gradient">(<a href="#ref-lu2022gradient" role="doc-biblioref">Lu 2022</a>,chapt 4)</span>. These are functions/algorithms that will change the rate (often to decrease it) over epochs. For example, to monotonically decrease the rate, two options are:</p>
<ul>
<li>inverse decay: <span class="math inline">\(f(\alpha_0, \delta, i) = \alpha_0/(1 + \delta i)\)</span></li>
<li>exponential decay: <span class="math inline">\(f(\alpha_0, \delta, i) = \alpha_0\exp(-\delta i)\)</span></li>
</ul>
<p>where <span class="math inline">\(i\)</span> is the epoch number and <span class="math inline">\(\delta\)</span> is a decay value. These two functions gradually decrease the rate so that, when near an optimum, it is slow enough not to exceed the optimal settings. Another approach is a step function that decreases the rate but maintains it at a constant level for a range of epochs. Those ranges can be predefined or set dynamically to decrease when the loss function is reduced. The function for the former is:</p>
<ul>
<li>step decay: <span class="math inline">\(f(\alpha_0, \zeta, r, i) = \alpha_0 \zeta^{\lfloor i/r\rfloor}\)</span></li>
</ul>
<p>where <span class="math inline">\(\zeta\)</span> is the reduction value, <span class="math inline">\(r\)</span> is the step length, and <span class="math inline">\(\lfloor x\rfloor\)</span> is the floor function.</p>
<p>Finally, some schedulers modulate the rate up and down. A cyclic schedule <span class="citation" data-cites="smith2017cyclical">(<a href="#ref-smith2017cyclical" role="doc-biblioref">Smith 2017</a>)</span> starts at a maximum value, decreases to a minimum value, and then increases cyclically. For some range <span class="math inline">\(r\)</span> and minimum rate <span class="math inline">\(\alpha_0\)</span>, an epoch’s location within the series is <span class="math inline">\(cycle = \lfloor 1 + (i / (2r) )\rfloor\)</span> and the rate is</p>
<ul>
<li>cyclic: <span class="math inline">\(f(\alpha_0, \alpha, r, i) = \alpha_0 + ( \alpha - \alpha_0 ) \max(0, 1 - x)\)</span></li>
</ul>
<p>where <span class="math inline">\(x = |(i/r) - (2\,cycle) + 1|\)</span>. This is a good option when we are unsure how fast or slow the convergence will be. If the rate is too high near an optimum value, it will soon decrease to an appropriate value. Additionally, if the search is stuck in a local optimum (or a saddle point), increasing the learning rate may help it escape.</p>
<p><a href="#fig-schedulers" class="quarto-xref">Figure&nbsp;<span>17.20</span></a> uses the loss function example from <a href="#fig-stationary" class="quarto-xref">Figure&nbsp;<span>17.18</span></a> to demonstrate the effect of rate schedulers. Two starting points are used. One (in orange) is very close to a saddle point, while the other starts from the upper right corner of the parameter space (in green).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-schedulers" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-schedulers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-schedulers-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-schedulers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.20: The effect of different schedulers on the rate and success of convergence.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Using a constant learning rate of <span class="math inline">\(\alpha = 0.1\)</span>, the search marches to the optimum value, but oscillates wildly on approach. Ultimately, it falls short of the best estimates, as it is constrained to make large jumps on each iteration. The cyclic schedule allowed rates to move between <span class="math inline">\(\alpha_0 = 0.001\)</span> to <span class="math inline">\(\alpha = 0.1\)</span> with a cycle range of <span class="math inline">\(r = 10\)</span> epochs. This appears to work well, or at least, it had good timing. The decay panel represents inverse decay with <span class="math inline">\(\delta = 0.025\)</span>. For the corner starting point, this is a very effective strategy. From the saddle point, we can observe that decreasing the learning rate results in fewer oscillations as the search progresses and eventually leads to the optimum. Finally, the panel for stepwise reduction used an initial learning rate of <span class="math inline">\(\alpha_0 = 0.1\)</span>, <span class="math inline">\(\eta = 0.5\)</span>, and a step length of 20 epochs. This approach worked well when the search began from the saddle location, but performed poorly when starting at the corner. The success depended on when the search was near the optimum; if not timed right, the oscillations were too wide to converge on the best point. A reduction in the step length might help.</p>
<p>The schedule type and its additional parameter(s) are added to the list of possible values that could be optimized during tuning.</p>
</section>
<section id="sec-nnet-regularization" class="level3" data-number="17.4.6">
<h3 data-number="17.4.6" class="anchored" data-anchor-id="sec-nnet-regularization"><span class="header-section-number">17.4.6</span> Regularization</h3>
<p>As seen in a previous chapter, we can add a penalty to the loss function to discourage particularly large model coefficients. This is especially beneficial for neural network models.</p>
<p>While <span class="citation" data-cites="hanson1988comparing">Hanson and Pratt (<a href="#ref-hanson1988comparing" role="doc-biblioref">1988</a>)</span> and <span class="citation" data-cites="weigend1990generalization">Weigend, Rumelhart, and Huberman (<a href="#ref-weigend1990generalization" role="doc-biblioref">1990</a>)</span> made some early proposals for penalizing the size of the estimated parameters, <span class="citation" data-cites="krogh1991simple">Krogh and Hertz (<a href="#ref-krogh1991simple" role="doc-biblioref">1991</a>)</span> proposed to use L<sub>2</sub> regularization to improve generalization. This is now a standard approach for training neural networks, and “weight decay” is now synonymous with squared penalties.</p>
<p>While either L<sub>1</sub>, L<sub>2</sub>, or a mixture of the two can be used, there is some reason to use just L<sub>2</sub> penalization. While either type of penalty might be able to combat multicollinearity, the quadratic penalty has the advantage of maintaining a smooth loss surface. With an L<sub>1</sub> penalty, we have at least one point on the loss surface that is not differentiable. Also note that using the L<sub>1</sub> penalty will not produce exact zeros in the parameter estimates when using stochastic gradient descen.</p>
<p>As we will describe later in this chapter, <span class="citation" data-cites="loshchilov2017decoupled">Loshchilov and Hutter (<a href="#ref-loshchilov2017decoupled" role="doc-biblioref">2017</a>)</span> note that there are cases when penalizing the loss function does not precisely match the definition of weight decay (in the neural network context of <span class="citation" data-cites="hanson1988comparing">Hanson and Pratt (<a href="#ref-hanson1988comparing" role="doc-biblioref">1988</a>)</span>). Their adjustment for these issues is called the “AdamW” method.</p>
<p>Another regularization technique is called <em>dropout</em>. This method randomly removes some units (hidden or not) from the network. This drops incoming and outgoing connections to other units, which propagates through all of the layers. These temporary coercions to zero recur with each evaluation of the gradient. In the final training step, a full set of parameters is used. Dropout has an effect similar to averaging many smaller networks. The amount of dropout should be tuned.</p>
</section>
<section id="sec-early-stopping" class="level3" data-number="17.4.7">
<h3 data-number="17.4.7" class="anchored" data-anchor-id="sec-early-stopping"><span class="header-section-number">17.4.7</span> Early Stopping</h3>
<p>When should training stop? We can estimate the number of epochs that might be needed, but if we make an incorrect guess, we risk under- or overfitting the model. It makes sense to include the number of epochs as a tuning parameter, but this could be costly<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
<p>If we have some data to spare, an efficient and effective approach is to use early stopping. If we set aside a small proportion of the training set to measure the search’s progress, we can set the <em>maximum</em> number of epochs and stop training when the loss worsens (based on our holdout set). Often, we can tune the number of “stopping epochs.”. If this value were five, we would stop training after five consecutive epochs where the loss function worsens (and use the parameter estimates from our last best epoch). Values greater than one protect against overreacting when the loss profile is noisy.</p>
</section>
<section id="sec-batch-learning" class="level3" data-number="17.4.8">
<h3 data-number="17.4.8" class="anchored" data-anchor-id="sec-batch-learning"><span class="header-section-number">17.4.8</span> Batch Learning</h3>
<p>Operationally, within each epoch, a random batch is selected to evaluate the gradient and corresponding update the parameters. This occurs for each batch until the entire set is evaluated. This is the end of the epoch. If we are monitoring a holdout set for early stopping, this is the point at which we evaluate the loss function at the current parameter estimate using the holdout set. After this, the process begins again by iterating through the batches.</p>
<p>SGD is thought to work well because it injects a considerable amount of variation into the gradients and updates. This would generally be considered a bad thing, but, as with simulated annealing, it makes the optimization less greedy. The somewhat noisy directions the optimizer takes can help the optimization escape from local optimums and be more resilient when the search veers near a saddle point or other regions where the gradient may be flat or consistently zero. <span class="citation" data-cites="hoffer2017train">Hoffer, Hubara, and Soudry (<a href="#ref-hoffer2017train" role="doc-biblioref">2017</a>)</span> referred to SGD as a “high-dimensional random walk on a random potential” process.</p>
<p>As the search proceeds, lowering the learning rate can mitigate the extra noise in the gradients so that, hopefully, the search does not move away from the best parameter estimates.</p>
<p>We can tune the batch size if there is no intuition for what a good value might be. Smallish batch sizes could range from 16 to 512 training set points<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, depending on the training set size. <span class="citation" data-cites="keskar2016large">Keskar et al. (<a href="#ref-keskar2016large" role="doc-biblioref">2016</a>)</span> remarks that smaller batches tend to explore the parameter space more thoroughly and that, as the batch size becomes larger, SGD will find a solution that is closer to the initial values.</p>
<p><span class="citation" data-cites="loshchilov2017decoupled">Loshchilov and Hutter (<a href="#ref-loshchilov2017decoupled" role="doc-biblioref">2017</a>)</span> make a few interesting observations related to L<sub>2</sub> penalization with stochastic gradient descent. Their results show that as the number of batches increases, the optimal penalization becomes smaller. In other words, as the batch size decreases, the amount of penalization should also decrease.</p>
</section>
<section id="sec-gradient-descent" class="level3" data-number="17.4.9">
<h3 data-number="17.4.9" class="anchored" data-anchor-id="sec-gradient-descent"><span class="header-section-number">17.4.9</span> Gradient Descent Algorithms</h3>
<p>Gradient descent methods for training neural networks have become a popular research topic over the last 15 years. We’ll discuss a few major techniques here, but there are myriad other tools for training these models. In this section, we’ll temporarily forgo discussing changes from epoch to epoch. Since we are focused on SGD, we’ll use the term update to refer to the point at which the parameters move to a different location due to a batch step.</p>
<section id="momentum" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="momentum">Momentum</h4>
<p>To start, let’s consider a more common form of momentum in gradient search<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> Momentum is a useful tool that can help in situations such as the results in <a href="#fig-schedulers" class="quarto-xref">Figure&nbsp;<span>17.20</span></a>, where the updates make the search path make large, ineffective jumps. When the learning rate is consistently large, the search route bounces back and forth since the gradients are substantial in magnitude but moving in different consecutive directions.</p>
<p>Momentum is one of several tools to adjust the gradient vector based on previous gradients. For iteration <span class="math inline">\(i\)</span>, a vector is computed that is a combination of the current and previous gradients<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>:</p>
<p><span class="math display">\[
v_i = \phi v_{i-1} + \alpha_i g(\boldsymbol{\theta}_i)
\]</span></p>
<p>and this is used to make the update <span class="math inline">\(\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - v_i\)</span>. This weighting system emphasizes recent gradients while also taking into account previous gradients. This can accelerate the search and diminish oscillations.</p>
<p>Values of <span class="math inline">\(\phi\)</span> are typically between 0.8 and 0.99, representing a weighted average of previous gradient directions, with a greater emphasis on recent iterations as <span class="math inline">\(\phi\)</span> increases. If the directions have been fairly stable, there is little change; however, if they are oscillating, the direction takes a more consistent central route with fewer directional jumps. It also helps if the search is in a region where gradients are close to zero; historical gradients will have some non-zero values that are unlikely to become stuck with updates near zero.</p>
<p><a href="#fig-sgd" class="quarto-xref">Figure&nbsp;<span>17.21</span></a> shows the results when momentum is used for our 2D toy example<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>. The cyclic rate settings were used for this example, and the results appear very similar to the previous path when initialized near the saddle point. The corner point starting value shows that momentum can lead to a location near the optimal value, but takes a more circuitous route.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-sgd" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sgd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-sgd-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sgd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.21: Examples of four different optimizers for the loss surface previously shown in <a href="#fig-schedulers" class="quarto-xref">Figure&nbsp;<span>17.20</span></a>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Momentum should be used in conjunction with a rate scheduler, or at the very least, without a consistently large learning rate. Making very large jumps in a central direction might lead the search to points far away from regions of improvement. Penalization also helps.</p>
</section>
<section id="adaptive-learning-rates" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="adaptive-learning-rates">Adaptive Learning Rates</h4>
<p>There is also a class of gradient descent methods that try to avoid using a single learning rate for all of the elements of <span class="math inline">\(\boldsymbol{\theta}\)</span>. Instead, they use a constant learning rate and, at each update, adjust the rate using functions of the gradient vector so that the rates are different from each <span class="math inline">\(\theta_j\)</span>.</p>
<p>One approach is the adaptive gradient algorithm, a.k.a. AdaGrad <span class="citation" data-cites="duchi2011adaptive">(<a href="#ref-duchi2011adaptive" role="doc-biblioref">Duchi, Hazan, and Singer 2011</a>)</span>, that scales the constant learning rate <span class="math inline">\(\alpha\)</span> by the accumulated square of the gradients:</p>
<p><span class="math display">\[
\boldsymbol{v}_i =\sum_{b=1}^t g(\boldsymbol{\theta}_b)^2
\]</span> so that</p>
<p><span class="math display">\[
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \frac{\alpha}{\sqrt{\boldsymbol{v}^2_i + \epsilon}}\, g(\boldsymbol{\theta}_i)
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is a small constant to avoid division by zero. The square root in the denominator is required to keep the denominator units the same as those in the numerator. AdaGrad has the advantage of keeping the adjusted learning rate steady for elements of <span class="math inline">\(g(\boldsymbol{\theta})\)</span> that do not change substantially from update to update. At update <span class="math inline">\(i\)</span>, the contribution to the learning rate is inversely proportional to the squared gradient value.</p>
<p>The problem is the <em>unweighted</em> accumulation of squared values. For a reasonable number of updates, elements of <span class="math inline">\(\boldsymbol{v}\)</span> can explode to very large values, and this prevents almost <em>any</em> movement of <span class="math inline">\(\boldsymbol{\theta}\)</span> from update to update. For the example shown in <a href="#fig-sgd" class="quarto-xref">Figure&nbsp;<span>17.21</span></a>, some of these values were in the thousands, and progress clearly halted before reaching the optimal parameters.</p>
<p>One method of fixing this problem is RMSprop <span class="citation" data-cites="hinton2012neural">(<a href="#ref-hinton2012neural" role="doc-biblioref">Hinton 2012</a>)</span>. Here, the denominator uses an exponentially weighted moving average of the squared gradients:</p>
<p><span class="math display">\[
\boldsymbol{v}_i = \rho\, \boldsymbol{v}_{i-1} + (1 - \rho)\, g(\boldsymbol{\theta}_i)^2
\]</span></p>
<p>This formulation places more weight on the current and recent gradients than the statistic used by AdaGrad. The RMSprop update becomes:</p>
<p><span class="math display">\[
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \frac{\alpha}{\sqrt{\boldsymbol{v}_i + \epsilon}}\, g(\boldsymbol{\theta}_i).
\]</span></p>
<p>The “RMS” in the name is due to taking the square root of the weighted sum of squared gradients (i.e., similar to a traditional root mean square).</p>
<p><span class="citation" data-cites="goodfellow2016deep">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow2016deep" role="doc-biblioref">2016</a>)</span> suggested using RMSprop with Nesterov momentum. This leads to a more complicated procedure. First, we compute our initial update: <span class="math inline">\(\quad\tilde{\boldsymbol{\theta}}_{i} = \boldsymbol{\theta}_{i} - \phi\,\boldsymbol{\delta}_{i-1}\)</span> where <span class="math inline">\(\boldsymbol{\delta}_0\)</span> is initialized with a vector of zeros. We compute the gradient at this temporary update and use this to compute <span class="math inline">\(\boldsymbol{v}_i\)</span>. From here:</p>
<p><span class="math display">\[
\boldsymbol{\delta}_{i} = \phi\,\boldsymbol{\delta}_{i-1} - \frac{\alpha}{\sqrt{\boldsymbol{v}_i + \epsilon}}\, g(\tilde{\boldsymbol{\theta}}_i)
\]</span></p>
<p>The results are in <a href="#fig-sgd" class="quarto-xref">Figure&nbsp;<span>17.21</span></a>. The curves are similar to those for AdaGrad except that they do reach the location of minimum loss. The path for the starting value near the saddle point exhibits several minor oscillations en route to the optimal result.</p>
<p>The adaptive moment estimation (Adam) optimizer from <span class="citation" data-cites="kinga2015method">Diederik and Ba (<a href="#ref-kinga2015method" role="doc-biblioref">2015</a>)</span> extends the moving average approach to the gradient term as well:</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{m}_i &amp;= \phi\, \boldsymbol{m}_{i-1} + (1 - \phi)\, g(\boldsymbol{\theta}_i) \notag \\
\boldsymbol{v}_i &amp;= \rho\, \boldsymbol{v}_{i-1} + (1 - \rho)\, g(\boldsymbol{\theta}_i)^2 \notag
\end{align}
\]</span></p>
<p>Adam also tries to normalize these quantities so that the first few updates have larger values:</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{m}^*_i &amp;= \frac{\boldsymbol{m}_i}{1 - \phi^2}\notag \\
\boldsymbol{v}^*_i &amp;= \frac{\boldsymbol{v}_i}{1 - \rho^2} \notag
\end{align}
\]</span></p>
<p>This helps offset the initialization of <span class="math inline">\(\boldsymbol{m}_0 = \boldsymbol{v}_0 = \boldsymbol{0}\)</span>. At update <span class="math inline">\(i\)</span>, Adam’s proposal is</p>
<p><span class="math display">\[
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \frac{\alpha}{\sqrt{\boldsymbol{v}^*_i + \epsilon}}\,\boldsymbol{m}^*_i.
\]</span></p>
<p>The results above show that Adam’s path towards the optimal parameters is slightly wobbly compared to RMSprop but otherwise effective.</p>
<p>Both <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\rho\)</span> are tunable in these models and typically range within [0.8, 0.99]. The constant learning rate <span class="math inline">\(\alpha\)</span> is also tunable. In <a href="#fig-sgd" class="quarto-xref">Figure&nbsp;<span>17.21</span></a>, <span class="math inline">\(\phi=\rho=0.9\)</span>. AdaGrad, Adam, and RMSprop used <span class="math inline">\(\alpha = 0.1\)</span>. For RMSprop, decreasing the rate to <span class="math inline">\(\alpha = 0.08\)</span> removed the oscillation effects shown in <a href="#fig-sgd" class="quarto-xref">Figure&nbsp;<span>17.21</span></a>.</p>
<p>As previously alluded to, <span class="citation" data-cites="loshchilov2017decoupled">Loshchilov and Hutter (<a href="#ref-loshchilov2017decoupled" role="doc-biblioref">2017</a>)</span> recognized that, for Adam, penalizing the loss function does not align with the definition of weight decay. Without a remedy, they found that Adam is not as effective as it could be. Their method, called AdamW, includes the penalty in the gradient definition. The gradient used to compute <span class="math inline">\(\boldsymbol{m}_i\)</span> and <span class="math inline">\(\boldsymbol{v}_i\)</span> is a combination of the previous “raw” gradient and the penalty (<span class="math inline">\(\lambda\)</span>):</p>
<p><span class="math display">\[
g(\boldsymbol{\theta}_i) = \nabla \psi(\boldsymbol{\theta}_{i}) - \lambda \boldsymbol{\theta}_{i}
\]</span></p>
<p>The updating equation also uses an extra term containing the penalty:</p>
<p><span class="math display">\[
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \alpha_i\left[\frac{\boldsymbol{m}^*_i}{\sqrt{\boldsymbol{v}^*_i + \epsilon}} + \lambda \boldsymbol{\theta}_{i}\right].
\]</span></p>
<p>Note that the learning rate <span class="math inline">\(\alpha\)</span> now has a subscript. They also recommend trying a cyclic learning rate scheduler with AdamW.</p>
</section>
<section id="other-methods" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="other-methods">Other Methods</h4>
<p>As previously mentioned, there are numerous other variations of SGD that will not be discussed here, including AdaMax <span class="citation" data-cites="kinga2015method">(<a href="#ref-kinga2015method" role="doc-biblioref">Diederik and Ba 2015</a>)</span>, Nadam <span class="citation" data-cites="dozat2016">(<a href="#ref-dozat2016" role="doc-biblioref">Dozat 2016</a>)</span>, MadGrad <span class="citation" data-cites="defazio2021">(<a href="#ref-defazio2021" role="doc-biblioref">Defazio and Jelassi 2021</a>)</span>, and others.</p>
</section>
</section>
<section id="sec-newtons-method" class="level3" data-number="17.4.10">
<h3 data-number="17.4.10" class="anchored" data-anchor-id="sec-newtons-method"><span class="header-section-number">17.4.10</span> Newton’s Method</h3>
<p>As previously shown, Newton’s method (a.k.a., the Newton–Raphson method) uses both gradients and the Hessian matrix to update parameters. This approach can be very effective when the local loss surface can be approximated by a second-degree polynomial. When the local search includes a stationary point, the signs of the eigenvalues of the Hessian can characterize it: all non-negative imply a maximum, all non-positive imply a minimum, and mixed signs indicate a saddle point.</p>
<p>One issue with the method is related to the Hessian. If there are <span class="math inline">\(p\)</span> parameters, the matrix needs to save <span class="math inline">\(p(p+1)/2\)</span> parameters. For neural networks, <span class="math inline">\(p\)</span> can be in the millions, so the basic application of Newton’s method can be slow and might require infeasible amounts of memory (depending on the model size). We also need to invert the Hessian, making it even more costly.</p>
<p>We can approximate Newton’s method using the Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm (L-BFGS) search <span class="citation" data-cites="lu2022gradient">(<a href="#ref-lu2022gradient" role="doc-biblioref">Lu 2022</a>,chapt 6)</span>. It uses past gradients to approximate the Hessian as the search proceeds. L-BFGS computes gradients at different values of <span class="math inline">\(\boldsymbol{\theta}\)</span> for more than just updating the parameters (i.e., Hessian approximation and the line search), so it is not a good solution when loss function or gradient evaluations are expensive.</p>
<p>However, if we have a small dataset, a low-to-medium complexity architecture, or both, L-BFGS can be a very effective tool for estimating parameters.</p>
</section>
<section id="sec-nnet-forestation" class="level3" data-number="17.4.11">
<h3 data-number="17.4.11" class="anchored" data-anchor-id="sec-nnet-forestation"><span class="header-section-number">17.4.11</span> Forestation Modeling</h3>
<p>For the forested data, there are a few options for preprocessing:</p>
<ul>
<li>Use binary indicators to represent the 39 counties or a supervised effect encoding strategy.</li>
<li>Several predictors are highly skewed. We can leave them as-is before centering/scaling, or use an orderNorm transformation to coerce each predictor to a standard normal distribution.</li>
<li>Deal with the high correlation for some predictors via PCA extraction or leave as-is in the data.</li>
</ul>
<p>Several combinations of these operations were used:</p>
<ul>
<li>An effect encoding with and without PCA feature extraction.</li>
<li>Standard binary indicators with and without PCA feature extraction and with and without a symmetry transformation (i.e., orderNorm versus centering/scaling).</li>
</ul>
<p>For the PCA extraction, all possible components were used.</p>
<p>For the model, there are a fair number of tuning parameters to combine:</p>
<ul>
<li>One or two layers.</li>
<li>The number of hidden units. For each layer, these ranged from two to fifty units.</li>
<li>The activation type: ReLU, elu, tanh, and tanhshrink.</li>
<li>L<sub>2</sub> regularization values between 10<sup>-10</sup> and 10<sup>-1</sup>.</li>
<li>Batch sizes between 2<sup>3</sup> and 2<sup>8</sup>.</li>
<li>Learning rates between 10<sup>-4</sup> and 10<sup>-1</sup>.</li>
<li>Learning rate schedules: constant, cyclic, and time decay.</li>
<li>Momentum values between [0.8, 0.99].</li>
</ul>
<p>AdamW was used to train the models, with a maximum of 25 epochs allowed, but early stopping could occur after five consecutive epochs with poor performance.</p>
<p>A space-filling design with 25 candidates was evaluated. The six preprocessing schemes listed above were each run with the same model grid, resulting in 300 candidates to be evaluated.</p>
<p><a href="#fig-forest-mlp-ranks" class="quarto-xref">Figure&nbsp;<span>17.22</span></a> shows the results for the 300 candidates. They are ranked by their resampling estimate of the Brier scores, and the figure presents these rankings from best to worst, breaking out the combinations of preprocessing scenarios. While each panel contains models with sufficiently low Brier scores, several trends are evident. The “Indicators” panel represents models using dummy variable indicators with no symmetry transformation, and this did not do well (with or without PCA feature extraction).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-forest-mlp-ranks" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-forest-mlp-ranks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-nonlinear_files/figure-html/fig-forest-mlp-ranks-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-forest-mlp-ranks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.22: Rankings of multilayer perceptron models for the forestation models using the Brier score. The panels represent different types of preprocessing, and the vertical lines are 90% confidence intervals.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In fact, PCA has suboptimal results in each of the three bottom panels. This could be due to the components being unsupervised; they are not optimized for prediction. Alternatively, the last few components might be measuring noise and have less relevance to the model. Adding irrelevant predictors to neural networks can diminish their effectiveness.</p>
<p>The two cases that did the best were the “Indicators + Symmetry” and “Effect Encoding + Symmetry” panels. While both used the orderNorm transformation, the former panel employed indicators, whereas the latter used a supervised effect encoding, where a single column was used to represent the county effect. This suggests that eliminating distributional skew in the predictors may yield a modest improvement in predictive performance.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-mlp-top-five" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-mlp-top-five-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="fybehlmray" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#fybehlmray table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#fybehlmray thead, #fybehlmray tbody, #fybehlmray tfoot, #fybehlmray tr, #fybehlmray td, #fybehlmray th {
  border-style: none;
}

#fybehlmray p {
  margin: 0;
  padding: 0;
}

#fybehlmray .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#fybehlmray .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#fybehlmray .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#fybehlmray .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#fybehlmray .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#fybehlmray .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#fybehlmray .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#fybehlmray .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#fybehlmray .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#fybehlmray .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#fybehlmray .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#fybehlmray .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#fybehlmray .gt_spanner_row {
  border-bottom-style: hidden;
}

#fybehlmray .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#fybehlmray .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#fybehlmray .gt_from_md > :first-child {
  margin-top: 0;
}

#fybehlmray .gt_from_md > :last-child {
  margin-bottom: 0;
}

#fybehlmray .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#fybehlmray .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#fybehlmray .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#fybehlmray .gt_row_group_first td {
  border-top-width: 2px;
}

#fybehlmray .gt_row_group_first th {
  border-top-width: 2px;
}

#fybehlmray .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#fybehlmray .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#fybehlmray .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#fybehlmray .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#fybehlmray .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#fybehlmray .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#fybehlmray .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#fybehlmray .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#fybehlmray .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#fybehlmray .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#fybehlmray .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#fybehlmray .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#fybehlmray .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#fybehlmray .gt_left {
  text-align: left;
}

#fybehlmray .gt_center {
  text-align: center;
}

#fybehlmray .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#fybehlmray .gt_font_normal {
  font-weight: normal;
}

#fybehlmray .gt_font_bold {
  font-weight: bold;
}

#fybehlmray .gt_font_italic {
  font-style: italic;
}

#fybehlmray .gt_super {
  font-size: 65%;
}

#fybehlmray .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#fybehlmray .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#fybehlmray .gt_indent_1 {
  text-indent: 5px;
}

#fybehlmray .gt_indent_2 {
  text-indent: 10px;
}

#fybehlmray .gt_indent_3 {
  text-indent: 15px;
}

#fybehlmray .gt_indent_4 {
  text-indent: 20px;
}

#fybehlmray .gt_indent_5 {
  text-indent: 25px;
}

#fybehlmray .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#fybehlmray div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table do-not-create-environment cell table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 20%">
<col style="width: 11%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 15%">
<col style="width: 10%">
<col style="width: 8%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="gt_col_headings gt_spanner_row header">
<th rowspan="2" id="Preprocessing" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Preprocessing</th>
<th rowspan="2" id="Architecture" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Architecture</th>
<th rowspan="2" id="a#-Parameters" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col"># Parameters</th>
<th rowspan="2" id="Penalty" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Penalty</th>
<th rowspan="2" id="Batch-Size" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Batch Size</th>
<th rowspan="2" id="Learning-Rate" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Learning Rate</th>
<th rowspan="2" id="Momentum" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Momentum</th>
<th colspan="2" id="Performance" class="gt_center gt_columns_top_border gt_column_spanner_outer" data-quarto-table-cell-role="th" scope="colgroup"><div class="gt_column_spanner">
Performance
</div></th>
</tr>
<tr class="gt_col_headings even">
<th id="Brier" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Brier</th>
<th id="ROC" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">ROC</th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="Preprocessing">Effect Encoding, Symmetry, No PCA</td>
<td class="gt_row gt_left" headers="Architecture">46 elu , 8 relu</td>
<td class="gt_row gt_right" headers="# Parameters">1176</td>
<td class="gt_row gt_left" headers="Penalty">10<sup>-7.38</sup></td>
<td class="gt_row gt_right" headers="Batch Size">16</td>
<td class="gt_row gt_left" headers="Learning Rate">10<sup>-3.25</sup> (none)</td>
<td class="gt_row gt_right" headers="Momentum">0.871</td>
<td class="gt_row gt_right" headers="Brier">0.0806</td>
<td class="gt_row gt_right" headers="ROC">0.9515</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Preprocessing">Indicators, Symmetry, No PCA</td>
<td class="gt_row gt_left" headers="Architecture">46 elu , 8 relu</td>
<td class="gt_row gt_right" headers="# Parameters">2878</td>
<td class="gt_row gt_left" headers="Penalty">10<sup>-7.38</sup></td>
<td class="gt_row gt_right" headers="Batch Size">16</td>
<td class="gt_row gt_left" headers="Learning Rate">10<sup>-3.25</sup> (none)</td>
<td class="gt_row gt_right" headers="Momentum">0.871</td>
<td class="gt_row gt_right" headers="Brier">0.0809</td>
<td class="gt_row gt_right" headers="ROC">0.9496</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Preprocessing">Effect Encoding, Symmetry, No PCA</td>
<td class="gt_row gt_left" headers="Architecture">32 tanh</td>
<td class="gt_row gt_right" headers="# Parameters">610</td>
<td class="gt_row gt_left" headers="Penalty">10<sup>-9.62</sup></td>
<td class="gt_row gt_right" headers="Batch Size">29</td>
<td class="gt_row gt_left" headers="Learning Rate">10<sup>-3.00</sup> (none)</td>
<td class="gt_row gt_right" headers="Momentum">0.950</td>
<td class="gt_row gt_right" headers="Brier">0.0828</td>
<td class="gt_row gt_right" headers="ROC">0.9472</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Preprocessing">Indicators, Symmetry, No PCA</td>
<td class="gt_row gt_left" headers="Architecture">32 tanh</td>
<td class="gt_row gt_right" headers="# Parameters">1794</td>
<td class="gt_row gt_left" headers="Penalty">10<sup>-9.62</sup></td>
<td class="gt_row gt_right" headers="Batch Size">29</td>
<td class="gt_row gt_left" headers="Learning Rate">10<sup>-3.00</sup> (none)</td>
<td class="gt_row gt_right" headers="Momentum">0.950</td>
<td class="gt_row gt_right" headers="Brier">0.0829</td>
<td class="gt_row gt_right" headers="ROC">0.9452</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Preprocessing">Effect Encoding, Symmetry, No PCA</td>
<td class="gt_row gt_left" headers="Architecture">10 elu</td>
<td class="gt_row gt_right" headers="# Parameters">192</td>
<td class="gt_row gt_left" headers="Penalty">10<sup>-6.62</sup></td>
<td class="gt_row gt_right" headers="Batch Size">52</td>
<td class="gt_row gt_left" headers="Learning Rate">10<sup>-3.12</sup> (inverse decay)</td>
<td class="gt_row gt_right" headers="Momentum">0.982</td>
<td class="gt_row gt_right" headers="Brier">0.0842</td>
<td class="gt_row gt_right" headers="ROC">0.9486</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-mlp-top-five-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;17.2: The best models as ranked by the Brier score.
</figcaption>
</figure>
</div>
</div>
<p>Digging in further, <a href="#tbl-mlp-top-five" class="quarto-xref">Table&nbsp;<span>17.2</span></a> displays the top five candidates from a pool of 300. They are a mixture of one- and two-layer networks, but note that we have to go to the third decimal place to distinguish their Brier scores. Constant learning rates did well as long as their rates were fairly low and batch sizes were fairly small. The number of parameters varied as well; increasing the model’s complexity did not appear to help for this dataset. In fact, the rank correlation between the Brier score and the number of parameters was -0.08, implying that there is little evidence that adding more layers and/or units to the MLP improves the model for these data.</p>
<p>The Brier scores are low and the areas under the ROC curves are all at least 0.94, indicating that the predictions effectively separate the classes and that the probability estimates are accurate. Across the candidates, the rank correlation between the Brier score and the ROC AUC was -0.95; in this case, their optimization results are well-aligned.</p>
<p>How do these results compare to the previous KNN model? The Brier scores are very close: 0.0828 for the neural network and 0.0778 for KNN The difference is very small, and a 90% confidence interval for the difference is (-0.0071, -0.0031), suggesting the FDA model has statistically better performance although the difference is very small. The difference in the area under the ROC curve is -0.0057 with interval (-0.0086, -0.00341), leading us to a similar conclusion. Given that the KNN model is simpler, we might favor that model over our neural network.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-nonlinear.html#sec-cls-nnet"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
</section>
<section id="sec-cls-tab-net" class="level2" data-number="17.5">
<h2 data-number="17.5" class="anchored" data-anchor-id="sec-cls-tab-net"><span class="header-section-number">17.5</span> Special Tabular Network Models</h2>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-nonlinear.html#sec-cls-tab-net"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-cls-svm" class="level2" data-number="17.6">
<h2 data-number="17.6" class="anchored" data-anchor-id="sec-cls-svm"><span class="header-section-number">17.6</span> Support Vector Machines</h2>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-nonlinear.html#sec-cls-svm"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="chapter-references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="chapter-references">Chapter References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-achiam2023gpt" class="csl-entry" role="listitem">
Achiam, J, S Adler, S Agarwal, L Ahmad, I Akkaya, F Leoni Aleman, D Almeida, et al. 2023. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=+GPT+4+technical+report&amp;as_ylo=2023&amp;as_yhi=2023&amp;btnG="><span>GPT</span>-4 Technical Report</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-baldi1989neural" class="csl-entry" role="listitem">
Baldi, P, and K Hornik. 1989. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Neural+networks+and+principal+component+analysis+Learning+from+examples+without+local+minima&amp;as_ylo=1989&amp;as_yhi=1989&amp;btnG=">Neural Networks and Principal Component Analysis: Learning from Examples Without Local Minima</a>.”</span> <em>Neural Networks</em> 2 (1): 53–58.
</div>
<div id="ref-Boykis_What_are_embeddings_2023" class="csl-entry" role="listitem">
Boykis, V. 2023. <span>“<span class="nocase">What are embeddings?</span>”</span> <a href="https://doi.org/10.5281/zenodo.8015028">https://doi.org/10.5281/zenodo.8015028</a>.
</div>
<div id="ref-breiman1984nonlinear" class="csl-entry" role="listitem">
Breiman, L, and R Ihaka. 1984. <span>“Nonlinear Discriminant Analysis via Scaling and <span>ACE</span>.”</span> University of. California, Berkeley.
</div>
<div id="ref-chen2017tutorial" class="csl-entry" role="listitem">
Chen, YC. 2017. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+tutorial+on+kernel+density+estimation+and+recent+advances&amp;as_ylo=2017&amp;as_yhi=2017&amp;btnG=">A Tutorial on Kernel Density Estimation and Recent Advances</a>.”</span> <em>Biostatistics &amp; Epidemiology</em> 1 (1): 161–87.
</div>
<div id="ref-clevert2015fast" class="csl-entry" role="listitem">
Clevert, DA, T Unterthiner, and S Hochreiter. 2015. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Fast+and+accurate+deep+network+learning+by+exponential+linear+units+elus+&amp;as_ylo=2015&amp;as_yhi=2015&amp;btnG=">Fast and Accurate Deep Network Learning by Exponential Linear Units (Elus)</a>.”</span> <em>arXiv</em> 4 (5): 11.
</div>
<div id="ref-cunningham2021k" class="csl-entry" role="listitem">
Cunningham, P, and SJ Delany. 2021. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=K+nearest+neighbour+classifiers+a+tutorial&amp;as_ylo=2021&amp;as_yhi=2021&amp;btnG=">K-Nearest Neighbour Classifiers-a Tutorial</a>.”</span> <em>ACM Computing Surveys (CSUR)</em> 54 (6): 1–25.
</div>
<div id="ref-dauphin2014identifying" class="csl-entry" role="listitem">
Dauphin, Y, R Pascanu, C Gulcehre, K Cho, S Ganguli, and Y Bengio. 2014. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Identifying+and+attacking+the+saddle+point+problem+in+high+dimensional+non+convex+optimization&amp;as_ylo=2014&amp;as_yhi=2014&amp;btnG=">Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization</a>.”</span> <em>Advances in Neural Information Processing Systems</em> 27.
</div>
<div id="ref-DeMaesschalck20001" class="csl-entry" role="listitem">
De Maesschalck, R., D. Jouan-Rimbaud, and D Massart. 2000. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+Mahalanobis+distance&amp;as_ylo=2000&amp;as_yhi=2000&amp;btnG=">The Mahalanobis Distance</a>.”</span> <em>Chemometrics and Intelligent Laboratory Systems</em> 50 (1): 1–18.
</div>
<div id="ref-defazio2021" class="csl-entry" role="listitem">
Defazio, A, and S Jelassi. 2021. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Adaptivity+without+Compromise+A+Momentumized+Adaptive+Dual+Averaged+Gradient+Method+for+Stochastic+Optimization&amp;as_ylo=2021&amp;as_yhi=2021&amp;btnG=">Adaptivity Without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-devlin2019bert" class="csl-entry" role="listitem">
Devlin, J, MW Chang, K Lee, and K Toutanova. 2019. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Bert+Pre+training+of+deep+bidirectional+transformers+for+language+understanding&amp;as_ylo=2019&amp;as_yhi=2019&amp;btnG=">Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding</a>.”</span> In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1</em>, 4171–86.
</div>
<div id="ref-kinga2015method" class="csl-entry" role="listitem">
Diederik, P, and J Ba. 2015. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+method+for+stochastic+optimization&amp;as_ylo=2015&amp;as_yhi=2015&amp;btnG=">A Method for Stochastic Optimization</a>.”</span> In <em>International Conference on Learning Representations (ICLR)</em>. Vol. 5. 6. California;
</div>
<div id="ref-dozat2016" class="csl-entry" role="listitem">
Dozat, T. 2016. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Incorporating+Nesterov+Momentum+into+Adam+&amp;as_ylo=2016&amp;as_yhi=2016&amp;btnG=">Incorporating <span class="nocase">Nesterov Momentum into Adam</span></a>.”</span> In <em>Proceedings of the 4th International Conference on Learning Representations</em>, 1–4.
</div>
<div id="ref-duchi2011adaptive" class="csl-entry" role="listitem">
Duchi, J, E Hazan, and Y Singer. 2011. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Adaptive+subgradient+methods+for+online+learning+and+stochastic+optimization+&amp;as_ylo=2011&amp;as_yhi=2011&amp;btnG=">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.</a>”</span> <em>Journal of Machine Learning Research</em> 12 (7).
</div>
<div id="ref-dudani1976distance" class="csl-entry" role="listitem">
Dudani, S. 1976. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+distance+weighted+k+nearest+neighbor+rule&amp;as_ylo=1976&amp;as_yhi=1976&amp;btnG=">The Distance-Weighted k-Nearest-Neighbor Rule</a>.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics</em>, no. 4: 325–27.
</div>
<div id="ref-dugas2000incorporating" class="csl-entry" role="listitem">
Dugas, C, E Bengio, F Bélisle, C Nadeau, and R Garcia. 2000. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Incorporating+second+order+functional+knowledge+for+better+option+pricing&amp;as_ylo=2000&amp;as_yhi=2000&amp;btnG=">Incorporating Second-Order Functional Knowledge for Better Option Pricing</a>.”</span> <em>Advances in Neural Information Processing Systems</em> 13.
</div>
<div id="ref-elfwing2018sigmoid" class="csl-entry" role="listitem">
Elfwing, S, E Uchibe, and K Doya. 2018. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Sigmoid+weighted+linear+units+for+neural+network+function+approximation+in+reinforcement+learning&amp;as_ylo=2018&amp;as_yhi=2018&amp;btnG=">Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning</a>.”</span> <em>Neural Networks</em> 107: 3–11.
</div>
<div id="ref-friedman1989regularized" class="csl-entry" role="listitem">
Friedman, J. 1989. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Regularized+discriminant+analysis&amp;as_ylo=1989&amp;as_yhi=1989&amp;btnG=">Regularized Discriminant Analysis</a>.”</span> <em>Journal of the American Statistical Association</em> 84 (405): 165–75.
</div>
<div id="ref-ghojogh2019linear" class="csl-entry" role="listitem">
Ghojogh, B, and M Crowley. 2019. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Linear+and+quadratic+discriminant+analysis+Tutorial&amp;as_ylo=2019&amp;as_yhi=2019&amp;btnG=">Linear and Quadratic Discriminant Analysis: Tutorial</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-glorot2010understanding" class="csl-entry" role="listitem">
Glorot, X, and Y Bengio. 2010. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Understanding+the+difficulty+of+training+deep+feedforward+neural+networks&amp;as_ylo=2010&amp;as_yhi=2010&amp;btnG=">Understanding the Difficulty of Training Deep Feedforward Neural Networks</a>.”</span> In <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 249–56. JMLR Workshop; Conference Proceedings.
</div>
<div id="ref-glorot2011deep" class="csl-entry" role="listitem">
Glorot, X, A Bordes, and Y Bengio. 2011. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Deep+sparse+rectifier+neural+networks&amp;as_ylo=2011&amp;as_yhi=2011&amp;btnG=">Deep Sparse Rectifier Neural Networks</a>.”</span> In <em>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</em>, 315–23. JMLR Workshop; Conference Proceedings.
</div>
<div id="ref-golub1979generalized" class="csl-entry" role="listitem">
Golub, G, M Heath, and G Wahba. 1979. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Generalized+cross+validation+as+a+method+for+choosing+a+good+ridge+parameter&amp;as_ylo=1979&amp;as_yhi=1979&amp;btnG=">Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter</a>.”</span> <em>Technometrics</em> 21 (2): 215–23.
</div>
<div id="ref-goodfellow2016deep" class="csl-entry" role="listitem">
Goodfellow, I, Y Bengio, and A Courville. 2016. <em><a href="https://www.deeplearningbook.org">Deep Learning</a></em>. MIT press.
</div>
<div id="ref-gower" class="csl-entry" role="listitem">
Gower, J. 1971. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+general+coefficient+of+similarity+and+some+of+its+properties&amp;as_ylo=1971&amp;as_yhi=1971&amp;btnG=">A General Coefficient of Similarity and Some of Its Properties</a>.”</span> <em>Biometrics</em> 27 (4): 857–71.
</div>
<div id="ref-guo2016entity" class="csl-entry" role="listitem">
Guo, C, and F Berkhahn. 2016. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Entity+embeddings+of+categorical+variables&amp;as_ylo=2016&amp;as_yhi=2016&amp;btnG=">Entity Embeddings of Categorical Variables</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-hand1981discrimination" class="csl-entry" role="listitem">
Hand, D. 1981. <em>Discrimination and Classification</em>. Wiley.
</div>
<div id="ref-hand2001idiot" class="csl-entry" role="listitem">
Hand, D, and K Yu. 2001. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Idiot+s+Bayes+not+so+stupid+after+all+&amp;as_ylo=2001&amp;as_yhi=2001&amp;btnG=">Idiot’s <span>Bayes</span> — Not so Stupid After All?</a>”</span> <em>International Statistical Review</em> 69 (3): 385–98.
</div>
<div id="ref-hanson1988comparing" class="csl-entry" role="listitem">
Hanson, S, and L Pratt. 1988. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Comparing+biases+for+minimal+network+construction+with+back+propagation&amp;as_ylo=1988&amp;as_yhi=1988&amp;btnG=">Comparing Biases for Minimal Network Construction with Back-Propagation</a>.”</span> <em>Advances in Neural Information Processing Systems</em> 1.
</div>
<div id="ref-hastie1994flexible" class="csl-entry" role="listitem">
Hastie, T, R Tibshirani, and A Buja. 1994. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Flexible+discriminant+analysis+by+optimal+scoring&amp;as_ylo=1994&amp;as_yhi=1994&amp;btnG=">Flexible Discriminant Analysis by Optimal Scoring</a>.”</span> <em>Journal of the American Statistical Association</em> 89 (428): 1255–70.
</div>
<div id="ref-mda_2024" class="csl-entry" role="listitem">
Hastie, T, and T Tibshirani. 2024. <span>“<span class="nocase">mda</span>: Mixture and Flexible Discriminant Analysis.”</span> <a href="https://doi.org/10.32614/CRAN.package.mda">https://doi.org/10.32614/CRAN.package.mda</a>.
</div>
<div id="ref-he2015delving" class="csl-entry" role="listitem">
He, K, X Zhang, S Ren, and J Sun. 2015. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Delving+deep+into+rectifiers+Surpassing+human+level+performance+on+imagenet+classification&amp;as_ylo=2015&amp;as_yhi=2015&amp;btnG=">Delving Deep into Rectifiers: Surpassing Human-Level Performance on Imagenet Classification</a>.”</span> In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 1026–34.
</div>
<div id="ref-he2016deep" class="csl-entry" role="listitem">
He, K, X Zhang, S Ren, and J Sun. 2016. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Deep+residual+learning+for+image+recognition&amp;as_ylo=2016&amp;as_yhi=2016&amp;btnG=">Deep Residual Learning for Image Recognition</a>.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 770–78.
</div>
<div id="ref-hechenbichler2004weighted" class="csl-entry" role="listitem">
Hechenbichler, K., and K. Schliep. 2004. <span>“Weighted k-Nearest-Neighbor Techniques and Ordinal Classification.”</span> Sfb386. <a href="http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-1769-9">http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-1769-9</a>.
</div>
<div id="ref-hendrycks2016gaussian" class="csl-entry" role="listitem">
Hendrycks, D, and K Gimpel. 2016. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Gaussian+error+linear+units+gelus+&amp;as_ylo=2016&amp;as_yhi=2016&amp;btnG=">Gaussian Error Linear Units (Gelus)</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-hinton2012neural" class="csl-entry" role="listitem">
Hinton, G. 2012. <span>“Neural Networks for Machine Learning, Lecture 6e.”</span> <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" class="uri">https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a>.
</div>
<div id="ref-hochreiter1998vanishing" class="csl-entry" role="listitem">
Hochreiter, S. 1998. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+vanishing+gradient+problem+during+learning+recurrent+neural+nets+and+problem+solutions&amp;as_ylo=1998&amp;as_yhi=1998&amp;btnG=">The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions</a>.”</span> <em>International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</em> 6 (02): 107–16.
</div>
<div id="ref-hochreiter1997long" class="csl-entry" role="listitem">
Hochreiter, S, and J Schmidhuber. 1997. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Long+short+term+memory&amp;as_ylo=1997&amp;as_yhi=1997&amp;btnG=">Long Short-Term Memory</a>.”</span> <em>Neural Computation</em> 9 (8): 1735–80.
</div>
<div id="ref-hoffer2017train" class="csl-entry" role="listitem">
Hoffer, E, I Hubara, and D Soudry. 2017. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Train+longer+generalize+better+closing+the+generalization+gap+in+large+batch+training+of+neural+networks&amp;as_ylo=2017&amp;as_yhi=2017&amp;btnG=">Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks</a>.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-hubert2024robust" class="csl-entry" role="listitem">
Hubert, M, J Raymaekers, and P Rousseeuw. 2024. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Robust+discriminant+analysis&amp;as_ylo=2024&amp;as_yhi=2024&amp;btnG=">Robust Discriminant Analysis</a>.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 16 (5): e70003.
</div>
<div id="ref-JohnsonWichern" class="csl-entry" role="listitem">
Johnson, R, and D Wichern. 2007. <em>Applied Multivariate Statistical Analysis</em>. 6th ed. Upper Saddle River, <span>NJ</span>: Prentice Hall.
</div>
<div id="ref-keskar2016large" class="csl-entry" role="listitem">
Keskar, NS, D Mudigere, J Nocedal, M Smelyanskiy, and PTP Tang. 2016. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=On+large+batch+training+for+deep+learning+Generalization+gap+and+sharp+minima&amp;as_ylo=2016&amp;as_yhi=2016&amp;btnG=">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-krogh1991simple" class="csl-entry" role="listitem">
Krogh, A, and J Hertz. 1991. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+simple+weight+decay+can+improve+generalization&amp;as_ylo=1991&amp;as_yhi=1991&amp;btnG=">A Simple Weight Decay Can Improve Generalization</a>.”</span> <em>Advances in Neural Information Processing Systems</em> 4.
</div>
<div id="ref-lecun2002efficient" class="csl-entry" role="listitem">
LeCun, Y, L Bottou, G B Orr, and KR Müller. 2002. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Efficient+backprop&amp;as_ylo=2002&amp;as_yhi=2002&amp;btnG=">Efficient Backprop</a>.”</span> In <em>Neural Networks: Tricks of the Trade</em>, 9–50. Springer.
</div>
<div id="ref-loshchilov2017decoupled" class="csl-entry" role="listitem">
Loshchilov, I, and F Hutter. 2017. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Decoupled+weight+decay+regularization&amp;as_ylo=2017&amp;as_yhi=2017&amp;btnG=">Decoupled Weight Decay Regularization</a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-lu2022gradient" class="csl-entry" role="listitem">
Lu, J. 2022. <em><a href="https://arxiv.org/abs/2205.00832">Gradient Descent, Stochastic Optimization, and Other Tales</a></em>. Eliva Press.
</div>
<div id="ref-Manning2009" class="csl-entry" role="listitem">
Manning, D, P Raghavan, and H Schutze. 2009. <em><a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html">Introduction to Information Retrieval</a></em>. Cambridge University Press.
</div>
<div id="ref-mclachlan2005discriminant" class="csl-entry" role="listitem">
McLachlan, G. 2005. <em>Discriminant Analysis and Statistical Pattern Recognition</em>. John Wiley &amp; Sons.
</div>
<div id="ref-murphy2012machine" class="csl-entry" role="listitem">
Murphy, K. 2012. <em><a href="https://probml.github.io/pml-book/book0.html">Machine Learning: A Probabilistic Perspective</a></em>. MIT press.
</div>
<div id="ref-pang2009shrinkage" class="csl-entry" role="listitem">
Pang, H, T Tong, and H Zhao. 2009. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Shrinkage+based+diagonal+discriminant+analysis+and+its+applications+in+high+dimensional+data&amp;as_ylo=2009&amp;as_yhi=2009&amp;btnG=">Shrinkage-Based Diagonal Discriminant Analysis and Its Applications in High-Dimensional Data</a>.”</span> <em>Biometrics</em> 65 (4): 1021–29.
</div>
<div id="ref-park1990comparison" class="csl-entry" role="listitem">
Park, B, and J Marron. 1990. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Comparison+of+data+driven+bandwidth+selectors&amp;as_ylo=1990&amp;as_yhi=1990&amp;btnG=">Comparison of Data-Driven Bandwidth Selectors</a>.”</span> <em>Journal of the American Statistical Association</em> 85 (409): 66–72.
</div>
<div id="ref-Samworth" class="csl-entry" role="listitem">
Samworth, R. 2012. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=+Optimal+weighted+nearest+neighbour+classifiers+&amp;as_ylo=2012&amp;as_yhi=2012&amp;btnG=">Optimal Weighted Nearest Neighbour Classifiers</a>.”</span> <em>The Annals of Statistics</em> 40 (5): 2733–63.
</div>
<div id="ref-Silverman86" class="csl-entry" role="listitem">
Silverman, B. 1986. <em>Density Estimation for Statistics and Data Analysis</em>. Chapman &amp; Hall.
</div>
<div id="ref-smith2017cyclical" class="csl-entry" role="listitem">
Smith, L. 2017. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Cyclical+learning+rates+for+training+neural+networks&amp;as_ylo=2017&amp;as_yhi=2017&amp;btnG=">Cyclical Learning Rates for Training Neural Networks</a>.”</span> In <em>2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 464–72. IEEE.
</div>
<div id="ref-sutskever2013importance" class="csl-entry" role="listitem">
Sutskever, I, J Martens, G Dahl, and G Hinton. 2013. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=On+the+importance+of+initialization+and+momentum+in+deep+learning&amp;as_ylo=2013&amp;as_yhi=2013&amp;btnG=">On the Importance of Initialization and Momentum in Deep Learning</a>.”</span> In <em>International Conference on Machine Learning</em>, 1139–47. pmlr.
</div>
<div id="ref-weigend1990generalization" class="csl-entry" role="listitem">
Weigend, A, D Rumelhart, and B Huberman. 1990. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Generalization+by+weight+elimination+with+application+to+forecasting&amp;as_ylo=1990&amp;as_yhi=1990&amp;btnG=">Generalization by Weight-Elimination with Application to Forecasting</a>.”</span> <em>Advances in Neural Information Processing Systems</em> 3.
</div>
<div id="ref-zuo2008kernel" class="csl-entry" role="listitem">
Zuo, W, D Zhang, and K Wang. 2008. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=On+kernel+difference+weighted+k+nearest+neighbor+classification&amp;as_ylo=2008&amp;as_yhi=2008&amp;btnG=">On Kernel Difference-Weighted k-Nearest Neighbor Classification</a>.”</span> <em>Pattern Analysis and Applications</em> 11 (3): 247–57.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>We’ll see kernel functions two more times in this chapter.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In units of Pa × 100.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>A different view of predictor importance will be seem later in <a href="#fig-forested-fda-imp" class="quarto-xref">Figure&nbsp;<span>17.7</span></a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The approximation involves scaling factors derived from the eigenvalues of <span class="math inline">\(\boldsymbol{\Phi}\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>For consistency, we’ll use “slopes” and “intercepts” here.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p> This representation and the one below were adapted from Izaak Neutelings’s excellent work, which can be found at <a href="https://tikz.net/neural_networks/"><code>https://tikz.net/neural_networks/</code></a> and is licensed under a CC BY-SA 4.0.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Generalized linear models are a good example of this<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>orderNorm is perhaps more attractive since it standardizes the predictors and coerces them to a symmetric distribution.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Although the submodel trick described in <a href="grid-search.html#sec-submodels" class="quarto-xref"><span>Section 11.3.1</span></a> could greatly improve the effectiveness of this approach.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Typically on log<sub>2</sub> units<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>In <a href="#sec-nnet-training" class="quarto-xref"><span>Section 17.4.2</span></a>, we previously described Nesterov momentum. This is unrelated to the current tool bearing the same name.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>For the first update, the value is simply <span class="math inline">\(v_1 = \alpha g(\boldsymbol{\theta}_i)\)</span>.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Since the loss function for this example is based on a single, deterministic value, the momentum value was set very low (<span class="math inline">\(\phi = 2/3\)</span>). However, the overall trend is consistent with results from a typical performance metric like cross-entropy.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/aml4td\.org");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/cls-linear.html" class="pagination-link" aria-label="Generalized Linear and Additive Classifiers">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Generalized Linear and Additive Classifiers</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/cls-trees.html" class="pagination-link" aria-label="Classification using Trees and Rules">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Classification using Trees and Rules</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js" type="text/javascript"></script>
<script type="text/javascript">
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>




</body></html>