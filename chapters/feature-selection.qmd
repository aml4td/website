---
knitr:
  opts_chunk:
    cache.path: "../_cache/feature-selection/"
---

# Feature Selection {#sec-feature-selection}

```{r}
#| label: feature-selection-setup
#| include: false
source("../R/_common.R")

# ------------------------------------------------------------------------------

library(heatmaply)
library(future)
library(tidymodels)

# ------------------------------------------------------------------------------
# Set options

tidymodels_prefer()
theme_set(theme_transparent())
plan("multisession")
set_options()

# ------------------------------------------------------------------------------
# load data and results

source("../R/setup_deliveries.R")
load("../RData/deliveries_cubist.RData")
```

In Chapters [-@sec-numeric-predictors] through [-@sec-interactions-nonlinear], we described different methods for adding new predictors to the model (e.g., interactions, spline columns, etc.). We can add any predictors (or functions of predictors) based on first principles or our intuition or contextual knowledge. 

We are also motivated to remove unwanted or irrelevant predictors. This can decrease model complexity, training time, computer memory, the cost of acquiring unimportant predictors, and so on. For some models, the presence of non-informative predictors can _decrease_ performance. 

We may want to remove predictors based on their distributions. Unsupervised feature selection focuses on just the predictor data. 

More often, though, we are interested in supervised feature selection^[Recall that we use the terms “predictors” and “features” interchangeably. We favor the former term, but “features” is more commonly used when discussing the filtering of columns.]: the removal of predictors that do not appear to have a relationship to the outcome. This implies that we have a statistic that can compute some measure of each predictor’s importance. 

While it is easy to add predictors to a model, it can be very difficult (and computationally expensive) to determine which predictors to remove properly. The reason comes back to the misuse (or over-use) of data. We’ve described the problem of using the same data to fit and evaluate a model. By naively re-predicting the training set, we can accidentally get overly optimistic performance statistics. To compensate for this, resampling repeatedly allocates some data for model fitting and other data for model evaluation. 

For feature selection, we have the same problem when the same data are used to measure importance, filter the predictors, and then train the model on the smaller feature set. It is similar to repeatedly taking the same test and then measuring our knowledge using the grade for the last test repetition. It doesn’t accurately measure the quantity that is important to us.  

This chapter will summarize the most important aspects of feature selection. This is a broad topic and a more extensive discussion can be found in @fes. First, we’ll use an example to quantify the effect of irrelevant predictors on different models. Next, the four main approaches for feature selection are described and illustrated. 

## The Cost of Irrelevant Predictors  {#sec-irrelevant-predictors}

```{r}
#| label: noise-sim
#| echo: false

load("../RData/noise_simulation_results.RData")
```

Does it matter if the model is given extra predictors that are unrelated to the outcome? It depends on the model. To quantify this, data were simulated using the equation described in @hooker2004discovering and also used in @sorokina2008detecting: 

$$
y_i = \pi^{x_{i1} x_{i2}}  \sqrt{ 2 x_{i3} } - asin(x_{i4}) + \log(x_{i3}  + x_{i5}) - (x_{i9} / x_{i10}) \sqrt{x_{i7} / x_{i8}} - x_{i2} x_{i7} + \epsilon_i
$$ {#eq-hooker}

where predictors 1, 2, 3, 6, 7, and 9 are standard uniform while the others are uniform on `[0.6, 1.0]`. The errors $\epsilon_i$ are Gaussian with mean zero and a standard deviation of 0.25. Training and testing data sizes where simulated to be 1,000 and 100,000, respectively. 

To assess the effect of useless predictors, between 10 and 100 extra columns of standard normal data were generated (unrelated to the outcome). Models were fit, and the RMSE of the test set was computed for each model. Twenty simulations were run for each combination of models and extra features^[Details and code can be found at [`https://github.com/topepo/noise_features_sim `](https://github.com/topepo/noise_features_sim).]. 

The percent difference from baseline RMSE was computed (baseline being the model with no extra columns). @fig-irrelevant-predictors shows the results for the following models: 

- Bagged regression trees 
- Bayesian additive regression trees (BART)
- Boosted trees (via lightGBM)
- Cubist
- Generalized additive models (GAMs)
- K-nearest neighbors (KNN)
- Multivariate adaptive regression splines (MARS)
- Penalized linear regression
- Random forest
- RuleFit
- Single-layer neural networks
- Support vector machines (radial basis function kernel)

These models are described in detail in subsequent chapters. The colors of the points/lines signifies whether the model automatically removes unused predictors from the model equation (see @sec-automatic-selection below). 

```{r}
#| label: fig-irrelevant-predictors
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 5
#| fig-cap: For different regression models, the percent increase in RMSE is shown as a function of the number of additional non-informative predictors. 

noise_simulation_results_pct %>%
  mutate(
    `Automatic Selection` = ifelse(fs, "yes", "no")
  ) %>%
  ggplot(aes(num_extra, increase)) +
  geom_hline(yintercept = 0, col = "green", alpha = 1 / 2) +
  geom_point(aes(col = `Automatic Selection`)) +
  geom_line(aes(col = `Automatic Selection`)) +
  facet_wrap(~ label) +
  labs(x = "# Noise Features", y = "RMSE Increase") +
  scale_y_continuous(labels = scales::percent)
```

The results show that a few models, K-nearest neighbors, neural networks, and support vector machines, have severely degraded performance as the number of predictors increases. This is most likely due to the use of cross- and/or dot-products of the predictor columns in their calculations. The extra predictors add significant noise to these calculations, and that noise propagates in a way that inhibits the models from accurately determining the underlying relationship with the outcome. 

However, several other models use these same calculations without being drastically affected. This is due to the models **automatically performing feature selection** during model training. Let’s briefly describe two clusters of models that were largely unaffected by the extra columns. 

 1. Penalized linear regression, generalized additive models, and RuleFit add a penalty to their calculations, restricting the model parameters from becoming abnormally large unless the underlying data warrant a large coefficient. This effectively reduces the impact of extra predictors by keeping their corresponding model coefficient at or near zero. 

 2. MARS and Cubist selectively include predictors in their regression equations, only including those specifically selected as having value. 

The other unaffected models are tree-based. As seen in @fig-reg-tree, the model makes different rules by selecting predictors to _split_ the data. If a predictor was not used in any split, it is functionally independent of the outcome in the prediction equation. This provides some insensitivity to the presence of non-informative predictors, although there is slight degradation in RMSE as the number of columns increases. 

In summary, irrelevant columns only slightly affect ML models that automatically select features. Models that do not have this attribute can be crippled under the same circumstances. 

## Different Levels of Features

When we talk about selecting features, we should be clear about what level of computation we mean. There are often two levels: 

- **Original predictors** are the unmodified version of the data. 
- **Derived predictors** refer to the set of columns that are present after feature engineering. 

For example, if our model requires all numeric inputs, a categorical predictor is often converted to binary indicators. A categorical column for the day of the week is the original predictor, while the binary indicators for individual days are the derived predictor. Similarly, the 550 measurement predictors in the barley data are the original predictors, and embedded values, such as the PCA components, are the derived predictors. 

Depending on the context, our interest in feature selection could be at either level. If the original predictors are expensive to estimate, we would be more interested in removing them at the original level; if _any_ of their derived predictors are important, we need the original. 

This idea will arise again in @sec-importance when we discuss _variable importance scores_ for explaining models. 

## Unsupervised Selection {#sec-unsupervised-selection}

There are occasions where the values of the predictors are unfavorable or possibly detrimental to the model. The simplest example is a zero-variance column, where all of the rows of the training set have the same value. There is no information in these data and, although the model might be able to tolerate such a predictor, taking the time to identify and remove such columns is advisable. 

A hypothetical example is a training set of 500 data points with a count-based predictor that has mostly zero counts. However, there are three data points with a count of 1 and one instance with a count of 2. The variance of these data is low, but it would be difficult to determine a cutpoint for removal. @kuhn2008building developed an algorithm to find near-zero predictors, as defined by two criteria:

- The _freqeuncy ratio_ is the frequency of the most prevalent value over the second most frequent value. For our example, the ratio was 496/3 = 165.3.
- The _proportion of unique values_, which is 3 / 500 = 0.006. 

We can determine thresholds for both of these criteria to remove the predictor. The default behavior is to remove the column of the frequency ratio is greater than 19 **and** the proportion of unique values is less than 0.1. For the example, the predictor would be eliminated. 

```{r}
#| label: corr-data
#| echo: false

relab <- c("E", "B", "F", "H", "A", "G", "C", "D", "I", "L", "J", "K")

cor_data <- 
  modeldata::steroidogenic_toxicity %>% 
  select(-class) %>% 
  rename(A = cyp_19a1, B = cyp_11b1, C = cyp_21a1, D = star, E = cyp_11a1,
         `F` = hsd3b2, G = cyp_11b2, H = cyp_17a1, I = progesterone, 
         J = cortisol, K = testosterone, L = dhea)

cor_mat <- 
  cor_data %>% 
  cor(use = "pairwise.complete.obs") 

cut_0.5 <- 
  cor_data %>% 
  recipes:::corr_filter(cutoff = .5) %>% 
  sort() 

lab_0.5 <- cli::format_inline("{xfun::numbers_to_words(length(cut_0.5))} predictors ({.code {cut_0.5}})")
```

Another example of an unsupervised filter reduces between-predictor correlations. A high degree of correlation can reduce the effectiveness of many models. We can filter out predictors by choosing a threshold for the (absolute) pairwise correlations and finding the smallest subset of predictors to meet this criterion. 

A good example is @fig-corr-plot, which visualizes the correlation matrix for twelve predictors (from @maglich2014more). Predictors `A` and `B` are highly correlated (corr = `r round(cor_mat["A", "B"], 2)`), and only one might be sufficient to produce a better model. It is less clear which of the other predictors should removed. If we apply a threshold of 1 / 2, then `r lab_0.5` would be removed before model training. 

```{r}
#| label: fig-corr-plot
#| echo: false
#| out-width: 90%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: A heatmap of a correlation matrix for twelve predictors, ordered using a hierarchical clustering algorithm.

heatmaply(
  cor_mat,
  branches_lwd = .5,
  colors = RdBu(50),
  symm = TRUE,
  revC = FALSE,
  label_format_fun = function(...) round(..., digits = 2),
  limits = c(-1, 1),
  margins = c(50,215,10,150)
)
```

Similar redundancy can occur in categorical predictors. In this case, measures of similarity can be used to filter the predictors in the same way. 

## The Classes of Supervised Methods {#sec-selection-methods}

There are three main strategies for _supervised_ feature selection. The first and most effective was discussed: automatic selection occurs when a subset of predictors is determined during model training. We’ll discuss this more in the next section. 

A second strategy is to use a **filter** to screen predictors before adding them to the model. For example, the analysis shown in @fig-delivery-increases computed how each item in a food order impacted the delivery time.  We could have applied a filter where we only used food item predictors whose lower confidence interval for the increase in time was greater than one. The idea behind the filter is that it is applied once prior to the model fit. 

The third supervised strategy is called **wrappers**. In this case, a sequential algorithm proposes feature subsets, fits the model with these subsets, and then determines a better subset from the results. This sounds similar to iterative optimization because it is. Many of the same tools can be used. Wrappers are the most thorough approach to searching the space of feature subsets, but this can come with an enormous computational cost. 

The next few sections will describe the mechanics of these methods. However, how we utilize these algorithms is tricky. We definitely want to avoid overfitting the predictor set to the training data. 

## Automatic Selection {#sec-automatic-selection}

```{r}
#| label: mars-deliveries
#| include: false

set.seed(3838)
mars_res <- 
  mars(prod_degree = 2, mode = "regression") %>% 
  fit(time_to_delivery ~ ., data = delivery_train)

pat <- paste0("(", levels(delivery_train$day), ")", collapse = "|")

mars_imp <- 
  mars_res$fit %>% 
  evimp() %>% 
  unclass() %>% 
  as.matrix()

mars_df <- 
  mars_imp %>% 
  as_tibble(rownames = "term") %>% 
  mutate(predictor = gsub(pat, "", term))

mars_active <- distinct(mars_df, predictor)
```

To more closely demonstrate how automatic feature selection works, let’s return to the Cubist model fit to the delivery data in @sec-model-development-whole-game. That model created a series of regression trees, and converted them to rules (i.e., a path through the tree), then fit regression models for every rule. Recall that there was a very large number of rules (`r format(nrow(all_rules), big.mark = ",")`) each with its own linear regression. Despite the size of these data, only `r length(cb_active)` of the original `r length(cb_active) + cb_unused` predictors in the data set were used in rules or model fits; `r cb_unused` were judged to be irrelevant for predicting delivery times. 

This shows that we can often reduce the number of features for free. 

Different models view the training data differently, and the list of relevant predictors will likely change from model to model. When we fit a MARS model [@sec-mars-reg] to the data, only `r nrow(mars_active)` original predictors were used. If we use automatic selection methods to understand what is important, we should fit a variety of different models and create a consensus estimate of which predictors are “active” for the data set.  


## Filter Methods {#sec-filters}

## Wrapper Methods  {#sec-wrappers}

## Overfitting the Feature Set {#sec-selection-overfitting}

## Valid Feature Selection {#sec-valid-selection}

## Chapter References {.unnumbered}
