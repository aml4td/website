---
knitr:
  opts_chunk:
    cache.path: "../_cache/feature-selection/"
---

# Feature Selection {#sec-feature-selection}

```{r}
#| label: feature-selection-setup
#| include: false
source("../R/_common.R")

# ------------------------------------------------------------------------------

library(heatmaply)
library(future)
library(tidymodels)
library(colino)

# ------------------------------------------------------------------------------
# Set options

tidymodels_prefer()
theme_set(theme_transparent())
plan("multisession")
set_options()

# ------------------------------------------------------------------------------
# load data and results

source("../R/setup_deliveries.R")
load("../RData/deliveries_cubist.RData")
load("../RData/deliveries_lm.RData")

ctrl_rs <-
  control_resamples(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE
  )

reg_metrics <- metric_set(mae, rmse, rsq)
```

In Chapters [-@sec-numeric-predictors] through [-@sec-interactions-nonlinear], we described different methods for adding new predictors to the model (e.g., interactions, spline columns, etc.). We can add any predictors (or functions of predictors) based on first principles or our intuition or contextual knowledge. 

We are also motivated to remove unwanted or irrelevant predictors. This can decrease model complexity, training time, computer memory, the cost of acquiring unimportant predictors, and so on. For some models, the presence of non-informative predictors can _decrease_ performance. 

We may want to remove predictors based on their distributions. Unsupervised feature selection focuses on just the predictor data. 

More often, though, we are interested in supervised feature selection^[Recall that we use the terms “predictors” and “features” interchangeably. We favor the former term, but “features” is more commonly used when discussing the filtering of columns.]: the removal of predictors that do not appear to have a relationship to the outcome. This implies that we have a statistic that can compute some measure of each predictor’s importance. 

While it is easy to add predictors to a model, it can be very difficult (and computationally expensive) to determine which predictors to remove properly. The reason comes back to the misuse (or over-use) of data. We’ve described the problem of using the same data to fit and evaluate a model. By naively re-predicting the training set, we can accidentally get overly optimistic performance statistics. To compensate for this, resampling repeatedly allocates some data for model fitting and other data for model evaluation. 

For feature selection, we have the same problem when the same data are used to measure importance, filter the predictors, and then train the model on the smaller feature set. It is similar to repeatedly taking the same test and then measuring our knowledge using the grade for the last test repetition. It doesn’t accurately measure the quantity that is important to us.  

This chapter will summarize the most important aspects of feature selection. This is a broad topic and a more extensive discussion can be found in @fes. First, we’ll use an example to quantify the effect of irrelevant predictors on different models. Next, the four main approaches for feature selection are described and illustrated. 

## The Cost of Irrelevant Predictors  {#sec-irrelevant-predictors}

```{r}
#| label: noise-sim
#| echo: false

load("../RData/noise_simulation_results.RData")
```

Does it matter if the model is given extra predictors that are unrelated to the outcome? It depends on the model. To quantify this, data were simulated using the equation described in @hooker2004discovering and also used in @sorokina2008detecting: 

$$
y_i = \pi^{x_{i1} x_{i2}}  \sqrt{ 2 x_{i3} } - asin(x_{i4}) + \log(x_{i3}  + x_{i5}) - (x_{i9} / x_{i10}) \sqrt{x_{i7} / x_{i8}} - x_{i2} x_{i7} + \epsilon_i
$$ {#eq-hooker}

where predictors 1, 2, 3, 6, 7, and 9 are standard uniform while the others are uniform on `[0.6, 1.0]`. The errors $\epsilon_i$ are Gaussian with mean zero and a standard deviation of 0.25. Training and testing data sizes where simulated to be 1,000 and 100,000, respectively. 

To assess the effect of useless predictors, between 10 and 100 extra columns of standard normal data were generated (unrelated to the outcome). Models were fit, and the RMSE of the test set was computed for each model. Twenty simulations were run for each combination of models and extra features^[Details and code can be found at [`https://github.com/topepo/noise_features_sim `](https://github.com/topepo/noise_features_sim).]. 

The percent difference from baseline RMSE was computed (baseline being the model with no extra columns). @fig-irrelevant-predictors shows the results for the following models: 

- Bagged regression trees 
- Bayesian additive regression trees (BART)
- Boosted trees (via lightGBM)
- Cubist
- Generalized additive models (GAMs)
- K-nearest neighbors (KNN)
- Multivariate adaptive regression splines (MARS)
- Penalized linear regression
- Random forest
- RuleFit
- Single-layer neural networks
- Support vector machines (radial basis function kernel)

These models are described in detail in subsequent chapters. The colors of the points/lines signifies whether the model automatically removes unused predictors from the model equation (see @sec-automatic-selection below). 

```{r}
#| label: fig-irrelevant-predictors
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 5
#| fig-cap: For different regression models, the percent increase in RMSE is shown as a function of the number of additional non-informative predictors. 

noise_simulation_results_pct %>%
  mutate(
    `Automatic Selection` = ifelse(fs, "yes", "no")
  ) %>%
  ggplot(aes(num_extra, increase)) +
  geom_hline(yintercept = 0, col = "green", alpha = 1 / 2) +
  geom_point(aes(col = `Automatic Selection`)) +
  geom_line(aes(col = `Automatic Selection`)) +
  facet_wrap(~ label) +
  labs(x = "# Noise Features", y = "RMSE Increase") +
  scale_y_continuous(labels = scales::percent)
```

The results show that a few models, K-nearest neighbors, neural networks, and support vector machines, have severely degraded performance as the number of predictors increases. This is most likely due to the use of cross- and/or dot-products of the predictor columns in their calculations. The extra predictors add significant noise to these calculations, and that noise propagates in a way that inhibits the models from accurately determining the underlying relationship with the outcome. 

However, several other models use these same calculations without being drastically affected. This is due to the models **automatically performing feature selection** during model training. Let’s briefly describe two clusters of models that were largely unaffected by the extra columns. 

 1. Penalized linear regression, generalized additive models, and RuleFit add a penalty to their calculations, restricting the model parameters from becoming abnormally large unless the underlying data warrant a large coefficient. This effectively reduces the impact of extra predictors by keeping their corresponding model coefficient at or near zero. 

 2. MARS and Cubist selectively include predictors in their regression equations, only including those specifically selected as having value. 

The other unaffected models are tree-based. As seen in @fig-reg-tree, the model makes different rules by selecting predictors to _split_ the data. If a predictor was not used in any split, it is functionally independent of the outcome in the prediction equation. This provides some insensitivity to the presence of non-informative predictors, although there is slight degradation in RMSE as the number of columns increases. 

In summary, irrelevant columns only slightly affect ML models that automatically select features. Models that do not have this attribute can be crippled under the same circumstances. 

## Different Levels of Features

When we talk about selecting features, we should be clear about what level of computation we mean. There are often two levels: 

- **Original predictors** are the unmodified version of the data. 
- **Derived predictors** refer to the set of columns that are present after feature engineering. 

For example, if our model requires all numeric inputs, a categorical predictor is often converted to binary indicators. A categorical column for the day of the week is the original predictor, while the binary indicators for individual days are the derived predictor. Similarly, the 550 measurement predictors in the barley data are the original predictors, and embedded values, such as the PCA components, are the derived predictors. 

Depending on the context, our interest in feature selection could be at either level. If the original predictors are expensive to estimate, we would be more interested in removing them at the original level; if _any_ of their derived predictors are important, we need the original. 

This idea will arise again in @sec-importance when we discuss _variable importance scores_ for explaining models. 


## Overfitting the Feature Set {#sec-selection-overfitting}

Take basic linear regression sse as an example

@Ambroise2002p1493


## Validating Feature Selection {#sec-validating-selection}

diagram showing the selection is part of preprocessing so that when we validate the pipeline, we must re-run the feature selection

note that the qualitative effect of selection has the largest variance compoment (compared to model-to-model etc)


## Unsupervised Selection {#sec-unsupervised-selection}

There are occasions where the values of the predictors are unfavorable or possibly detrimental to the model. The simplest example is a zero-variance column, where all of the rows of the training set have the same value. There is no information in these data and, although the model might be able to tolerate such a predictor, taking the time to identify and remove such columns is advisable. 

A hypothetical example is a training set of 500 data points with a count-based predictor that has mostly zero counts. However, there are three data points with a count of 1 and one instance with a count of 2. The variance of these data is low, but it would be difficult to determine a cutpoint for removal. @kuhn2008building developed an algorithm to find **near-zero predictors**, as defined by two criteria:

- The _freqeuncy ratio_ is the frequency of the most prevalent value over the second most frequent value. For our example, the ratio was 496/3 = 165.3.
- The _proportion of unique values_, which is 3 / 500 = 0.006. 

We can determine thresholds for both of these criteria to remove the predictor. The default behavior is to remove the column of the frequency ratio is greater than 19 **and** the proportion of unique values is less than 0.1. For the example, the predictor would be eliminated. 

```{r}
#| label: corr-data
#| echo: false

relab <- c("E", "B", "F", "H", "A", "G", "C", "D", "I", "L", "J", "K")

cor_data <- 
  modeldata::steroidogenic_toxicity %>% 
  select(-class) %>% 
  rename(A = cyp_19a1, B = cyp_11b1, C = cyp_21a1, D = star, E = cyp_11a1,
         `F` = hsd3b2, G = cyp_11b2, H = cyp_17a1, I = progesterone, 
         J = cortisol, K = testosterone, L = dhea)

cor_mat <- 
  cor_data %>% 
  cor(use = "pairwise.complete.obs") 

cut_0.25 <- 
  cor_data %>% 
  recipes:::corr_filter(cutoff = .25) %>% 
  sort() 

lab_0.25 <- cli::format_inline("{xfun::numbers_to_words(length(cut_0.25))} predictors ({.code {cut_0.25}})")
```

Another example of an unsupervised filter reduces between-predictor correlations. A high degree of correlation can reduce the effectiveness of many models. We can filter out predictors by choosing a threshold for the (absolute) pairwise correlations and finding the smallest subset of predictors to meet this criterion. 

A good example is @fig-corr-plot, which visualizes the correlation matrix for twelve predictors (from @maglich2014more). Predictors `A` and `B` are highly correlated (corr = `r round(cor_mat["A", "B"], 2)`), and only one might be sufficient to produce a better model. Similar issues occur with predictor pairs `D`/`E` and `K`/`L`. It is less clear which of the other predictors should removed (if any). If we apply a threshold of 0.25, then `r lab_0.25` would be removed before model training. 

```{r}
#| label: fig-corr-plot
#| echo: false
#| out-width: 90%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: A heatmap of a correlation matrix for twelve predictors, ordered using a hierarchical clustering algorithm.

heatmaply(
  cor_mat,
  branches_lwd = .5,
  colors = RdBu(50),
  symm = TRUE,
  revC = FALSE,
  label_format_fun = function(...) round(..., digits = 2),
  limits = c(-1, 1),
  margins = c(50,215,10,150)
)
```

Similar redundancy can occur in categorical predictors. In this case, measures of similarity can be used to filter the predictors in the same way. 

## The Classes of Supervised Methods {#sec-selection-methods}

There are three main strategies for _supervised_ feature selection. The first and most effective was discussed: automatic selection occurs when a subset of predictors is determined during model training. We’ll discuss this more in the next section. 

The second supervised strategy is called **wrappers**. In this case, a sequential algorithm proposes feature subsets, fits the model with these subsets, and then determines a better subset from the results. This sounds similar to iterative optimization because it is. Many of the same tools can be used. Wrappers are the most thorough approach to searching the space of feature subsets, but this can come with an enormous computational cost. 

A third strategy is to use a **filter** to screen predictors before adding them to the model. For example, the analysis shown in @fig-delivery-increases computed how each item in a food order impacted the delivery time.  We could have applied a filter where we only used food item predictors whose lower confidence interval for the increase in time was greater than one. The idea behind the filter is that it is applied once prior to the model fit. 

The next few sections will describe the mechanics of these methods. However, how we utilize these algorithms is tricky. We definitely want to avoid overfitting the predictor set to the training data. 

## Automatic Selection {#sec-automatic-selection}

```{r}
#| label: mars-deliveries
#| include: false

set.seed(3838)
mars_res <- 
  mars(prod_degree = 2, mode = "regression") %>% 
  fit(time_to_delivery ~ ., data = delivery_train)

pat <- paste0("(", levels(delivery_train$day), ")", collapse = "|")

mars_imp <- 
  mars_res$fit %>% 
  evimp() %>% 
  unclass() %>% 
  as.matrix()

mars_df <- 
  mars_imp %>% 
  as_tibble(rownames = "term") %>% 
  mutate(predictor = gsub(pat, "", term))

mars_active <- distinct(mars_df, predictor)
```

To more closely demonstrate how automatic feature selection works, let’s return to the Cubist model fit to the delivery data in @sec-model-development-whole-game. That model created a series of regression trees, and converted them to rules (i.e., a path through the tree), then fit regression models for every rule. Recall that there was a very large number of rules (`r format(nrow(all_rules), big.mark = ",")`) each with its own linear regression. Despite the size of these data, only `r length(cb_active)` of the original `r length(cb_active) + cb_unused` predictors in the data set were used in rules or model fits; `r cb_unused` were judged to be irrelevant for predicting delivery times. 

::: {.important-box}
This shows that we can often reduce the number of features for free. 
:::

Different models view the training data differently, and the list of relevant predictors will likely change from model to model. When we fit a MARS model [@sec-mars-reg] to the data, only `r nrow(mars_active)` original predictors were used. If we use automatic selection methods to understand what is important, we should fit a variety of different models and create a consensus estimate of which predictors are “active” for the data set.  

## Wrapper Methods  {#sec-wrappers}

Wrapper methods use an overarching search method to optimize binary indicators, one for each potential predictor, that control which predictors are given to the model.

The most popular approach is recursive feature elimination (RFE), which starts with the complete set of predictors and uses a mechanism to rank each in terms of their utility for predicting the outcome. Using this ordering, predictors are successively removed while performance is monitored. The performance profile determines the optimal subset size, and the most important predictors are included in the model. RFE is greedy, though; it predetermines the order of the predictors and will never consider a subset that is inconsistent with this order. 

Two global search methods described in @sec-iterative, genetic algorithms and simulated annealing, can also be used. In this instance, the search space is the predictor space represented as binary indicators. The same process applies, though. For example, a GA would contain a population of different subsets, each producing a fitted model. The performance values enable the creation of the next generation via selection, reproduction, and mutation. In the case of simulated annealing, an initial subset is created and evaluated, and perturbations are created by randomly altering a small set of indicators (to create a “nearby” subset). Suboptimal subsets can be accepted similarly to our application for tuning parameters. 

However, for each of these cases, the details matter. First, we should probably use separate data to 

- estimate performance for each candidate subset and 
- determining how far the search should proceed.

If we have large data sets, we can partition them into separate validation sets for each purpose. Otherwise, resampling is probably the answer to solve one or both of these problems. 

The need to optimize tuning parameters can greatly complicate this entire process. A set of optimal parameter values for large subset sizes may perform very poorly for small subsets. In some cases, such as $m_{try}$ and the number of embedding dimensions, the tuning parameter depends on the subset size. 

In this case, the traditional approach uses a nested resampling procedure similar to the one shown in @sec-nested-resampling. An inner resampling scheme tunes the model, and the optimized model is used to predict the assessment set from the outer resample. Even with parallel processing, this can become an onerous computational task. 

However, let’s illustrate a wrapper method using an artificial data set from the simulations described in @sec-irrelevant-predictors. We’ll use @eq-hooker and supplement the nine real predictors with twenty noise columns. For simplicity, we’ll use a neural network to predict the outcome with a static set of tuning parameters. The training set consisted of 1,000 data points. 

Since feature selection is part of the broader model pipeline, 10-fold cross-validation was used to measure the effectiveness of 150 iterations of simulated annealing. A separate SA run is created for each resample; we use the analysis set to train the model on the current subset, and the assessment set is used to measure the RMSE. 

@fig-selection-sa shows the results, where each point is the average of the 10 RMSE values for the SA at each iteration. The results clearly improve as the selection process mostly begins to include important predictors and discards a few of the noise columns. After approximately 120 iterations, the solution stabilizes.  This appears to be a very straightforward search process that clearly indicates when to stop the SA search. 

```{r}
#| label: fig-selection-sa
#| echo: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 3.25
#| fig-cap: The progress of using simualted annealing to select features for.a neural network.  
load("../RData/nnet_sa_res.RData")

nnet_sa_res$internal %>% 
  summarize(mean = mean(RMSE), .by = c(Iter)) %>% 
  ggplot(aes(Iter, mean)) + 
  geom_line() +
  geom_point(cex = 1 / 2) +
  labs(x = "SA Iteration", y = "RMSE (resampled)")
```


However, looks can be deceiving. The feature selection process is incredibly variable, and adding or removing an important predictor can cause severe changes in performance. @fig-selection-sa is showing the _average_ change in performance, and the relative smoothness of this curve is somewhat deceiving. 

@fig-selection-sa-resamples shows what could have happened by visualizing each of the 10 searches. The results are much more dynamic and significantly vary from resample to resample. Most (but not all) searches contain one or two precipitous drops in the RMSE with many spikes of high errors (caused by a bad permutation of the solution). These cliffs and spikes do not reproducibly occur at the same iteration or in the same way. 


```{r}
#| label: fig-selection-sa-resamples
#| echo: false
#| out-width: 90%
#| fig-width: 8
#| fig-height: 6
#| fig-cap: The individual SA runs that make up the data in @fig-selection-sa.  
load("../RData/nnet_sa_res.RData")

nnet_sa_res$internal %>% 
  ggplot(aes(Iter, RMSE)) + 
  geom_line() +
  facet_wrap(~ Resample) +
  labs(x = "SA Iteration", y = "RMSE")
```

We should keep in mind that @fig-selection-sa lets us know where, on average, the search should stop, but our actual search result (that uses the entire training set) is more likely to look like what we see in @fig-selection-sa-resamples.

For this particular search, the numerically best result occurred at iteration `r nnet_sa_res$optIter` and the final model contained `r length(nnet_sa_res$optVariables)` predictors where `r xfun::numbers_to_words(sum(grepl("noise", nnet_sa_res$optVariables)))` of the twenty noise columns are still contained in the model. Our resampled estimate of the RMSE, at this iteration, was `r round(mean(nnet_sa_res$internal$RMSE[nnet_sa_res$internal$Iter == nnet_sa_res$optIter]), 3)` (a perfect model would have a value of 0.25). We also simulated a large “test set” and the RMSE for that data set was XXX with the optimized neural network.   



## Filter Methods {#sec-filters}

```{r}
#| label: filter-deliveries
#| include: false
#| cache: true

spline_filter_rec <-
  recipe(time_to_delivery ~ ., data = delivery_train) %>%
  step_dummy(all_factor_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_spline_natural(hour, distance, deg_free = 10) %>%
  step_interact(~starts_with("hour"):starts_with("day")) %>% 
  step_bin2factor(starts_with("item")) %>% 
  step_select_aov(starts_with("item"), outcome = "time_to_delivery", 
                  threshold = 0.1)

filter_wflow <-
  workflow() %>%
    add_model(linear_reg()) %>%
    add_recipe(spline_filter_rec)

set.seed(3838)
filter_res <-
  fit_resamples(
    filter_wflow,
    delivery_rs,
    control = ctrl_rs,
    metrics = reg_metrics
  )

filter_metrics <- collect_metrics(filter_res)
filter_mae <- filter_metrics$mean[filter_metrics$.metric == "mae"]

spline_metrics <- collect_metrics(lin_reg_res)
spline_mae <- spline_metrics$mean[spline_metrics$.metric == "mae"]

# ------------------------------------------------------------------------------

spline_tune_filter_rec <-
  recipe(time_to_delivery ~ ., data = delivery_train) %>%
  step_dummy(all_factor_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_spline_natural(hour, distance, deg_free = 10) %>%
  step_interact(~starts_with("hour"):starts_with("day")) %>% 
  step_bin2factor(starts_with("item")) %>% 
  step_select_aov(starts_with("item"), outcome = "time_to_delivery", 
                  top_p = tune())

filter_tune_wflow <-
  workflow() %>%
    add_model(linear_reg()) %>%
    add_recipe(spline_tune_filter_rec)

set.seed(3838)
filter_tune_res <-
  tune_grid(
    filter_tune_wflow,
    delivery_rs,
    control = ctrl_rs,
    grid = tibble(top_p = 1:27),
    metrics = reg_metrics
  )

# set.seed(382)
# filter_tune_ci <- 
#   filter_tune_res %>% 
#   int_pctl(times = 5000, alpha = 0.10)

filter_tune_mae <- 
  filter_tune_res %>% 
  collect_metrics() %>% 
  filter(.metric == "mae") %>% 
  mutate(txt = map_chr(mean, dec_to_time))

filter_tune_mae_min <- 
  filter_tune_mae %>% 
  slice_min(mean) %>% 
  pluck("txt")

filter_tune_mae_max <- 
  filter_tune_mae %>% 
  slice_max(mean) %>% 
  pluck("txt")
```



```{r}
#| label: fig-tune-filter
#| echo: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 3.25
#| fig-cap: The effect of a simple t-test filter from the delivery item predictors on performance.  

autoplot(filter_tune_res, metric = "mae") +
  labs(y = "MAE (in minutes)")

# filter_tune_ci %>% 
#   filter(.metric == "mae") %>% 
#   ggplot(aes(top_p)) + 
#   geom_point(aes(y = .estimate)) + 
#   # geom_line(aes(y = .estimate), alpha = 1 / 2) +
#   geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 1 /2, alpha = 1 / 2) + 
#   labs(y = "MAE (in minutes)", x = "# Selected Predictors")
```

The result is that, based on performance, it is best to keep almost all of the delivery items in the model. As we start to remove them, performance values decrease, although degradation is negligible (MAE ranges from `r filter_tune_mae_min` to `r filter_tune_mae_max`). The takeaway from this analysis is that these predictors matter statistically but not practically. 

This is counter to the data analysis shown in @fig-delivery-increases, where many of the item predictors were found to be unlikely to affect delivery times (again, statistically).  The difference is key: @fig-delivery-increases shows a set of univariate analyses while the results shown in @fig-tune-filter evaluate the item predictors in a linear model that contains many other predictors (including others for delivery items). 

This surfaces an important idea that we’ll get different results when filters (and feature selection in general) are applied in different ways. 


## Subset Size as a Tuning Parameter {#sec-tuning-subset-size}

An approach somewhere between filters and wrappers can be used. In this approach, a measure of importance for each predictor is computed, and a tuning parameter is added for how these are to be retained. This is like RFE, but we optimize all the tuning parameters simultaneously and evaluate subsets in a potentially less greedy way. 

There are two potential downsides, though. First, there is still the potential for enormous computational costs, especially if the ranking process is expensive. For example, if we use a grid search, there is significant potential for the same subset size to occur more than once in the grid^[The same issue can also occur with iterative search]. In this case, we are repeating the same importance calculation multiple times. 

The second potential problem is that we will use the same data set to determine optimal parameters and how many features to retain. This could very easily lead to overfitting and/or optimization bias, especially if our training set is not large. 

```{r}
#| label: filter-tune
#| include: false
#| cache: true

n_train <- 10^3
n_test <- 10^5
num_extra <- 20

# ------------------------------------------------------------------------------

set.seed(212)
sim_tr <- sim_regression(n_train, method = "hooker_2004")
sim_te <- sim_regression(n_test,  method = "hooker_2004")

set.seed(348)
sim_tr <- sim_tr %>% bind_cols(sim_noise(n_train, num_extra))
sim_te <- sim_te %>% bind_cols(sim_noise(n_test,  num_extra))

num_pred <- ncol(sim_tr) - 1

set.seed(260)
sim_rs <- vfold_cv(sim_tr)

# ------------------------------------------------------------------------------

sim_rec <-
  recipe(outcome ~ ., data = sim_tr) %>%
  step_select_forests(all_predictors(), outcome = "outcome", top_p = tune()) %>%
  step_normalize(all_predictors())

mlp_spec <-
  mlp(hidden_units = tune(), penalty = tune(), epochs = 250,
      activation = tune(), learn_rate = tune()) %>%
  set_engine("brulee", stop_iter = 5) %>%
  set_mode("regression")

mlp_wflow <- workflow(sim_rec, mlp_spec)

mlp_param <-
  mlp_wflow %>%
  extract_parameter_set_dials() %>%
  update(
    learn_rate = learn_rate(c(-3, - 1/ 2)),
    top_p = top_p(c(1, num_pred)),
    hidden_units = hidden_units(c(2, 20)),
    activation = activation(c("relu", "tanh", "log_sigmoid")),
  )

# ------------------------------------------------------------------------------

mlp_rf_sfd_res <-
  mlp_wflow %>%
  tune_grid(
    resamples = sim_rs,
    param_info = mlp_param,
    grid = 25,
    metrics = metric_set(rmse),
    control = control_grid(save_workflow = TRUE)
  )

mlp_rf_fit <- fit_best(mlp_rf_sfd_res, , metric = "rmse")

mlp_rf_mtr <- 
  mlp_rf_fit %>% 
  augment(sim_te) %>% 
  rmse(outcome, .pred)
```

add extract, 

```{r}
#| label: fig-mlp-rf
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: Joint tuning of model and wrapper parameters.  

autoplot(mlp_rf_sfd_res, metric = "rmse") + ylim(c(.25, .72))
```

desirability functions; penalize by subset size; blend multiple criteria

`r round(mlp_rf_mtr$.estimate, 3)`


## Chapter References {.unnumbered}
