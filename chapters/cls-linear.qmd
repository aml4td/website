---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-linear/"
---

# Generalized Linear and Additive Classifiers {#sec-cls-linear}

```{r}
#| label: cls-linear-setup
#| include: false

source("../R/_common.R")
source("../R/_themes.R")
source("../R/_themes_ggplot.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(embed)
library(bestNormalize)
library(probably)
library(patchwork)
library(future.mirai)
library(gt)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_bw())
set_options()
plan(mirai_multisession)

# ------------------------------------------------------------------------------
# load data 

load("../RData/forested_data.RData")
load("../RData/logistic_bayes.RData")
```

## Exploring Forestation Data  {#sec-forestation-eda}


```{r}
#| label: county
#| include: false

obs_rates <- 
  forested_train |> 
  summarize(
    rate = mean(class == "Yes"),
    events = sum(class == "Yes"),
    total = length(class),
    .by = c(county)
  ) |> 
  mutate(
    hstat = map2(events, total, binom.test, conf.level = 0.9),
    pct = rate * 100,
    .lower = map_dbl(hstat, ~ .x$conf.int[1]),
    .upper = map_dbl(hstat, ~ .x$conf.int[2]),
    county = tools::toTitleCase(gsub("_", " ", county)),
    county = factor(county),
    county = reorder(county, rate),
    `# Locations` = total
  )

# obs_rates |> 
#   ggplot(aes(y = county, col = `# Locations`)) + 
#   geom_point(aes(x = rate)) + 
#   geom_errorbar(aes(xmin = .lower, xmax = .upper)) + 
#   labs(x = "Percent Forested", y = NULL)
```

## Logistic Regression {#sec-logistic-reg}

Logistic regression is a classification model that can be used when there are $C = 2$ classes. As with all binary classification models, we want to accurately estimate the probability of an event^[We'll assume that class $j = 1$ is the class that is considered the event, for $j = 1, \ldots, C$ classes.]. As discussed in the previous chapter, the probability parameter $\pi$ should be between zero and one, and we often frame the problem via the Bernoulli distribution (i.e., $y \sim Bernoulli(\pi)$). The likelihood function is:  

$$
\ell(\pi_i; y_{ij}) = \prod_{i=1}^{n_{tr}} \pi_i^{y_{i1}} (1-\pi_i)^{y_{i2}}
$$ {#eq-bernoulli}

where $\pi_i$ is the theoretical probability of an event for sample $i$ ($i = 1, \ldots, n_{tr}$) and $y_{ic}$ is the binary indicator for class $c$. 

Generally, we have a tabular data set of predictors that we want to affect $\pi_i$ in some effective and understandable way. One pattern that we've seen in previous chapters is a linear combination of parameters for the $p$ predictors: 

$$
\boldsymbol{x}_i'\boldsymbol{\beta} = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}
$$ {#eq-linear-predictor}

This is often referred to as the "linear predictor" and is commonly denoted as $\eta_i$ The range of this value is [-$\infty$, $\infty$], so we can't directly model the probability of the event using this equation as it would easily exceed the natural boundaries for $\pi_i$. One approach is to embed the linear predictor inside a nonlinear function. The logistic model takes the form:

$$
\pi_i = \frac{1}{1 + \exp(-\boldsymbol{x}_i'\boldsymbol{\beta})}
$$ {#eq-logistic}

This keeps the probability values within [0, 1] no matter how large or small the linear predictor becomes. This nonlinear function can be derived using differential equations for growth curves. Although the model has been rediscovered several times, it was first derived in the 1800s and was not commonly known as the "logistic" model until 1925. See @cramer2004early for a history. 

```{r}
#| label: precipitation-fit
#| include: false

linear_percip_fit <- 
  logistic_reg() %>% 
  fit(class ~ log10(precip_annual), data = forested_train)

linear_percip_coef <- 
  linear_percip_fit %>% 
  extract_fit_engine() %>% 
  coef() 
# glm models the probability of the second factor level
linear_percip_coef <- -linear_percip_coef
linear_percip_coef <- format(linear_percip_coef, digits = 3)
```

For a single predictor, $x$, the logistic model results in a symmetric, sigmoidal function across values of $x$. To demonstrate, a simple logistic regression model was fit to the binary outcome data with a single predictor: the annual precipitation. That predictor has a fairly right-skewed distribution, so a log (base 10) transformation was used on the data prior to training the model. 

There are various ways to find parameter estimates, which will be discussed in subsequent subsections. The most basic method is maximum likelihood estimation, where we use a numerical optimization method to minimize the loss function, which is the negative log-likelihood. The log-likelihood is more numerically stable to work with, and it is generally not challenging to find maximum likelihood estimates (MLEs) of our $\boldsymbol{\beta}$ parameters. Statistically, the MLEs are estimates that best explain the data. They are also unbiased estimates of the true parameters^[Recall from @sec-variance-bias that unbiased estimates might have high variance. For the forestation data, we'll see that some of our logistic regression parameter estimates have irrationally large variances. Later in this chapter, biased methods (via regularization) are also discussed.].

We solved the log-likelihood equations (based on @eq-bernoulli) to estimate the parameters, producing the linear predictor function:  

$$
\boldsymbol{x}_i'\boldsymbol{\beta} = `r linear_percip_coef[1]` + `r linear_percip_coef[2]` \log_{10}(precipitation_i)
$$

@fig-precipitation shows this pattern using the annual precipitation predictor in the forestation data in green. The curve shows a smooth function that increases in $x$ and has an asymptote of 1.0. 

Since the outcome data have only two qualitative values, it is difficult to visualize how well the curve fits the data. However, our training set for this example is fairly large, and we can show the observed event rate over the range of precipitation by binning the data into 50 groups. The breakpoints for the bins are quantiles of the log-10 training set predictor values. @fig-precipitation has an option to overlay these data with the model fit. 

::: {#fig-precipitation}

::: {.figure-content}

```{shinylive-r}
#| label: fig-precipitation
#| out-width: "80%"
#| viewerHeight: 600
#| standalone: true

library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(parsnip)
library(mgcv)
library(splines2)
library(scales)

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

theme_light_bl<- function(...) {
  
  ret <- ggplot2::theme_bw(...)
  
  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect
  
  ret$legend.position <- "top"
  
  ret
}

# ------------------------------------------------------------------------------

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-1, 5, 5, -1)),
    column(
      width = 4,
      checkboxGroupInput(
        inputId = "include",
        label = "Include",
        choices = list("Binned Data" = "Data", "Untransformed" = "fit_linear", 
                       "Splines" = "fit_spline", GAM = "fit_gam"),
        selected = c("fit_linear", "Data"),
        inline = TRUE
      )
    ),
    column(
      width = 4,
      radioButtons(
        inputId = "yaxis",
        label = "y-axis",
        choices = c("Event Rate" = "rate", "Logit" = "logit"),
        inline = TRUE
      )
    )
  ),
  as_fill_carrier(plotOutput("plot"))
)


server <- function(input, output) {
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/forested_data.RData"))
  
  percip <- 
    forested_train %>% 
    mutate(precip_annual = log10(precip_annual))
  
  percip_bin <- 
    percip %>% 
    dplyr::select(class, precip_annual) %>% 
    mutate(percip_bin = ntile(precip_annual, 50)) %>% 
    dplyr::summarize(
      rate = mean(class == "Yes"),
      percip = median(precip_annual),
      n = length(precip_annual),
      .by = c(percip_bin)
    ) %>% 
    mutate(logit = binomial()$linkfun(rate))
  
  percip_rng <- extendrange(percip_bin$percip)
  # percip_rng[1] <- 0.0
  
  percip_grid <- tibble(precip_annual = seq(percip_rng[1], percip_rng[2], length.out = 100))
  
  linear_fit <- 
    logistic_reg() %>% 
    fit(class ~ precip_annual, data = percip)
  
  linear_pred <- 
    augment(linear_fit, new_data = percip_grid) %>% 
    mutate(Model = "Linear Term", group = "fit_linear")
  
  num_spline <- 9
  spline_fit <- 
    logistic_reg() %>% 
    fit(class ~ naturalSpline(precip_annual, df = num_spline), data = percip)
  
  spline_lab <- paste0("Natural Splines (", num_spline, " df)")
  
  spline_pred <- 
    augment(spline_fit, new_data = percip_grid) %>% 
    mutate(Model = spline_lab, group = "fit_spline")
  
  gam_fit <- 
    gen_additive_mod() %>% 
    set_mode("classification") %>% 
    fit(class ~ s(precip_annual), data = percip)
  gam_lab <- paste0("GAM (", round(sum(gam_fit$fit$edf[-1]), 1), " df)")
  
  gam_pred <- 
    augment(gam_fit, new_data = percip_grid) %>% 
    mutate(Model = gam_lab, group = "fit_gam")
  
  predictions <- 
    bind_rows(linear_pred, spline_pred, gam_pred) %>% 
    mutate(Model = factor(Model, levels = c("Linear Term", spline_lab, gam_lab))) %>% 
    mutate(logit = binomial()$linkfun(.pred_Yes))
  
  output$plot <-
    renderPlot({
      
      if (input$yaxis == "rate") {
        p <- 
          percip_bin %>% 
          ggplot(aes(percip)) + 
          labs(x = "Log10 Annual Percipitation", y = "Probability of Forestation") + 
          lims(y = 0:1)
      } else {
        p <- 
          percip_bin %>% 
          ggplot(aes(percip)) + 
          labs(x = "Log10 Annual Percipitation", y = "Logit")
      }
      
      
      if (any(input$include == "Data")) {
        if (input$yaxis == "rate") {
          p <- p +  geom_point(aes(y = rate), alpha = 1 / 3, cex = 3) 
        } else {
          p <- p +  geom_point(aes(y = logit), alpha = 1 / 3, cex = 3) 
        }
      }
      
      if (any(grepl("fit_", input$include))) {
        curve_data <- dplyr::filter(predictions, group %in% input$include)
        
        if (input$yaxis == "rate") {
          p <- p +
            geom_line(data = curve_data, 
                      aes(x = precip_annual, y = .pred_Yes, color = Model),
                      linewidth = 1)
        } else {
          p <- p +
            geom_line(data = curve_data, 
                      aes(x = precip_annual, y = logit, color = Model),
                      linewidth = 1) 
        }
        
        p <- p + 
          theme(legend.position = "top") + 
          scale_color_brewer(drop = FALSE, palette = "Dark2")
      }
      
      p <- p + theme_light_bl()
      print(p)
    }, res = 100)
}

app <- shinyApp(ui = ui, server = server)
```

:::

An example of a logistic regression model with a single predictor (annual precipitation, in log10 units)

:::

Overlaying the estimated event rates shows that the model does a poor job of representing the relationship between the probability of forestation and precipitation. The data trend is not monotonically increasing; in the middle of the precipitation range, there is a small drop in the binned rates that recovers and begins to increase once more. Our simple linear predictor is not flexible enough to emulate this motif.

### Generalized Linear Models {.unnumbered}

Logistic regression belongs to a broader class of _generalized linear models_, or GLMs [@glm]. These models can be used with different types of outcomes, such as numeric outcomes or count data. The idea is to have a model a function of the outcome data using the linear predictor. In our current case, the _logit_ function (the inverse of @eq-logistic) is applicable: 

$$
log\left(\frac{\pi_i}{1 - \pi_i}\right) = \eta_i = \boldsymbol{x}_i'\boldsymbol{\beta}
$$ {#eq-logit}

This nonlinear function is called the *link function*, and most models have multiple options. Technically, logistic regression uses the logit link, but there are many other functions to choose from. The probit and complementary log-log transformations can be used [@glm] as well as many others. See @morgan2018analysis for more specialized "link functions." From a broader view, we might refer to our model as "binary regression" or "binomial regression." 

Looking back to @fig-precipitation, we can change the y-axis data and show how the logit changes for our fitted model. The line visualizes the estimated linear predictor for these data. Sadly, when the representation of our data is similarly changed, the pattern is not linear; our linear model isn't effectively emulating the nonlinear pattern in the data. 

There are significant benefits to framing logistic regression inside the class of generalized linear models. First, it gives us some nice theoretical guarantees about the likelihood function, allowing us to use simple gradient-based optimization methods to optimize the log-likelihood. Second, it provides an inferential framework for our parameters. This enables us to understand how important our predictors are by way of hypothesis tests and confidence intervals. 

It's easy to take these benefits for granted. Suppose we were to find parameter estimates using some other log function, such as the Brier score or the area under the ROC curve. In that case, it might be difficult to solve the required equations, let alone derive the methods to make inferences regarding parameters. 

Before proceeding further, let's talk about the linear predictor. The "linear" in "linear predictor" and "generalized linear model" means that the model is _linear in the parameters_ $\boldsymbol{\beta}$. This does not mean that the model can only produce linear class boundaries. Our model terms $x_{ij}$ can represent any function of one or more parameters. We can use the feature engineering tools shown in Chapters [-@sec-numeric-predictors] through [-@sec-interactions-nonlinear] to create a better model. 

For example, in @fig-precipitation, we saw that using the log precipitation values as our single predictor $x$ didn't work well. When plotted on the logit scale, there is a clear nonlinear function in the binned rates. We know how to estimate a general nonlinear function of a parameter: spline functions. @fig-precipitation can also show the results when nine natural spline features are used in the model. Visually, we can see that this expansion is far more effective at predicting the outcome. We can also quantify this using several metrics mentioned in the previous chapter (e.g., Brier score, cross-entropy, etc).  

Logistic regression models can be drastically improved by including better representations of our predictors. Since it is an interpretable and stable model, we might be motivated to spend more time developing this model via exploratory data analysis and feature engineering. In many teaching materials and websites, we see analyses that quickly label the results of a logistic model as ineffective (due to the use of simple model terms) before moving on to more black-box machine-learning methods. The virtue of learning about your data and how the predictors related to the outcome can not only improve the logistic model but enable a more complete understanding and description of _why_ it works. 

```{r}
#| label: forest-logistic-elements
#| include: false

bare_rec <- 
  recipe(class ~ ., data = forested_train) %>% 
  step_dummy(county) %>% 
  step_zv(all_predictors())

transformed_rec <- 
  recipe(class ~ ., data = forested_train) %>% 
  step_orderNorm(all_numeric_predictors()) %>% 
  step_dummy(county) %>% 
  step_zv(all_predictors())

forest_rec <- 
  recipe(class ~ ., data = forested_train) %>% 
  step_orderNorm(all_numeric_predictors()) %>% 
  step_lencode_mixed(county, outcome = "class")

forest_int_rec <- 
  forest_rec %>% 
  step_interact(
    ~ dew_temp:elevation + longitude:vapor_min + 
      longitude:vapor_max + dew_temp:temp_january_min + 
      dew_temp:vapor_max + latitude:longitude + 
      longitude:roughness + temp_annual_max:dew_temp + 
      temp_annual_min:dew_temp + 
      temp_annual_max:vapor_max
  )

forest_spline_rec <- 
  forest_int_rec %>% 
  step_spline_natural(
    all_numeric_predictors(), -county, -eastness, -northness, -year, -contains("_x_"),
    deg_free = 10) %>% 
  step_lincomb(all_predictors())

forest_set <- 
  workflow_set(
    preproc = list(simple = bare_rec, "encoded + transformations" = transformed_rec, 
                   encoded = forest_rec, "encoded + interactions" = forest_int_rec, 
                   "encoded + transformations + splines" = forest_spline_rec),
    model = list(logistic_mle = logistic_reg())
  )

cls_mtr <- metric_set(brier_class, roc_auc, pr_auc, mn_log_loss)
```


```{r}
#| label: forest-logistic-run
#| include: false
#| cache: true
forest_res <-
  forest_set %>% 
  workflow_map(
    fn = "fit_resamples",
    resamples = forested_rs,
    control = control_resamples(save_pred = TRUE),
    metrics = cls_mtr, 
    seed = 693,
    verbose = FALSE
  )

set.seed(365)
brier_perm <- 
  forest_res %>% 
  extract_workflow_set_result("simple_logistic_mle") %>% 
  collect_predictions(summarize = TRUE) %>% 
  permutations(permute = "class", times = 50) %>% 
  mutate(
    data = map(splits, analysis),
    brier = map_dbl(data, ~ brier_class(.x, class, .pred_Yes)$.estimate)
  ) %>% 
  summarize(
    permutation = mean(brier),
    n = length(brier),
    std_err = sd(brier) / sqrt(n)
  ) %>% 
  mutate(
    lower = permutation - qnorm(.95) * std_err,
    upper = permutation + qnorm(.95) * std_err
  )
```

### Forestation Model Development {.unnumbered}

We can assess each of these feature engineering steps by fitting logistic regression pipelines that sequentially add steps: 

- A **simple** preprocessor that only converts the `r length(levels(forested_train$county))` counties to binary indicators and removes potential zero-variance predictors. 
- We can add a **normalization** step that transforms the numeric predictors using the ORD transformation. 
- We'll replace the county  code dummy variable operation with an effect **encoding** step (from the previous pipeline).
- We can add our potential **interactions** to our preprocessor operations. 
- Finally, we can supplement our interaction pipeline with ten **spline** terms for the previously listed predictor set. 

For each of these preprocessing/model configurations, we’ll resample the training set and estimate several performance metrics (but will focus on the Brier score). 

We can see the resampling results in @fig-forest-logistic. It shows that most steps incrementally reduce the Brier score. One of the largest drops occurs when we avoid making many county indicators and use an effect encoding instead. The normalization step is estimated to help, but the error bars indicate that this improvement might be within the statistical noise of resampling. The end result is a 1/3 reduction in the Brier score by adding better representations of our predictors. 

Note about linear dependencies and widly large coefficients

```{r}
#| label: fig-forest-logistic
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 3.25
#| fig-cap: Cross-validated Brier scores for several stages of feature engineering of the forestation data using basic logistic regression. The red dashed line represents the permutation estimate of the no-information rate. The error bars are 90% confidence intervals based on the naive standard error estimates produced during resampling. 

forest_res %>% 
  rank_results(rank_metric = "brier_class") %>% 
  filter(.metric == "brier_class") %>% 
  mutate(
    pipeline = gsub("_logistic_mle", "", wflow_id), 
    pipeline = factor(pipeline),
    pipeline = reorder(pipeline, -mean),
    ci_pm = qnorm(0.95) * std_err
  ) %>% 
  ggplot(aes(mean, pipeline)) + 
  geom_vline(xintercept = brier_perm$permutation, col = "red", lty = 2) +
  geom_vline(xintercept = 0, col = "green", lty = 2) +
  geom_point() +
  geom_errorbar(aes(xmin = mean - ci_pm, xmax = mean + ci_pm), width = 1 / 3) +
  labs(x = "Brier Score", y = NULL)
```

```{r}
#| label: logistic-spline-mtr
#| include: false
logistic_mtr <- 
  forest_res %>% 
  extract_workflow_set_result("encoded + transformations + splines_logistic_mle") %>% 
  collect_metrics(summarize = TRUE)
```

Along with the Brier score values of `r signif(logistic_mtr$mean[logistic_mtr$.metric == "brier_class"], 3)`, the spline pipeline had the following resampling estimates: ROC AUC = `r signif(logistic_mtr$mean[logistic_mtr$.metric == "roc_auc"], 3)`, a PR AUC = `r signif(logistic_mtr$mean[logistic_mtr$.metric == "pr_auc"], 3)`, and a cross-entropy = `r signif(logistic_mtr$mean[logistic_mtr$.metric == "mn_log_loss"], 3)`. During resampling, there were `r logistic_mtr$n[1]` resamples, leading to `r logistic_mtr$n[1]` ROC curves, and so on. We can pool the `r logistic_mtr$n[1]` sets of assessment set predictions and produce approximate ROC and calibration curves. These are shown in @fig-forest-logistic-diag. The calibration curve indicates that the model has trouble reliably predicting samples whose true probabilities are less than one-half. The ROC curve is unremarkable. 

```{r}
#| label: fig-forest-logistic-diag
#| echo: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 4
#| fig-cap: Calibration and ROC curves for the pooled assessment sets for the logistic regression model and the full preprocessing pipeline (e.g. up to spline terms). 

logistic_pred <- 
  forest_res %>% 
  extract_workflow_set_result("encoded + transformations + splines_logistic_mle") %>% 
  collect_predictions(summarize = TRUE)

logitic_cal <- cal_plot_windowed(logistic_pred, class, .pred_Yes, step_size = 0.02)
logitic_roc <- 
  logistic_pred %>% 
  roc_curve(class, .pred_Yes) %>% 
  ggplot(aes(1 - specificity, sensitivity)) + 
  geom_abline(col = "red", lty = 2) +
  geom_step(direction = "vh") + 
  coord_obs_pred()

logitic_cal + logitic_roc
```

From here, we could investigate which terms had the greatest influence on the model. Were the 10 interactions that we identified important or statistically significant? Should we have added more or less? We might plot the predicted values to see if there are specific geographic locations that corresponded to the poorly calibrated values. Perhaps we could also refine the model by using a supervised feature filter, and so on. 

Instead, we’ll move on to a different way to estimate the logistic model’s unknown parameters using different objective functions that use the likelihood but in different ways. 

### Interpretation {.unnumbered}

```{r}
#| label: odds-calc
#| include: false
ref_level <- "yakima"
ref_cnty <- tools::toTitleCase(gsub("_", " ", ref_level))
other_level <- "skamania"
other_term <- paste0("county", other_level)
other_cnty <- tools::toTitleCase(gsub("_", " ", other_level))

releveled <- forested_train |> 
  mutate(county = relevel(county, ref = ref_level))

county_coef <- 
  logistic_reg() |> 
  fit(class ~ county, data = releveled) |> 
  tidy(conf.int = TRUE, conf.level = 0.90)

ref_coef <- 
  county_coef |> 
  filter(term %in% c("(Intercept)")) |> 
  mutate(
    coef = format(-estimate, digits = 3, scientific = FALSE),
    odds = format(exp(-estimate), digits = 3, scientific = FALSE),
    prob = binomial()$linkinv(-estimate),
    xin = format(1/prob, digits = 2, scientific = FALSE),
    prob = format(prob * 100, digits = 3)
    )

other_coef <- 
  county_coef |> 
  filter(term %in% c(other_term)) |> 
  mutate(
    coef = format(-estimate, digits = 3, scientific = FALSE),
    .upper = format(exp(-conf.low), digits = 3, scientific = FALSE),
    .lower= format(exp(-conf.high), digits = 3, scientific = FALSE),
    odds = format(exp(-estimate), digits = 3, scientific = FALSE),
    prob = binomial()$linkinv(-estimate),
    xin = format(1/prob, digits = 2, scientific = FALSE),
    prob = format(prob * 100, digits = 3)
  )

other_change_coef <- 
  county_coef |> 
  filter(term %in% c("(Intercept)", other_term)) |> 
  summarize(estimate = sum(estimate)) |> 
  mutate(
    coef = format(-estimate, digits = 3, scientific = FALSE),
    odds = format(exp(-estimate), digits = 3, scientific = FALSE),
    prob = binomial()$linkinv(-estimate),
    xin = format(1/prob, digits = 2, scientific = FALSE),
    prob = format(prob * 100, digits = 3)
  )
```

The logit function has a meaningful interpretation since it contains the _odds_ of an event occurring. Let's consider a simple case where we are modeling the change by county. If we use `r ref_cnty` county as the reference cell (recall @sec-indicators), the intercept of a logistic model is `r ref_coef$coef`. This corresponds to a probability of `r ref_coef$prob`%. The odds of some event that occurs with probability $\pi$ is 

$$
odds = \pi / (1-\pi)
$$ 

or "one in $1/\pi$." For `r ref_cnty`, this relates to the logistic regression model since

$$
log\left(\frac{Pr[\text{`r ref_cnty` forested}]}{Pr[\text{`r ref_cnty` unforested}]}\right) = log\left(\text{`r ref_cnty` odds}\right) = \beta_0
$$

Therefore, the odds of a tree being forested in this county are _exp_(`r ref_coef$coef`) = `r ref_coef$odds` or, one in about `r ref_coef$xin` locations. 

Alternatively, consider Skamania County, which contains  Gifford Pinchot National Forest. It's model coefficient is `r other_coef$coef` However, recall that with a reference cell encoding, this is the change in the logit relative to the reference value (`r ref_cnty` County). If we are interested in the probability of forestation for Skamania, we have to look at the sum of both coefficients (`r other_change_coef$coef` logit units) which means that the probability of forestation estimated to be about `r other_change_coef$prob`%.

One way of assessing how much more likely an event is to occur is the odds ratio. For example, if we wanted to compute the odds ratio of forestation between `r ref_cnty` and Skamania counties, we can use different sets of coefficients from the linear predictor. Since the forestation rate is higher in Skamania than `r ref_cnty`, the odds ratio of interest is

$$
\frac{\text{Skamania odds}}{\text{`r ref_cnty` odds}}
$$

We know that 

$$
log\left(\text{Skamania odds}\right) = log\left(\frac{Pr[\text{Skamania forested}]}{Pr[\text{Skamania unforested}]}\right) = \beta_0 +\beta_1
$$

so that 

$$
\frac{\text{Skamania odds}}{\text{`r ref_cnty` odds}} = \frac{exp(\beta_0 +\beta_1)}{exp(\beta_0)} = exp(\beta_0 + \beta_1 - \beta_0) = exp(\beta_1)
$$

From our estimated coefficient, the odds of being forested in Skamania is `r other_coef$odds` times larger than in `r ref_cnty`. Since we are using maximum likelihood estimation, we can also compute confidence intervals for this odds ratio; the 90% interval is (`r other_coef$.lower`, `r other_coef$.upper`). 

For numeric predictors, such as eastness or elevation, exponentiating the coefficient gives us a _per unit_ odds ratio. Since the predictors might be in different units, the interpretation can be more complex depending on the situation^[This might be one factor that leads some to discretize numeric predictors; they can compute odds ratios for "low" and "high" predictor ranges.].

In either case, the logistic regression model does have a structure that lends itself to natural probabilistic interpretation. See @hosmer2013applied for a broader discussion. 

### Multicollinearity

Look at coefs

Look at 2 predictor demo

correlation of features

harms interpretation and rankings

PCA/PLS/filters

what about interactions; hierarchy principle

### Regularized {#sec-logistic-penalized}

ridge
lasso
elasticnet/glmnet
adaptive lasso
Univariate-Guided
MCP
SCAD

relaxed lasso


### Bayesian Estimation {#sec-logistic-bayes} 

Now that we've seen how to find parameter estimates for logistic regression via maximum likelihood (penalized or unpenalized), let's discuss how Bayesian estimation works for logistic regression. This explanation will be very reductive; the mechanisms for serious Bayesian analysis are highly optimized and sophisticated Our example will demonstrate the main points but is insufficient for solving real problems. 

Recall that the Bayesian posterior distribution is the endpoint of a Bayesian analysis. It combines our prior belief with the observed data to make a complete probabilistic description of the parameters, given the data. For small-scale illustration, we'll use a single predictor model: 

$$
log\left(\frac{\pi_i}{1 - \pi_i}\right) = \beta_0 + \beta_1 {x}_i
$$ 

where $i=1, \ldots, 100$. We'll simulate the 100 training set data points using $\beta_0 = 1.0$ and $\beta_1 = -1.5$. Let's assume that the data are independent Bernoulli random variables. The predictor data were uniformly distributed between $\pm 1$. Using these specifications, our data set has an event rate of `r round(mean(tr_data$class== "A")*100, 1)`%. 

Instead of maximizing the likelihood, the goal is to optimize the posterior probability:

$$
Pr[\boldsymbol{\beta}| x] = \frac{Pr[\boldsymbol{\beta}]  \ell(x| \boldsymbol{\beta})}{Pr[x]}
$$

As mentioned in @sec-bayes, we are trying to estimate a distribution and not just the values of our two unknown parameters. To do so, we need two prior distributions. If we know a lot about the data, we might be able to inject some opinion into the analysis. For example, if we were extremely sure that the slope parameter $\beta_1$ should be non-negative, our prior could be a right-skewed distribution with no negative values. 

In many cases, though, we might err on the side of choosing very broad, diffuse distributions. The regression parameters generally take up values on the entire real line, so we'll choose our two priors to be independently Gaussian $N(0, 10)$. Considering that most values of these distributions will be within $\pm 3$ standard deviations, these are fairly flat and offer very limited opinions about what the parameters are likely to be. While @gelman2008weakly makes a recommendation for a weakly informative default prior, we'll discuss more complex methods later in this section. 

At this point, we can calculate the likelihood and the prior probabilities for proposed parameter values. Note that Bayes Rule above also includes $Pr[x]$. This is a normalizing constant and is not a function of the unknown parameters. It is not generally ignorable, but, in this case, the methodology used here does not require its calculation; we'll multiply the prior and likelihood to compute an un-normalized posterior probability. 

One small problem is numerical. As we multiply a collection of small numbers, the product can become so small that it is indistinguishable from absolute zero. The solution is often to compute logarithms of the probabilities and add them (maximizing the sum of the log posterior is the same as maximizing it in the natural units). This may seem trivial, but it also helps us understand the effect of the prior. The prior has two probability values in the example, and $Pr[\boldsymbol{\beta}]$ is their product. The likelihood for this example is the product of 100 probability density values. When taking the log of the posterior, we're summing 102 log probabilities, two of which are from the prior. Although these two values come from different probability scales, they contribute to less than 2% of the total sum. Suppose that our training set consisted of eight data points. Now, the prior accounts for 20% of the posterior. This shows how, all other things being equal, the effect of the prior can be limited (unless it is extraordinarily opinionated and/or narrow). 

The Metropolis-Hastings (MH) algorithm is used to demonstrate the process of estimating parameters [@Robert2004;@chib1995understanding;@van2018simple]. Again, currently, technology for estimating the posterior is more complex and effective, but this method illustrates MCMC optimization. The process will seem similar; simulated annealing is an application of MH. 

We start with initial samples from the two prior distributions, denoted as $\boldsymbol{\beta}^{(0)}$. Using these and our data, we calculate the unnormalized posterior probability. As with simulated annealing, we can make a small more within a "local neighborhood.". We'll take a simple approach and simulate proposed values, $\boldsymbol{\beta}^{(1)}$, that are within $\pm 2.0$ of our current initial values. After computing the second posterior value: 

* If the new posterior is larger than the first, accept $\boldsymbol{\beta}^{(1)}$ as our new best estimate. 
* Otherwise, we can accept $\boldsymbol{\beta}^{(1)}$ if a random uniform number is larger than the ratio of the first posterior value to the second. 

If $\boldsymbol{\beta}^{(1)}$ is close (in posterior probability) to $\boldsymbol{\beta}^{(0)}$, we have a higher probability of accepting it (as with simulated annealing). 

@tbl-mcmc shows some of the initial iterations of the MH algorithm. After the initial parameter value, another is generated to be in a nearby space. The log-posterior for this new value is smaller than the first, so we do not automatically accept it. The probability of acceptance is essentially zero though, since the difference in the log-posterior values is `r round(param$log_post[2] - param$log_post[1])`, the exponentiated value is very close to zero. The random uniform value for this decision (`r signif(param$random[2], 3)`) formally rejects it. The next proposal is again based on the initial value and is rejected. On the third try, the new parameter value has a larger log-posterior probability and is automatically accepted as the current best.  

```{r}
#| label: tbl-mcmc
#| tbl-cap: "The first ten iterations of the Metropolis-Hastings algorithm for Bayesian logistic regression."
#| echo: false

dec_lvls <- c("initial", "improvement", "acceptable", "rejected")

param <- param %>% 
  mutate(
    Decision =
      case_when(
        better ~ "improvement",
        !better & accept ~ "acceptable",
        TRUE ~ "rejected"
      ),
    Decision = ifelse(row_number() == 1, "initial", Decision),
    Decision = factor(Decision, levels = dec_lvls)
  )

param %>% 
  slice(1:11) %>% 
  mutate(random = ifelse(is.na(accept_prob), NA, random)) %>% 
  select(Iteration = iteration, Parent = parent, Intercept = beta_0, 
         Slope = beta_1, `(Log) Posterior` = log_post, `Pr[Accept]` = accept_prob,
         Random = random, Decision) %>% 
  gt() |> 
  fmt_number(columns = c(Intercept, Slope, `(Log) Posterior`), n_sigfig = 3) %>%
  fmt_number(columns = c(`(Log) Posterior`), n_sigfig = 4) %>%
  fmt_number(columns = c(`Pr[Accept]`, Random), decimals = 3) |>
  sub_missing(columns = c(`Pr[Accept]`, Parent, Random), missing_text = "---")
```

This process proceeds a large number of times. We can monitor the proposed parameter values and assess if they appear to converge to a stable parameter space. @fig-mcmc-warmup shows the progress of MH for the first `r warmup` iterations. The process starts far away from the true parameters (represented by the dotted lines), and the next 25 iterations happen to sample slope values in a narrow, negative range. There are some improved values until the slope meanders to larger values, where new proposals are consistently accepted. 

```{r}
#| label: fig-mcmc-warmup
#| echo: false
#| out-width: 50%
#| fig-width: 4.2
#| fig-height: 4.5
#| fig-align: center
#| fig-cap: The coefficient path during the warmup period for Metropolis-Hastings optimization. 
true_means <- c(1.0, -1.5)

warmup_data %>%
	ggplot(aes(beta_0, beta_1, col = Decision, pch = Decision)) +
	geom_point() +
	geom_vline(xintercept = true_means[1], lty = 3) +
	geom_hline(yintercept = true_means[2], lty = 3) +
	labs(x = expression(beta[0]), y = expression(beta[1])) +
	scale_color_manual(
		values = c(rgb(0, 0, 0, 3 / 4), rgb(0, 0, 1, 3 / 4), rgb(0, 0, 0, 1 / 4))
	) +
	scale_shape_manual(values = c(16, 17, 4)) +
  theme(legend.position = "top")
```



Around 55 iterations, the MH process has settled near the true values. For iterations six throught `r warmup`, the proposals remain in that area with `r round(100 * mean(warmup_data$Decision[55:warmup] == "rejected"), 1)`% being outright rejected, `r round(100 * mean(warmup_data$Decision[55:warmup] == "acceptable"), 1)`% are provisionally accepted, and `r round(100 * mean(warmup_data$Decision[55:warmup] == "improvement"), 1)`% are improvements. 

```{r}
#| label: post-summary
#| echo: false

post_point <- 
  posterior %>% 
  dplyr::select(-iteration) %>% 
  colMeans() %>% 
  signif(digits = 3)
post_lower <- 
  posterior %>% 
  dplyr::select(-iteration) %>% 
  map_dbl(~ quantile(.x, prob = 0.05)) %>% 
  signif(digits = 3)
post_upper <- 
  posterior %>% 
  dplyr::select(-iteration) %>% 
  map_dbl(~ quantile(.x, prob = 0.95)) %>% 
  signif(digits = 3)

mle_coef <- 
  glm(class ~ x, data = tr_data, family = binomial) %>% 
  tidy()
```

At this point, it is reasonable to believe the MH has converged to the location of the optimal values. Since we may not be able to analytically determine the true distribution of the posterior, we spend a good deal of time running new iterations. The non-rejected proposals form a sample of the posterior, and we continue to sample until there are enough data points to make precise numerical summaries. After a total of `r format(nrow(param) - warmup - 1, big.mark = ",")` additional iterations, there were `r format(sum(param$Decision[(warmup+1):nrow(param)] != "rejected"), big.mark = ",")` usable samples. @fig-logistic-posterior shows the univariate distributions of our parameters. The slope's posterior mean was `r post_point[2]`, with 90% of the samples within (`r post_lower[2]`, `r post_upper[2]`). This interval is credible and can come from a more rational probability statement about this parameter (compared to a confidence interval). 

```{r}
#| label: fig-logistic-posterior
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 3.1
#| fig-align: center
#| fig-cap: Samples of the posterior distribution (post-warm) for the two logistic regression parameters. 

true_means <- c(1.0, -1.5)

posterior %>%
  pivot_longer(cols = c(intercept, slope), names_to = "parameter", values_to = "value") %>%
  ggplot(aes(value)) +
  geom_histogram(bins = 20, col = "white") +
  facet_wrap(~ parameter, scales = "free_x") +
  labs(x = "Parameter Values")
```

The previous section showed how penalizing the likelihood can mitigate issues with our model related to multicollinearity or irrelevant predictions. Penalization produces biased estimates for those models since they are shrunken towards zero. From a Frequentist perspective, using priors in Bayesian estimation also produces biased estimates since the posterior is shrunken to the prior (to some degree). 

We mentioned that there are more specialized priors that can be used. For example, we can choose priors to encourage sparsity in our regression parameters (as we did with the lasso model). It turns out that the regularization of the lasso model is equivalent to using a Laplace distribution (a.k.a the double exponential distribution) as a prior [@tibshirani1996regression]. @fig-sparse-posteriors (left) show an example of the Laplace distribution. Compared to a similar Gaussian distribution, the Laplacian has larger probabilities near zero and in the distribution's tails.  Bayesian models using this prior are discussed by @park2008bayesian.

```{r}
#| label: fig-sparse-posteriors
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 3.1
#| fig-align: center
#| fig-cap: Examples of priors that can be used to encourage sparsity in the parameter values. The spike distribution has been scaled to be smaller for illustration. 
x_seq <- seq(-25, 25, by = 0.005)

dbl_expo <- function(x, mu = 0, scale = 1) {
  1 / (2 * scale) * exp(-abs(x - mu) / scale)
}

gau_lap <- 
  bind_rows(
    tibble(Parameter = x_seq, density = dbl_expo(x_seq, scale = 10), Distribution = "Laplacian"),
    tibble(Parameter = x_seq, density = dnorm(x_seq, sd = 10), Distribution = "Gaussian")
  )

p_gau_lap <-
  ggplot(gau_lap, aes(x = Parameter, y = density, col = Distribution)) +
  geom_line(linewidth = 3 / 4, alpha = 3 / 4) +
  theme(legend.position = "top") + 
  scale_color_manual(values = c("#A8BE74FF", "#BF7417FF"))

slab <- dnorm(x_seq, mean = 0, sd = 10)

# Normalize for printing
spike <- dnorm(x_seq, mean = 0, sd = 0.1)
spike <- spike / max(spike)
spike <- spike * max(slab) * 3
zero_spike <- spike > .Machine$double.eps

spike_slab <- bind_rows(
  tibble(Parameter = x_seq, density = slab, Distribution = "Slab"),
  tibble(
    Parameter = x_seq[zero_spike],
    density = spike[zero_spike],
    Distribution = "Spike"
  )
)

p_spike_slab <- 
  ggplot(spike_slab, aes(x = Parameter, y = density, col = Distribution)) +
  labs(y = NULL) +
  geom_line(linewidth = 3 / 4) +
  theme(legend.position = "top") + 
  scale_color_manual(values = c("#D64358FF", "#4F5791FF"))

p_gau_lap + p_spike_slab
```

Another idea is to use a composite distribution. The "spike and slab" prior contains three distributions [@mitchell1988bayesian;@malsiner2011comparing]. The first is a slab: a very wide distribution such as our earlier N(0, 10). This is designed to represent our (weak) belief for parameters that should have a non-sparse/non-zero value. The spike element is an incredibly tight distribution centered around zero (such as an N(0, 0.1)) and typifies parameters that should have little or no effect on the model. These two choices are shown in @fig-sparse-posteriors (right). Finally, the mixture distribution has values on [0, 1] that indicate how to blend the spike and slab elements. For example, with the values given above, the spike-slab prior could be: 

$$
Pr[\beta|\alpha] = (1 - \alpha) N(0, 10) + \alpha N(0, 0.1)
$$

What should we use for the mixture prior $Pr[\alpha]$? With a Beta distribution, we could have the prior take values of the entire range. This would include a uniform distribution if we lack an opinion on the sparsity rate or enable more informative beliefs. Alternatively, we could choose a Bernoulli distribution that produces values in $\alpha \in \{0, 1\}$ with some probability. We would choose a value for $Pr[\alpha = 1]$ or put yet another prior on that parameter. 

While these approaches do regularize the parameters to be closer to zero, remember that Bayesian models _do not_ produce a single parameter value but a distribution of values. Whereas the lasso and glmnet can estimate the parameters to be absolute zero, eliminating the feature from the prediction equation entirely. For Bayesian models, it will not occur. The priors shown in @fig-sparse-posteriors indicate that, theoretically, there is an infinitesimally small probability that the posterior will have many values at absolute zero. In the case of the spike-slab model, the posterior for these parameters is likely to be bimodal with one mode at zero. In summary, the Bayesian approach _does_ move values very close to zero as needed but will not exclude any features from the prediction equation. 

The _posterior predictive distribution_ is used when predicting new data. If we are predicting a value of $x_0$, the posterior predictive distribution takes into account our existing posterior but also accounts for the variability that we sampled the new value, i.e., $Pr[X = x_0]$ where $X$ is the random variable for our predictor. The draws from this distribution are used to estimate the class probability. We often produce a single value for a new prediction, which could be the mean (or mode) of the posterior draws. We can also produce credible intervals for our probability. This alone may be a good reason to use Bayesian estimation for these models.  

## Multinomial Regression {#sec-multinomial-reg}

## Generalized Additive Models {#sec-cls-gam}

## Linear Discriminants {#sec-lda}

## Chapter References {.unnumbered}
