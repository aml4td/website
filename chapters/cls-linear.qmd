---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-linear/"
---

# Generalized Linear and Additive Classifiers {#sec-cls-linear}

```{r}
#| label: cls-linear-setup
#| include: false

source("../R/_common.R")
source("../R/_themes_ggplot.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(embed)
library(bestNormalize)
library(probably)
library(patchwork)
library(mirai)
library(gt)
library(gtExtras)
library(ggrepel)
library(mirai)
library(car)
library(DALEXtra)
library(discrim)
library(spatialsample)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_bw())
set_options()
daemons(parallel::detectCores())

# ------------------------------------------------------------------------------
# load data

load("../RData/forested_data.RData")
load("../RData/logistic_bayes.RData")
load("../RData/forested_interactions.RData")
```

```{r}
#| label: cls-boundaries
#| include: false
data(ad_data, package = "modeldata")

ad_sub <-
  ad_data |>
  select(truth = Class, x_1 = Ab_42, x_2 = p_tau) |>
  mutate(
    x_1 = scale(x_1)[,1], x_2 = scale(x_2)[,1],
    truth = factor(ifelse(truth == "Impaired", "A", "B"))
  )

rngs <- map(ad_sub |> select(-truth), extendrange)

ad_grid <- crossing(x_1 = seq(rngs[[1]][1], rngs[[1]][2], length.out = 200),
                    x_2 = seq(rngs[[2]][1], rngs[[2]][2], length.out = 200))

# ------------------------------------------------------------------------------

high_fit <-
  discrim_flexible(num_terms = 30, prod_degree = 2, prune_method = "none") |>
  set_engine("earth", nk = 100) |>
  fit(truth ~ ., data = ad_sub)

high_pred <- augment(high_fit, ad_grid) |> mutate(group = "High Complexity")

low_fit <-
  logistic_reg() |>
  fit(truth ~ ., data = ad_sub)

low_pred <- augment(low_fit, ad_grid) |> mutate(group = "Low Complexity")

set.seed(12)
mid_fit <-
  discrim_flexible(num_terms = 4, prod_degree = 2, prune_method = "none") |>
  set_engine("earth") |>
  fit(truth ~ ., data = ad_sub)

mid_pred <- augment(mid_fit, ad_grid) |> mutate(group = "Medium Complexity")

all_pred <-
  bind_rows(low_pred, mid_pred, high_pred) |>
  rename(probability = .pred_A) |>
  mutate(group = factor(group, levels = c("Low Complexity", "Medium Complexity",  "High Complexity")))
```

We often conceptualize classification models by the type of class boundary they produce. For example, in @fig-cls-boundaries, two predictors are visualized, and the colors and shapes of the data indicated their class memberships. The class boundaries are visualized as black lines, dividing the data into two (or more) regions where the model predicted a specific class. When there are more than two predictors, visualizing this boundary becomes impossible, but we still use the idea of a line that demarcates different regions that have the same (hard) class prediction. 

The low-complexity model attempts to bisect the two classes using a simple straight line. Points to the northwestern side of the line are classified as rose-colored triangles, and those on the opposite side are green circles. The background colors in the plot represent the estimated probabilities. This model underperforms since there are a high number of triangles on the wrong side of the line. The medium complexity model divides the data using a boundary that has three straight line segments. These linear segments result in a nonlinear boundary function. It also does not do a very good job of predicting the data, but it has adapted to the data more than the simple linear boundary. The high complexity boundary uses the same underlying model as the medium complexity fit, but a tuning parameter was increased to allow the model to adapt more to the training data (perhaps a little too much). There are four distinct regions that are produced by these boundaries. Unfortunately, the model is moderately overfitting the training set to these data. The tuning parameter in question should be better tuned to balance complexity and overfitting. 


```{r}
#| label: fig-cls-boundaries
#| echo: false
#| out-width: 100%
#| fig-width: 11
#| fig-height: 5
#| dev: "ragg_png"
#| fig-cap: Three different levels of complexity for class boundaries where the data has two predictors and two classes. 

all_pred |>
  ggplot(aes(x_1, x_2)) +
  scale_fill_gradient2(
    low = "#A3AD62FF",
    mid = "white",
    high = "#DF91A3FF",
    midpoint = 1 / 2,
    limits = 0:1,
    breaks = (1:3) / 4
  ) +
  geom_point(
    data = ad_sub |> filter(truth == "B"),
    aes(col = truth, pch = truth),
    cex = 2,
    alpha = 2 / 4
  ) +
  geom_point(
    data = ad_sub |> filter(truth == "A"),
    aes(col = truth, pch = truth),
    cex = 2,
    alpha = 3 / 4
  ) +
  scale_color_manual(values = c("#798234FF", "#D46780FF")) +
  scale_shape_manual(values = c(16, 17)) +
  coord_fixed(ratio = 1) +
  geom_contour(aes(z = probability),
               breaks = 1 / 2,
               col = "black",
               linewidth = 1) +
  geom_tile(aes(fill = probability), alpha = 1 / 6) +
  facet_wrap(~ group) +
  theme(legend.position = "top") +
  labs(x = "Predictor 1", y = "Predictor 2")
```

This chapter focuses on models that, at first appearance, produce strictly linear class boundaries. Apart from feature engineering, they tend to be relatively easy, fast to train, and are more likely to be interpretable due to their simplicity. We’ll start by discussing the most used classification model: logistic regression. This model has many important aspects to explore, as well as numerous ways to estimate model parameters. An extension of this model for more than two classes, a.k.a. multinomial regression, is also described. Finally, we review a fairly antiquated classification model (discriminant analysis) that will lead to some effective generalizations in the next chapter. 

However, before diving into modeling techniques, let’s take a deep look at the Washington State forestation data originally introduced in @sec-spatial-splitting. These data will be used to demonstrate the nuances of different classifications from this chapter through @sec-cls-ensembles.

## Exploring Forestation Data  {#sec-forestation-eda}

These data have been discussed in Sections [-@sec-spatial-splitting], [-@sec-spatial-resampling], and [-@sec-unsupervised-selection]. As a refresher, locations in Washington state were surveyed, and specific criteria were applied to determine whether they were sufficiently forested. Using predictors on the climate,  terrain, and location, we want to accurately predict the probability of forestation at other sites within the state. 

As previously mentioned, this type of data exhibits spatial autocorrelation, where objects close to each other tend to have similar attributes. This is not a book specific to spatial analysis; ordinary machine learning tools will be used to analyze these data. Our analyses might be, to some degree, suboptimal for the task. However, for our data, @FIA2015 describes the sampling methodology, in which the on-site inspection locations are sampled from within a collection of 6,000-acre hexagonal regions. While spatial autocorrelation is very relevant, the large space between these points may reduce the risk of using spatially ignorant modeling methodologies. Fortunately, our data spending methodologies _are_ spatially aware, and these might further mitigate any issues caused by ordinary ML models. For example, when initially splitting (and resampling them), recall that we used a buffer to add some space between the data used to fit the model and those used for assessment (e.g., Figures [-@fig-forested-split] and [-@fig-forested-blockcv]). This can reduce the risk of ignoring the autocorrelation when estimating model parameters. 

To learn more about spatial machine learning and data analysis, @kopczewska2022spatial is a nice overview. We also recommend @nikparvar2021machine, @kanevski2009machine, and @cressie2015statistics.

```{r}
#| label: vert-forest
#| include: false
#| cache: true

vert_forest <-
  forested_train |>
  pivot_longer(
    cols = c(-class, -county),
    names_to = "Predictor",
    values_to = "value"
  ) |>
  full_join(name_key |> rename(Predictor = variable), by = "Predictor") |>
  mutate(
    text = ifelse(is.na(text), Predictor, text),
    Predictor = tools::toTitleCase(text)
  ) |>
  select(-text)

num_unique <-
  vert_forest |>
  select(-class, -county) |>
  summarize(
    num_vals = min(21, vctrs::vec_unique_count(value)),
    .by = c(Predictor)
  )

percentiles <-
  vert_forest |>
  full_join(num_unique, by = "Predictor") |>
  group_nest(Predictor, num_vals) |>
  mutate(
    pctl = map2(
      data,
      num_vals,
      ~ tibble(group = ntile(.x$value, n = .y), class = .x$class)
    ),
    mid = map_dbl(data, ~ median(.x$value)),
  ) |>
  select(-data) |>
  unnest(c(pctl)) |>
  mutate(pctl = (group - 1) / num_vals * 100) |>
  summarize(
    events = sum(class == "Yes"),
    total = length(class),
    .by = c(Predictor, pctl)
  ) |>
  mutate(
    prop_obj = map2(events, total, ~ tidy(binom.test(.x, .y, conf.level = 0.9)))
  ) |>
  select(-events) |>
  unnest(c(prop_obj))

obs_rates <-
  forested_train |>
  summarize(
    rate = mean(class == "Yes"),
    events = sum(class == "Yes"),
    total = length(class),
    .by = c(county)
  ) |>
  mutate(
    hstat = map2(events, total, binom.test, conf.level = 0.9),
    pct = rate * 100,
    .lower = map_dbl(hstat, ~ .x$conf.int[1]),
    .upper = map_dbl(hstat, ~ .x$conf.int[2]),
    county = tools::toTitleCase(gsub("_", " ", county)),
    county = factor(county),
    county = reorder(county, rate),
    `# Locations` = total
  )
```

As with any ML project, we conduct preliminary exploratory data analysis to determine whether any data characteristics might affect how we model them. @tbl-forested-numeric has statistical and visual summaries of the `r ncol(forested_train) - 2` numeric predictors using the training data. Several of the predictors exhibit pronounced skew (right or left leaning). By coercing the distributions of some predictors to be more symmetric, we might gain robustness and perhaps an incremental improvement in performance (for some models). 

Also, the annual minimum temperature, dew temperature, January minimum temperature, and maximum vapor show bimodality in the distributions. The year of inspection is also interesting; the data collection was sparse before 2011, and subsequent years contain a few hundred data points per year before beginning to drop off in 2021. These characteristics are not indicative of problems with data quality, but it can be important to know that they exist when debugging why a model is underperforming or showing odd results.

::: {#tbl-forested-numeric}

```{r}
#| label: forested-numeric
#| echo: false

vert_forest |>
  summarize(
    Minimum = min(value),
    Mean = mean(value),
    Max = max(value),
    `Std. Dev` = sd(value),
    Skewness = e1071::skewness(value),
    Distribution = list(value),
    .by = c(Predictor)
  ) |>
  arrange(Predictor) |>
  gt() |>
  fmt_number(columns = c(-Distribution), n_sigfig = 3) |>
  gt_plt_dist(
    Distribution,
    type = "histogram",
    same_limit = FALSE,
    line_color = "#222",
    fill_color = "white"
  ) |>
  cols_width(Predictor ~ pct(25))

```

Histograms and statistical summaries of the numeric predictors in the Washington State training set. 

:::

Recall that @fig-corr-plot previously described the correlation structure of these predictors. There were several clusters of predictors with strong magnitudes of correlation. This implies that there is some redundancy of information in these features. However, machine learning is often "a game of inches," where even redundant predictors can contribute incremental improvements in performance. In any case, the analyses in this chapter will be profoundly affected by this characteristic; it will be investigated in in more detail below. 

Individually, how does each of these features appear to relate to the outcome? To assess this, we binned each predictor into roughly 20 groups based on percentiles and used these groups (each containing about `r floor(mean(percentiles$total))` locations) to compute the rate of forestation and 90% confidence intervals^[Binning is used here as a visualization tool; we re-emphasize that converting numeric predictors into categorical features is problematic.]. As an exploratory tool, we can use these binned versions of the data to see potential relationships with the outcome. @fig-forest-percentiles shows the profiles. Note that there is enough data in each bin to make the confidence intervals very close to the estimated rates. 

```{r}
#| label: fig-forest-percentiles
#| echo: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 6
#| fig-cap: Binned rates of forestation over percentiles of the numeric predictors. The shaded regions are 90% confidence intervals.

percentiles |>
  ggplot(aes(pctl, estimate)) +
  geom_line() +
  geom_ribbon(
    aes(ymin = conf.low, ymax = conf.high),
    alpha = 1 / 5,
    fill = "#D85434FF"
  ) +
  facet_wrap(~Predictor, ncol = 3) +
  labs(x = "Percentile", y = "Rate of Forestation")
```

Quite a few predictors show considerable nonlinear trends, and a few are not monotonic (i.e., the sign of the slope changes over the range of values). The Eastness and Northness features, which capture the landscape orientation at the location, show flat trends. This means that these predictors are less likely to be important. However, once in a model with other predictors, the model may be able to extract some utility from them, perhaps via interaction terms. The primary takeaway from this visualization is that models that are able to express nonlinear trends will probably do better than those restricted to linear classification boundaries. 

In addition to longitude and latitude, the data contains a qualitative location-based predictor: the county in Washington. There are data on `r nrow(obs_rates)` counties. The number of locations within each county can vary with `r obs_rates$county[which.min(obs_rates$total)]` county having the least training set samples (`r min(obs_rates$total)`) and `r obs_rates$county[which.max(obs_rates$total)]` having the most (`r max(obs_rates$total)`). @fig-counties shows how the rate of forestation changes and the uncertainty in these estimates. Several counties in the training set have no forested locations. Given the number of counties and their varying frequencies of data, an effect encoding strategy might be appropriate for this predictor. 

```{r}
#| label: fig-counties
#| echo: false
#| out-width: 40%
#| fig-width: 4
#| fig-height: 6
#| fig-cap: Outcome rates for different counties in Washington State.

obs_rates |>
  ggplot(aes(y = county, col = `# Locations`)) +
  geom_point(aes(x = rate)) +
  geom_errorbar(aes(xmin = .lower, xmax = .upper)) +
  labs(x = "Rate of Forestation", y = NULL) +
  theme(legend.position = "top") +
  scale_color_viridis_c(option = "mako", begin = .2, end = .8)
```

Finally, it might be a good idea to assess potential interaction effects prior to modeling. Since almost all of our features are numeric, it can be difficult to assess interactions visually, so the H-statistics for two-way interactions were calculated using a boosted tree as the base model using the numeric features. Since there are only `r choose(15, 2)` possible interactions, the H-statistics were recomputed 25 times using different random number generators so that we can compute a mean H-statistic and its associated standard error. 
 
```{r}
#| label: fig-forested-interactions
#| echo: false
#| out-width: 90%
#| fig-width: 8
#| fig-height: 4.5
#| fig-cap: Results for the top 25 H-statistic interactions. The error bars are 90% intervals based on replicate computations.

forested_hstats_text |>
  slice_max(mean_score, n = 25) |>
  ggplot(aes(y = term)) +
  geom_point(aes(x = mean_score)) +
  geom_errorbar(
    aes(xmin = .lower, xmax = .upper),
    alpha = 1 / 2,
    width = 1 / 2
  ) +
  labs(x = "Mean H-Statistic", y = NULL)
```

The vast majority of the `r choose(15, 2)`  H-statistics are less than `r signif(quantile(forested_hstats_text$mean_score, probs = 0.95), 3)` and there are a handful of interactions that are greater than that value; we’ll take the top five interactions and use them in a logistic regression shown below.

The next section will describe a mainstay of machine learning models for two classes: logistic regression. 

## Logistic Regression {#sec-logistic-reg}

Logistic regression is a classification model that can be used when there are $C = 2$ classes. As with all binary classification models, we want to accurately estimate the probability of an event^[We'll assume that class $j = 1$ is the class that is considered the event, for $j = 1, \ldots, C$ classes.]. As discussed in the previous chapter, the probability parameter $\pi$ should be between zero and one, and we often frame the problem via the Bernoulli distribution^[We often talk about logistic regression as being driven by a **Binomial** distribution. Either distribution can be used for this model, but the Binomial is used for _grouped_ data where each row of the data set has $n_i$ "trials" and $r_i$ "successes". In ML, we are not usually in this situation and have a single outcome result per row, hence why we use the Bernoulli (where $n_i = 1$).] (i.e., $y \sim Bernoulli(\pi)$). The likelihood function is:  

$$
\ell(\pi_i; y_{ij}) = \prod_{i=1}^{n_{tr}} \pi_i^{y_{i1}} (1-\pi_i)^{y_{i2}}
$$ {#eq-bernoulli}

where $\pi_i$ is the theoretical probability of an event for sample $i$ ($i = 1, \ldots, n_{tr}$) and $y_{ic}$ is the binary indicator for class $c$. As written above, each row of the data has its own probability estimate. This is not useful since it does not attribute changes in the probabilities to any particular feature.

Generally, we have a tabular data set of predictors that we want to affect $\pi_i$ in some effective and understandable way. One pattern that we've seen in previous chapters is a linear combination of parameters for the $p$ predictors: 

$$
\boldsymbol{x}_i'\boldsymbol{\beta} = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}
$$ {#eq-linear-predictor}

This is often referred to as the "linear predictor" and is commonly denoted as $\eta_i$ The range of this value is [-$\infty$, $\infty$], so we can't directly model the probability of the event using this equation as it would easily exceed the natural boundaries for $\pi_i$. One approach is to embed the linear predictor inside a nonlinear function. The **logistic model** takes the form: 

$$
\pi_i = \frac{1}{1 + \exp(-\boldsymbol{x}_i'\boldsymbol{\beta})}
$$ {#eq-logistic}

This keeps the probability values within [0, 1] no matter how large or small the linear predictor becomes. This nonlinear function can be derived using differential equations for growth curves. Although the model has been rediscovered several times, it was first derived in the 1800s and was not commonly known as the "logistic" model until 1925. See @cramer2004early for a history. 

```{r}
#| label: longitude-fit
#| include: false

linear_long_fit <-
  logistic_reg() |>
  fit(class ~ longitude, data = forested_train)

linear_long_coef <-
  linear_long_fit |>
  extract_fit_engine() |>
  coef()
# glm models the probability of the second factor level
linear_long_coef <- -linear_long_coef
linear_long_coef <- format(linear_long_coef, digits = 3)

iter_text <- cli::format_inline("{linear_long_fit$fit$iter} iteration{?s}")
```

There are various ways to find parameter estimates, which will be discussed in subsequent subsections. The most basic method is maximum likelihood estimation, where we use a numerical optimization method to minimize the loss function, which is the negative log-likelihood:

$$
-\log \ell(\pi_i; y_{ij}) = \sum_{i=1}^{n_{tr}} \left[ y_{i1}\log(\pi_i) + (1- y_{i1}) \log(1-\pi_i)\right]
$$ {#eq-bernoulli-log-lik}

The log-likelihood is more numerically stable to work with, and it is generally not challenging to find maximum likelihood estimates (MLEs) of our $\boldsymbol{\beta}$ parameters. @glm describes an effective gradient-based method for estimating parameters. Their method, called iteratively reweighted least squares (IRLS), minimizes the sum of weighted residuals, where the weights are defined by a function of the data’s variance. This method tends to converge very quickly for logistic regression^[In fact, as we'll see shortly, their theory is not limited to logistic regression; it encompasses linear regression, Poisson regression, and other models.] and has theoretical guarantees. 

Overall, for logistic regression, maximum likelihood estimation generates a relatively stable estimator. While it can be sensitive to outliers and high influence points [@pregibon1981logistic], a limited amount of data jittering or omissions will not widely affect the parameter estimates unless the training set is fairly small. We’ll see an exception below in @sec-logistic-multicollinearity,  where specific characteristics of the _training data_ make the estimators unstable.   

Statistically, the MLEs are estimates that best explain the data. They are also unbiased estimates of the true parameters^[Recall from @sec-variance-bias that unbiased estimates might have high variance. For the forestation data, we'll see that some of our logistic regression parameter estimates have irrationally large variances. Later in this chapter, biased methods (via regularization) are also discussed.].

To illustrate the model, let’s use the training data to model the probability of forestation as a function of the location’s longitude; the logit function has just simple slope and intercept parameters. Using the entire training set, we solved the log-likelihood equations (based on @eq-bernoulli-log-lik) to estimate the parameters. The optimization converges after `r iter_text`, producing the linear predictor function:  

$$
\boldsymbol{x}_i'\boldsymbol{\beta} = `r linear_long_coef[1]`  `r linear_long_coef[2]`\: longitude_i
$$ {#eq-longitude-logistic}

@fig-longitude shows this pattern using the longitude predictor in the forestation data in green. The data points shown are the binned data from @fig-forest-percentiles; the model is actually fit on the individual locations in the training set (i.e., not using the binned version in the figure). 

Why does the rate swing up and down? Looking back at @fig-forested, we see that most of the initial western portion of the state is forested. However, Seattle is adjacent to the water of Puget Sound; this accounts for the area of nonforested locations around longitudes of about -122.3°. Further east of Seattle is mostly forested.  East of this lies a large swath of unforested land (especially in the mid to south east). This is reflected in the drop in the rates from about -120° to -118.5°. In the easternmost section of the state, there is a region of forested land to the north, accounting for the rate increase at the right-hand side of @fig-longitude.  

::: {#fig-longitude}

::: {.figure-content}

```{shinylive-r}
#| label: fig-longitude
#| out-width: "80%"
#| viewerHeight: 600
#| standalone: true

library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(parsnip)
library(mgcv)
library(splines2)
library(scales)

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg,
  fg = "#595959"
)

theme_light_bl <- function(...) {
  ret <- ggplot2::theme_bw(...)

  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background <- col_rect
  ret$plot.background <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key <- col_rect

  ret$legend.position <- "top"

  ret
}

# ------------------------------------------------------------------------------

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-1, 5, 5, -1)),
    column(
      width = 4,
      checkboxGroupInput(
        inputId = "include",
        label = "Include",
        choices = list(
          "Binned Data" = "Data",
          "Untransformed" = "fit_linear",
          "Splines" = "fit_spline",
          GAM = "fit_gam"
        ),
        selected = c("fit_linear", "Data"),
        inline = TRUE
      )
    ),
    column(
      width = 4,
      radioButtons(
        inputId = "yaxis",
        label = "y-axis",
        choices = c("Event Rate" = "rate", "Logit" = "logit"),
        inline = TRUE
      )
    )
  ),
  as_fill_carrier(plotOutput("plot"))
)


server <- function(input, output) {
  load(url(
    "https://raw.githubusercontent.com/aml4td/website/main/RData/forested_data.RData"
  ))

  lng_bin <-
    forested_train |>
    dplyr::select(class, longitude) |>
    mutate(lng_bin = ntile(longitude, 21)) |>
    dplyr::summarize(
      rate = mean(class == "Yes"),
      lng = median(longitude),
      n = length(longitude),
      .by = c(lng_bin)
    ) |>
    mutate(logit = binomial()$linkfun(rate))

  lng_rng <- extendrange(lng_bin$lng)
  # lng_rng[1] <- 0.0

  lng_grid <- tibble(longitude = seq(lng_rng[1], lng_rng[2], length.out = 100))

  linear_fit <-
    logistic_reg() |>
    fit(class ~ longitude, data = forested_train)

  linear_pred <-
    augment(linear_fit, new_data = lng_grid) |>
    mutate(Model = "Linear Term", group = "fit_linear")

  num_spline <- 10
  spline_fit <-
    logistic_reg() |>
    fit(
      class ~ naturalSpline(longitude, df = num_spline),
      data = forested_train
    )

  spline_lab <- paste0("Natural Splines (", num_spline, " df)")

  spline_pred <-
    augment(spline_fit, new_data = lng_grid) |>
    mutate(Model = spline_lab, group = "fit_spline")

  gam_fit <-
    gen_additive_mod() |>
    set_mode("classification") |>
    fit(class ~ s(longitude), data = forested_train)
  gam_lab <- paste0("GAM (", round(sum(gam_fit$fit$edf[-1]), 1), " df)")

  gam_pred <-
    augment(gam_fit, new_data = lng_grid) |>
    mutate(Model = gam_lab, group = "fit_gam")

  predictions <-
    bind_rows(linear_pred, spline_pred, gam_pred) |>
    mutate(
      Model = factor(Model, levels = c("Linear Term", spline_lab, gam_lab))
    ) |>
    mutate(logit = binomial()$linkfun(.pred_Yes))

  output$plot <-
    renderPlot(
      {
        if (input$yaxis == "rate") {
          p <-
            lng_bin |>
            ggplot(aes(lng)) +
            labs(x = "Longitude", y = "Probability of Forestation") +
            lims(y = 0:1)
        } else {
          p <-
            lng_bin |>
            ggplot(aes(lng)) +
            labs(x = "Longitude", y = "Logit")
        }

        if (any(input$include == "Data")) {
          if (input$yaxis == "rate") {
            p <- p + geom_point(aes(y = rate), alpha = 1 / 3, cex = 3)
          } else {
            p <- p + geom_point(aes(y = logit), alpha = 1 / 3, cex = 3)
          }
        }

        if (any(grepl("fit_", input$include))) {
          curve_data <- dplyr::filter(predictions, group %in% input$include)

          if (input$yaxis == "rate") {
            p <- p +
              geom_line(
                data = curve_data,
                aes(x = longitude, y = .pred_Yes, color = Model),
                linewidth = 1
              )
          } else {
            p <- p +
              geom_line(
                data = curve_data,
                aes(x = longitude, y = logit, color = Model),
                linewidth = 1
              )
          }

          p <- p +
            theme(legend.position = "top") +
            scale_color_brewer(drop = FALSE, palette = "Dark2")
        }

        p <- p + lims(y = 0:1) + theme_light_bl()
        print(p)
      },
      res = 100
    )
}

app <- shinyApp(ui = ui, server = server)
app
```

:::

An example of a logistic regression model with a single predictor (longitude). The binned data points are the same as those shown in @fig-forest-percentiles and are for visualization purposes only; the models are trained on the individual rows in the training set. 

:::

The model fit initially underestimates the rate of forestation for the most western locations, and the predicted probability slowly decreases in a fairly flat sigmoid. Given the ups and downs of the observed event rates over longitude, this model fit misses almost all of the nuance in the data.  The data trend is not monotonically decreasing, so our simple linear predictor is doing its best but is not flexible enough to emulate what is actually occurring. 

#### Generalized Linear Models {.unnumbered}

Logistic regression belongs to a broader class of _generalized linear models_, or GLMs [@glm]. These models can be used with different types of outcomes, such as real-valued outcomes or integer counts. The idea is to model a function of the outcome using the linear predictor. In our current case, the _logit_ function (the inverse of @eq-logistic) is applicable: 

$$
log\left(\frac{\pi_i}{1 - \pi_i}\right) = \eta_i = \boldsymbol{x}_i'\boldsymbol{\beta}
$$ {#eq-logit}

This nonlinear function is called the *link function*, and there are often multiple functions that could be used. Technically, logistic regression uses the logit link, but there are many other functions to choose from. The probit and complementary log-log transformations can be used [@glm] as well as many others. See @morgan2018analysis for more specialized "link functions." From a broader view, we might refer to our model as "binary regression." 

Looking back to @fig-longitude, we can change the y-axis data and show how the logit changes for our fitted model. The line visualizes the estimated linear predictor for these data. Sadly, when the representation of our data is similarly changed, the data pattern is not linear; our linear model isn't effectively emulating the nonlinear pattern in the data. 

There are significant benefits to framing logistic regression inside the class of generalized linear models. First, it gives us some nice theoretical guarantees about the likelihood function, allowing us to use simple gradient-based optimization methods to optimize the log-likelihood. Second, it provides an inferential framework for our parameters. This enables us to understand how important our predictors are by way of hypothesis tests and confidence intervals. 

It's easy to take these benefits for granted. Suppose we were to find parameter estimates using some other log function, such as the Brier score or the area under the ROC curve. In that case, it might be difficult to solve the required equations, let alone derive the methods to make inferences regarding parameters. 

::: {.important-box}
Before proceeding further, let's talk about the linear predictor. The "linear" in "linear predictor" and "generalized linear model" means that the model is _linear in the parameters_ $\boldsymbol{\beta}$. This does **not** mean that the model can only produce linear class boundaries. Our model terms $x_{ij}$ can represent any function of one or more parameters. We can use the feature engineering tools shown in Chapters [-@sec-numeric-predictors] through [-@sec-interactions-nonlinear] to create a better model. 
:::

For example, in @fig-longitude, we saw that using the longitude values as our single predictor $x$ didn't work well. When plotted on the logit scale, there is a clear nonlinear function in the binned rates. We know how to estimate a general nonlinear function of a feature: spline functions. @fig-longitude can also show the results when ten natural spline features are used in the model. Visually, we can see that this expansion is far more effective at predicting the outcome.

Logistic regression models can be drastically improved by including better representations of our predictors. Since it is an interpretable and stable model, we might be motivated to spend more time developing this model via exploratory data analysis and feature engineering. In many teaching materials and websites, we see analyses that quickly label the results of a logistic model as ineffective (due to the use of simple model terms) before moving on to more black-box machine-learning methods. The virtue of learning about your data and how the predictors related to the outcome can not only improve the logistic model but enable a more complete understanding and description of _why_ it works. 

```{r}
#| label: forest-logistic-elements
#| include: false


bare_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_dummy(county) |>
  step_zv(all_predictors())

transformed_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_dummy(county) |>
  step_zv(all_predictors())

forest_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_lencode_mixed(county, outcome = "class")

forest_int_rec <-
  forest_rec |>
  step_interact(!!forested_int_form)

forest_spline_rec <-
  forest_int_rec |>
  step_spline_natural(
    all_numeric_predictors(),
    -county,
    -eastness,
    -northness,
    -year,
    -contains("_x_"),
    deg_free = 10
  ) |>
  step_lincomb(all_predictors())

forest_set <-
  workflow_set(
    preproc = list(
      simple = bare_rec,
      "encoded + transformations" = transformed_rec,
      encoded = forest_rec,
      "encoded + interactions" = forest_int_rec,
      "encoded + transformations + splines" = forest_spline_rec
    ),
    model = list(logistic_mle = logistic_reg())
  )

cls_mtr <- metric_set(brier_class, roc_auc, pr_auc, mn_log_loss)
```


```{r}
#| label: forest-logistic-run
#| include: false
#| cache: true
forest_res <-
  forest_set |>
  workflow_map(
    fn = "fit_resamples",
    resamples = forested_rs,
    control = control_resamples(save_pred = TRUE),
    metrics = cls_mtr,
    seed = 693,
    verbose = FALSE
  )

set.seed(365)
brier_perm <-
  forest_res |>
  extract_workflow_set_result("simple_logistic_mle") |>
  collect_predictions(summarize = TRUE) |>
  permutations(permute = "class", times = 50) |>
  mutate(
    data = map(splits, analysis),
    brier = map_dbl(data, ~ brier_class(.x, class, .pred_Yes)$.estimate)
  ) |>
  summarize(
    permutation = mean(brier),
    n = length(brier),
    std_err = sd(brier) / sqrt(n)
  ) |>
  mutate(
    lower = permutation - qnorm(.95) * std_err,
    upper = permutation + qnorm(.95) * std_err
  )

glm_best <-
  forest_res |>
  rank_results(rank_metric = "brier_class") |>
  filter(.metric == "brier_class") |>
  slice_min(mean, n = 1)

set.seed(799)
glm_best_int <-
  forest_res |>
  extract_workflow_set_result(glm_best$wflow_id) |>
  int_pctl(times = 2000, metrics = cls_mtr)
```

#### Forestation Model Development {.unnumbered}

We can assess each of these feature engineering steps by fitting logistic regression pipelines that sequentially add steps. Our five models are:

 1. A **simple** preprocessor that only converts the `r length(levels(forested_train$county))` counties to binary indicators and removes potential zero-variance predictors. 
 2. We can add a **normalization** step that transforms the numeric predictors using the ORD transformation. 
 3. We'll replace the county code dummy variable operation in model #2 with an effect **encoding** step.
 4. We can add our potential **interactions** to to model #3. 
 5. Finally, we can supplement our interaction pipeline with ten **spline** terms for the previously listed predictor set. 

For each of these preprocessing/model configurations, we’ll resample the training set and estimate several performance metrics (but will focus on the Brier score). 

We can see the resampling results in @fig-forest-logistic. It shows that most steps incrementally reduce the Brier score. One of the largest drops occurs when we avoid making many county indicators and use an effect encoding instead. The normalization step is estimated to help, but the error bars indicate that this improvement might be within the statistical noise of resampling. The end result is a 1/3 reduction in the Brier score by adding better representations of our predictors^[We’ll discuss regularized models shortly, but in practice, we might _start_ with a complex model (such as Model #5) and use a penalized model to determine which model terms to keep. We’ll do exactly that in the next section.]. 

```{r}
#| label: fig-forest-logistic
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 3
#| fig-cap: Cross-validated Brier scores for several stages of feature engineering of the forestation data using basic logistic regression. The red dashed line represents the permutation estimate of the no-information rate. The error bars are 90% confidence intervals based on the naive standard error estimates produced during resampling.

forest_res |>
  rank_results(rank_metric = "brier_class") |>
  filter(.metric == "brier_class") |>
  mutate(
    pipeline = gsub("_logistic_mle", "", wflow_id),
    pipeline = factor(pipeline),
    pipeline = reorder(pipeline, -mean),
    ci_pm = qnorm(0.95) * std_err
  ) |>
  ggplot(aes(mean, pipeline)) +
  geom_vline(xintercept = brier_perm$permutation, col = "red", lty = 2) +
  geom_vline(xintercept = 0, col = "green", lty = 2) +
  geom_point() +
  geom_errorbar(aes(xmin = mean - ci_pm, xmax = mean + ci_pm), width = 1 / 3) +
  labs(x = "Brier Score", y = NULL)
```

```{r}
#| label: logistic-spline-mtr
#| include: false
logistic_mtr <-
  forest_res |>
  extract_workflow_set_result(
    "encoded + transformations + splines_logistic_mle"
  ) |>
  collect_metrics(summarize = TRUE)
```

Along with the Brier score values of `r signif(logistic_mtr$mean[logistic_mtr$.metric == "brier_class"], 3)`, the spline pipeline (i.e., Model #5) had the following resampling estimates: ROC AUC = `r signif(logistic_mtr$mean[logistic_mtr$.metric == "roc_auc"], 3)`, a PR AUC = `r signif(logistic_mtr$mean[logistic_mtr$.metric == "pr_auc"], 3)`, and a cross-entropy = `r signif(logistic_mtr$mean[logistic_mtr$.metric == "mn_log_loss"], 3)`. During resampling, there were `r logistic_mtr$n[1]` resamples, leading to `r logistic_mtr$n[1]` ROC curves, and so on. We can pool the `r logistic_mtr$n[1]` sets of assessment set predictions and produce _approximate_ ROC and calibration curves. These are shown in @fig-forest-logistic-diag. The calibration curve indicates that the model reliably predicts samples with accuracy  The ROC curve shows good separation between classes but is otherise unremarkable. 

```{r}
#| label: fig-forest-logistic-diag
#| echo: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 4
#| fig-cap: Calibration and ROC curves for the pooled assessment sets for the logistic regression model and the full preprocessing pipeline (e.g. up to spline terms). The calibration plot used a moving window, where windows that span 10% of the data are computed with moving increments of 2%.

logistic_pred <-
  forest_res |>
  extract_workflow_set_result(
    "encoded + transformations + splines_logistic_mle"
  ) |>
  augment()

logitic_cal <- cal_plot_windowed(
  logistic_pred,
  class,
  .pred_Yes,
  step_size = 0.02
)
logitic_roc <-
  logistic_pred |>
  roc_curve(class, .pred_Yes) |>
  ggplot(aes(1 - specificity, sensitivity)) +
  geom_abline(col = "red", lty = 2) +
  geom_step(direction = "vh") +
  coord_obs_pred()

logitic_cal + logitic_roc
```

From here, we should investigate which terms had the greatest influence on the model. Were the 10 interactions that we identified important or statistically significant? Should we have added more or less? We might plot the predicted values to see if there are specific geographic locations that corresponded to the poorly calibrated values. Perhaps we could also refine the model by using a supervised feature filter, and so on. 

### Examining the Model {#logistic-diagnostic}

There are many ways to appraise the model. One (unfortunate) focus is often on the statistical significance of the model terms. If the model’s purpose is solely for interpretation, this is understandable. For ML applications, p-values are not particularly insightful since they don’t necessarily correlate with predictive performance. This is especially true for our logistic model, which contains numerous spline terms; knowing whether the fourth spline term has a small p-value probably isn’t insightful. However, we’ll return to the model inferential capacity shortly as a diagnostic tool. 

Our suggestion is to assess the model’s quality by using the held-out predictions to conduct *residual analysis* by finding systematic patterns in those values. The most basic residual is the simple difference in the “observed” and predicted probabilities: $e_{ic} = y_{ic} - \widehat{p}_{ic}$. The issue with these values is that the variance of the probability estimates changes over their [0,1] range. A residual of 0.1 has a different context when $\widehat{p}_i = 0.01$ compared to $\widehat{p}_i = 0.5$.  To normalize these values, we can use _Pearson residuals_, which are $e_{ic}$  divided by the standard deviation: 

 $$
 e^p_{ic} = \frac{y_{ic} - \widehat{p}_{ic}}{\widehat{p}_{ic} (1 - \widehat{p}_{ic})}.
 $$

Alternatively, we can look at _deviance residuals_. The deviance is a likelihood-based comparison between two models: our current model and one that is _saturated_ where the model perfectly fits the training data. For logistic regression, these take the form: 

$$
e^d_{ic} = \text{sign}(e_{ic}) \left[-2(y _{ic}\, \text{log}(\widehat{p} _{ic}) + (1 - y _{ic})\, \text{log}(1 - \widehat{p}_{ic}))\right]^{1/2}
$$

See @pregibon1981logistic and  @agresti2015foundations for discussions.  

In our application, it would be relevant to plot residuals versus values of individual predictors. If we see systematic, non-random patterns, it might indicate that how we represent that predictor in the model is inadequate. For example, in @fig-longitude-resdiauls the deviance residuals are plotted against longitude. These residuals are computed with the holdout predictions for each resampling fold. 

The left-hand panel shows residuals for the model with simple linear terms for the predictors. For each class, we can see large spikes in residuals for longitude values corresponding to the previous ups and downs shown in @fig-longitude. It is interesting that this visualization shows different patterns for each class; they ar enot mirrored reflections of one another. For example, for the region between longitudes of approximately -120.5° and -118°, the unforested locations have very small residuals, and the forested locations have a spike.

```{r}
#| label: fig-longitude-resdiauls
#| fig-cap: "Deviance residuals plotted against longitude for models with and without spline terms for predictors (including longitude)."
#| out-width: "70%"
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| warning: false

logistic_resdiduals <-
  forest_res |>
  extract_workflow_set_result("encoded_logistic_mle") |>
  collect_predictions() |>
  mutate(Model = "No Splines") |>
  bind_rows(
    forest_res |>
      extract_workflow_set_result(
        "encoded + transformations + splines_logistic_mle"
      ) |>
      collect_predictions() |>
      mutate(Model = "Spline Terms")
  ) |>
  mutate(
    indicator = ifelse(class == "Yes", 1, 0),
    error = indicator - .pred_Yes,
    dev_1 = indicator * log(.pred_Yes),
    dev_2 = (1 - indicator) * log(1 - .pred_Yes),
    deviance = sign(error) * sqrt((-2 * (dev_1 + dev_2)))
  ) |>
  inner_join(forested_train |> add_rowindex() |> select(-class), by = ".row")

logistic_resdiduals |>
  ggplot(aes(longitude, deviance, col = class)) +
  geom_point(alpha = 1 / 2, cex = 1) +
  facet_wrap(~Model) +
  scale_color_manual(values = c("#218239", "#d4ad42")) +
  labs(x = "Longitude", y = "Deviance Residiual") +
  theme(legend.position = "top")
```

In contrast, the right-hand panel has many more residuals near zero and significantly attenuated spikes/patterns. This is indicative of an improvement of model fit. However, some points have absolute residuals _larger_ than the simpler model. Overall, though, the model with spline terms has fewer systematic patterns. 

Given the spatial nature of these data, mapping the residuals can be instructive. It could show us locations that tend to be poorly predicted. We can also determine the largest (absolute) residuals and investigate what might be different about these particular data points. 

Let's move on to consider how the logistic model can facilitate interpretaion. 

### Interpreting the Logistic Model {#logistic-interpretability}

```{r}
#| label: odds-calc
#| include: false
ref_level <- "yakima"
ref_cnty <- tools::toTitleCase(gsub("_", " ", ref_level))
other_level <- "skamania"
other_term <- paste0("county", other_level)
other_cnty <- tools::toTitleCase(gsub("_", " ", other_level))

releveled <- forested_train |>
  mutate(county = relevel(county, ref = ref_level))

county_coef <-
  logistic_reg() |>
  fit(class ~ county, data = releveled) |>
  tidy(conf.int = TRUE, conf.level = 0.90)

ref_coef <-
  county_coef |>
  filter(term %in% c("(Intercept)")) |>
  mutate(
    coef = format(-estimate, digits = 3, scientific = FALSE),
    odds = format(exp(-estimate), digits = 3, scientific = FALSE),
    prob = binomial()$linkinv(-estimate),
    xin = format(1 / prob, digits = 2, scientific = FALSE),
    prob = format(prob * 100, digits = 3)
  )

other_coef <-
  county_coef |>
  filter(term %in% c(other_term)) |>
  mutate(
    coef = format(-estimate, digits = 3, scientific = FALSE),
    .upper = format(exp(-conf.low), digits = 3, scientific = FALSE),
    .lower = format(exp(-conf.high), digits = 3, scientific = FALSE),
    odds = format(exp(-estimate), digits = 3, scientific = FALSE),
    prob = binomial()$linkinv(-estimate),
    xin = format(1 / prob, digits = 2, scientific = FALSE),
    prob = format(prob * 100, digits = 3)
  )

other_change_coef <-
  county_coef |>
  filter(term %in% c("(Intercept)", other_term)) |>
  summarize(estimate = sum(estimate)) |>
  mutate(
    coef = format(-estimate, digits = 3, scientific = FALSE),
    odds = format(exp(-estimate), digits = 3, scientific = FALSE),
    prob = binomial()$linkinv(-estimate),
    xin = format(1 / prob, digits = 2, scientific = FALSE),
    prob = format(prob * 100, digits = 3)
  )
```

The logit function has a meaningful interpretation since it contains the _odds_ of an event occurring. Let's consider a simple case where we are modeling the change by county. If we use `r ref_cnty` county as the reference cell (recall @sec-indicators), the intercept of a logistic model is `r ref_coef$coef`. This corresponds to a probability of `r ref_coef$prob`%. The odds of some event that occurs with probability $\pi$ is 

$$
odds = \pi / (1-\pi)
$$ 

or "one in $1/\pi$." For `r ref_cnty`, this relates to the logistic regression model since

$$
log\left(\frac{Pr[\text{`r ref_cnty` forested}]}{Pr[\text{`r ref_cnty` unforested}]}\right) = log\left(\text{`r ref_cnty` odds}\right) = \beta_0
$$

Therefore, the odds of a tree being forested in this county are _exp_(`r ref_coef$coef`) = `r ref_coef$odds` or, one in about `r ref_coef$xin` locations. 

Alternatively, consider Skamania County, which contains  Gifford Pinchot National Forest. It's model coefficient is `r other_coef$coef` However, recall that with a reference cell encoding, this is the change in the logit relative to the reference value (`r ref_cnty` County). If we are interested in the probability of forestation for Skamania, we have to look at the sum of both coefficients (`r other_change_coef$coef` logit units) which means that the probability of forestation estimated to be about `r other_change_coef$prob`%.

One way of assessing how much more likely an event is to occur is the odds ratio. For example, if we wanted to compute the odds ratio of forestation between `r ref_cnty` and Skamania counties, we can use different sets of coefficients from the linear predictor. Since the forestation rate is higher in Skamania than `r ref_cnty`, the odds ratio of interest is

$$
\frac{\text{Skamania odds}}{\text{`r ref_cnty` odds}}
$$

We know that 

$$
log\left(\text{Skamania odds}\right) = log\left(\frac{Pr[\text{Skamania forested}]}{Pr[\text{Skamania unforested}]}\right) = \beta_0 +\beta_1
$$

so that 

$$
\frac{\text{Skamania odds}}{\text{`r ref_cnty` odds}} = \frac{exp(\beta_0 +\beta_1)}{exp(\beta_0)} = exp(\beta_0 + \beta_1 - \beta_0) = exp(\beta_1)
$$

From our estimated coefficient, the odds of being forested in Skamania is `r other_coef$odds` times larger than in `r ref_cnty`. For each categorical predictor, we can use its model coefficient to compute odds ratios relative to the reference cell^[Recall that if there are multiple categorical predictors, the reference cell is multidimensional.]. Since we are using maximum likelihood estimation, we can also compute confidence intervals for this odds ratio; the 90% interval is (`r other_coef$.lower`, `r other_coef$.upper`), the lower bound being very far from a value of 1.0. 

For numeric predictors, such as eastness or elevation, exponentiating the coefficient gives us a _per unit_ odds ratio. Since the predictors might be in different units, the interpretation can be more complex depending on the situation^[This might be one factor that leads some people to discretize numeric predictors; they can compute odds ratios for "low" and "high" predictor ranges.].

In either case, the logistic regression model does have a structure that lends itself to natural probabilistic interpretation. See @hosmer2013applied for a broader discussion. 

```{r}
#| label: best-glm-fit
#| include: false

best_mle_wflow <-
  forest_set |>
  extract_workflow(id = "encoded + transformations + splines_logistic_mle")

best_mle_fit <-
  best_mle_wflow |>
  fit(forested_train)

best_mle_coef <-
  tidy(best_mle_fit) |>
  mutate(
    Type = case_when(
      grepl("(_10)|(_0[1-9])", term) ~ "Spline",
      grepl("_x_", term) ~ "Interaction",
      TRUE ~ "Main Effect"
    ),
    Type = factor(Type, levels = c("Spline", "Interaction", "Main Effect"))
  )

best_mle_vifs <-
  best_mle_fit |>
  extract_fit_engine() |>
  vif() |>
  tibble::enframe() |>
  set_names(c("term", "vif"))

best_mle_coef <- full_join(best_mle_coef, best_mle_vifs, by = "term")

best_mle_sig <-
  best_mle_coef |>
  mutate(signif = p.value <= 0.1) |>
  filter(term != "(Intercept)") |>
  summarize(
    sig_rate = round(mean(signif) * 100, 1),
    num = length(signif),
    .by = c(Type)
  )
```

```{r}
#| label: logistic-pdp
#| include: false
#| cache: true

explainer_logistic <-
  explain_tidymodels(
    best_mle_fit,
    data = forested_train |> select(-class),
    y = forested_train$class,
    label = "",
    type = "prob",
    verbose = FALSE
  )

set.seed(1805)
pdp_longitude <- model_profile(
  explainer_logistic,
  N = 1000,
  variables = "longitude"
)
```

Methods are also available to help explain a model or a specific prediction, independent of the type of model used. The field of **explainable machine learning** uses tools related to the H-statistic previously described in @sec-interactions-detection. These will be discussed in detail in @sec-explanations, but we’ll preview the technique here. 

We’ve used the longitude predictor to demonstrate potential nonlinear patterns with the outcome values. There definitely appears to be a complex pattern between longitude and the probability of forestation. However, when placed in a model with other predictors, how does longitude affect how the model predictions?

Partial dependence plots (PDP) are visualization tools to answer this question. Suppose we were to sample a single location $x_i$ from the training set (or any other set of data). We could keep all other predictors constant for that sampled data point and create a grid of possible values for the feature of interest. Using our model, we can create a _profile_ showing how the predictions change over these values. However, if we had sampled a different data point, the pattern probably would not be the same (unless the model was very simple). We can sample a large number of data points and use their profiles to compute the average effect of the predictor across its range of values. 

@fig-longitude-pdp shows a PDP for longitude from our most complex model (including interactions and splines). The 1,000 grey lines show the individual profiles, and the red curve shows the average effect of this predictor, in the presence of the other model terms.

```{r}
#| label: fig-longitude-pdp
#| fig-cap: "A partial dependency plot visualizing how longitude affects the probability when averaged over 1,000 random draws of the data. The red line shows the average trend while the grey lines show the trends for the 1,000 location samples. This is an interesting contrast to @fig-longitude."
#| out-width: "70%"
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| warning: false

ggplot_pdp <- function(obj, x) {
  p <-
    as_tibble(obj$agr_profiles) %>%
    mutate(`_label_` = stringr::str_remove(`_label_`, "^[^_]*_")) %>%
    ggplot(aes(`_x_`, 1 - `_yhat_`)) +
    geom_line(
      data = as_tibble(obj$cp_profiles),
      aes(x = {{ x }}, group = `_ids_`),
      linewidth = 0.5,
      alpha = 0.05,
      color = "gray50"
    )

  num_colors <- n_distinct(obj$agr_profiles$`_label_`)

  if (num_colors > 1) {
    p <- p + geom_line(aes(color = `_label_`), linewidth = 1.2, alpha = 0.8)
  } else {
    p <- p + geom_line(color = "darkred", linewidth = 1.2, alpha = 0.8)
  }

  p
}

ggplot_pdp(pdp_longitude, longitude) +
  labs(y = "Probability of Forestation", x = "Longitude")
```

The pattern is different from @fig-longitude. For example, there is no decreasing trend in the eastern part of the state; probabilities for locations greater than longitudes of -121° are fairly constant. The difference is due to some other predictor having additional influence on locations in that area of the state (perhaps the county). 

This model appears to be adequate, but _it is not_. There is a major issue related to our parameter estimates that, while not obviously affecting the predictive ability, reflects a flaw that could make the fit unreliable. 

### A Potential Problem: Multicollinearity {#sec-logistic-multicollinearity}

It might be interesting to explore which model terms show statistical significance as a proxy for importance. Basic logistic regression via MLE enables standard hypothesis tests. The top of @tbl-logistic-reg shows the five terms, apart from the intercept, with the smallest p-values. This includes the effect encoding for the county, which should be unsurprising since it captures the shrunken mean logit for each county. The remaining terms all correspond to spline basis expansion terms. Using an $\alpha$ value that corresponds to a 10% false positive rate, `r sum(best_mle_coef$p.value <= 0.1)` terms out of `r nrow(best_mle_coef)` are "significant" from the statistical perspective. When considering these terms, recall that the original predictors were normalized to look like they follow a standard normal distribution, meaning that most of their preditor values should be between $\pm 3$, prior to making spline and/or interaction terms. 

```{r}
#| label: tbl-logistic-reg
#| tbl-cap: "The top five most signficant model terms (top) and the five terms with the highest standard errors (bottom)."
#| echo: false

logistic_fit_tbl <-
  bind_rows(
    best_mle_coef |>
      filter(term != "(Intercept)") |>
      slice_min(p.value, n = 5) |>
      mutate(listing = "Smallest p-values"),
    best_mle_coef |>
      filter(term != "(Intercept)") |>
      slice_max(std.error, n = 5) |>
      mutate(listing = "Largest Standard Errors") |>
      arrange(desc(std.error))
  ) |>
  mutate(
    term = gsub("_0", " (spline #", term),
    term = gsub(")", " ", term),
    term = ifelse(grepl("spline", term), paste0(term, ")"), term),
    term = gsub("_10", " (spline #10)", term),
    term = gsub("precip_annual", "annual preciptation", term),
    p.value = format.pval(p.value)
  ) |>
  select(-Type, -vif)

logistic_fit_tbl |>
  group_by(listing) |>
  gt() |>
  fmt_number(
    columns = c(estimate, std.error, statistic),
    n_sigfig = 3,
    drop_trailing_zeros = FALSE
  ) |>
  cols_label(
    term = "Model Term",
    estimate = "Estimate",
    statistic = "t-statistic",
    std.error = "Std. Err.",
    p.value = "p-value"
  ) |>
  tab_style(
    style = cell_text(style = "italic"),
    locations = cells_row_groups()
  )
```

```{r}
#| label: max-spline
#| include: false
features <- 
  forest_spline_rec |> 
  prep() |> 
  bake(new_data = NULL, -class)

abs_cor <- 
  features |> 
  cor() |> 
  abs()
abs_cor <- abs_cor[upper.tri(abs_cor)]

upper_vals <- 
  features |> 
  select(contains("precip_annual")) |> 
  map_dbl(~ max(.x)) |> 
  mean() |> 
  round(3)

cor_data <- forested_train |>
  select(-class, -county)

cor_mat <- cor_data %>%
  cor(use = "pairwise.complete.obs")
```

For the curious, we might also examine which predictors showed no evidence of being different from zero. The bottom of the table shows five terms with large p-values. These are also spline terms, all for annual precipitation. However, notice that the estimated coefficients and their standard error terms are in the _hundreds_. For these spline terms, the predictor values range between zero and `r upper_vals` (on average). Given this, it is difficult to imagine that the coefficients and their values are reasonable. 

TODO have the splot predictors been normalized? 


```{r}
#| label: two-param-colin-calc
#| include: false

elevation_fit <-
  logistic_reg() |>
  fit(class ~ elevation, data = forested_train) |>
  tidy() |>
  mutate(Model = "elevation only")

annual_mean_fit <-
  logistic_reg() |>
  fit(class ~ temp_annual_mean, data = forested_train) |>
  tidy() |>
  mutate(Model = "mean temperature only")

both_fit <-
  logistic_reg() |>
  fit(class ~ elevation + temp_annual_mean, data = forested_train)

both_vif <-
  both_fit |>
  extract_fit_engine() |>
  vif() |>
  tibble::enframe() |>
  set_names(c("term", "VIF"))

both_fit <-
  both_fit |>
  tidy() |>
  mutate(Model = "both") |>
  full_join(both_vif, by = "term")

all_terms <-
  bind_rows(elevation_fit, annual_mean_fit, both_fit) |>
  left_join(name_key |> rename(term = variable), by = "term") |>
  mutate(
    text = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "elevation" ~ "Elevation",
      TRUE ~ tools::toTitleCase(text)
    )
  ) |>
  select(Model, text, estimate, std_err = std.error, VIF)
```


The reason for these excessively large values goes back to @fig-corr-plot, where we saw that several predictors had large (absolute) correlations with one another. It can be problematic when multiple predictors have a high degree of correlation with one another, commonly called _multicollinearity_^[Meaning that multiple predictors are “co-linear” or have a strong linear relationship with one another] and is a fairly frequent occurrence in many data sets. For example, recall the visualization of the correlation matrix in @fig-corr-plot. The heatmap shows that there are a number of climate-related predictors that have high degrees of correlation with one another. Once we create spline and interaction terms, there are `r sum(abs_cor > 3/4)` pairwise correlations between model terms that have correlations greater than 0.75 and `r sum(abs_cor > 9/10)` term pairs with correlations greater than 0.90.  

While this model's predictive performance is good, the model coefficients are extremely unstable. Multicollinearity not only inflates the variance of the parameter estimates but can also reverse the coefficient's sign, leading to an incorrect interpretation (see below). 

A statistical diagnostic called the variable inflation factor (VIF) measures how much larger the variance is for a set of potentially correlated predictors compared to a set of completely uncorrelated predictors [@belsley2005regression]. Originally developed for linear regression, @weissfeld1991multicollinearity extended the diagnostic to generalized linear models. A general rule of thumb is that VIF statistics greater than 5.0 or 10.0 should be considered larger. For our model, the median VIF across the parameters was `r signif(median(best_mle_vifs$vif), 3)` with the largest being `r format(max(best_mle_vifs$vif), big.mark = ",", scientific = FALSE)`, which corresponds to an annual precipitation spline term. 

To illustrate the issue on a smaller scale, @tbl-logistic-colin shows models with two simple (untransformed) predictors: elevation and mean temperature. The correlation between these two predictors is `r signif(cor(forested_train$elevation, forested_train$temp_annual_mean), 2)`. The first two rows of the table correspond to models with a single predictor, and the last row shows how the model changes when both are included in the model. The parameter estimates radically change when both predictors are included: the sign of elevation changes, and the coefficient for mean temperature increases several orders of magnitude. The variable inflation factor is fairly high and should alert us to the problem. 

:::: {.columns}

::: {.column width="8%"}
:::

::: {.column width="84%"}

::: {#tbl-logistic-colin}

```{r}
#| label: logistic-colin
#| echo: false

all_terms |>
  pivot_wider(
    id_cols = c(Model),
    names_from = c(text),
    values_from = c(estimate, std_err, VIF)
  ) |>
  relocate(
    Model,
    contains("Intercept"),
    contains("Elevation"),
    contains("Mean")
  ) |>
  gt() |>
  tab_spanner(label = "Intercept", columns = contains("Intercept")) |>
  tab_spanner(label = "Elevation", columns = contains("Elevation")) |>
  tab_spanner(label = "Annual Mean Temperature", columns = contains("Mean")) |>
  sub_missing(columns = c(everything()), missing_text = "---") |>
  fmt_number(columns = c(contains("Intercept")), n_sigfig = 3) |>
  fmt_number(columns = c(contains("Elevation")), n_sigfig = 3) |>
  fmt_number(columns = c(contains("Mean")), n_sigfig = 3) |>
  cols_label(
    contains("estimate_") ~ "Estimate",
    contains("std_er") ~ "Std. Error",
    contains("VIF") ~ "VIF"
  ) |>
  cols_width(Model ~ pct(25))
```

An example of how highly colinear predictors can affect parameter estimates and standard errors in a logistic regression model.

:::

:::

::: {.column width="8%"}
:::

:::: 


What can we do about this? There are a few solutions. First, we could transform the original predictors using PCA or PLS so that they enter the model with significantly reduced correlations. However, it may be more difficult to incorporate interactions and nonlinear terms when each derived predictor is a composite of all of the original predictors. Second, we can use a correlation filter to remove the smallest set of predictors that reduces the overall effect of multicollinearity. This is a pretty reasonable approach, but as was said earlier, ML is a game of inches. There could be a reduction in performance by removing near-redundant predictors. 

For logistic regression, the best solution is to keep the predictor set the same but use regularization, a different technique to estimate parameters

### Regularization Through Penalization {#sec-logistic-penalized}

Regularization is a technique that coerces a potentially complex solution into one that is less extreme. We’ve already seen regularization a few times: 

- Early stopping when fitting neural networks (@sec-model-development-whole-game, footnote) prevented model training to become worse.
- Effect encodings that pull a naive statistical estimate to a more moderate value (@sec-effect-encodings).
- Bayes’ Rule for estimating a proportion (@sec-single-proportion) shrank the naive probability estimate towards the prior. 
- The $n_{min}$ tuning parameter that prohibits trees from becoming too deep by preventing splits on small data sets (@sec-external-validation).

In this section, we’ll discuss commonly used regularization techniques for regression models, mostly via _penalization_ of model coefficients. The next section acquaints readers with how Bayesian estimation can result in regularization. 

Why would we regularize a logistic regression model? We want to avoid coefficients with values larger in magnitude than they should be. For example, the previous section shows that highly correlated predictors in a model can result in abnormally large coefficients (potentially with the wrong sign) and huge standard errors. Another example is related to feature selection. If a predictor has no underlying relationship to the outcome, any corresponding coefficient that is not zero is too large. 

In logistic regression, we have shown that parameters can be estimated by maximizing the Bernoulli log-likelihood function (@eq-bernoulli). The most common methods for regularizing logistic models are to add a penalty to the log-likelihood that discourages large parameter values. We'll denote a penalty function as $\mathcal{P}(\boldsymbol{\beta})$ (which may also include tuning parmaeters.

::: {.important-box}
All of the techniques discussed in this section make the implicit assumption that the model features have been standardized to be in the same units (including indicator features). 
:::

Probably the first method devised to penalize models was **ridge regression** [@hoerl1970a;@hoerl1970b;@hoerl2020ridge]. The ridge penalty function detracts from the log-likelihood by using the sum of the squared regression coefficients^[This is often referred to as an $L_2$ penalty since the quadratic norm of the coefficients is used. Also, in the neural network literature, this penalty is odften called "weight decay."]: 

$$
\mathcal{P}_{rr}(\boldsymbol{\beta};\lambda) =  \lambda\sum_{j=1}^{p} \beta_j^2
$$ {#eq-ridge-penalty}

The summation index starts at one and assumes the convention that the intercept parameter is $\beta_0$. *Intercepts are generally excluded from penalties*. There are a few reasons for this. First, the intercept does not have any meaningful null value to shrink towards. A value $\beta_0 = 0$ corresponds to a probability estimate of 0.5, and that value may be wildly different from the population intercept (which is driven by the outcome’s prevalence). Shrinking towards this value may detrimentally harm the model since it is likely moving towards the wrong location. Second, as previously mentioned, the predictors need to be in the same units to make the summation in @eq-ridge-penalty effective. Let’s say that we standardize the data to have the same mean and variance. The mean and variance of the intercept are deterministically one and zero, respectively. Using them in the summation is like comparing apples to horses; it doesn’t make any sense. Third, for some data sets, the intercept may be naturally large due to very low (or high) prevalence. _Squaring_ such a naturally large number, then adding it to the sum, forces the penalty to be mostly driven by this large value. This overwhelms the effect of the other predictors and harms the model’s ability to effectively regularize the parameters associated with predictors (and these are the ones that we care about). 

For a generalized linear model and a fixed value of the $\lambda$ penalty, we estimate parameters that maximize the penalized log-likelihood: 

$$
\underset{\boldsymbol{\beta}}{\text{minimize}}:  -\log \ell(x| \boldsymbol{\beta}) + \mathcal{P}(\boldsymbol{\beta};\lambda)
$$ 

Ridge regression is effective when the matrix of features is _ill-conditioned_, meaning that we cannot compute the inverse of some important matrices (namely $X'X$). This primarily occurs when there are either more features than training set points or severe multicollinearity. The penalty has the effect of **shrinking** the model coefficients towards zero (but at different rates for each predictor). This prevents the parameters from becoming too large due to multicollinearity and stabilizes the standard errors of the estimates. It can also help mitigate the issue of incorrect signs of coefficients; the signs can often change as penalties change. 

The amount of regularization is a tuning parameter, and effective values are between zero and 0.1 (although they can be larger). When the penalty becomes excessively large, all of the coefficients are shrunk so close to zero that the model is essentially mean only, and all predictions will be approximately equal. We typically think about the penalty in 1og-10 units. 

@fig-two-feature-penalty shows how the parameter estimates for the forestry model containing elevation and the mean annual percipitation are used. To standardize the parameters, the orderNorm transformation was used on each so that the percipitation data are no longer skewed and both predictors are in the same units. The dashed horizontal lines indicate the parameter estimates from using ordinary maximum likelihood estimation with no penalty. As the ridge penalty increases, each estimate smoothly decreases towards zero. At a very large penalty, the elevation coefficient changes to become negative before approaching values that are nearly zero.


::: {#fig-two-feature-penalty}

::: {.figure-content}

```{shinylive-r}
#| label: shiny-two-feature-penalty
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(bslib)
library(dplyr)
library(ggplot2)
library(scales)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
source("https://raw.githubusercontent.com/aml4td/website/logistic-reg/R/shiny-penalty.R")

app
```

:::

The path of coefficients for a logistic regression with two features when using different penalization methods. The horizontal dashed lines indicate the unpenalized coefficients. 

:::



We choose the proper amount of regularization just like any other parameter: using a performance metric to decide an appropriate value. As we’ll see later, there is often a plateau where the results are fairly similar for a wide range of penalties. It is nearly impossible to choose a penalty that causes the model to overfit the data. Poor choices for this tuning parameter occur when the results over-penalize the model to a point where all the features are too close to zero (i.e., underfitting). 

As previously mentioned, standard maximum likelihood estimation yields unbiased estimates; the estimates $\hat{\beta}$ have expected values that equal the true, unknown parameters $\beta$.  We can conceptualize this as “MLE is aiming for the right target.” The penalization methods discussed here produce _biased_ estimates, and that bias increases with the penalty. The upside is that, for ridge regression, the variance of the parameter estimates is likely to decrease substantially as the bias grows incrementally. This strategy is another case of the variance-bias tradeoff. By reducing the variance more than increasing the squared bias, we can improve the overall mean squared error of our estimate. However, if our predictor set does not exhibit multicollinearity, biasing the model will have purely adverse effects since there cannot be an appreciable reduction in variance. 
 
We can also penalize the regression using sum of the absolute value of the parameters: 

$$
\mathcal{P}_{lasso}(\boldsymbol{\beta};\lambda) =  \lambda\sum_{j=1}^{p} |\beta_j|
$$ {#eq-lasso-penalty}

There are various methods that can be used to optimize this version of the penalized log-likelihood [@breiman1995better;@lar] but the Lasso model of @tibshirani1996regression is the most common method. The Lasso penalty^[The name is an acronym for the “least absolute shrinkage and selection operator”. Also, we refer to a penalty that uses the absolute values of parameters as an $L_1$ penalty.] has an interesting theoretical property that, for some value of the penalty, some parameter estimates can take a value of **absolute zero**, meaning that they are functionally eliminated from the model. This is one of the most effective tools for automated feature selection. However, due to numerical issues, note that simply optimizing the penalized log-likelihood with basic gradient-based optimization _will not_ find values of actual zero (but come very close). Specialized optimization tools exist that will result in features being selected for removal. 

There are a few ways that Lasso penalization regularizes the parameter estimates that are different than ridge regression. First, while ridge penalization simultaneously moves groups of correlated features towards zero, the Lasso will often choose a single feature out of a group of correlated features. @friedman2010regularization summarized this nicely:

> "Ridge regression is known to shrink the coefficients of correlated predictors towards each other, allowing them to borrow strength from each other. In the extreme case of $k$ identical predictors, they each get identical coefficients with $1/k^{th}$ the size that any single one would get if fit alone.[...]
>
> Lasso, on the other hand, is somewhat indifferent to very correlated predictors, and will tend to pick one and ignore the rest."

Another phenomenon occurs when one or more features are set to zero: the rate of shrinkage to zero can change from the remaining parameters, and, in some rare cases, a parameter may become non-zero again. 

For our model with two features, @fig-two-feature-penalty shows how the parameters change as the Lasso penalty is used. The elevation predictor is completely removed before the perception predictor reaches zero. Notice that once elevation is removed, the other parameter is pulled towards zero is more slowly than before. 

As the comments above indicate, the Lasso model is more strongly associated with feature selection than mitigating the effects of multicollinearity (where the ridge penalty is favored). Why not use a combination to accomplish both goals? @zou2005regularization describe the _elastic net_ model that includes a separate penalty for both criteria:  

$$
\mathcal{P}_{en}(\boldsymbol{\beta};\lambda_1 \lambda_2) =  \lambda_1\sum_{j=1}^{p} \beta_j^2 + \lambda_2\sum_{j=1}^{p} |\beta_j|
$$ {#eq-elastic-penalty}

Alternatively, @friedman2010regularization described what is referred to as the _glmnet penalty_ that uses the form: 

$$
\mathcal{P}_{gn}(\boldsymbol{\beta};\lambda, \alpha) =  \lambda \left[(1 - \alpha) \sum_{j=1}^{p} \beta_j^2 + \alpha \sum_{j=1}^{p} |\beta_j|  \right]
$$ {#eq-glmnet-penalty}

where $\lambda$ is the total amount of regularization and $\alpha$  controls the mixture of the two methods so that $\alpha = 1$ is a Lasso model and $\alpha = 0$ is ridge regression. The glmnet penalty formulation tends to be favored by users. 

The combination strategy can help the model move between the goals of dealing with multicollinearity (and similar issues) and feature selection. By tuning the penalty and the mixing parameters, we can let the data decide which penalty it needs most (and how much). As we’ll see below, a typical pattern is a fairly flat plain where there isn’t much change in our preferred metric for a wide range of penalty values. Also, the relationship between the penalty and performance can be similar for different mixtures but shows a lagging pattern over values of $\alpha$. As such, we can achieve virtually the same performance but with different numbers of non-zero coefficients. When choosing the best parameters for glmnet, we might want to factor in the number of “active features” along with performance. For example, we could use desirability functions to blend the two outcomes or create a more direct rule, such as “choose $\lambda$ and $\alpha$ with the fewest number of predictors as long as the area under the ROC curve is greater than 0.85.”

@fig-two-feature-penalty shows the glmnet results for the two-feature model. Starting with $\alpha = 1.0$, we have the same solution as the Lasso results. Using the slider to decrease the proportion of Lasso in the penalty slowly shifts the curves to the right, where more penalization is needed to eliminate both parameters (since more ridge regression is used). Also, as $\alpha$ moves closer to the minimum of 10%, there becomes less difference in the slopes for the mean annual temperature before and after elevation is removed. In any case, elevation is removed before mean annual temperature for each setting. 

The nature of penalized models, such as Lasso and glmnet, makes them likely to be able to exploit submodels (@sec-submodels) for very efficient model tuning. For a specific mixture percentage, we can fit one glmnet model and evaluate a large number of penalty values almost for free^[This depends on the implementation, though.]. This is only the case when the tuning grid has many $\lambda$ values for each $\alpha$. Space-filling designs and iterative search would not be good choices here; regular grids for these model parameters can be _more_ computationally efficient. 

The glmnet model is probably the most popular method for regularizing models, but it can suffer when one or more parameters are excessively large. When using lasso or ridge penalties, a parameter estimate outlier can have a linear or quadratic influence on the penalty, respectively. This can pull the other estimates towards the outlier, thereby giving it undue influence on the penalization. @fan2001variable and @mcp defined the smoothly clipped absolute deviation (SCAD)  and minimax concave penalty (MCP) models, respectively. There configurations are:  

$$
\mathcal{P}_{scad}(\boldsymbol{\beta};\lambda, \gamma) =     \begin{cases}
	 \sum_{j=1}^{p}\lambda|\beta_j| & |\beta_j| \le \lambda \\
	 \sum_{j=1}^{p} \lambda|\beta_j| \frac{2\gamma\lambda|\beta_j|-\beta_j^2-\lambda^2}{2(\gamma - 1)} & \lambda < |\beta_j| \le \gamma\lambda \\
	 \frac{\lambda^2(\gamma + 1)}{2} & |\beta_j| \ge \gamma\lambda
	\end{cases} 
$$ {#eq-scad-penalty}

and

$$
\mathcal{P}_{mcp}(\boldsymbol{\beta};\lambda, \gamma) =   \begin{cases}
	 \sum_{j=1}^{p}\frac{\lambda|\beta_j| - \beta_j^2}{2\gamma} & |\beta_j| \le \gamma\lambda \\
	 \sum_{j=1}^{p}\frac{\lambda^2\gamma}{2} & |\beta_j| > \gamma\lambda
	\end{cases} 
$$ {#eq-mcp-penalty}

The models include an additional tuning parameter, $\gamma$, that provides a “clipping” effect where, once the sum of the absolute values is “large” ($\gamma\lambda$) the penalty becomes constant. These methods have the advantage of limiting the effect that abnormally large parameters have on the regularization procedure. This may make them more attractive for cases where the ratio of features to training set samples is exceedingly large.

These two penalties are also shown in @fig-two-feature-penalty where there is an extra control for the clipping parameter ($\gamma$). For clipping values $\gamma < 10$, there is a sharp change in the parameter paths where they abruptly jump to their unpenalized values. This is related to the limit of $\lambda\gamma$; values above this combination are clipped at a value that does not involve the $\beta$ parameters. As $\gamma$ becomes relatively large for these data, the static penalty has a less abrupt effect on the parameter estimates, and the change is not as instantaneous as it is for smaller clipping values. For this example, there is very little difference between the SCAD and MCP approaches. 

Penalization of generalized linear models has been a very fertile area of research, and there are many more variations of these models.

- _Groups_ of predictors can be defined that have a common penalty so that they all stay in (or leave) the model _en masse_ [@simon2013sparse;@bien2013lasso]. This might be helpful to have specific penalization for natural groupings, such as spline terms generated by a specific parameter, or a set of dummy indicators for a high cardinality predictor, etc. 

- We can regularize models with a large set of interactions so that they can only be removed if their main effects are also removed from the model (enforcing the hierarchy principle for interactions) [@lim2015learning;@bien2013lasso]. 

- Two-stage methods and adaptive methods use an initial fit to inform the estimation and penalization [@zou2006adaptive;@meinshausen2007relaxed]. For example, the UniLasos method of @chatterjee2025univariate uses the slope coefficient from a collection of unpenalized models with a single predictor to improve lasso penalization.  

Most of these penalization methods have the disadvantage that they disable traditional statistical inference^[The main exception is the pure ridge regression model, where we can compute a covariance matrix once we have finalized the penalty value.]. It is difficult to measure uncertainty for models using feature selection via a lasso penalty. Even bootstrapping the model is difficult; the bootstrap distribution for many parameters is bimodal since it is a mixture of parameter estimates when the coefficient is retained and a large spike at zero when it was not. @EfronHastie2016a

MORE ABOUT THIS

#### Refitting the Forestation Model  {.unnumbered}

```{r}
#| label: glmn-calc
#| include: false
#| cache: true

glmn_pen <- 10^seq(-6, -1, length.out = 50)
glmn_grid <- crossing(penalty = glmn_pen, mixture = c(0:4) / 4)

glmn_spec <-
  logistic_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet", path_values = !!glmn_pen)

glmn_wflow <-
  best_mle_wflow |>
  update_model(glmn_spec)

glmn_res <-
  glmn_wflow |>
  tune_grid(
    resamples = forested_rs,
    grid = glmn_grid,
    control = control_grid(save_pred = TRUE, save_workflow = TRUE),
    metrics = cls_mtr
  )

glmn_best_param <- select_best(glmn_res, metric = "brier_class")
glmn_best_metric <- show_best(glmn_res, metric = "brier_class", n = 1)
```

```{r}
#| label: tuned-glmn-penalties
#| include: false
#| cache: true

pen_target <- -3
glmn_sel_param <-
  glmn_res |>
  collect_metrics() |>
  mutate(diff = abs(log10(penalty) - pen_target)) |>
  slice_min(diff, n = 1) |>
  filter(mixture == glmn_best_param$mixture) |>
  slice_head(n = 1) |>
  select(penalty, mixture, .config)

glmn_best_fit <- fit_best(glmn_res, parameters = glmn_sel_param)

set.seed(799)
glmn_sel_int <-
  glmn_res |>
  int_pctl(
    times = 2000,
    metrics = cls_mtr,
    allow_par = FALSE,
    parameters = glmn_sel_param
  )

lr_coefs <-
  inner_join(
    tidy(best_mle_fit) |> select(term, MLE = estimate),
    tidy(glmn_best_fit) |> select(term, glmnet = estimate),
    by = "term"
  ) |>
  filter(term != "(Intercept)") |>
  mutate(
    Type = case_when(
      grepl("(_10)|(_0[1-9])", term) ~ "Spline",
      grepl("_x_", term) ~ "Interaction",
      TRUE ~ "Main Effect"
    ),
    Type = factor(Type, levels = c("Spline", "Interaction", "Main Effect"))
  )

num_rm <- sum(lr_coefs$glmnet == 0)

p_glmnet_tune <-
  autoplot(glmn_res, metric = "brier_class") +
  theme(legend.position = "top") +
  labs(y = "Brier (Resampled)")

# ------------------------------------------------------------------------------

glmn_best_coef <-
  glmn_best_fit |>
  extract_fit_engine() |>
  tidy() |>
  filter(term != "(Intercept)")

feature_nms <- unname(name_list)
for (i in seq_along(name_list)) {
  glmn_best_coef$term <- gsub(
    name_list[[i]],
    names(name_list)[i],
    glmn_best_coef$term,
    fixed = TRUE
  )
}

glmn_best_coef$term <- gsub("_x_", " X ", glmn_best_coef$term)
glmn_best_coef$term <- gsub("_", " ", glmn_best_coef$term)

top_best <- 5
show_pen <- glmn_pen[which.min(abs(glmn_pen - 10^(-4)))]
at_best <-
  glmn_best_coef |>
  filter(lambda == glmn_best_param$penalty) |>
  slice_max(abs(estimate), n = top_best)

best_path <-
  inner_join(glmn_best_coef, at_best |> select(term), by = "term")

at_show <-
  best_path |>
  filter(lambda == show_pen) |>
  mutate(label = term)
```

Let’s take our previous logistic regression Model #5 and create a penalized fit via the glmnet. We’ll tune over the amount of penalization and the mixture of lasso. For the former we evaluated `r length(glmn_pen)` values from 10<sup>`r log10(min(glmn_pen))`</sup> to 10<sup>`r log10(max(glmn_pen))`</sup>. For mixing, we looked as pure ridge and lasso mixes along with 25%, 50%, and 75% lasso. A grid with all combinations of these parameters (i.e., a regular grid) was used.

@fig-logistic-glmnet (top) shows the results. The Brier score was used to measure model effectiveness. The curves show that, when the penalty is too large, important features are eliminated and the model begins to underfit. At the other end of the range, there is very little difference in the Brier scores across the mixture values. The numerically best value corresponds to  `r glmn_best_param$mixture * 100`% lasso and a penalty value of 10<sup>`r signif(log10(glmn_best_param$penalty), 2)`</sup>; however, there are many different combinations with near equal Brier scores. 

The bottom of @fig-logistic-glmnet shows that, despite the Brier scores being relatively constant, the parameter estimates fluctuate significantly. The labeled predictors are those with the largest absolute coefficient values at the best penalty/mixture combination. These values are spline basis expansion terms for several different predictors. 

```{r}
#| label: fig-logistic-glmnet
#| fig-width: 7
#| fig-height: 8
#| fig-align: center
#| out-width: 70%
#| fig-cap: "Top: Results of tuning the complex logistic model using the glmnet penalty. The dotted vertical line designates the numerically optimal result while the dashed line is the selected penalty value. Bottom: Parameter estimates over penalty values for the best model. The features with the largest values at the selected penalty are highlighted."
#| warning: false

set.seed(836)
p_glmnet_param <-
  glmn_best_coef |>
  ggplot(aes(lambda, estimate, group = term)) +
  geom_vline(xintercept = glmn_sel_param$penalty, lty = 2) +
  geom_vline(xintercept = glmn_best_param$penalty, lty = 3) +
  geom_line(show.legend = FALSE, alpha = 1 / 3, col = "grey") +
  geom_line(
    data = best_path,
    aes(col = term),
    show.legend = TRUE,
    alpha = 1,
    linewidth = 1.1
  ) +
  # geom_label_repel(data = at_show, aes(col = term, label = label), show.legend = FALSE, alpha = 0.9) +
  scale_x_log10() +
  labs(x = penalty()$label, y = "Estimate") +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "bottom") +
  guides(colour = guide_legend(ncol = 3, title = ""))

mix_colors <- scales::brewer_pal(palette = "Blues")(9)[-(1:2)]

(p_glmnet_tune +
  geom_vline(xintercept = glmn_sel_param$penalty, lty = 2) +
  geom_vline(xintercept = glmn_best_param$penalty, lty = 3) +
  scale_color_manual(values = mix_colors)) /
  p_glmnet_param +
  plot_layout(heights = c(2, 3))
```

However, when we use the numerically optimal settings, a strong case can be made that there is appreciable multicollinearity occurring; the magnitudes of the parameters are still large. For this analysis, we decided to reduce the penalty value back to a value of about 10<sup>`r pen_target`</sup> (designated as the vertical dashed line). The parameters are more attenuated at this value, and there is no real increase in the Brier score, which is estimated to be `r signif(glmn_sel_int$.estimate[glmn_sel_int$.metric == "brier_class"], 3)` with 90% confidence interval (`r signif(glmn_sel_int$.lower[glmn_sel_int$.metric == "brier_class"], 3)`, `r signif(glmn_sel_int$.upper[glmn_sel_int$.metric == "brier_class"], 3)`). With these tuning parameter values, `r cli::format_inline("{num_rm} parameter{?s} {?was/were}")` were set to zero. 

### Bayesian Estimation {#sec-logistic-bayes} 

Now that we've seen how to find parameter estimates for logistic regression via maximum likelihood (penalized or unpenalized), let's discuss how Bayesian estimation works for logistic regression. This explanation will be very reductive; the mechanisms for serious Bayesian analysis are highly optimized and sophisticated. Our example will demonstrate the main points but is insufficient for solving real problems. 

Recall that the Bayesian posterior distribution is the endpoint of a Bayesian analysis. It combines our prior belief with the observed data to make a complete probabilistic description of the parameters, given the data. For small-scale illustration, we'll use a single predictor model: 

$$
log\left(\frac{\pi_i}{1 - \pi_i}\right) = \beta_0 + \beta_1 {x}_i
$$ 

where $i=1, \ldots, 100$. We'll simulate the 100 training set data points using $\beta_0 = 1.0$ and $\beta_1 = -1.5$. The predictor data were uniformly distributed between $\pm 1$. Using these specifications, our data set has an event rate of `r round(mean(tr_data$class== "A")*100, 1)`%. 

Let's assume that the data are independent Bernoulli random variables. Instead of maximizing the likelihood, the goal is to optimize the posterior probability:

$$
Pr[\boldsymbol{\beta}| x] = \frac{Pr[\boldsymbol{\beta}]  \ell(x| \boldsymbol{\beta})}{Pr[x]}
$$

As mentioned in @sec-bayes, we are trying to estimate a distribution and not just the scalar values of our two unknown parameters. To do so, we need two prior distributions. If we know a lot about the data, we might be able to inject some opinion into the analysis. For example, if we were extremely sure that the slope parameter $\beta_1$ should be non-negative, our prior could be a right-skewed distribution with no negative values.  However, keep in mind that our priors define what is possible for the parameters. For example, if we used a uniform prior with limits of $\pm 2.0$, the posterior distribution will never have values outside of that limit since those have zero prior probabilities of occurring. 

In many cases, though, we might err on the side of choosing very broad, diffuse distributions. The regression parameters generally take up values on the entire real line, so we'll choose our two priors to be independently Gaussian $N(0, 10)$. Considering that most values of these distributions will be within $\pm 3$ standard deviations, these are fairly flat and offer very limited opinions about what the parameters are likely to be. While @gelman2008weakly makes a recommendation for a weakly informative default prior, we'll discuss more complex methods later in this section. 

At this point, we can calculate the likelihood and the prior probabilities for proposed parameter values. Note that Bayes Rule above also includes $Pr[x]$. This is a normalizing constant and is not a function of the unknown parameters. It is not generally ignorable, but, in this case, the methodology used here does not require its calculation; we'll multiply the prior and likelihood to compute an un-normalized posterior probability. 

One small problem is numerical. As we multiply a collection of small numbers, the product can become so small that it is indistinguishable from absolute zero. The solution is often to compute logarithms of the probabilities and add them (maximizing the sum of the log posterior is the same as maximizing it in the natural units). This may seem trivial, but it also helps us understand the effect of the prior. The prior has two probability values in the example, and $Pr[\boldsymbol{\beta}]$ is their product. The likelihood for this example is the product of $n_{tr} = 100$ probability density values. When taking the log of the posterior, we're summing 102 log probabilities, two of which are from the prior. Although these two values come from different probability scales, they contribute to less than 2% of the total sum. Suppose that our training set consisted of $n_{tr} = 8$ data points. Now, the prior accounts for 20% of the posterior. This shows how, all other things being equal, the effect of the prior can be limited (unless it is extraordinarily opinionated and/or narrow). 

The Metropolis-Hastings (MH) algorithm is used to demonstrate the process of estimating parameters [@Robert2004;@chib1995understanding;@van2018simple]. Again, currently, technology for estimating the posterior is more complex and effective, but this method illustrates Markov Chain Monte Carlo (MCMC) optimization. The process will seem similar; simulated annealing is an application of MH. 

We start with initial samples from the two prior distributions, denoted as $\boldsymbol{\beta}^{(0)}$. Using these and our data, we calculate the unnormalized posterior probability. As with simulated annealing, we can make a small more within a "local neighborhood.". We'll take a simple approach and simulate proposed values, $\boldsymbol{\beta}^{(1)}$, that are within $\pm 2.0$ of our current initial values. After computing the second posterior value: 

* If the new posterior is larger than the first, accept $\boldsymbol{\beta}^{(1)}$ as our new best estimate. 
* Otherwise, we can accept $\boldsymbol{\beta}^{(1)}$ if a random uniform number is larger than the ratio of the first posterior value to the second. 

If $\boldsymbol{\beta}^{(1)}$ is close (in posterior probability) to $\boldsymbol{\beta}^{(0)}$, we have a higher probability of accepting it (as with simulated annealing). 

@tbl-mcmc shows some of the initial iterations of the MH algorithm. After the initial parameter value, another is generated to be in a nearby space. The log-posterior for this new value is smaller than the first, so we do not automatically accept it. The probability of acceptance is essentially zero though, since the difference in the log-posterior values is `r round(param$log_post[2] - param$log_post[1])`, the exponentiated value is very close to zero. The random uniform value for this decision (`r signif(param$random[2], 3)`) formally rejects it. The next proposal is again based on the initial value and is rejected. On the third try, the new parameter value has a larger log-posterior probability and is automatically accepted as the current best.  

```{r}
#| label: tbl-mcmc
#| tbl-cap: "A set of iterations of the Metropolis-Hastings algorithm for Bayesian logistic regression at the beginning and ending phases of the optimization."
#| echo: false

dec_lvls <- c("initial", "improvement", "acceptable", "rejected")

param <- param |>
  mutate(
    Decision = case_when(
      better ~ "improvement",
      !better & accept ~ "acceptable",
      TRUE ~ "rejected"
    ),
    Decision = ifelse(row_number() == 1, "initial", Decision),
    Decision = factor(Decision, levels = dec_lvls)
  )

slice_size <- 8
param |>
  slice_head(n = slice_size + 1) |>
  mutate(Phase = "Beginning") |>
  bind_rows(
    param |>
      slice_tail(n = slice_size + 1) |>
      mutate(Phase = "Ending")
  ) |>
  mutate(random = ifelse(is.na(accept_prob), NA, random)) |>
  select(
    Iteration = iteration,
    Parent = parent,
    Intercept = beta_0,
    Slope = beta_1,
    `(Log) Posterior` = log_post,
    `Pr[Accept]` = accept_prob,
    Random = random,
    Decision
  ) |>
  gt() |>
  fmt_number(columns = c(Intercept, Slope, `(Log) Posterior`), n_sigfig = 3) |>
  fmt_number(columns = c(`(Log) Posterior`), n_sigfig = 4) |>
  fmt_number(columns = c(`Pr[Accept]`, Random), decimals = 3) |>
  fmt_number(columns = c(Iteration, Parent), decimals = 0) |>
  sub_missing(
    columns = c(`Pr[Accept]`, Parent, Random),
    missing_text = "---"
  ) |>
  tab_row_group(label = " ", rows = Iteration <= slice_size + 1)
```

This process, called the "warm up phase," proceeds a large number of times (as defined by the user). We can monitor the proposed parameter values and assess if they appear to converge to a stable parameter space. @fig-mcmc-warmup shows the progress of MH for the first `r warmup` iterations. The process starts far away from the true parameters (represented by the dotted lines), and the next 25 iterations happen to sample slope values ($\beta_1$) in a narrow, negative range. There are some improved values until the slope meanders to larger values, where new proposals are consistently accepted. 

```{r}
#| label: fig-mcmc-warmup
#| echo: false
#| out-width: 50%
#| fig-width: 4.2
#| fig-height: 4.5
#| fig-align: center
#| fig-cap: The coefficient path during the warmup period for Metropolis-Hastings optimization.
true_means <- c(1.0, -1.5)

warmup_data |>
  ggplot(aes(beta_0, beta_1, col = Decision, pch = Decision)) +
  geom_point() +
  geom_vline(xintercept = true_means[1], lty = 3) +
  geom_hline(yintercept = true_means[2], lty = 3) +
  labs(x = expression(beta[0]), y = expression(beta[1])) +
  scale_color_manual(
    values = c(rgb(0, 0, 0, 3 / 4), rgb(0, 0, 1, 3 / 4), rgb(0, 0, 0, 1 / 4))
  ) +
  scale_shape_manual(values = c(16, 17, 4)) +
  theme(legend.position = "top")
```

Around 55 iterations, the MH process has settled near the true values. After this, the iterations in our warm up phase of `r `warmup` iterations, the proposals remain in that area with `r round(100 * mean(warmup_data$Decision[55:warmup] == "rejected"), 1)`% being outright rejected, `r round(100 * mean(warmup_data$Decision[55:warmup] == "acceptable"), 1)`% are provisionally accepted, and `r round(100 * mean(warmup_data$Decision[55:warmup] == "improvement"), 1)`% are improvements. 

```{r}
#| label: post-summary
#| echo: false

term_rate <-
  param |>
  slice_tail(n = 500) |>
  summarize(rate = round(mean(better | accept) * 100, 1)) |>
  pluck("rate")

post_point <-
  posterior |>
  dplyr::select(-iteration) |>
  colMeans() |>
  signif(digits = 3)

post_lower <-
  posterior |>
  dplyr::select(-iteration) |>
  map_dbl(~ quantile(.x, prob = 0.05)) |>
  signif(digits = 3)

post_upper <-
  posterior |>
  dplyr::select(-iteration) |>
  map_dbl(~ quantile(.x, prob = 0.95)) |>
  signif(digits = 3)

mle_coef <-
  glm(class ~ x, data = tr_data, family = binomial) |>
  tidy()
```

At this point, it is reasonable to believe the MH has converged to the location of the optimal values. Since we may not be able to analytically determine the true distribution of the posterior, we spend a good deal of time running new iterations. The non-rejected proposals form a sample of the posterior, and we continue to sample until there are enough data points to make precise numerical summaries. After a total of `r format(nrow(param) - warmup - 1, big.mark = ",")` additional iterations, there were `r format(sum(param$Decision[(warmup+1):nrow(param)] != "rejected"), big.mark = ",")` usable samples. In the last 500 iterations of MH, approximately `r term_rate`% of proposals were not rejected. 

@fig-logistic-posterior shows the univariate distributions of our parameters. The slope's posterior mean was `r post_point[2]`, with 90% of the samples within (`r post_lower[2]`, `r post_upper[2]`). This interval is credible and can come from a more rational probability statement about this parameter (compared to a confidence interval). 

```{r}
#| label: fig-logistic-posterior
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 3.1
#| fig-align: center
#| fig-cap: Samples of the posterior distribution (post-warm) for the two logistic regression parameters.

true_means <- c(1.0, -1.5)

posterior |>
  pivot_longer(
    cols = c(intercept, slope),
    names_to = "parameter",
    values_to = "value"
  ) |>
  ggplot(aes(value)) +
  geom_histogram(bins = 20, col = "white") +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = "Parameter Values")
```

The previous section showed how penalizing the likelihood can mitigate issues with our model related to multicollinearity or irrelevant predictions. Penalization produces biased estimates for those models since they are shrunken towards zero. From a Frequentist perspective, using priors in Bayesian estimation also produces biased estimates since the posterior is shrunken to the prior (to some degree). 

We mentioned that there are more specialized priors that can be used. For example, we can choose priors to encourage sparsity in our regression parameters (as we did with the lasso model). It turns out that the regularization of the lasso model is equivalent to using a Laplace distribution (a.k.a the double exponential distribution) as a prior [@tibshirani1996regression]. @fig-sparse-posteriors (left) show an example of the Laplace distribution. Compared to a similar Gaussian distribution, the Laplacian has larger probabilities near zero and in the distribution's tails.  Bayesian models using this prior are discussed by @park2008bayesian.

```{r}
#| label: fig-sparse-posteriors
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 3.1
#| fig-align: center
#| fig-cap: Examples of priors that can be used to encourage sparsity in the parameter values. The spike distribution has been scaled to be smaller for illustration.
x_seq <- seq(-25, 25, by = 0.005)

dbl_expo <- function(x, mu = 0, scale = 1) {
  1 / (2 * scale) * exp(-abs(x - mu) / scale)
}

gau_lap <-
  bind_rows(
    tibble(
      Parameter = x_seq,
      density = dbl_expo(x_seq, scale = 10),
      Distribution = "Laplacian"
    ),
    tibble(
      Parameter = x_seq,
      density = dnorm(x_seq, sd = 10),
      Distribution = "Gaussian"
    )
  )

p_gau_lap <-
  ggplot(gau_lap, aes(x = Parameter, y = density, col = Distribution)) +
  geom_line(linewidth = 3 / 4, alpha = 3 / 4) +
  theme(legend.position = "top") +
  scale_color_manual(values = c("#A8BE74FF", "#BF7417FF"))

slab <- dnorm(x_seq, mean = 0, sd = 10)

# Normalize for printing
spike <- dnorm(x_seq, mean = 0, sd = 0.1)
spike <- spike / max(spike)
spike <- spike * max(slab) * 3
zero_spike <- spike > .Machine$double.eps

spike_slab <- bind_rows(
  tibble(Parameter = x_seq, density = slab, Distribution = "Slab"),
  tibble(
    Parameter = x_seq[zero_spike],
    density = spike[zero_spike],
    Distribution = "Spike"
  )
)

p_spike_slab <-
  ggplot(spike_slab, aes(x = Parameter, y = density, col = Distribution)) +
  labs(y = NULL) +
  geom_line(linewidth = 3 / 4) +
  theme(legend.position = "top") +
  scale_color_manual(values = c("#D64358FF", "#4F5791FF"))

p_gau_lap + p_spike_slab
```

Another idea is to use a composite distribution. The "spike and slab" prior contains three distributions [@mitchell1988bayesian;@malsiner2011comparing]. The first is a slab: a very wide distribution such as our earlier N(0, 10). This is designed to represent our (weak) belief for parameters that should have a non-sparse/non-zero value. The spike element is an incredibly tight distribution centered around zero (such as an N(0, 0.1)) and typifies parameters that should have little or no effect on the model. These two choices are shown in @fig-sparse-posteriors (right). Finally, the mixture distribution has values on [0, 1] that indicate how to blend the spike and slab elements. For example, with the values given above, the spike-slab prior could be: 

$$
Pr[\beta|\alpha] = (1 - \alpha) N(0, 10) + \alpha N(0, 0.1)
$$

What should we use for the mixture prior $Pr[\alpha]$? With a Beta distribution, we could have the prior take values of the entire range. This would include a uniform distribution if we lack an opinion on the sparsity rate or enable more informative beliefs. Alternatively, we could choose a Bernoulli distribution that produces values in $\alpha \in \{0, 1\}$ with some probability. We would choose a value for $Pr[\alpha = 1]$ or put yet another prior on that parameter. 

While these approaches do regularize the parameters to be closer to zero, remember that Bayesian models _do not_ produce a single parameter value but a distribution of values. Whereas the lasso and glmnet can estimate the parameters to be absolute zero, eliminating the feature from the prediction equation entirely. For Bayesian models, it will not occur. The priors shown in @fig-sparse-posteriors indicate that, theoretically, there is an infinitesimally small probability that the posterior will have many values at absolute zero. In the case of the spike-slab model, the posterior for these parameters is likely to be bimodal with one mode at zero. In summary, the Bayesian approach _does_ move values very close to zero as needed but will not functionally exclude any features from the prediction equation. 

The _posterior predictive distribution_ is used when predicting new data. If we are predicting a value of $x_0$, the posterior predictive distribution takes into account our existing posterior but also accounts for the variability that we sampled the new value, i.e., $Pr[X = x_0]$ where $X$ is the random variable for our predictor. The draws from this distribution are used to estimate the class probability. We often produce a single value for a new prediction, which could be the mean (or mode) of the posterior draws. We can also produce credible intervals for our probability. This alone may be a good reason to use Bayesian estimation for these models.  

## Multinomial Regression {#sec-multinomial-reg}

Analogous models exist for outcomes with three or more classes. Notationally, we can again denote the binary indicators as $y_{ik}$ with $k = 1\ldots C \ge 3$. Generally, this type of target value is assumed to come from a multinomial distribution, with likelihood: 

$$
\ell(\pi_i; y_{ij}) = \prod_{i=1}^{n_{tr}} \pi_1^{y_{i1}}\pi_2^{y_{i2}}\cdots \pi_C^{y_{iC}}
$$ {#eq-multinomial}

where $\pi_{k}$ is the probability that the outcome is class $k = 1, \ldots, C$. 

One complication is the implicit constraint that $\pi_1 + \cdots + \pi_C = 1$. Directly maximizing the log-likelihood is feasible but would not lead to unique parameter estimates^[In fact, we’ll see this approach in @sec-cls-mlp.]. 

The primary method for estimating parameters is the **multinomial logit model**, a subset of a class of techniques called _discrete choice models_. One estimation method uses a collection of $C-1$ logistic regression models that contrast specific outcome classes versus a reference class. The linear predictor is related to a logit function with a different denominator: 

$$
log\left(\frac{\pi_{ik}}{1 - \pi_{iC}}\right) = \eta_{ik} = \boldsymbol{x}_i'\boldsymbol{\beta}_k
$$ {#eq-multinomial-logit}

Note that, in this formulation, the linear predictor has the same structure across classes. While the parameter estimates are different for different classes, the same predictor set is used for all models. 

When making predictions, we normalize the probabilities from one logit model using the total: 

$$
\hat{\pi}_{ik} = \frac{\exp(\hat{\eta}_{ik})}{\sum_\limits{k=1}^{C-1} \exp(\hat{\eta}_{ik})}
$$


note Dirichlet as prior, penalization, etc


## Generalized Additive Models {#sec-cls-gam}

## Discriminants {#sec-lda}

Linear discriminant analysis (LDA) is a classical method for classification models that can be used when all predictors are quantitative. There is more than one way to motivate the model, and we’ll frame it in terms of Bayes’ Rule, which, in this context, is

$$
Pr\left[y| \boldsymbol{x}\right] = \frac{Pr\left[Y\right]\: Pr\left[\boldsymbol{x}|y\right]}{Pr\left[\boldsymbol{x}\right]}
$$  {#eq-lda-bayes}

where $y$ represents the outcome classes. The prior $Pr\left[y| \boldsymbol{x}\right]$ is represented by a vector of probabilities that represents our beliefs for how often each class occurs in the wild. Often, though, the prior in LDA is estimated from the training set. 

Traditional LDA makes the assumption that the training set predictors follow a multivariate Gaussian distribution. It assumes that the overall distribution has a single mean vector $\boldsymbol{\mu}$ ($p\times 1$) and covariance matrix $\boldsymbol{\Sigma}$ ($p\times p$) so that:

$$
Pr\left[\boldsymbol{x}\right]= (2\pi )^{-p/2}\det({\boldsymbol{\Sigma}})^{-1/2}\exp \left[-\frac{1}{2}\left(\boldsymbol{x}-\boldsymbol{\mu}\right)'\boldsymbol{\Sigma}\left(\boldsymbol{x}-\boldsymbol{\mu}\right)\right] 
$$ {#eq-multivariate-norm}

The conditional probabilities $Pr\left[\boldsymbol{x}|y\right]$ are also assumed to be multivariate normal with different mean vectors  $\boldsymbol{\mu}_k$ but a common covariance matrix $\boldsymbol{\Sigma}$. This latter assumption leads to classification boundaries that are strictly linear. 

In practice, the model is trained by estimating the the mean and covariance parameters. The standard equations for the sample means, variances, and covariances are commonly used. In this scenario, we compute the probabilities and the corresponding hard class estimates based on the class with the largest posterior probability. 

Fisher's LDA

A special case of this model is the naive Bayes model. This assumes that the predictor data are _independent_, meaning that the covariance matrix $\boldsymbol{\Sigma}$ is diagonal. This is an incredibly unlikely assumption, but it offers a significant reduction in complexity. It means that both $Pr\left[\boldsymbol{x}\right]$ and $Pr\left[\boldsymbol{x}|y\right]$ can be computed as a product of univariate densities. We could still assume Gaussian distributions for each of the predictors. However, we might also use more complex methods of estimating the probabilities. For example, nonparametric density estimates make minimal assumptions regarding the predictor data. As an example, ...

```{r}
#| label: fig-cls-densities
#| echo: false
#| out-width: 50%
#| message: false
#| warning: false
#| fig-width: 5
#| fig-height: 5
#| fig-cap: "Conditional probability distributions $Pr\\left[\\boldsymbol{x}|y\\right]$ that assume that the predictors are independent but use two different methods for estimating the density functions."

varpor_data <- 
  forested_train |> 
  mutate(group = as.character(class)) |> 
  select(group, vapor_max)


norm_dens <- function(x) {
  mn <- mean(x)
  stdev <- sd(x)
  rngs <- extendrange(x)
  grid <- seq(rngs[1], rngs[2], length.out = 2^10)
  tibble(vapor_max = grid, dens = dnorm(grid, mn, stdev), Estimate = "Gaussian")
}

flex_dens <- function(x) {
  x |> 
    density(n = 2^10) |>
    tidy() |> 
    set_names(c("vapor_max", "dens")) |> 
    mutate(Estimate = "nonparametric")
}

both_estimates <- 
  forested_train |> 
  nest(vapor_max, .by = class) |> 
  mutate(
    gaussian = map(data, ~ norm_dens(.x$vapor_max)),
    flexible = map(data, ~ flex_dens(.x$vapor_max))
  ) |> 
  select(-data)


two_densities <- 
  both_estimates |> 
  select(class, gaussian) |> 
  unnest(gaussian) |> 
  bind_rows(
    both_estimates |> 
      select(class, flexible) |> 
      unnest(flexible)
  )

ggplot(two_densities, aes(x = vapor_max)) +
  geom_histogram(
    data = forested_train,
    aes(y = after_stat(density)),
    col = "white",
    alpha = .5,
    bins = 30
  ) +
  geom_line(aes(vapor_max, y = dens, col = Estimate), linewidth = 1.5) +
  facet_wrap( ~ class, ncol = 1, labeller = label_both) +
  theme(legend.position = "top") +
  scale_color_brewer(palette = "Dark2") +
  labs(x = "Maximum Vapor")
```

The naive Bayes model’s reduced complexity also enables us to include qualitative predictors since we can compute the rate of each predictor’s probabilities from the training set. 



## Chapter References {.unnumbered}
