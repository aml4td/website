---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-linear/"
---

# Generalized Linear and Additive Classifiers {#sec-cls-linear}

```{r}
#| label: cls-linear-setup
#| include: false

source("../R/_common.R")
source("../R/_themes_ggplot.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(embed)
library(bestNormalize)
library(probably)
library(patchwork)
library(mirai)
library(gt)
library(gtExtras)
library(ggrepel)
library(car)
library(DALEXtra)
library(discrim)
library(spatialsample)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_bw())
set_options()
daemons(parallel::detectCores())

# ------------------------------------------------------------------------------
# load data

load("../RData/forested_data.RData")
load("../RData/logistic_bayes.RData")
load("../RData/forested_interactions.RData")
```

```{r}
#| label: cls-boundaries
#| include: false
data(ad_data, package = "modeldata")

ad_sub <-
  ad_data |>
  select(truth = Class, x_1 = Ab_42, x_2 = p_tau) |>
  mutate(
    x_1 = scale(x_1)[,1], x_2 = scale(x_2)[,1],
    truth = factor(ifelse(truth == "Impaired", "A", "B"))
  )

rngs <- map(ad_sub |> select(-truth), extendrange)

ad_grid <- crossing(x_1 = seq(rngs[[1]][1], rngs[[1]][2], length.out = 200),
                    x_2 = seq(rngs[[2]][1], rngs[[2]][2], length.out = 200))

# ------------------------------------------------------------------------------

high_fit <-
  discrim_flexible(num_terms = 30, prod_degree = 2, prune_method = "none") |>
  set_engine("earth", nk = 100) |>
  fit(truth ~ ., data = ad_sub)

high_pred <- augment(high_fit, ad_grid) |> mutate(group = "High Complexity")

low_fit <-
  logistic_reg() |>
  fit(truth ~ ., data = ad_sub)

low_pred <- augment(low_fit, ad_grid) |> mutate(group = "Low Complexity")

set.seed(12)
mid_fit <-
  discrim_flexible(num_terms = 4, prod_degree = 2, prune_method = "none") |>
  set_engine("earth") |>
  fit(truth ~ ., data = ad_sub)

mid_pred <- augment(mid_fit, ad_grid) |> mutate(group = "Medium Complexity")

all_pred <-
  bind_rows(low_pred, mid_pred, high_pred) |>
  rename(probability = .pred_A) |>
  mutate(group = factor(group, levels = c("Low Complexity", "Medium Complexity",  "High Complexity")))
```

We often conceptualize classification models by the type of class boundary they produce. For example, in @fig-cls-boundaries, two predictors are visualized, and the colors and shapes of the data indicated their class memberships. The class boundaries are visualized as black lines, dividing the data into two (or more) regions where the model predicted a specific class. When there are more than two predictors, visualizing this boundary becomes impossible, but we still use the idea of a line that demarcates different regions that have the same (hard) class prediction. 

The low-complexity model attempts to bisect the two classes using a simple straight line. Points to the northwestern side of the line are classified as rose-colored triangles, and those on the opposite side are green circles. The background colors in the plot represent the estimated probabilities. This model underperforms since there are a high number of triangles on the wrong side of the line. The medium complexity model divides the data using a boundary that has three straight line segments. These linear segments result in a nonlinear boundary function. It also does not do a very good job of predicting the data, but it has adapted to the data more than the simple linear boundary. The high complexity boundary uses the same underlying model as the medium complexity fit, but a tuning parameter was increased to allow the model to adapt more to the training data (perhaps a little too much). There are four distinct regions that are produced by these boundaries. Unfortunately, the model is moderately overfitting the training set to these data. The tuning parameter in question should be better tuned to balance complexity and overfitting. 

```{r}
#| label: fig-cls-boundaries
#| echo: false
#| out-width: 100%
#| fig-width: 11
#| fig-height: 5
#| dev: "ragg_png"
#| fig-cap: Three different levels of complexity for class boundaries where the data has two predictors and two classes. 

all_pred |>
  ggplot(aes(x_1, x_2)) +
  scale_fill_gradient2(
    low = "#A3AD62FF",
    mid = "white",
    high = "#DF91A3FF",
    midpoint = 1 / 2,
    limits = 0:1,
    breaks = (1:3) / 4
  ) +
  geom_point(
    data = ad_sub |> filter(truth == "B"),
    aes(col = truth, pch = truth),
    cex = 2,
    alpha = 2 / 4
  ) +
  geom_point(
    data = ad_sub |> filter(truth == "A"),
    aes(col = truth, pch = truth),
    cex = 2,
    alpha = 3 / 4
  ) +
  scale_color_manual(values = c("#798234FF", "#D46780FF")) +
  scale_shape_manual(values = c(16, 17)) +
  coord_fixed(ratio = 1) +
  geom_contour(aes(z = probability),
               breaks = 1 / 2,
               col = "black",
               linewidth = 1) +
  geom_tile(aes(fill = probability), alpha = 1 / 6) +
  facet_wrap(~ group) +
  theme(legend.position = "top") +
  labs(x = "Predictor 1", y = "Predictor 2")
```

This chapter focuses on models that, at first appearance, produce strictly linear class boundaries. Apart from feature engineering, they tend to be relatively easy, fast to train, and are more likely to be interpretable due to their simplicity. We’ll start by discussing the most used classification model: logistic regression. This model has many important aspects to explore, as well as numerous ways to estimate model parameters. An extension of this model for more than two classes, a.k.a. multinomial regression, is also described. Finally, we review a fairly antiquated classification model (discriminant analysis) that will lead to some effective generalizations in the next chapter. 

However, before diving into modeling techniques, let’s take a deep look at the Washington State forestation data originally introduced in @sec-spatial-splitting. These data will be used to demonstrate the nuances of different classifications from this chapter through @sec-cls-ensembles.

## Exploring Forestation Data  {#sec-forestation-eda}

These data have been discussed in Sections [-@sec-spatial-splitting], [-@sec-spatial-resampling], and [-@sec-unsupervised-selection]. As a refresher, locations in Washington state were surveyed, and specific criteria were applied to determine whether they were sufficiently forested. Using predictors on the climate,  terrain, and location, we want to accurately predict the probability of forestation at other sites within the state. 

As previously mentioned, this type of data exhibits spatial autocorrelation, where objects close to each other tend to have similar attributes. This is not a book specific to spatial analysis; ordinary machine learning tools will be used to analyze these data. Our analyses might be, to some degree, suboptimal for the task. However, for our data, @FIA2015 describes the sampling methodology, in which the on-site inspection locations are sampled from within a collection of 6,000-acre hexagonal regions. While spatial autocorrelation is very relevant, the large space between these points may reduce the risk of using spatially ignorant modeling methodologies. Fortunately, our data spending methodologies _are_ spatially aware, and these might further mitigate any issues caused by ordinary ML models. For example, when initially splitting (and resampling them), recall that we used a buffer to add some space between the data used to fit the model and those used for assessment (e.g., Figures [-@fig-forested-split] and [-@fig-forested-blockcv]). This can reduce the risk of ignoring the autocorrelation when estimating model parameters. 

To learn more about spatial machine learning and data analysis, @kopczewska2022spatial is a nice overview. We also recommend @nikparvar2021machine, @kanevski2009machine, and @cressie2015statistics.

```{r}
#| label: vert-forest
#| include: false
#| cache: true

vert_forest <-
  forested_train |>
  pivot_longer(
    cols = c(-class, -county),
    names_to = "Predictor",
    values_to = "value"
  ) |>
  full_join(name_key |> rename(Predictor = variable), by = "Predictor") |>
  mutate(
    text = ifelse(is.na(text), Predictor, text),
    Predictor = tools::toTitleCase(text)
  ) |>
  select(-text)

num_unique <-
  vert_forest |>
  select(-class, -county) |>
  summarize(
    num_vals = min(21, vctrs::vec_unique_count(value)),
    .by = c(Predictor)
  )

percentiles <-
  vert_forest |>
  full_join(num_unique, by = "Predictor") |>
  group_nest(Predictor, num_vals) |>
  mutate(
    pctl = map2(
      data,
      num_vals,
      ~ tibble(group = ntile(.x$value, n = .y), class = .x$class)
    ),
    mid = map_dbl(data, ~ median(.x$value)),
  ) |>
  select(-data) |>
  unnest(c(pctl)) |>
  mutate(pctl = (group - 1) / num_vals * 100) |>
  summarize(
    events = sum(class == "Yes"),
    total = length(class),
    .by = c(Predictor, pctl)
  ) |>
  mutate(
    prop_obj = map2(events, total, ~ tidy(binom.test(.x, .y, conf.level = 0.9)))
  ) |>
  select(-events) |>
  unnest(c(prop_obj))

obs_rates <-
  forested_train |>
  summarize(
    rate = mean(class == "Yes"),
    events = sum(class == "Yes"),
    total = length(class),
    .by = c(county)
  ) |>
  mutate(
    hstat = map2(events, total, binom.test, conf.level = 0.9),
    pct = rate * 100,
    .lower = map_dbl(hstat, ~ .x$conf.int[1]),
    .upper = map_dbl(hstat, ~ .x$conf.int[2]),
    county = tools::toTitleCase(gsub("_", " ", county)),
    county = factor(county),
    county = reorder(county, rate),
    `# Locations` = total
  )
```

As with any ML project, we conduct preliminary exploratory data analysis to determine whether any data characteristics might affect how we model them. @tbl-forested-numeric has statistical and visual summaries of the `r ncol(forested_train) - 2` numeric predictors using the training data. Several of the predictors exhibit pronounced skew (right or left leaning). By coercing the distributions of some predictors to be more symmetric, we might gain robustness and perhaps an incremental improvement in performance (for some models). 

Also, the annual minimum temperature, dew temperature, January minimum temperature, and maximum vapor show bimodality in the distributions. The year of inspection is also interesting; the data collection was sparse before 2011, and subsequent years contain a few hundred data points per year before beginning to drop off in 2021. These characteristics are not indicative of problems with data quality, but it can be important to know that they exist when debugging why a model is underperforming or showing odd results.

::: {#tbl-forested-numeric}

```{r}
#| label: forested-numeric
#| echo: false

vert_forest |>
  summarize(
    Minimum = min(value),
    Mean = mean(value),
    Max = max(value),
    `Std. Dev` = sd(value),
    Skewness = e1071::skewness(value),
    Distribution = list(value),
    .by = c(Predictor)
  ) |>
  arrange(Predictor) |>
  gt() |>
  fmt_number(columns = c(-Distribution), n_sigfig = 3) |>
  gt_plt_dist(
    Distribution,
    type = "histogram",
    same_limit = FALSE,
    line_color = "#222",
    fill_color = "white"
  ) |>
  cols_width(Predictor ~ pct(25))

```

Histograms and statistical summaries of the numeric predictors in the Washington State training set. 

:::

Recall that @fig-corr-plot previously described the correlation structure of these predictors. There were several clusters of predictors with strong magnitudes of correlation. This implies that there is some redundancy of information in these features. However, machine learning is often "a game of inches," where even redundant predictors can contribute incremental improvements in performance. In any case, the analyses in this chapter will be profoundly affected by this characteristic; it will be investigated in in more detail below. 

Individually, how does each of these features appear to relate to the outcome? To assess this, we binned each predictor into roughly 20 groups based on percentiles and used these groups (each containing about `r floor(mean(percentiles$total))` locations) to compute the rate of forestation and 90% confidence intervals^[Binning is used here as a visualization tool; we re-emphasize that converting numeric predictors into categorical features is problematic.]. As an exploratory tool, we can use these binned versions of the data to see potential relationships with the outcome. @fig-forest-percentiles shows the profiles. Note that there is enough data in each bin to make the confidence intervals very close to the estimated rates. 

```{r}
#| label: fig-forest-percentiles
#| echo: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 6
#| fig-cap: Binned rates of forestation over percentiles of the numeric predictors. The shaded regions are 90% confidence intervals.

percentiles |>
  ggplot(aes(pctl, estimate)) +
  geom_line() +
  geom_ribbon(
    aes(ymin = conf.low, ymax = conf.high),
    alpha = 1 / 5,
    fill = "#D85434FF"
  ) +
  facet_wrap(~Predictor, ncol = 3) +
  labs(x = "Percentile", y = "Rate of Forestation")
```

Quite a few predictors show considerable nonlinear trends, and a few are not monotonic (i.e., the sign of the slope changes over the range of values). The Eastness and Northness features, which capture the landscape orientation at the location, show flat trends. This means that these predictors are less likely to be important. However, once in a model with other predictors, the model may be able to extract some utility from them, perhaps via interaction terms. The primary takeaway from this visualization is that models that are able to express nonlinear trends will probably do better than those restricted to linear classification boundaries. 

In addition to longitude and latitude, the data contains a qualitative location-based predictor: the county in Washington. There are data on `r nrow(obs_rates)` counties. The number of locations within each county can vary with `r obs_rates$county[which.min(obs_rates$total)]` county having the least training set samples (`r min(obs_rates$total)`) and `r obs_rates$county[which.max(obs_rates$total)]` having the most (`r max(obs_rates$total)`). @fig-counties shows how the rate of forestation changes and the uncertainty in these estimates. Several counties in the training set have no forested locations. Given the number of counties and their varying frequencies of data, an effect encoding strategy might be appropriate for this predictor. 

```{r}
#| label: fig-counties
#| echo: false
#| out-width: 40%
#| fig-width: 4
#| fig-height: 6
#| fig-cap: Outcome rates for different counties in Washington State.

obs_rates |>
  ggplot(aes(y = county, col = `# Locations`)) +
  geom_point(aes(x = rate)) +
  geom_errorbar(aes(xmin = .lower, xmax = .upper)) +
  labs(x = "Rate of Forestation", y = NULL) +
  theme(legend.position = "top") +
  scale_color_viridis_c(option = "mako", begin = .2, end = .8)
```

Finally, it might be a good idea to assess potential interaction effects prior to modeling. Since almost all of our features are numeric, it can be difficult to assess interactions visually, so the H-statistics for two-way interactions were calculated using a boosted tree as the base model using the numeric features. Since there are only `r choose(15, 2)` possible interactions, the H-statistics were recomputed 25 times using different random number generators so that we can compute a mean H-statistic and its associated standard error. 
 
```{r}
#| label: fig-forested-interactions
#| echo: false
#| out-width: 90%
#| fig-width: 8
#| fig-height: 4.5
#| fig-cap: Results for the top 25 H-statistic interactions. The error bars are 90% intervals based on replicate computations.

forested_hstats_text |>
  slice_max(mean_score, n = 25) |>
  ggplot(aes(y = term)) +
  geom_point(aes(x = mean_score)) +
  geom_errorbar(
    aes(xmin = .lower, xmax = .upper),
    alpha = 1 / 2,
    width = 1 / 2
  ) +
  labs(x = "Mean H-Statistic", y = NULL)
```

The vast majority of the `r choose(15, 2)`  H-statistics are less than `r signif(quantile(forested_hstats_text$mean_score, probs = 0.95), 3)` and there are a handful of interactions that are greater than that value; we’ll take the top five interactions and use them in a logistic regression shown below.

The next section will describe a mainstay of machine learning models for two classes: logistic regression. 

## Logistic Regression {#sec-logistic-reg}

Logistic regression is a classification model that can be used when there are $C = 2$ classes. As with all binary classification models, we want to accurately estimate the probability of an event^[We'll assume that class $j = 1$ is the class that is considered the event, for $j = 1, \ldots, C$ classes.]. As discussed in the previous chapter, the probability parameter $\pi$ should be between zero and one, and we often frame the problem via the Bernoulli distribution^[We often talk about logistic regression as being driven by a **Binomial** distribution. Either distribution can be used for this model, but the Binomial is used for _grouped_ data where each row of the data set has $n_i$ "trials" and $r_i$ "successes". In ML, we are not usually in this situation and have a single outcome result per row, hence why we use the Bernoulli (where $n_i = 1$).] (i.e., $y \sim Bernoulli(\pi)$). The likelihood function is:  

$$
\ell(\pi_i; y_{ij}) = \prod_{i=1}^{n_{tr}} \pi_i^{y_{i1}} (1-\pi_i)^{y_{i2}}
$$ {#eq-bernoulli}

where $\pi_i$ is the theoretical probability of an event for sample $i$ ($i = 1, \ldots, n_{tr}$) and $y_{ic}$ is the binary indicator for class $c$. As written above, each row of the data has its own probability estimate. This is not useful since it does not attribute changes in the probabilities to any particular feature.

Generally, we have a tabular data set of predictors that we want to affect $\pi_i$ in some effective and understandable way. One pattern that we've seen in previous chapters is a linear combination of parameters for the $p$ predictors: 

$$
\boldsymbol{x}_i'\boldsymbol{\beta} = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}
$$ {#eq-linear-predictor}

This is often referred to as the "linear predictor" and is commonly denoted as $\eta_i$ The range of this value is [-$\infty$, $\infty$], so we can't directly model the probability of the event using this equation as it would easily exceed the natural boundaries for $\pi_i$. One approach is to embed the linear predictor inside a nonlinear function. The **logistic model** takes the form: 

$$
\pi_i = \frac{1}{1 + \exp(-\boldsymbol{x}_i'\boldsymbol{\beta})}
$$ {#eq-logistic}

This keeps the probability values within [0, 1] no matter how large or small the linear predictor becomes. This nonlinear function can be derived using differential equations for growth curves. Although the model has been rediscovered several times, it was first derived in the 1800s and was not commonly known as the "logistic" model until 1925. See @cramer2004early for a history. 

```{r}
#| label: longitude-fit
#| include: false

linear_long_fit <-
  logistic_reg() |>
  fit(class ~ longitude, data = forested_train)

linear_long_coef <-
  linear_long_fit |>
  extract_fit_engine() |>
  coef()
# glm models the probability of the second factor level
linear_long_coef <- -linear_long_coef
linear_long_coef <- format(linear_long_coef, digits = 3)

iter_text <- cli::format_inline("{linear_long_fit$fit$iter} iteration{?s}")
```

There are various ways to find parameter estimates, which will be discussed in subsequent subsections. The most basic method is maximum likelihood estimation, where we use a numerical optimization method to minimize the loss function, which is the negative log-likelihood:

$$
-\log \ell(\pi_i; y_{ij}) = \sum_{i=1}^{n_{tr}} \left[ y_{i1}\log(\pi_i) + (1- y_{i1}) \log(1-\pi_i)\right]
$$ {#eq-bernoulli-log-lik}

The log-likelihood is more numerically stable to work with, and it is generally not challenging to find maximum likelihood estimates (MLEs) of our $\boldsymbol{\beta}$ parameters. @glm describes an effective gradient-based method for estimating parameters. Their method, called iteratively reweighted least squares (IRLS), minimizes the sum of weighted residuals, where the weights are defined by a function of the data’s variance. This method tends to converge very quickly for logistic regression^[In fact, as we'll see shortly, their theory is not limited to logistic regression; it encompasses linear regression, Poisson regression, and other models.] and has theoretical guarantees. 

Overall, for logistic regression, maximum likelihood estimation generates a relatively stable estimator. While it can be sensitive to outliers and high influence points [@pregibon1981logistic], a limited amount of data jittering or omissions will not widely affect the parameter estimates unless the training set is fairly small. We’ll see an exception below in @sec-logistic-multicollinearity,  where specific characteristics of the _training data_ make the estimators unstable.   

Statistically, the MLEs are estimates that best explain the data. They are also unbiased estimates of the true parameters^[Recall from @sec-variance-bias that unbiased estimates might have high variance. For the forestation data, we'll see that some of our logistic regression parameter estimates have irrationally large variances. Later in this chapter, biased methods (via regularization) are also discussed.].

To illustrate the model, let’s use the training data to model the probability of forestation as a function of the location’s longitude; the logit function has just simple slope and intercept parameters. Using the entire training set, we solved the log-likelihood equations (based on @eq-bernoulli-log-lik) to estimate the parameters. The optimization converges after `r iter_text`, producing the linear predictor function:  

$$
\boldsymbol{x}_i'\boldsymbol{\beta} = `r linear_long_coef[1]`  `r linear_long_coef[2]`\: longitude_i
$$ {#eq-longitude-logistic}

@fig-longitude shows this pattern using the longitude predictor in the forestation data in green. The data points shown are the binned data from @fig-forest-percentiles; the model is actually fit on the individual locations in the training set (i.e., not using the binned version in the figure). 

Why does the rate swing up and down? Looking back at @fig-forested, we see that most of the initial western portion of the state is forested. However, Seattle is adjacent to the water of Puget Sound; this accounts for the area of nonforested locations around longitudes of about -122.3°. Further east of Seattle is mostly forested.  East of this lies a large swath of unforested land (especially in the mid to south east). This is reflected in the drop in the rates from about -120° to -118.5°. In the easternmost section of the state, there is a region of forested land to the north, accounting for the rate increase at the right-hand side of @fig-longitude.  

::: {#fig-longitude}

::: {.figure-content}

```{shinylive-r}
#| label: fig-longitude
#| out-width: "80%"
#| viewerHeight: 600
#| standalone: true

library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(parsnip)
library(mgcv)
library(splines2)
library(scales)

light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg,
  fg = "#595959"
)

theme_light_bl <- function(...) {
  ret <- ggplot2::theme_bw(...)

  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background <- col_rect
  ret$plot.background <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key <- col_rect

  ret$legend.position <- "top"

  ret
}

# ------------------------------------------------------------------------------

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-1, 5, 5, -1)),
    column(
      width = 4,
      checkboxGroupInput(
        inputId = "include",
        label = "Include",
        choices = list(
          "Binned Data" = "Data",
          "Untransformed" = "fit_linear",
          "Splines" = "fit_spline",
          GAM = "fit_gam"
        ),
        selected = c("fit_linear", "Data"),
        inline = TRUE
      )
    ),
    column(
      width = 4,
      radioButtons(
        inputId = "yaxis",
        label = "y-axis",
        choices = c("Event Rate" = "rate", "Logit" = "logit"),
        inline = TRUE
      )
    )
  ),
  as_fill_carrier(plotOutput("plot"))
)

server <- function(input, output) {
  load(url(
    "https://raw.githubusercontent.com/aml4td/website/main/RData/forested_data.RData"
  ))

  lng_bin <-
    forested_train |>
    dplyr::select(class, longitude) |>
    mutate(lng_bin = ntile(longitude, 21)) |>
    dplyr::summarize(
      rate = mean(class == "Yes"),
      lng = median(longitude),
      n = length(longitude),
      .by = c(lng_bin)
    ) |>
    mutate(logit = binomial()$linkfun(rate))

  lng_rng <- extendrange(lng_bin$lng)
  # lng_rng[1] <- 0.0

  lng_grid <- tibble(longitude = seq(lng_rng[1], lng_rng[2], length.out = 100))

  linear_fit <-
    logistic_reg() |>
    fit(class ~ longitude, data = forested_train)

  linear_pred <-
    augment(linear_fit, new_data = lng_grid) |>
    mutate(Model = "Linear Term", group = "fit_linear")

  num_spline <- 10
  spline_fit <-
    logistic_reg() |>
    fit(
      class ~ naturalSpline(longitude, df = num_spline),
      data = forested_train
    )

  spline_lab <- paste0("Natural Splines (", num_spline, " df)")

  spline_pred <-
    augment(spline_fit, new_data = lng_grid) |>
    mutate(Model = spline_lab, group = "fit_spline")

  gam_fit <-
    gen_additive_mod() |>
    set_mode("classification") |>
    fit(class ~ s(longitude), data = forested_train)
  gam_lab <- paste0("GAM (", round(sum(gam_fit$fit$edf[-1]), 1), " df)")

  gam_pred <-
    augment(gam_fit, new_data = lng_grid) |>
    mutate(Model = gam_lab, group = "fit_gam")

  predictions <-
    bind_rows(linear_pred, spline_pred, gam_pred) |>
    mutate(
      Model = factor(Model, levels = c("Linear Term", spline_lab, gam_lab))
    ) |>
    mutate(logit = binomial()$linkfun(.pred_Yes))

  output$plot <-
    renderPlot(
      {
        if (input$yaxis == "rate") {
          p <-
            lng_bin |>
            ggplot(aes(lng)) +
            labs(x = "Longitude", y = "Probability of Forestation") +
            lims(y = 0:1)
        } else {
          p <-
            lng_bin |>
            ggplot(aes(lng)) +
            labs(x = "Longitude", y = "Logit")
        }

        if (any(input$include == "Data")) {
          if (input$yaxis == "rate") {
            p <- p + geom_point(aes(y = rate), alpha = 1 / 3, cex = 3)
          } else {
            p <- p + geom_point(aes(y = logit), alpha = 1 / 3, cex = 3)
          }
        }

        if (any(grepl("fit_", input$include))) {
          curve_data <- dplyr::filter(predictions, group %in% input$include)

          if (input$yaxis == "rate") {
            p <- p +
              geom_line(
                data = curve_data,
                aes(x = longitude, y = .pred_Yes, color = Model),
                linewidth = 1
              )
          } else {
            p <- p +
              geom_line(
                data = curve_data,
                aes(x = longitude, y = logit, color = Model),
                linewidth = 1
              )
          }

          p <- p +
            theme(legend.position = "top") +
            scale_color_brewer(drop = FALSE, palette = "Dark2")
        }

        p <- p  + theme_light_bl()
        print(p)
      },
      res = 100
    )
}

app <- shinyApp(ui = ui, server = server)
app
```

:::

An example of a logistic regression model with a single predictor (longitude). The binned data points are the same as those shown in @fig-forest-percentiles and are for visualization purposes only; the models are trained on the individual rows in the training set. 

:::

The model fit initially underestimates the rate of forestation for the most western locations, and the predicted probability slowly decreases in a fairly flat sigmoid. Given the ups and downs of the observed event rates over longitude, this model fit misses almost all of the nuance in the data.  The data trend is not monotonically decreasing, so our simple linear predictor is doing its best but is not flexible enough to emulate what is actually occurring. 

#### Generalized Linear Models {.unnumbered}

Logistic regression belongs to a broader class of _generalized linear models_, or GLMs [@glm]. These models can be used with different types of outcomes, such as real-valued outcomes or integer counts. The idea is to model a function of the outcome using the linear predictor. In our current case, the _logit_ function (the inverse of @eq-logistic) is applicable: 

$$
log\left(\frac{\pi_i}{1 - \pi_i}\right) = \eta_i = \boldsymbol{x}_i'\boldsymbol{\beta}
$$ {#eq-logit}

This nonlinear function is called the *link function*, and there are often multiple functions that could be used. Technically, logistic regression uses the logit link, but there are many other functions to choose from. The probit and complementary log-log transformations can be used [@glm] as well as many others. See @morgan2018analysis for more specialized "link functions." From a broader view, we might refer to our model as "binary regression." 

Looking back to @fig-longitude, we can change the y-axis data and show how the logit changes for our fitted model. The line visualizes the estimated linear predictor for these data. Sadly, when the representation of our data is similarly changed, the data pattern is not linear; our linear model isn't effectively emulating the nonlinear pattern in the data. 

There are significant benefits to framing logistic regression inside the class of generalized linear models. First, it gives us some nice theoretical guarantees about the likelihood function, allowing us to use simple gradient-based optimization methods to optimize the log-likelihood. Second, it provides an inferential framework for our parameters. This enables us to understand how important our predictors are by way of hypothesis tests and confidence intervals. 

It's easy to take these benefits for granted. Suppose we were to find parameter estimates using some other log function, such as the Brier score or the area under the ROC curve. In that case, it might be difficult to solve the required equations, let alone derive the methods to make inferences regarding parameters. 

::: {.important-box}
Before proceeding further, let's talk about the linear predictor. The "linear" in "linear predictor" and "generalized linear model" means that the model is _linear in the parameters_ $\boldsymbol{\beta}$. This does **not** mean that the model can only produce linear class boundaries. Our model terms $x_{ij}$ can represent any function of one or more parameters. We can use the feature engineering tools shown in Chapters [-@sec-numeric-predictors] through [-@sec-interactions-nonlinear] to create a better model. 
:::

For example, in @fig-longitude, we saw that using the longitude values as our single predictor $x$ didn't work well. When plotted on the logit scale, there is a clear nonlinear function in the binned rates. We know how to estimate a general nonlinear function of a feature: spline functions. @fig-longitude can also show the results when ten natural spline features are used in the model. Visually, we can see that this expansion is far more effective at predicting the outcome.

Logistic regression models can be drastically improved by including better representations of our predictors. Since it is an interpretable and stable model, we might be motivated to spend more time developing this model via exploratory data analysis and feature engineering. In many teaching materials and websites, we see analyses that quickly label the results of a logistic model as ineffective (due to the use of simple model terms) before moving on to more black-box machine-learning methods. The virtue of learning about your data and how the predictors related to the outcome can not only improve the logistic model but enable a more complete understanding and description of _why_ it works. 

```{r}
#| label: forest-logistic-elements
#| include: false

bare_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_dummy(county) |>
  step_zv(all_predictors())

transformed_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_dummy(county) |>
  step_zv(all_predictors())

forest_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_lencode_mixed(county, outcome = "class")

forest_int_rec <-
  forest_rec |>
  step_interact(!!forested_int_form)

forest_spline_rec <-
  forest_int_rec |>
  step_spline_natural(
    all_numeric_predictors(),
    -county,
    -eastness,
    -northness,
    -year,
    -contains("_x_"),
    deg_free = 10
  ) |>
  step_lincomb(all_predictors())

forest_set <-
  workflow_set(
    preproc = list(
      simple = bare_rec,
      "encoded + transformations" = transformed_rec,
      encoded = forest_rec,
      "encoded + interactions" = forest_int_rec,
      "encoded + transformations + splines" = forest_spline_rec
    ),
    model = list(logistic_mle = logistic_reg())
  )

cls_mtr <- metric_set(brier_class, roc_auc, pr_auc, mn_log_loss)
```

```{r}
#| label: forest-logistic-run
#| include: false
#| cache: true
forest_res <-
  forest_set |>
  workflow_map(
    fn = "fit_resamples",
    resamples = forested_rs,
    control = control_resamples(save_pred = TRUE),
    metrics = cls_mtr,
    seed = 693,
    verbose = FALSE
  )

set.seed(365)
brier_perm <-
  forest_res |>
  extract_workflow_set_result("simple_logistic_mle") |>
  collect_predictions(summarize = TRUE) |>
  permutations(permute = "class", times = 50) |>
  mutate(
    data = map(splits, analysis),
    brier = map_dbl(data, ~ brier_class(.x, class, .pred_Yes)$.estimate)
  ) |>
  summarize(
    permutation = mean(brier),
    n = length(brier),
    std_err = sd(brier) / sqrt(n)
  ) |>
  mutate(
    lower = permutation - qnorm(.95) * std_err,
    upper = permutation + qnorm(.95) * std_err
  )

glm_best <-
  forest_res |>
  rank_results(rank_metric = "brier_class") |>
  filter(.metric == "brier_class") |>
  slice_min(mean, n = 1)

set.seed(799)
glm_best_int <-
  forest_res |>
  extract_workflow_set_result(glm_best$wflow_id) |>
  int_pctl(times = 2000, metrics = cls_mtr)
```

#### Forestation Model Development {.unnumbered}

We can assess each of these feature engineering steps by fitting logistic regression pipelines that sequentially add steps. Our five models are:

 1. A **simple** preprocessor that only converts the `r length(levels(forested_train$county))` counties to binary indicators and removes potential zero-variance predictors. 
 2. We can add a **normalization** step that transforms the numeric predictors using the ORD transformation. 
 3. We'll replace the county code dummy variable operation in model #2 with an effect **encoding** step.
 4. We can add our potential **interactions** to to model #3. 
 5. Finally, we can supplement our interaction pipeline with ten **spline** terms for the previously listed predictor set. 

For each of these preprocessing/model configurations, we’ll resample the training set and estimate several performance metrics (but will focus on the Brier score). 

We can see the resampling results in @fig-forest-logistic. It shows that most steps incrementally reduce the Brier score. One of the largest drops occurs when we avoid making many county indicators and use an effect encoding instead. The normalization step is estimated to help, but the error bars indicate that this improvement might be within the statistical noise of resampling. The end result is a 1/3 reduction in the Brier score by adding better representations of our predictors^[We’ll discuss regularized models shortly, but in practice, we might _start_ with a complex model (such as Model #5) and use a penalized model to determine which model terms to keep. We’ll do exactly that in the next section.]. 

```{r}
#| label: fig-forest-logistic
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 3
#| fig-cap: Cross-validated Brier scores for several stages of feature engineering of the forestation data using basic logistic regression. The red dashed line represents the permutation estimate of the no-information rate. The error bars are 90% confidence intervals based on the naive standard error estimates produced during resampling.

forest_res |>
  rank_results(rank_metric = "brier_class") |>
  filter(.metric == "brier_class") |>
  mutate(
    pipeline = gsub("_logistic_mle", "", wflow_id),
    pipeline = factor(pipeline),
    pipeline = reorder(pipeline, -mean),
    ci_pm = qnorm(0.95) * std_err
  ) |>
  ggplot(aes(mean, pipeline)) +
  geom_vline(xintercept = brier_perm$permutation, col = "red", lty = 2) +
  geom_vline(xintercept = 0, col = "green", lty = 2) +
  geom_point() +
  geom_errorbar(aes(xmin = mean - ci_pm, xmax = mean + ci_pm), width = 1 / 3) +
  labs(x = "Brier Score", y = NULL)
```

```{r}
#| label: logistic-spline-mtr
#| include: false
logistic_mtr <-
  forest_res |>
  extract_workflow_set_result(
    "encoded + transformations + splines_logistic_mle"
  ) |>
  collect_metrics(summarize = TRUE)
```

Along with the Brier score values of `r signif(logistic_mtr$mean[logistic_mtr$.metric == "brier_class"], 3)`, the spline pipeline (i.e., Model #5) had the following resampling estimates: ROC AUC = `r signif(logistic_mtr$mean[logistic_mtr$.metric == "roc_auc"], 3)`, a PR AUC = `r signif(logistic_mtr$mean[logistic_mtr$.metric == "pr_auc"], 3)`, and a cross-entropy = `r signif(logistic_mtr$mean[logistic_mtr$.metric == "mn_log_loss"], 3)`. During resampling, there were `r logistic_mtr$n[1]` resamples, leading to `r logistic_mtr$n[1]` ROC curves, and so on. We can pool the `r logistic_mtr$n[1]` sets of assessment set predictions and produce _approximate_ ROC and calibration curves. These are shown in @fig-forest-logistic-diag. The calibration curve indicates that the model reliably predicts samples with accuracy  The ROC curve shows good separation between classes but is otherise unremarkable. 

```{r}
#| label: fig-forest-logistic-diag
#| echo: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 4
#| fig-cap: Calibration and ROC curves for the pooled assessment sets for the logistic regression model and the full preprocessing pipeline (e.g. up to spline terms). The calibration plot used a moving window, where windows that span 10% of the data are computed with moving increments of 2%.

logistic_pred <-
  forest_res |>
  extract_workflow_set_result(
    "encoded + transformations + splines_logistic_mle"
  ) |>
  augment()

logitic_cal <- cal_plot_windowed(
  logistic_pred,
  class,
  .pred_Yes,
  step_size = 0.02
)
logitic_roc <-
  logistic_pred |>
  roc_curve(class, .pred_Yes) |>
  ggplot(aes(1 - specificity, sensitivity)) +
  geom_abline(col = "red", lty = 2) +
  geom_step(direction = "vh") +
  coord_obs_pred()

logitic_cal + logitic_roc
```

From here, we should investigate which terms had the greatest influence on the model. Were the 10 interactions that we identified important or statistically significant? Should we have added more or less? We might plot the predicted values to see if there are specific geographic locations that corresponded to the poorly calibrated values. Perhaps we could also refine the model by using a supervised feature filter, and so on. 

### Examining the Model {#sec-logistic-diagnostic}

There are many ways to appraise the model. One (unfortunate) focus is often on the statistical significance of the model terms. If the model’s purpose is solely for interpretation, this is understandable. For ML applications, p-values are not particularly insightful since they don’t necessarily correlate with predictive performance. This is especially true for our logistic model, which contains numerous spline terms; knowing whether the fourth spline term has a small p-value probably isn’t insightful. However, we’ll return to the model inferential capacity shortly as a diagnostic tool. 

Our suggestion is to assess the model’s quality by using the held-out predictions to conduct *residual analysis* by finding systematic patterns in those values. The most basic residual is the simple difference in the "observed" and predicted probabilities: $e_{ic} = y_{ic} - \widehat{p}_{ic}$. The issue with these values is that the variance of the probability estimates changes over their [0,1] range. A residual of 0.1 has a different context when $\widehat{p}_i = 0.01$ compared to $\widehat{p}_i = 0.5$.  To normalize these values, we can use _Pearson residuals_, which are $e_{ic}$  divided by the standard deviation: 

 $$
 e^p_{ic} = \frac{y_{ic} - \widehat{p}_{ic}}{\widehat{p}_{ic} (1 - \widehat{p}_{ic})}.
 $$

Alternatively, we can look at _deviance residuals_. The deviance is a likelihood-based comparison between two models: our current model and one that is _saturated_ where the model perfectly fits the training data. For logistic regression, these take the form: 

$$
e^d_{ic} = \text{sign}(e_{ic}) \left[-2(y _{ic}\, \text{log}(\widehat{p} _{ic}) + (1 - y _{ic})\, \text{log}(1 - \widehat{p}_{ic}))\right]^{1/2}
$$

See @pregibon1981logistic and  @agresti2015foundations for discussions.  

In our application, it would be relevant to plot residuals versus values of individual predictors. If we see systematic, non-random patterns, it might indicate that how we represent that predictor in the model is inadequate. For example, in @fig-longitude-resdiauls the deviance residuals are plotted against longitude. These residuals are computed with the holdout predictions for each resampling fold. 

The left-hand panel shows residuals for the model with simple linear terms for the predictors. For each class, we can see large spikes in residuals for longitude values corresponding to the previous ups and downs shown in @fig-longitude. It is interesting that this visualization shows different patterns for each class; they ar enot mirrored reflections of one another. For example, for the region between longitudes of approximately -120.5° and -118°, the unforested locations have very small residuals, and the forested locations have a spike.

```{r}
#| label: fig-longitude-resdiauls
#| fig-cap: "Deviance residuals plotted against longitude for models with and without spline terms for predictors (including longitude)."
#| out-width: "70%"
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| warning: false

logistic_resdiduals <-
  forest_res |>
  extract_workflow_set_result("encoded_logistic_mle") |>
  collect_predictions() |>
  mutate(Model = "No Splines") |>
  bind_rows(
    forest_res |>
      extract_workflow_set_result(
        "encoded + transformations + splines_logistic_mle"
      ) |>
      collect_predictions() |>
      mutate(Model = "Spline Terms")
  ) |>
  mutate(
    indicator = ifelse(class == "Yes", 1, 0),
    error = indicator - .pred_Yes,
    dev_1 = indicator * log(.pred_Yes),
    dev_2 = (1 - indicator) * log(1 - .pred_Yes),
    deviance = sign(error) * sqrt((-2 * (dev_1 + dev_2)))
  ) |>
  inner_join(forested_train |> add_rowindex() |> select(-class), by = ".row")

logistic_resdiduals |>
  ggplot(aes(longitude, deviance, col = class)) +
  geom_point(alpha = 1 / 2, cex = 1) +
  facet_wrap(~Model) +
  scale_color_manual(values = c("#218239", "#d4ad42")) +
  labs(x = "Longitude", y = "Deviance Residiual") +
  theme(legend.position = "top")
```

In contrast, the right-hand panel has many more residuals near zero and significantly attenuated spikes/patterns. This is indicative of an improvement of model fit. However, some points have absolute residuals _larger_ than the simpler model. Overall, though, the model with spline terms has fewer systematic patterns. 

Given the spatial nature of these data, mapping the residuals can be instructive. It could show us locations that tend to be poorly predicted. We can also determine the largest (absolute) residuals and investigate what might be different about these particular data points. 

Let's move on to consider how the logistic model can facilitate interpretaion. 

### Interpreting the Logistic Model {#logistic-interpretability}

```{r}
#| label: odds-calc
#| include: false
ref_level <- "yakima"
ref_cnty <- tools::toTitleCase(gsub("_", " ", ref_level))
other_level <- "skamania"
other_term <- paste0("county", other_level)
other_cnty <- tools::toTitleCase(gsub("_", " ", other_level))

releveled <- forested_train |>
  mutate(county = relevel(county, ref = ref_level))

county_coef <-
  logistic_reg() |>
  fit(class ~ county, data = releveled) |>
  tidy(conf.int = TRUE, conf.level = 0.90)

ref_coef <-
  county_coef |>
  filter(term %in% c("(Intercept)")) |>
  mutate(
    coef = format(-estimate, digits = 3, scientific = FALSE),
    odds = format(exp(-estimate), digits = 3, scientific = FALSE),
    prob = binomial()$linkinv(-estimate),
    xin = format(1 / prob, digits = 2, scientific = FALSE),
    prob = format(prob * 100, digits = 3)
  )

other_coef <-
  county_coef |>
  filter(term %in% c(other_term)) |>
  mutate(
    coef = format(-estimate, digits = 3, scientific = FALSE),
    .upper = format(exp(-conf.low), digits = 3, scientific = FALSE),
    .lower = format(exp(-conf.high), digits = 3, scientific = FALSE),
    odds = format(exp(-estimate), digits = 3, scientific = FALSE),
    prob = binomial()$linkinv(-estimate),
    xin = format(1 / prob, digits = 2, scientific = FALSE),
    prob = format(prob * 100, digits = 3)
  )

other_change_coef <-
  county_coef |>
  filter(term %in% c("(Intercept)", other_term)) |>
  summarize(estimate = sum(estimate)) |>
  mutate(
    coef = format(-estimate, digits = 3, scientific = FALSE),
    odds = format(exp(-estimate), digits = 3, scientific = FALSE),
    prob = binomial()$linkinv(-estimate),
    xin = format(1 / prob, digits = 2, scientific = FALSE),
    prob = format(prob * 100, digits = 3)
  )
```

The logit function has a meaningful interpretation since it contains the _odds_ of an event occurring. Let's consider a simple case where we are modeling the change by county. If we use `r ref_cnty` county as the reference cell (recall @sec-indicators), the intercept of a logistic model is `r ref_coef$coef`. This corresponds to a probability of `r ref_coef$prob`%. The odds of some event that occurs with probability $\pi$ is 

$$
odds = \pi / (1-\pi)
$$ 

or "one in $1/\pi$." For `r ref_cnty`, this relates to the logistic regression model since

$$
log\left(\frac{Pr[\text{`r ref_cnty` forested}]}{Pr[\text{`r ref_cnty` unforested}]}\right) = log\left(\text{`r ref_cnty` odds}\right) = \beta_0
$$

Therefore, the odds of a tree being forested in this county are _exp_(`r ref_coef$coef`) = `r ref_coef$odds` or, one in about `r ref_coef$xin` locations. 

Alternatively, consider Skamania County, which contains  Gifford Pinchot National Forest. It's model coefficient is `r other_coef$coef` However, recall that with a reference cell encoding, this is the change in the logit relative to the reference value (`r ref_cnty` County). If we are interested in the probability of forestation for Skamania, we have to look at the sum of both coefficients (`r other_change_coef$coef` logit units) which means that the probability of forestation estimated to be about `r other_change_coef$prob`%.

One way of assessing how much more likely an event is to occur is the odds ratio. For example, if we wanted to compute the odds ratio of forestation between `r ref_cnty` and Skamania counties, we can use different sets of coefficients from the linear predictor. Since the forestation rate is higher in Skamania than `r ref_cnty`, the odds ratio of interest is

$$
\frac{\text{Skamania odds}}{\text{`r ref_cnty` odds}}
$$

We know that 

$$
log\left(\text{Skamania odds}\right) = log\left(\frac{Pr[\text{Skamania forested}]}{Pr[\text{Skamania unforested}]}\right) = \beta_0 +\beta_1
$$

so that 

$$
\frac{\text{Skamania odds}}{\text{`r ref_cnty` odds}} = \frac{exp(\beta_0 +\beta_1)}{exp(\beta_0)} = exp(\beta_0 + \beta_1 - \beta_0) = exp(\beta_1)
$$

From our estimated coefficient, the odds of being forested in Skamania is `r other_coef$odds` times larger than in `r ref_cnty`. For each categorical predictor, we can use its model coefficient to compute odds ratios relative to the reference cell^[Recall that if there are multiple categorical predictors, the reference cell is multidimensional.]. Since we are using maximum likelihood estimation, we can also compute confidence intervals for this odds ratio; the 90% interval is (`r other_coef$.lower`, `r other_coef$.upper`), the lower bound being very far from a value of 1.0. 

For numeric predictors, such as eastness or elevation, exponentiating the coefficient gives us a _per unit_ odds ratio. Since the predictors might be in different units, the interpretation can be more complex depending on the situation^[This might be one factor that leads some people to discretize numeric predictors; they can compute odds ratios for "low" and "high" predictor ranges.].

In either case, the logistic regression model does have a structure that lends itself to natural probabilistic interpretation. See @hosmer2013applied for a broader discussion. 

```{r}
#| label: best-glm-fit
#| include: false

best_mle_wflow <-
  forest_set |>
  extract_workflow(id = "encoded + transformations + splines_logistic_mle")

best_mle_fit <-
  best_mle_wflow |>
  fit(forested_train)

best_mle_coef <-
  tidy(best_mle_fit) |>
  mutate(
    Type = case_when(
      grepl("(_10)|(_0[1-9])", term) ~ "Spline",
      grepl("_x_", term) ~ "Interaction",
      TRUE ~ "Main Effect"
    ),
    Type = factor(Type, levels = c("Spline", "Interaction", "Main Effect"))
  )

best_mle_vifs <-
  best_mle_fit |>
  extract_fit_engine() |>
  vif() |>
  tibble::enframe() |>
  set_names(c("term", "vif"))

best_mle_coef <- full_join(best_mle_coef, best_mle_vifs, by = "term")

best_mle_sig <-
  best_mle_coef |>
  mutate(signif = p.value <= 0.1) |>
  filter(term != "(Intercept)") |>
  summarize(
    sig_rate = round(mean(signif) * 100, 1),
    num = length(signif),
    .by = c(Type)
  )
```

```{r}
#| label: logistic-pdp
#| include: false
#| cache: true

explainer_logistic <-
  explain_tidymodels(
    best_mle_fit,
    data = forested_train |> select(-class),
    y = forested_train$class,
    label = "",
    type = "prob",
    verbose = FALSE
  )

set.seed(1805)
pdp_longitude <- model_profile(
  explainer_logistic,
  N = 1000,
  variables = "longitude"
)
```

Methods are also available to help explain a model or a specific prediction, independent of the type of model used. The field of **explainable machine learning** uses tools related to the H-statistic previously described in @sec-interactions-detection. These will be discussed in detail in @sec-explanations, but we’ll preview the technique here. 

We’ve used the longitude predictor to demonstrate potential nonlinear patterns with the outcome values. There definitely appears to be a complex pattern between longitude and the probability of forestation. However, when placed in a model with other predictors, how does longitude affect how the model predictions?

Partial dependence plots (PDP) are visualization tools to answer this question. Suppose we were to sample a single location $x_i$ from the training set (or any other set of data). We could keep all other predictors constant for that sampled data point and create a grid of possible values for the feature of interest. Using our model, we can create a _profile_ showing how the predictions change over these values. However, if we had sampled a different data point, the pattern probably would not be the same (unless the model was very simple). We can sample a large number of data points and use their profiles to compute the average effect of the predictor across its range of values. 

@fig-longitude-pdp shows a PDP for longitude from our most complex model (including interactions and splines). The 1,000 grey lines show the individual profiles, and the red curve shows the average effect of this predictor, in the presence of the other model terms.

```{r}
#| label: fig-longitude-pdp
#| fig-cap: "A partial dependency plot visualizing how longitude affects the probability when averaged over 1,000 random draws of the data. The red line shows the average trend while the grey lines show the trends for the 1,000 location samples. This is an interesting contrast to @fig-longitude."
#| out-width: "70%"
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| warning: false

ggplot_pdp <- function(obj, x) {
  p <-
    as_tibble(obj$agr_profiles) %>%
    mutate(`_label_` = stringr::str_remove(`_label_`, "^[^_]*_")) %>%
    ggplot(aes(`_x_`, 1 - `_yhat_`)) +
    geom_line(
      data = as_tibble(obj$cp_profiles),
      aes(x = {{ x }}, group = `_ids_`),
      linewidth = 0.5,
      alpha = 0.05,
      color = "gray50"
    )

  num_colors <- n_distinct(obj$agr_profiles$`_label_`)

  if (num_colors > 1) {
    p <- p + geom_line(aes(color = `_label_`), linewidth = 1.2, alpha = 0.8)
  } else {
    p <- p + geom_line(color = "darkred", linewidth = 1.2, alpha = 0.8)
  }

  p
}

ggplot_pdp(pdp_longitude, longitude) +
  labs(y = "Probability of Forestation", x = "Longitude")
```

The pattern is different from @fig-longitude. For example, there is no decreasing trend in the eastern part of the state; probabilities for locations greater than longitudes of -121° are fairly constant. The difference is due to some other predictor having additional influence on locations in that area of the state (perhaps the county). 

This model appears to be adequate, but _it is not_. There is a major issue related to our parameter estimates that, while not obviously affecting the predictive ability, reflects a flaw that could make the fit unreliable. 

### A Potential Problem: Multicollinearity {#sec-logistic-multicollinearity}

### Regularization Through Penalization {#sec-logistic-penalized}

#### Refitting the Forestation Model  {.unnumbered}

### Bayesian Estimation {#sec-logistic-bayes} 

## Multinomial Regression {#sec-multinomial-reg}

## Generalized Additive Models {#sec-cls-gam}

## Discriminants {#sec-lda}

## Chapter References {.unnumbered}
