---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-linear/"
---

# Generalized Linear and Additive Classifiers {#sec-cls-linear}

```{r}
#| label: cls-linear-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(future)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_transparent())
set_options()
plan("multisession")

# ------------------------------------------------------------------------------
# load data 

load("../RData/forested_data.RData")
```

## Exploring Forestation Data  {#sec-forestation-eda}

### Feature engineering 

## Logistic Regression {#sec-logistic-reg}

Logistic regression is a classification model that can be used when there are $C = 2$ classes. As with all binary classification models, we want to accurately estimate the probability of an event^[We'll assume that class $j = 1$ is the class that is considered the event, for $j = 1, \ldots, C$ classes.]. As discussed in the previous chapter, the probability parameter $\pi$ should be between zero and one, and we often frame the problem in terms of the probability model based on the Bernoulli likelihood function: 

$$
\ell(\pi_i; y_{ij}) = \prod_{i=1}^{n_{tr}} \pi_i^{y_{i1}} (1-\pi_i)^{y_{i2}}
$$ {#eq-bernoulli}

where $\pi_i$ is the theortical probability of an event for sample $i$ ($i = 1, \ldots, n_{tr}$). 

Generally, we have a tabular data set of predictors that we want to affect $\pi_i$ in some effective and understandable way. One pattern that we've seen in previous chapters is a linear combination of predictors and parameters: 

$$
\boldsymbol{x}_i'\boldsymbol{\beta} = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}
$$ {#eq-linear-predictor}

This is often referred to as the “linear predictor.” The range of this value is [-$\infty$, $\infty$], so we can't directly model the probability of the event using this equation as it would easily exceed the natural boundaries for $\pi_i$. One approach is to embed the linear predictor inside a nonlinear function. The logistic model takes the form:

$$
\pi_i = \frac{1}{1 + exp(-\boldsymbol{x}_i'\boldsymbol{\beta})}
$$ {#eq-logistic}

This keeps the probability values within [0, 1] no matter how large or small the linear predictor becomes^[Technically, logistic regression uses this nonlinear function, but there are many other functions to choose from. Examples are the probit, complementary log-log, or TODO function. In these cases, we might refer to our model as “binary regression.” We TODO for a historical perspective.]. 

```{r}
#| label: precipitation-fit
#| include: false

linear_percip_fit <- 
  logistic_reg() %>% 
  fit(class ~ log10(`annual precipitation`), data = forested_train)

linear_percip_coef <- 
  linear_percip_fit %>% 
  extract_fit_engine() %>% 
  coef() 
# glm models the probability of the second factor level
linear_percip_coef <- -linear_percip_coef
linear_percip_coef <- format(linear_percip_coef, digits = 3)

```

For a single predictor, $x$, the logistic model results in a symmetric, sigmoidal function across values of $x$. To demonstrate, a simple logistic regression model was fit to the binary outcome data with a single predictor: the annual precipitation. That predictor has a fairly right-skewed distribution, so a log (base 10) transformation was used on the data prior to training the model. 

There are various ways to find parameter estimates, which will be discussed in subsequent subsections. The most basic method is maximum likelihood estimation, where we use a numerical optimization method to minimize the loss function, which is the negative log-likelihood. The log-likelihood is more numerically stable to work with, and it is generally not challenging to find maximum likelihood estimates (MLEs) of our $\boldsymbol{\beta}$ parameters. Statistically, the MLEs are estimates that maximize the probability of the training set data (TODO say this better). 

We solved the log-likelihood equations (based on @eq-bernoulli) to estimate the parameters, producing the linear predictor function:  

$$
\boldsymbol{x}_i'\boldsymbol{\beta} = `r linear_percip_coef[1]` + `r linear_percip_coef[2]` \log_{10}(precipitation_i)
$$

@fig-precipitation shows this pattern using the annual precipitation predictor in the forestation data in green. The curve shows a smooth function that increases in $x$ and has an asymptote of 1.0. 

Since the outcome data have only two qualitative values, it is difficult to visualize how well the curve fits the data. However, our training set for this example is fairly large, and we can show the observed event rate over the range of precipitation by binning the data into 100 groups. The breakpoints for the bins are quantiles of the log-10 training set predictor values. @fig-precipitation has an option to overlay these data with the model fit. 


::: {#fig-precipitation}

::: {.figure-content}

```{shinylive-r}
#| label: fig-precipitation
#| out-width: "80%"
#| viewerHeight: 600
#| standalone: true

library(shiny)
library(bslib)
library(tidymodels)
library(splines2)
library(scales)


light_bg <- "#fcfefe" # from aml4td.scss
grid_theme <- bs_theme(
  bg = light_bg, fg = "#595959"
)

theme_light_bl<- function(...) {
  
  ret <- ggplot2::theme_bw(...)
  
  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)
  ret$panel.background  <- col_rect
  ret$plot.background   <- col_rect
  ret$legend.background <- col_rect
  ret$legend.key        <- col_rect
  
  ret$legend.position <- "top"
  
  ret
}

# ------------------------------------------------------------------------------


ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-1, 5, 5, -1)),
    column(
      width = 4,
      checkboxGroupInput(
        inputId = "include",
        label = "Include",
        choices = list("Data" = "Data", "Linear" = "fit_linear", "Splines" = "fit_spline"),
        selected = c("fit_linear"),
        inline = TRUE
      )
    ),
    column(
      width = 4,
      radioButtons(
        inputId = "yaxis",
        label = "y-axis",
        choices = c("Event Rate" = "rate", "Logit" = "logit"),
        inline = TRUE
      )
    )
  ),
  as_fill_carrier(plotOutput("plot"))
)


server <- function(input, output) {
  load(url("https://raw.githubusercontent.com/aml4td/website/main/RData/forested_data.RData"))
  
  percip <- 
    forested_train %>% 
    mutate(`annual precipitation` = log10(`annual precipitation`))
  
  percip_bin <- 
    percip %>% 
    select(class, `annual precipitation`) %>% 
    mutate(percip_bin = ntile(`annual precipitation`, 100)) %>% 
    summarize(
      rate = mean(class == "Yes"),
      percip = median(`annual precipitation`),
      n = length(`annual precipitation`),
      .by = c(percip_bin)
    ) %>% 
    mutate(logit = binomial()$linkfun(rate))
  
  percip_rng <- extendrange(percip_bin$percip)
  # percip_rng[1] <- 0.0
  
  percip_grid <- tibble(`annual precipitation` = seq(percip_rng[1], percip_rng[2], length.out = 100))
  
  linear_fit <- 
    logistic_reg() %>% 
    fit(class ~ `annual precipitation`, data = percip)
  
  linear_pred <- 
    augment(linear_fit, new_data = percip_grid) %>% 
    mutate(Model = "Linear Term", group = "fit_linear")
  
  spline_fit <- 
    logistic_reg() %>% 
    fit(class ~ naturalSpline(`annual precipitation`, df = 7), data = percip)
  
  spline_pred <- 
    augment(spline_fit, new_data = percip_grid) %>% 
    mutate(Model = "Seven Natural Spline Terms", group = "fit_spline")
  
  
  predictions <- 
    bind_rows(linear_pred, spline_pred) %>% 
    mutate(Model = factor(Model, levels = c("Linear Term", "Seven Natural Spline Terms"))) %>% 
    mutate(logit = binomial()$linkfun(.pred_Yes))
  
  output$plot <-
    renderPlot({
      
      if (input$yaxis == "rate") {
        p <- 
          percip_bin %>% 
          ggplot(aes(percip)) + 
          labs(x = "Log10 Annual precipitation", y = "Probability of Forestation") + 
          lims(y = 0:1)
      } else {
        p <- 
          percip_bin %>% 
          ggplot(aes(percip)) + 
          labs(x = "Log10 Annual precipitation", y = "Logit")
      }
      
      
      if (any(input$include == "Data")) {
        if (input$yaxis == "rate") {
          p <- p +  geom_point(aes(y = rate), alpha = 1 / 3, cex = 3) 
        } else {
          p <- p +  geom_point(aes(y = logit), alpha = 1 / 3, cex = 3) 
        }
      }
      
      if (any(grepl("fit_", input$include))) {
        curve_data <- dplyr::filter(predictions, group %in% input$include)
        
        if (input$yaxis == "rate") {
          p <- p +
            geom_line(data = curve_data, 
                      aes(x = `annual precipitation`, y = .pred_Yes, color = Model),
                      linewidth = 1)
        } else {
          p <- p +
            geom_line(data = curve_data, 
                      aes(x = `annual precipitation`, y = logit, color = Model),
                      linewidth = 1) 
        }
        
        p <- p + 
          theme(legend.position = "top") + 
          scale_color_brewer(drop = FALSE, palette = "Dark2")
      }
      
      p <- p + theme_light_bl()
      print(p)
    }, res = 100)
}

app <- shinyApp(ui = ui, server = server)
```
:::

An example of a logistic regression model with a single predictor (annual precipitation, in log10 units)

:::

Overlaying the estimated event rates shows that the model does a poor job of representing the relationship between the probability of forestation and precipitation. The data trend is not monotonically increasing; in the middle of the precipitation range, there is a small drop in the binned rates that recovers and begins to increase once more. Our simple linear predictor is not flexible enough to emulate this motif.

TODO Segue to GLMs

Logistic regression belongs to a broader class of _generalized linear models_, or GLMs (REF). These models can be used with different types of outcomes, such as numeric outcomes or count data. The idea is to have a model regress the linear predictor against a function of the outcome data. In our current case, the _logit_ function is applicable^[ref oragins paper]: 

$$
log\left(\frac{\pi_i}{1 - \pi_i}\right) =\boldsymbol{x}'\boldsymbol{\beta}
$$ {#eq-logit}

This form can be found by solving @eq-logistic for the linear predictor. Looking back to @fig-precipitation, we can change the y-axis data and show how the logit changes for our fitted model. The line visualizes the estimated linear predictor for these data. Sadly, when the representation of our data is similarly changed, the pattern is not linear; our linear model isn't effectively emulating the nonlinear pattern in the data. 

The logit function has a meaningful interpretation since it contains the _odds_ of an event occurring. TODO more here. 

There are many benefits to using a generalized linear model. First, it gives us some nice theoretical guarantees about the likelihood function, allowing us to use simple gradient-based optimization methods to optimize the log-likelihood. Second, it provides an inferential framework for our parameters. This enables us to understand how important our predictors are by way of hypothesis tests and confidence intervals. 

It's easy to take these benefits for granted. Suppose we were to find parameter estimates using some other log function, such as the Brier score or the area under the ROC curve. In that case, it might be difficult to solve the required equations, let alone derive the methods to make inferences regarding parameters. 

Before proceeding further, let's talk about the linear predictor. The “linear” in “linear predictor” and “generalized linear model” means that the model is _linear in the parameters_ $\boldsymbol{\beta}$. This does not mean that the model can only produce linear class boundaries. Our model terms $x_{ij}$ can represent any function of one or more parameters. We can use the feature engineering tools shown in Chapters [-@sec-numeric-predictors] through [-@sec-interactions-nonlinear] to create a better model. 

For example, in @fig-precipitation, we saw that using the raw roughness value as our single predictor $x$ didn't work well. When plotted on the logit scale, there is a clear nonlinear function. We know how to estimate a general nonlinear function of a parameter: spline functions. @fig-precipitation can also show the results when seven natural spline features are used in the model. Visually, we can see that this expansion is far more effective at predicting the outcome. We can also quantify this using several metrics mentioned in the previous chapter. 

Logistic regression models can be drastically improved by including better representations of our predictors. Since it is an interpretable and stable model, we might be motivated to spend more time developing this model via exploratory data analysis and feature engineering. In many teaching materials and websites, we see analyses that quickly label the results of a logistic model as ineffective (due to the use of simple model terms) before moving on to more black-box machine-learning methods. The virtue of learning about your data and how the predictors related to the outcome can not only improve the logistic model but enable a more complete understanding and description of _why_ it works. 


### Maximum Likelihood Estimation  {#sec-logistic-mle}

### Regularized {#sec-logistic-penalized}

### Bayesian Estimation {#sec-logistic-bayes}

## Multinomial Regression {#sec-multinomial-reg}

## Generalized Additive Models {#sec-cls-gam}

## Linear Discriminants {#sec-lda}

## Naive Bayes {#sec-naive-bayes}

## Chapter References {.unnumbered}
