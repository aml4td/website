<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; Characterizing Classification Models – Applied Machine Learning for Tabular Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/cls-linear.html" rel="next">
<link href="../chapters/comparing-models.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ac94f01b84e0cf733daf4ed4b084c36a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<meta name="shinylive:serviceworker_dir" content="..">
<script src="../site_libs/quarto-contrib/shinylive-0.9.1/shinylive/load-shinylive-sw.js" type="module"></script>
<script src="../site_libs/quarto-contrib/shinylive-0.9.1/shinylive/run-python-blocks.js" type="module"></script>
<link href="../site_libs/quarto-contrib/shinylive-0.9.1/shinylive/shinylive.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/shinylive-quarto-css/shinylive-quarto.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-7T996NL20Z"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-7T996NL20Z', { 'anonymize_ip': true});
</script>
<script src="../site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="../site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title"><span id="sec-cls-metrics" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Characterizing Classification Models</span></span></h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../"></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/aml4td/website/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/news.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">News</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/contributing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributing</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/whole-game.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Whole Game</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Preparation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/initial-data-splitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Initial Data Splitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/missing-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Missing Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/numeric-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transforming Numeric Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/categorical-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Working with Categorical Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/interactions-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Interactions and Nonlinear Features</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/overfitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Overfitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/resampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Measuring Performance with Resampling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/grid-search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Grid Search</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/iterative-search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Iterative Search</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/feature-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/comparing-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Comparing Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-metrics.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Characterizing Classification Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-linear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Generalized Linear and Additive Classifiers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Complex Nonlinear Boundaries</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-trees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Classification using Trees and Rules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-imbalance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Class Imbalances</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-case-study.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Classification Case Study</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/cls-summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Classification Summary</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Regression</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Characterization</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Finalization</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-cls-distributions" id="toc-sec-cls-distributions" class="nav-link active" data-scroll-target="#sec-cls-distributions"><span class="header-section-number">15.1</span> Class Distributions</a></li>
  <li><a href="#sec-metric-information" id="toc-sec-metric-information" class="nav-link" data-scroll-target="#sec-metric-information"><span class="header-section-number">15.2</span> Choosing Appropriate Metrics</a></li>
  <li><a href="#sec-mushrooms" id="toc-sec-mushrooms" class="nav-link" data-scroll-target="#sec-mushrooms"><span class="header-section-number">15.3</span> Example Data: Poisonous Mushrooms</a></li>
  <li><a href="#sec-cls-hard-metrics" id="toc-sec-cls-hard-metrics" class="nav-link" data-scroll-target="#sec-cls-hard-metrics"><span class="header-section-number">15.4</span> Assessing Hard Class Predictions</a></li>
  <li><a href="#sec-cls-two-classes" id="toc-sec-cls-two-classes" class="nav-link" data-scroll-target="#sec-cls-two-classes"><span class="header-section-number">15.5</span> Metrics for Two Classes</a></li>
  <li><a href="#sec-cls-metrics-wts" id="toc-sec-cls-metrics-wts" class="nav-link" data-scroll-target="#sec-cls-metrics-wts"><span class="header-section-number">15.6</span> Weighted Performance Metrics</a></li>
  <li><a href="#sec-cls-metrics-soft" id="toc-sec-cls-metrics-soft" class="nav-link" data-scroll-target="#sec-cls-metrics-soft"><span class="header-section-number">15.7</span> Evaluating Probabilistic Predictions</a>
  <ul class="collapse">
  <li><a href="#sec-cross-entropy" id="toc-sec-cross-entropy" class="nav-link" data-scroll-target="#sec-cross-entropy"><span class="header-section-number">15.7.1</span> Cross-Entropy</a></li>
  <li><a href="#sec-brier" id="toc-sec-brier" class="nav-link" data-scroll-target="#sec-brier"><span class="header-section-number">15.7.2</span> Brier Scores</a></li>
  <li><a href="#sec-roc" id="toc-sec-roc" class="nav-link" data-scroll-target="#sec-roc"><span class="header-section-number">15.7.3</span> Reciever Operating Characteristic Curves</a></li>
  <li><a href="#sec-pr" id="toc-sec-pr" class="nav-link" data-scroll-target="#sec-pr"><span class="header-section-number">15.7.4</span> Precision-Recall Curves</a></li>
  <li><a href="#sec-compare-roc-pr" id="toc-sec-compare-roc-pr" class="nav-link" data-scroll-target="#sec-compare-roc-pr"><span class="header-section-number">15.7.5</span> Comparing ROC and PR Curves</a></li>
  <li><a href="#sec-gain" id="toc-sec-gain" class="nav-link" data-scroll-target="#sec-gain"><span class="header-section-number">15.7.6</span> Gain Charts</a></li>
  </ul></li>
  <li><a href="#sec-cls-calibration" id="toc-sec-cls-calibration" class="nav-link" data-scroll-target="#sec-cls-calibration"><span class="header-section-number">15.8</span> Measuring and Improving Calibration</a></li>
  <li><a href="#sec-ordered-categories" id="toc-sec-ordered-categories" class="nav-link" data-scroll-target="#sec-ordered-categories"><span class="header-section-number">15.9</span> Ordered Categories</a></li>
  <li><a href="#sec-cls-multi-objectives" id="toc-sec-cls-multi-objectives" class="nav-link" data-scroll-target="#sec-cls-multi-objectives"><span class="header-section-number">15.10</span> Multi-Objective Assessments</a></li>
  <li><a href="#chapter-references" id="toc-chapter-references" class="nav-link" data-scroll-target="#chapter-references">Chapter References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span id="sec-cls-metrics" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Characterizing Classification Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Classification models are used to predict outcomes that are categorical in nature. There are two types of predictions:</p>
<ul>
<li><em>Hard class predictions</em> are the most likely category in the form of a single qualitative value.</li>
<li><em>Soft class predictions</em> are a set of numeric scores, one per category, reflecting the likelihood of each. We’ll focus solely on probability estimates of each class for soft predictions.</li>
</ul>
<p>Hard class predictions are almost always created by finding the category corresponding to the largest value of the soft predictions.</p>
<p>This chapter discussed important characteristics of qualitative outcomes and appropriate metrics for quantifying how well the model functions. There is an abundance of metrics; we describe many of them here. Additionally, we’ll highlight different aspects of “performance” and when one might be more important than others.</p>
<section id="sec-cls-distributions" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="sec-cls-distributions"><span class="header-section-number">15.1</span> Class Distributions</h2>
<p>An important characteristic of classification outcomes is the <em>prevalence</em> of the classes. This is the frequency at which each class level occurs <em>in the wild</em>. For example, in late 2024, the prevalence of COVID-19 was believed to be between 5% and 7%. The prevalence is not a universal, static constant; it can change by geography and time. However, as we’ll see below, the prevalence can affect the results of many metrics.</p>
<p>For classification models, the prevalence is directly related to Bayes’ Rule. It is the <em>prior distribution</em> for the outcome variable (i.e., <span class="math inline">\(Pr[y]\)</span> in <a href="iterative-search.html#eq-bayes-rule" class="quarto-xref">Equation&nbsp;<span>12.10</span></a>). While the prevalence can be estimated from our training set, we should also realize that it should reflect our <em>a priori</em> beliefs (if any) about the outcome data.</p>
<p>For example, <a href="#fig-wordle-prior" class="quarto-xref">Figure&nbsp;<span>15.1</span></a> (left panel) shows the prior for Wordle<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> scores as defined by one of the authors. Wordle is a game where the player has six attempts to guess a five-letter word. There are seven levels, the last (an <code>X</code>) representing the case where the player failed to guess the word. The prior for guessing the word on a single try should not be zero but incredibly small. There are thousands of possible words, although many players choose a starter word that they believe optimal<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Guessing the word on two tries is considerably fortuitous, and the author placed a 4% probability of this occurring. There is also the opinion that correctly guessing in three or four turns should consume the bulk of the probability, and values of 40% and 35% were assigned to these. Not guessing the word was also thought to be about 5%. The final prior was (in order) 0.1%, 4%, 40%, 35%, 10%, 5.9%, and 5%.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-wordle-prior" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wordle-prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-wordle-prior-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wordle-prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.1: Two potential prior distributions for Wordle scores, one based on the prior belief of a single person and another extracted from a social media platform.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The figure also shows the distribution of 296,534 self-reported scores across 805 words on the Bluesky social media platform. While more bell-shaped, these data have some potential issues. The first two probabilities, 0.75% and 7.14%, are abnormally high. It is exceedingly unlikely that more than one half of a percent of guesses were correct on the first try, given that there are 805 unique words in this data set<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Also, the second guess probability is only achievable if a highly optimized first word is used rigorously and the player is smiled upon by fortune.</p>
<p>Theoretically, these two distributions should be different since one is the belief of a single person and may not generalize well to the general population of players. However, the prior is about belief and only has a significant effect when the size of the data is small (as previously discussed in Sections <a href="categorical-predictors.html#sec-effect-encodings" class="quarto-xref"><span>6.4.3</span></a> and <a href="iterative-search.html#sec-single-proportion" class="quarto-xref"><span>12.5.1</span></a>).</p>
<p>This chapter describes myriad approaches for quantifying how well a classification model functions via performance metrics.</p>
<div class="important-box">
<p>It is very important to carefully choose which metric (or metrics) to optimize a model. The metric defines what your model should consider important, and it will almost always believe that your chosen metric is the only important criterion. This can often lead to unintended consequences.</p>
</div>
<p>The next section discusses important characteristics of performance metrics. After this, two sections discuss specific metrics for both types of predictions. Another section discussed the special case where the class levels are ordered (such as with the Wordle scores). There is also a section specifically on model calibration and another describing the use of multiple metrics.</p>
</section>
<section id="sec-metric-information" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="sec-metric-information"><span class="header-section-number">15.2</span> Choosing Appropriate Metrics</h2>
<p>Let’s consider a few critical aspects of classification metrics. We’ll focus on specific metrics computed with hard or soft class predictions to further this discussion. We’ve seen the Brier score used repeatedly in the previous chapters for class probability predictions. As a reminder, this statistic is similar to the sum of squared errors in regression: for each class, the estimated class probabilities are contrasted with their corresponding binary indicators. These differences are squared and averaged. Let’s juxtapose the Brier score with the simplest metric for hard predictions: the model’s overall accuracy (i.e., the proportion of correctly predicted samples).</p>
<p>To start, should our metric be based on hard or soft predictions? This, of course, depends on how the model will be applied to data and how users will consume its predictions. However, a strong argument can be made that performance metrics that use soft predictions should be favored over those using hard predictions. Information theory introduces the idea of <em>self-information</em>: if the value of a variable has some probability <span class="math inline">\(\pi\)</span> of occurring, we can measure the amount of information using <span class="math inline">\(I = -log_2(\pi)\)</span> where the units are called bits<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>For binary classification, there are two possible hard prediction values, so the information is <span class="math inline">\(-log_2(1/2)\)</span> = 1.0 bit. For simplicity, suppose the probability estimates are measured to the second decimal place. In this case, there are 101 possible probability estimates, so the data contain <span class="math inline">\(-log_2(1/101)\)</span> bits (about 6.7).</p>
<p>This simple analysis implies that metrics that use probabilities will contain much more information (literally and figuratively) than class predictions. As a result, probability-based metrics will be more capable of discriminating between models.</p>
<p>Let’s also consider two examples of how metrics can lead to unintended consequences.</p>
<p>First, accuracy is very sensitive to class imbalances (where one or more classes do not occur very often). Suppose that we have two classes, and one only occurs 1% of the time (called the “minority class” in this context). When accuracy is used for these data, an inferior model can easily achieve 99% accuracy simply by always predicting the majority class. If accuracy is all that we care about, then we can satisfy the literal goal, but our model will probably fail to fulfill the intention of the user<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>The second example contrasts the concepts of <em>discrimination</em> and <em>calibration</em>.</p>
<ul>
<li><p>A model can discriminate the classes well when the probability estimates of the true class are larger than the probabilities for the others. In other words, being good at discrimination means that you can, to some degree, successfully discern the true classes</p></li>
<li><p>A classification model is well-calibrated when the estimated probability predictions are close to their real theoretical counterparts. In other words, if we estimate a probability to be 55%, the event should occur at this same rate if realized multiple times.</p></li>
</ul>
<p>Some metrics favor one of these characteristics over the other. For example, neural networks for classification use binary indicators for their outcome data and try to produce models so that their predictions are near these values. To ensure that the results are “probability like,” meaning that they add up to one and are on [0, 1], the softmax transformation <span class="citation" data-cites="Bridle1990">(<a href="#ref-Bridle1990" role="doc-biblioref">Bridle 1990</a>)</span> is applied to the raw predictions:</p>
<p><span id="eq-softmax"><span class="math display">\[
\hat{p}_k = \frac{e^{\hat{y}_k}}{{\displaystyle  \sum_{l=1}^Ce^{\hat{y}_l}}}
\tag{15.1}\]</span></span></p>
<p>where <span class="math inline">\(C\)</span> is the number of classes, and <span class="math inline">\(\hat{y}_k\)</span> is a predicted score for class <span class="math inline">\(k\)</span>.</p>
<p>While the softmax method ensures they have the correction range, it does not guarantee that a model with good discrimination is well-calibrated. <a href="#fig-discrim-vs-cal" class="quarto-xref">Figure&nbsp;<span>15.2</span></a> shows a hypothetical example that illustrates what can occur when the softmax method is used. The distributions of the two classes have a good degree of separation, indicating that the model can discriminate the true classes. The observed accuracy is large (98.2%) but the predicted probabilities only range from 38.2% to 63%. It is improbable that the true probability values have such a narrow range, and we can assume that the calibration is poor here. The observed Brier score is large (0.203), supporting this assumption.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-discrim-vs-cal" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-discrim-vs-cal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-discrim-vs-cal-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-discrim-vs-cal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.2: An example of a model that has good discrimination but poor calibration.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In some cases, the pattern shown in <a href="#fig-discrim-vs-cal" class="quarto-xref">Figure&nbsp;<span>15.2</span></a> may be acceptable. For example, if the goal of the model is to identify “events” (such as the most profitable consumers in a database), the model’s calibration may not be important; your main interest is in ranking the rows in a data set. However, we suggest that probability predictions are far more informative than class predictions, and thus, good calibration is a key feature of most classification models.</p>
</section>
<section id="sec-mushrooms" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="sec-mushrooms"><span class="header-section-number">15.3</span> Example Data: Poisonous Mushrooms</h2>
<p>To demonstrate classification metrics, we’ll primarily use a data set where mushroom characteristics are used to predict whether specific mushrooms are edible or poisonous <span class="citation" data-cites="wagner2021mushroom">(<a href="#ref-wagner2021mushroom" role="doc-biblioref">Wagner, Heider, and Hattab 2021</a>)</span>. These data were obtained from the <a href="https://archive.ics.uci.edu/dataset/848/secondary+mushroom+dataset">UCI Machine Learning Archive</a> and we’ll use them in conjunction with a naive Bayes classification model<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. This model uses a simplified version of Bayes’ Rule (see <a href="cls-nonlinear.html#sec-naive-bayes" class="quarto-xref"><span>Section 17.1.4</span></a>).</p>
<p>The initial data pool contained 61,069 data points. The main partition of the data was into training (<span class="math inline">\(n_{tr}\)</span> = 42,748), validation (<span class="math inline">\(n_{val}\)</span> = 6,107), and test (<span class="math inline">\(n_{te}\)</span> = 6,107). Additionally, a <em>calibration</em> set of <span class="math inline">\(n_{cal}\)</span> = 6,107 mushrooms were allocated for the procedures described in <span class="quarto-unresolved-ref">?sec-calibration</span>.</p>
<p>The predictors of the outcome were:</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>does bruise or bleed (yes or no)</li>
<li>cap color (categorical, 12 values)</li>
<li>cap diameter (numeric)</li>
<li>cap shape (categorical, 7 values)</li>
<li>cap surface (categorical, 8 values)</li>
<li>gill attachment (categorical, 8 values, contains unknowns)</li>
<li>gill color (categorical, 12 values)</li>
<li>gill spacing (close, distant, or none)</li>
<li>habitat (categorical, 8 values)</li>
<li>has ring (yes or no)</li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li>ring type (categorical, 9 values, contains unknowns)</li>
<li>season (categorical, 4 values)</li>
<li>spore print color (categorical, 7 values)</li>
<li>stem color (categorical, 13 values)</li>
<li>stem height (numeric)</li>
<li>stem root (categorical, 4 values)</li>
<li>stem surface (categorical, 8 values)</li>
<li>stem width (numeric)</li>
<li>veil color (categorical, 7 values)</li>
<li>partial veil (partial or unknown)</li>
</ul>
</div>
</div>
<p>The outcome data has relatively balanced classes with 55.5% of the mushrooms being poisonous. It is unclear whether this rate is consistent with mushrooms in the wild.</p>
<p>No preprocessing was applied to the data for this model. Naive Bayes models do not require the categorical predictors to be decomposed into binary indicator columns or transformations of the numeric predictors. As shown above, some predictors have missing data. In these cases, “missing” was added as an additional category.</p>
<p>We’ll fit the model to the training set and, for the most part, use the validation set to demonstrate the various metrics. <a href="#fig-naive-bayes-probs" class="quarto-xref">Figure&nbsp;<span>15.3</span></a> shows the distribution of the probability of the event (poisonous) for the validation set.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-naive-bayes-probs" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-naive-bayes-probs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-naive-bayes-probs-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-naive-bayes-probs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.3: The distribution of the validation set class probability estimates for a naive Bayes model, split by the true class of the mushroom.
</figcaption>
</figure>
</div>
</div>
</div>
<p>These distributions demonstrate a few attractive characteristics. First, the probability estimates span the entire range of possible values, and each distribution peaks at appropriately extreme values (i.e., zero or one). Conversely, the model rarely produces “confidently incorrect” predictions (i.e., erroneously predicting with a large probability estimate). We’ll see this model’s calibration and separation properties in later sections.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-metrics.html#sec-mushrooms"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-cls-hard-metrics" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="sec-cls-hard-metrics"><span class="header-section-number">15.4</span> Assessing Hard Class Predictions</h2>
<p>We’ll start by focusing on the qualitative predictions: <code>"poisonous"</code> or <code>"edible"</code>. Unless otherwise stated, hard class predictions for two-class problems are assigned in this chapter using a 50% cutoff. Alternate cutoffs are investigated in <a href="#sec-roc" class="quarto-xref"><span>Section 15.7.3</span></a>.</p>
<p>One of the most fundamental ways to inspect and understand how well hard class prediction performs is the <strong>confusion matrix</strong>, simply a cross-tabulation of the observed and predicted classes. From this, we can conceptualize where the model underperforms. An excellent model has the largest counts along the diagonal where the observed and predicted classes are the same. For the naive Bayes model, <a href="#tbl-confusion-matrix" class="quarto-xref">Table&nbsp;<span>15.1</span></a> shows the confusion matrix for the mushroom data’s validation set.</p>
<div class="columns">
<div class="column" style="width:37%;">

</div><div class="column" style="width:26%;">
<div id="tbl-confusion-matrix" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-confusion-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th colspan="2" data-quarto-table-cell-role="th" style="text-align: left; empty-cells: hide; border-bottom: hidden;"></th>
<th colspan="2" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Truth
</div></th>
</tr>
<tr class="even">
<th style="text-align: left;" data-quarto-table-cell-role="th">Prediction</th>
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: right;" data-quarto-table-cell-role="th">poisonous</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">edible</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">poisonous</td>
<td style="text-align: left;"></td>
<td style="text-align: right;">2,613</td>
<td style="text-align: right;">564</td>
</tr>
<tr class="even">
<td style="text-align: left;">edible</td>
<td style="text-align: left;"></td>
<td style="text-align: right;">750</td>
<td style="text-align: right;">2,180</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-confusion-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.1: Confusion matrix for validation set predictions from the naive Bayes model.
</figcaption>
</figure>
</div>
</div><div class="column" style="width:37%;">

</div>
</div>
<p>It may be helpful to present the confusion matrix in terms of percentages. For example, we might think that the model is worse at predicting the poisonous than edible mushrooms since the lower left cell has a larger count (750) than the upper right cell (564). However, this difference is partially explained by the presence of more poisonous mushrooms in the left column than in the left column of edible samples. It can also be helpful to visualize the matrix as a heatmap where darker colors represent larger counts, especially when there are many possible classes.</p>
<p>We can also view the confusion matrix as a “sufficient statistic” since it has all the information required to compute the metrics based on hard class predictions. For example, the accuracy computed from our validation set was 78.5%, which is</p>
<p><span id="eq-accuracy"><span class="math display">\[
accuracy = \frac{1}{n}\sum_{k=1}^Cn_{kk}
\tag{15.2}\]</span></span></p>
<p>where <span class="math inline">\(n_{ij}\)</span> are the number of counts in the <em>i</em> th row and the <em>j</em> th column of the confusion matrix, <span class="math inline">\(n\)</span> is the total number of data points.</p>
<p>Many of the metrics in this section are in the form of a proportion. For these, confidence intervals can be computed using typical methods for binomial proportions. We suggest using asymmetric intervals, such as the “exact” technique in <span class="citation" data-cites="clopper1934use">Clopper and Pearson (<a href="#ref-clopper1934use" role="doc-biblioref">1934</a>)</span>. See <span class="citation" data-cites="newcombe1998two">Newcombe (<a href="#ref-newcombe1998two" role="doc-biblioref">1998</a>)</span> and <span class="citation" data-cites="meeker2017statistical">Meeker, Hahn, and Escobar (<a href="#ref-meeker2017statistical" role="doc-biblioref">2017</a>)</span> for discussions of different methods for computing intervals on proportions. For the validation set results, the 90% confidence interval for accuracy was (77.6%, 79.3%). For other types of metrics, we can use the bootstrap methods described in <span class="quarto-unresolved-ref">?sec-boot-ci</span> to obtain confidence limits.</p>
<p>As previously mentioned, accuracy is misleading when there is a class imbalance. There are a few ways to correct for imbalances, most notably the measure of agreement called Kappa <span class="citation" data-cites="Cohen1960">(<a href="#ref-Cohen1960" role="doc-biblioref">Cohen 1960</a>)</span>. This normalizes the accuracy by a measure of expected accuracy based on chance agreement (<span class="math inline">\(E\)</span>):</p>
<p><span id="eq-kappa"><span class="math display">\[
Kappa = \frac{accuracy - E}{1 - E}
\tag{15.3}\]</span></span></p>
<p>where <span class="math inline">\(E\)</span> is the expected accuracy based on the marginal totals of the confusion matrix. For two classes, this is:</p>
<p><span id="eq-exp-agree"><span class="math display">\[
E =\frac{1}{n^{2}}\sum_{k=1}^2n_{k1}n_{k2}
\tag{15.4}\]</span></span></p>
<p>Much like correlation statistics, the range of Kappa is <span class="math inline">\(\pm\)</span> 1, with values of one corresponding to perfect agreement and zero meaning negative agreement. While negative values are possible, they are uncommon. Interpreting Kappa can be difficult since no well-defined values indicate the results are good enough. This is compounded by the fact that the values can change with different prevalences of the classes <span class="citation" data-cites="thompson1988reappraisal guggenmoos1996meaning">(<a href="#ref-thompson1988reappraisal" role="doc-biblioref">Thompson and Walter 1988</a>; <a href="#ref-guggenmoos1996meaning" role="doc-biblioref">Guggenmoos-Holzmann 1996</a>)</span>. For this reason, it isn’t easy to compare values between data sets or to have a single standard. There are also differences in range when the outcome data has different numbers of classes; a Kappa value of 0.5 has a different meaning when <span class="math inline">\(C = 2\)</span> than with some other number of possible classes.</p>
<p>The Kappa statistic can also incorporate different weights for different types of errors <span class="citation" data-cites="cohen1968weighted">(<a href="#ref-cohen1968weighted" role="doc-biblioref">Cohen 1968</a>)</span>. This is discussed in more detail in <a href="#sec-ordered-categories" class="quarto-xref"><span>Section 15.9</span></a> below.</p>
<p>Using the data from <a href="#tbl-confusion-matrix" class="quarto-xref">Table&nbsp;<span>15.1</span></a>, the Kappa value was 0.568. While subjective, this value probably indicates satisfactory agreement between the true classes and the predictions.</p>
<p>Despite the qualitative nature of the classes, we can also compute a correlation statistic between the observed and predicted classes. Initially developed in the early twentieth century <span class="citation" data-cites="ekstrom2011phi">(<a href="#ref-ekstrom2011phi" role="doc-biblioref">Ekstrom 2011</a>)</span>, the Phi-coefficient was rediscovered by <span class="citation" data-cites="MATTHEWS1975442">Matthews (<a href="#ref-MATTHEWS1975442" role="doc-biblioref">1975</a>)</span> and, more recently, is referred to as the Matthews correlation coefficient (MCC). For <span class="math inline">\(C = 2\)</span>, the statistic is:</p>
<p><span id="eq-mcc-two"><span class="math display">\[
MCC = \frac {n_{11}n_{22}-n_{12}n_{21}}{\sqrt {n_{1\boldsymbol{\cdot} }n_{2\boldsymbol{\cdot} }n_{\boldsymbol{\cdot} 2}n_{\boldsymbol{\cdot} 1}}}
\tag{15.5}\]</span></span></p>
<p>where the dot notation indicates the sum of the row or column. More generally, for <span class="math inline">\(C\)</span> classes:</p>
<p><span id="eq-mcc-multi"><span class="math display">\[
{\displaystyle {\text{MCC}}={\frac {\displaystyle{\sum_{k=1}^C\sum_{l=1}^C\sum _{m=1}^C(n_{kk}n_{lm}-n_{kl}n_{mk})}}{{\sqrt {{\displaystyle \sum_{k=1}^C\left(\sum _{l=1}^Cn_{kl}\right)\left(\sum _{\substack{k'=1\\k'\neq k}}^C\sum _{l'=1}^Cn_{k'l'}\right)}}}{\sqrt {{\displaystyle\sum_{k=1}^C\left(\sum _{l=1}^Cn_{lk}\right)\left(\sum _{\substack{k'=1\\k'\neq k}}^C\sum _{l'=1}^Cn_{l'k'}\right)}}}}}}
\tag{15.6}\]</span></span></p>
<p>Like Kappa, the statistic mirrors the correlation coefficient for numeric data in that it ranges from -1 to +1 and a similar interpretation to ordinary correlation coefficients. For the mushroom data, the MCC estimate, 0.569, is very close to the Kappa estimate shown above. This is often the case, and the similarity was investigated more in <span class="citation" data-cites="delgado2019cohen">Delgado and Tibau (<a href="#ref-delgado2019cohen" role="doc-biblioref">2019</a>)</span>.</p>
<p>Accuracy, Kappa, and MCC can be computed regardless of the number of classes. There are numerous specialized metrics for <span class="math inline">\(C = 2\)</span>, i.e., “binary classification.”. The next section reviews these, and <a href="#sec-cls-metrics-wts" class="quarto-xref"><span>Section 15.6</span></a> describes a few methods for using these binary metrics when <span class="math inline">\(C &gt; 2\)</span>.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-metrics.html#sec-cls-hard-metrics"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-cls-two-classes" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="sec-cls-two-classes"><span class="header-section-number">15.5</span> Metrics for Two Classes</h2>
<p>It helps to conceptualize metrics for two classes when one class can be defined as the <em>event of interest</em> (i.e., which of the two is most important). The choice mainly affects the interpretation of metrics. For the mushroom data, either class would be appropriate, but we will choose <code>"poisonous"</code> as the event for our descriptions. We’ll also use “positives” to refer to the event of interest and “negatives” for the other class value. From this, we can name those positives correctly predicted as “true positives” (TP) and the accurately predicted negatives as “true negatives” (TN). If a positive sample was <em>incorrectly</em> predicted as negative, we have “false negatives” (FN) and, conversely, “false positives” (FP) for poorly predicted negative samples. <a href="#tbl-confusion-matrix-two-class" class="quarto-xref">Table&nbsp;<span>15.2</span></a> arranges these cases in a 2 <span class="math inline">\(\times\)</span> 2 confusion matrix.</p>
<div class="columns">
<div class="column" style="width:30%;">

</div><div class="column" style="width:40%;">
<div id="tbl-confusion-matrix-two-class" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-confusion-matrix-two-class-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: left; empty-cells: hide; border-bottom: hidden;"></th>
<th colspan="2" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Truth
</div></th>
</tr>
<tr class="even">
<th style="text-align: left;" data-quarto-table-cell-role="th">Prediction</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">event</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">non-event</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">event</td>
<td style="text-align: left;">TP</td>
<td style="text-align: left;">FP</td>
</tr>
<tr class="even">
<td style="text-align: left;">non-event</td>
<td style="text-align: left;">FN</td>
<td style="text-align: left;">TN</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-confusion-matrix-two-class-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.2: A general confusion matrix for outcomes with two classes.
</figcaption>
</figure>
</div>
</div><div class="column" style="width:30%;">

</div>
</div>
<p>There are particular metrics associated with the accuracy of each class. Sensitivity is the probably of correctly predicting the event of interest:</p>
<p><span id="eq-sens"><span class="math display">\[
Sensitivity = \frac{\text{correctly predicted events}}{\text{total events}} = \frac{TP}{TP + FN}
\tag{15.7}\]</span></span></p>
<p>The accuracy of non-events is measured by specificity:</p>
<p><span id="eq-spec"><span class="math display">\[
Specificity = \frac{\text{correctly predicted non-events}}{\text{total non-events}} = \frac{TN}{TN + FP}
\tag{15.8}\]</span></span></p>
<p>We often refer to the false positive rate which is simply one minus the specificity.</p>
<p>Sensitivity and specificity are bedrock metrics for two classes since they represent the two possible ways models can go wrong. In the Statistics literature, they correspond to “Type I” and “Type II” errors, respectively. We’ll examine their relationship below when receiver operating characteristic (ROC) curves are discussed.</p>
<p>With the mushroom data, the sensitivity was 77.7% and the specificity was nearly the same: 79.4%. Since these are simple ratios, confidence intervals can be computed using the same technique for overall accuracy.</p>
<p>These two metrics are commonly reported when a two-class model is evaluated. However, there is a subtle issue in their usage. If we want to know how well the model works, they don’t really answer a question that anyone would ask. For example, sensitivity is the answer to:</p>
<blockquote class="blockquote">
<p><strong>Q1</strong>: If I know my mushroom is poisonous, what percentage of the time do I correctly predict it to be poisonous?</p>
</blockquote>
<p>This is because sensitivity <em>conditions</em> on the sample being positive. If we knew that, we would not require a model to make a prediction. What we would really like to know:</p>
<blockquote class="blockquote">
<p><strong>Q2</strong>: If I predict this mushroom to be poisonous, what is the probability that it is actually poisonous?</p>
</blockquote>
<p>This is a different question that requires more data than just the sensitivity estimate.</p>
<p>To explain, we must return to Bayes’ Rule (<a href="iterative-search.html#eq-bayes-rule" class="quarto-xref">Equation&nbsp;<span>12.10</span></a>). In this context, the sensitivity is the likelihood term <span class="math inline">\(Pr[x = poison | y = poison]\)</span> (where <span class="math inline">\(x\)</span> represents the predicted class). To answer question Q2, what we need is:</p>
<p><span id="eq-bayes-ppv"><span class="math display">\[
Pr[y = poison| x = poison] = \frac{Pr[y = poison] Pr[x = poison | y = poison]}{Pr[x = poison]}
\tag{15.9}\]</span></span></p>
<p>This equation reflects the idea that we need to know how often the event really occurs (independent of our model). This is the Bayesian prior probability (i.e., the prevalence). If the event rarely happens anyway, that should affect the probability of us predicting it to happen.</p>
<p>For the event class, Q2 is measured by a statistic called the <em>positive predictive value</em> (PPV), which requires sensitivity, specificity, and prevalence:</p>
<p><span id="eq-ppv"><span class="math display">\[
PPV= \frac{Prev \times Sens}{(Sens \times Prev) + \left((1 - Spec) \times (1 - Prev)\right)}
\tag{15.10}\]</span></span></p>
<p>Without knowing anything about how often the average mushroom is poisonous, we could choose a prevalence of 50% as an uninformative prior. Our estimate from the training set (55.5%) is close to uninformative so that we will use that in our computations.</p>
<p>With our estimates in hand, the answer to Q2 is</p>
<blockquote class="blockquote">
<p>A2: 82.2% of the time, a mushroom is poisonous when we predict it to be.”</p>
</blockquote>
<p>The Bayesian analog to specificity is called the <em>negative predictive value</em> (NPV) and has a similar formula:</p>
<p><span id="eq-npv"><span class="math display">\[
NPV= \frac{(1 - Prev) \times Spec}{\left(Prev \times (1 - Sens)\right) + \left(Spec \times (1 - Prev)\right)}
\tag{15.11}\]</span></span></p>
<p>Using the prevalence from our training set and the metrics computed on the validation set, the NPV estimate is 74.4%.</p>
<p>The PPV and NPV are functions of three quantities, and it might be challenging to understand how the prevalence can affect the statistics. <a href="#fig-prevalence" class="quarto-xref">Figure&nbsp;<span>15.4</span></a> can help us understand these relationships. For example, if the prevalence is near zero, achieving an acceptable PPV value may be difficult unless the specificity is perfect (in which case it is quite easy to attain a good NPV).</p>
<div id="fig-prevalence" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-prevalence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| label: fig-prevalence
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(dplyr)
library(purrr)
library(ggplot2)
library(patchwork)
library(bslib)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")

# ------------------------------------------------------------------------------

ui &lt;- fluidPage(
#  theme = grid_theme,
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-3, 6, -3)),
    column(
      width = 10,
      sliderInput(
        inputId = "prev",
        label = "Prevelance",
        min = 0,
        max = 1,
        value = 1/2,
        width = "100%",
        step = 1/20
      )
    )
  ),
  as_fill_carrier(plotOutput('results'))
)

server &lt;- function(input, output) {
  
  ppv_fn &lt;- function(sens, spec, prev = 1 / 2) {
    ( sens * prev ) / ( (sens * prev) + ( (1 - spec) * (1 - prev) ) )
  }
  npv_fn &lt;- function(sens, spec, prev = 1 / 2) {
    ( spec * (1 - prev) ) / ( ( (1 - sens) * prev) + ( (spec) * (1 - prev) ) )
  }
  
  output$results &lt;-
    renderPlot({
      
      x &lt;- seq(0, 1, length.out = 50)
      n &lt;- length(x)
      cond_x &lt;- (0:4) / 4
      rng &lt;- extendrange(0:1, f = 0.02)
      
      ###
      
      pos_df &lt;- expand.grid(spec = cond_x, sens = x)
      pos_df$prev &lt;- input$prev
      pos_df$PPV &lt;- ppv_fn(pos_df$sens, pos_df$spec, prev = pos_df$prev)
      pos_df$Specificity &lt;- format(pos_df$spec, digits = 3)
      
      pos_p &lt;- 
        pos_df %&gt;% 
        ggplot(aes(x = sens, y = PPV, col = Specificity)) +
        geom_line(linewidth = 1) + 
        lims(x = 0:1, y = rng) + 
        theme_bw() +
        theme(
          legend.position = "top",
          legend.text = element_text(size = 9)
        ) +
        scale_color_brewer(palette = "Reds") +
        labs(x = "Sensitivity")
      
      ###
      
      neg_df &lt;- expand.grid(sens = cond_x, spec = x)
      neg_df$prev &lt;- input$prev
      neg_df$NPV &lt;- npv_fn(neg_df$sens, neg_df$spec, prev = neg_df$prev)
      neg_df$Sensitivity &lt;- format(neg_df$sens, digits = 3)
      
      neg_p &lt;- 
        neg_df %&gt;% 
        ggplot(aes(x = spec, y = NPV, col = Sensitivity)) +
        geom_line(linewidth = 1) + 
        lims(x = 0:1, y = rng) + 
        theme_bw() +
        theme(
          legend.position = "top",
          legend.text = element_text(size = 9)
        ) +
        scale_color_brewer(palette = "Blues") +
        labs(x = "Specificity")
      
      print(pos_p + plot_spacer() + neg_p + plot_layout(widths = c(6, 1 , 6)))
      
    }, res = 100)
}

app &lt;- shinyApp(ui, server)

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prevalence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.4: A visualization of how the ingredients to the positive and negative predictive values affect the statistics.
</figcaption>
</figure>
</div>
<p>The predictive value metrics are rarely quoted since they are more complex and rely on good prevalence estimates. They also tend to give worse impressions of model efficacy than sensitivity and specificity.</p>
<p>Sensitivity and specificity (and PPV/NPV) measure opposite types of errors. When paired, they are most appropriate when the overall model quality is important. In contrast, we’ll now consider a few other metrics for two-class problems, focusing primarily on the event class.</p>
<p>The field of Information Retrieval (IR) focuses on finding relevant documents from a large collection. Once a training set of documents is labeled (as relevant or not), IR systems consume the document’s contents (mostly text) and use those data to predict whether a new document is relevant. In our previous discussion, the event of interest is when the true class of the outcome is “relevant.” The term “retrieved” is also used to define the identified documents, meaning the documents that have been predicted to be relevant (i.e., TP + FP).</p>
<p>The primary two metrics used in information retrieval are precision and recall. Precision is the proportion of relevant documents in the data set that have been identified:</p>
<p><span id="eq-precision"><span class="math display">\[
Precision = \frac{\text{relevant and retrieved}}{\text{total retrieved}}=\frac{TP}{TP + FP}
\tag{15.12}\]</span></span></p>
<p>For example, in the validation set, our naive Bayes model has predicted that 3,177 mushrooms were predicted/identified to be poisonous using a 50% cutoff. Of these, 2,613 were truely poisonous. The ratio of these values produced a precision value of 74.4%.</p>
<p>It is sometimes said that precision is the same as the positive predicted value. However, this is only the case when the prevalence is 50% (i.e., balanced classes). For information retrieval, it is doubtful that the event rate would be so large (otherwise, documents would be very easily found).</p>
<p>The recall is the rate that identified documents are actually relevant:</p>
<p><span id="eq-recall"><span class="math display">\[
Recall = \frac{\text{relevant and retrieved}}{\text{total relevant}} = \frac{TP}{TP + FN}
\tag{15.13}\]</span></span></p>
<p>This is the same as sensitivity.</p>
<p>These two metrics are sometimes qualified by how many of the top K documents are found (e.g., “precision@K”).</p>
<p>Also related are F-measures, which are combinations of precision and recall. For example, the <span class="math inline">\(F_1\)</span> score is the harmonic mean of precision and recall:</p>
<p><span id="eq-f1"><span class="math display">\[
F_{1}=2\left(\frac{precision \times recall}{precision + recall}\right)
\tag{15.14}\]</span></span></p>
<p>More generally, the F-score can be written as</p>
<p><span id="eq-f-score"><span class="math display">\[
F_{\beta}=(1+\beta_2)\left(\frac{precision \times recall}{(\beta^2 \times precision) + recall}\right)
\tag{15.15}\]</span></span></p>
<p>Statistics called <em>gain</em> (or sometimes <em>lift</em>) quantify the number of relevant data points found relative to chance. These depend on a probability cutoff defining which samples are retrieved. For a set of <span class="math inline">\(n\)</span> samples, suppose that <span class="math inline">\(n_{prev}\)</span> is the number expected to be relevant by chance. For some threshold <span class="math inline">\(t\)</span>, let <span class="math inline">\(n_t\)</span> be the number of retrieved documents that are truly relevant (such that <span class="math inline">\(Pr[event]\ge t\)</span>). The gain at threshold <span class="math inline">\(t\)</span> is then</p>
<p><span id="eq-gain"><span class="math display">\[
Gain_t = n_t / n_{prev}
\tag{15.16}\]</span></span></p>
<p>so that <span class="math inline">\(Gain_0 = 1\)</span> and <span class="math inline">\(Gain_1 = 0\)</span>. We’ll consider this metric again in <a href="#sec-gain" class="quarto-xref"><span>Section 15.7.6</span></a>.</p>
<p>It is worth reiterating that precision, recall, <span class="math inline">\(F_{\beta}\)</span>, and Gain serve a completely different goal than sensitivity and specificity. They are focused on finding events in the data, while sensitivity and specificity are concerned with overall model quality by balancing the two possible error types.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-metrics.html#sec-cls-two-classes"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-cls-metrics-wts" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="sec-cls-metrics-wts"><span class="header-section-number">15.6</span> Weighted Performance Metrics</h2>
<p>Two class performance metrics are convenient summaries and are generally interpretable. As we saw earlier in this chapter, the metrics of accuracy and Kappa naturally handle more than two classes. Because of the desirable characteristics of other two-class performance metrics, conventions have been constructed to extend these metrics <span class="citation" data-cites="lewis1991evaluating">(<a href="#ref-lewis1991evaluating" role="doc-biblioref">Lewis 1991</a>)</span>. These conventions can be applied to any two-class metric. We will use sensitivity, <em>S</em>, to illustrate the conventions.</p>
<p>The first approach, <em>macro averaging</em>, computes <span class="math inline">\(C\)</span> versions of the statistic using a “one-versus-all” approach. For example, when <span class="math inline">\(C = 3\)</span>, <span class="math inline">\(S_1\)</span> is the sensitivity when the first class is considered the event and the second and third classes are pooled into the non-event category. <span class="math inline">\(S_2\)</span> is the sensitivity when the second class is considered the event, and so on. The macro estimate of sensitivity is computed as their average:</p>
<p><span id="eq-sens-macro"><span class="math display">\[
S_{macro} = \frac{1}{C}\sum_{k=1}^C S_k
\tag{15.17}\]</span></span></p>
<p>where the <span class="math inline">\(S_k\)</span> are the one-versus-all metrics.</p>
<p>A naive average of sensitivities is appropriate when the classes are balanced. When the classes are unbalanced, an unweighted average will bias this metric towards the sensitivity of the largest class. To account for class imbalance, <em>weighted macro averaging</em> uses the class frequencies <span class="math inline">\(n_k\)</span> as weights</p>
<p><span id="eq-sens-macro-wts"><span class="math display">\[
S_{macro, wts} = \sum_{k=1}^C \frac{S_k}{n_k}
\tag{15.18}\]</span></span></p>
<p>Finally, <em>micro averaging</em> is more complex because it aggregates the ingredients that go into the specific calculations. Sensitivity, for example, requires the number of true-positive and false negatives. These are summed and used in the regular computation of sensitivity:</p>
<p><span id="eq-sens-micro"><span class="math display">\[
S_{micro} = \frac{{\displaystyle \sum_{k=1}^C TP_k}}{\left( {\displaystyle \sum_{k=1}^C TP_k} \right) + \left( {\displaystyle \sum_{k=1}^C FN_k} \right) }
\tag{15.19}\]</span></span></p>
<p>Note that many performance metrics naturally work with three or more classes. There is little point in using these averaging tools for: accuracy, Kappa, MCC, ROC AUC, and others.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-metrics.html#cls-metrics-wts"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-cls-metrics-soft" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="sec-cls-metrics-soft"><span class="header-section-number">15.7</span> Evaluating Probabilistic Predictions</h2>
<p>As previously stated, class probabilities have far more information than categorical class predictions. We’ll describe common metrics that use these quantitative estimates. A few of these utilize indicators for different classes. We’ll denote binary indicators for class <span class="math inline">\(k = 1, \ldots, C\)</span> and sample <span class="math inline">\(i = 1, \ldots, n\)</span> as <span class="math inline">\(y_{ik}\)</span>. For binary data, we use <span class="math inline">\(k=1\)</span> to define the event of interest and <span class="math inline">\(k=2\)</span> as the non-event so that <span class="math inline">\(y_{i1} = 1\)</span> and <span class="math inline">\(y_{i2} = 0\)</span>.</p>
<section id="sec-cross-entropy" class="level3" data-number="15.7.1">
<h3 data-number="15.7.1" class="anchored" data-anchor-id="sec-cross-entropy"><span class="header-section-number">15.7.1</span> Cross-Entropy</h3>
<p>Cross-entropy is one of the most commonly used metrics for evaluating class probability estimates. It evaluates the difference between two probability distributions. For classification models, it is the distance between the predicted probabilities (<span class="math inline">\(\hat{p}_{ik}\)</span>) and their true values. The latter terms are represented by their binary indicators (<span class="math inline">\(y_{ik}\)</span>). The general equation is:</p>
<p><span id="eq-cross-entropy"><span class="math display">\[
CE = -\frac{1}{n}\sum_{i=1}^n\sum_{k=1}^Cy_{ik}\log(\hat{p}_{ik})
\tag{15.20}\]</span></span></p>
<p>where we assume that none of the estimated probabilities are zero. For the special case of <span class="math inline">\(C=2\)</span> classes, it simplifies to:</p>
<p><span id="eq-cross-entropy-binary"><span class="math display">\[
CE = -\frac{1}{n}\sum_{i=1}^n \left[y_{i1}\log(\hat{p}_{i1}) + y_{i2}\log(1 - \hat{p}_{i1})\right]
\tag{15.21}\]</span></span></p>
<p>since <span class="math inline">\(\hat{p}_{i2} = 1 - \hat{p}_{i1}\)</span>. A good model minimizes cross-entropy.</p>
<p>There is an alternative way to view this statistic. It turns out that if we assume the outcome data independently arise from a Bernoulli distribution with an event probability of <span class="math inline">\(\pi\)</span>, the corresponding likelihood statistic is</p>
<p><span id="eq-binomial-lh"><span class="math display">\[
\ell(y_{ik}; \pi) = \prod_{i=1}^n \pi^{y_{i1}} (1 - \pi)^{y_{i2}}
\tag{15.22}\]</span></span></p>
<p>We’ve described that, by maximizing the likelihood, we find values(s) of the parameter(s) that explain the data best. To improve numerical stability, we typically try to minimize the negative log-likelihood. Logging <span class="math inline">\(\ell(y_{ik}; \pi)\)</span> and substituting our estimate <span class="math inline">\(\hat{p}_{i1}\)</span> for <span class="math inline">\(\pi\)</span>, we also arrive at:</p>
<p><span id="eq-binomial-neg-llh"><span class="math display">\[
-\log \ell(y_{ik}; \hat{p}_{i1}) = -\sum_{i=1}^n \left[y_{i1}\log(\hat{p}_{i1}) + y_{i2}\log(1 - \hat{p}_{i1})\right]
\tag{15.23}\]</span></span></p>
<p>which is the same as cross-entropy. When there are more than two classes, the same equivalence can be shown to the multinomial log-likelihood. Often, with classification models, this statistic will be referred to as “log loss<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>”.</p>
<p>Cross-entropy can be an effective objective function for classification. On the theoretical side, it also has the characteristic of being a <em>proper scoring rule</em>; it is optimized at the true probability distribution. We’ll see a little more about this below when Brier scores are discussed.</p>
<p>One issue is that the scale of the value is not interpretable. For example, knowing that the cross-entropy value for our naive Bayes model is 0.455 does not obviously indicate whether that value is good or bad. We can determine what a poor value of cross-entropy is for this data set by repeatedly permuting the true class column of the validation set, recomputing the metrics, and taking the average. This gives us an estimate of the metrics when there is no connection between the observed classes and their predictions. Following this approach with 100 permutations, the baseline cross-entropy of a poor model was 0.551.</p>
<p>Also, there can be numerical issues when some probability estimates are at or near zero; <span class="math inline">\(\log(\hat{p}_{ik})\)</span> can diverge quickly to a large negative number and is undefined as zero. The common correction for these issues is to cap the probability estimates to a very small number. This helps avoid <span class="math inline">\(\log(0)\)</span>, but there is still the issue that extremes of the probability scale have more dramatic emphasis (and influence) on the metric.</p>
</section>
<section id="sec-brier" class="level3" data-number="15.7.2">
<h3 data-number="15.7.2" class="anchored" data-anchor-id="sec-brier"><span class="header-section-number">15.7.2</span> Brier Scores</h3>
<p>We want our probability estimates to be as close as possible to their true values. For classification, we don’t know the true probability values; we only have the observed class and the corresponding indicators for each class level. Similar to regression, we can use a metric that measures the squared distance between the observed and predicted probabilities, <span class="math inline">\(y_{ik}\)</span> and <span class="math inline">\(\hat{p}_{ik}\)</span>, respectively. For a binary outcome, that metric is called the Brier score<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> and has the form:</p>
<p><span id="eq-brier-two"><span class="math display">\[
Brier = \frac{1}{n}\sum_{i=1}^n(y_{i1} - \hat{p}_{i1})^2
\tag{15.24}\]</span></span></p>
<p>See <span class="citation" data-cites="brier1950verification">Brier (<a href="#ref-brier1950verification" role="doc-biblioref">1950</a>)</span> and, more recently, <span class="citation" data-cites="bella2013effect">Bella et al. (<a href="#ref-bella2013effect" role="doc-biblioref">2013</a>)</span>. A perfect model would estimate probabilities to be either 1.0 or 0.0, depending on the class, and would minimize this statistic. What would an ineffective model produce? While there is only one way to be perfect, there are many ways in which a model could produce poor estimates. Let’s say the model is completely equivocal and every prediction for this binary outcome is 0.5. In this case, we are summing repeated values of 0.5<sup>2</sup> so that the bad models would have scores <span class="math inline">\(\ge\)</span> 0.25. This serves as a good rule of thumb, but a better (but more complex) approach is via the permutation method previously described.</p>
<p>Looking at our mushroom model, the Brier score was estimated as 0.1502 with 90% bootstrap confidence limits of (0.1452, 0.1553). The permutation estimate shows that a poor model would have an average score of 0.5202. While not perfect, the model can achieve a fairly small average squared error.</p>
<p>In some cases, <a href="#eq-brier-two" class="quarto-xref">Equation&nbsp;<span>15.24</span></a> is written with the components for <em>both</em> classes. In this case, the possible range of values and the rule of thumb value would be doubled. While <span class="citation" data-cites="stanski1989survey">Stanski, Wilson, and Burrows (<a href="#ref-stanski1989survey" role="doc-biblioref">1989</a>)</span> recommends dividing the score by the number of classes to maintain a range of [0, 1], there does not appear to be an established convention for this aspect of the Brier score. The more general equation for any number of classes is:</p>
<p><span id="eq-brier"><span class="math display">\[
Brier = \frac{1}{nC}\sum_{i=1}^n\sum_{k=1}^C (y_{ik} - \hat{p}_{ik})^2
\tag{15.25}\]</span></span></p>
<p>Based on this equation, a more general version of our rule of thumb would be <span class="math inline">\((1 - (1/C))^2\)</span> for the worst-case Brier score. For example, with ten class levels, an unsatisfactory model would have a score around 0.81. We’ll use this version of the Brier score so that values are always between zero and one.</p>
<p>This metric has some interesting qualities that make it attractive. As previously mentioned, a perfect model has a score of zero. If a performance metric achieves its smallest value when the probability estimate is perfect, it is called a <em>proper scoring rule</em>. There are theoretical benefits to using such a metric which <span class="citation" data-cites="kruppa2014probability">Kruppa et al. (<a href="#ref-kruppa2014probability" role="doc-biblioref">2014</a>)</span> describes succinctly:</p>
<blockquote class="blockquote">
<p>The Brier score is a strictly proper score <span class="citation" data-cites="gneiting2007strictly">(<a href="#ref-gneiting2007strictly" role="doc-biblioref">Gneiting and Raftery 2007</a>)</span>, which means that it takes its minimal value only when the true probabilities are inserted as predictions. Phrased differently, it is not possible to improve the Brier score by systematically predicting probability values other than the best estimate.</p>
</blockquote>
<p>Additionally, the Brier score can be partitioned into more specific components that measure different aspects of the model fit (similar to the variance-bias decomposition from <a href="interactions-nonlinear.html#sec-variance-bias" class="quarto-xref"><span>Section 8.4</span></a>). <span class="citation" data-cites="sanders1963subjective">Sanders (<a href="#ref-sanders1963subjective" role="doc-biblioref">1963</a>)</span> split the Brier score into two components: “reliability” and “resolution.” This requires the probability estimates to have replicate values, i.e., the estimated probability values are not unique.</p>
<p>Let’s demonstrate with a binary outcome (<span class="math inline">\(C = 2\)</span>). Suppose out of <span class="math inline">\(n\)</span> data points there are only <span class="math inline">\(n_p\)</span> unique values of <span class="math inline">\(\hat{p}_{i1}\)</span>. We’ll denoted these unique values as <span class="math inline">\(\hat{p}_{j}\)</span> (<span class="math inline">\(j=1, \ldots, n_p\)</span>) each occuring with frequency <span class="math inline">\(n_j\)</span>.</p>
<p>Earlier, we remarked with dissatisfaction that the only representations we have of the <em>true</em> probability estimates are the binary indicators <span class="math inline">\(y_{i1}\)</span>. Now, we are in a more interesting situation: for each unique probability estimate, we have <span class="math inline">\(n_j\)</span> values, and the mean of the <span class="math inline">\(y_{i1}\)</span> is a higher resolution representation of the true values. This lets us get a little closer to a situation where we can contrast our estimates with the truth (to some degree). We’ll denote the mean of the <span class="math inline">\(y_{i1}\)</span> indicators corresponding to probability <span class="math inline">\(j\)</span> as <span class="math inline">\(\bar{y}_j\)</span>.</p>
<p>We can then partition the Brier score into:</p>
<p><span class="math display">\[\begin{align}
Brier &amp;= \frac {1}{n_p} \sum_{j=1}^{n_p}n_{j}(\hat{p}_{j} - \bar{y}_j)^2 + \frac {1}{n_p} \sum_{j=1}^{n_p}n_{j} \bar{y}_j (1 - \bar{y}_j) \label{eq-brier-cal} \\
&amp;= \text{reliability} + \text{refinement} \notag \\
\end{align}\]</span></p>
<p>Reliability compares our more refined values of “truth” with our model’s estimates; this is as close as we can get to the previously stated goal of <strong>calibration</strong>. We should minimize this value.</p>
<p>Refinement is interesting because it indirectly uses the model probability estimates (by contributing the <span class="math inline">\(j\)</span> data groupings). The pattern <span class="math inline">\(\bar{y}_j (1 - \bar{y}_j)\)</span> also drives the binomial variance and the Gini Index<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. This pattern measures <em>impurity</em>. The best-case scenario is that the true outcome values for every unique model probability estimate <em>are the same as possible</em>. We want to minimize this term too. <span class="citation" data-cites="murphy1973new">Murphy (<a href="#ref-murphy1973new" role="doc-biblioref">1973</a>)</span> shows how to further divide refinement, and <span class="citation" data-cites="yates1982external">Yates (<a href="#ref-yates1982external" role="doc-biblioref">1982</a>)</span> compares the two decompositions.</p>
<p>This partition could be very helpful, except for the problem that most models produce nearly unique probability estimates. For example, there are 6,107 mushrooms in our validation set and 6,100 unique predictions. To show how the decomposition could be useful, we’ll artificially round our probabilities to the second decimal place, yielding 101 unique predictions. <a href="#fig-brier-decomp" class="quarto-xref">Figure&nbsp;<span>15.5</span></a> shows the <span class="math inline">\(n_p\)</span> = 101 values for the reliability and refinement terms. On the left, we can see a strong linear relationship between the estimates and the <span class="math inline">\(\bar{y}_j\)</span>. The calibration is more variable in the middle of the probability range, and there is some deviation from the diagonal line when the estimates are in the range of [0.1, 0.5] (a smoothed line is shown in blue).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-brier-decomp" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-brier-decomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-brier-decomp-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-brier-decomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.5: The data making up the two-component partition of the Brier score (reliability/calibration and refinement). The left panel plots <span class="math inline">\(\hat{p}_{j}\)</span> versus <span class="math inline">\(\bar{y}_j\)</span> and the right panels plots <span class="math inline">\(\hat{p}_{j}\)</span> versus <span class="math inline">\(n_{j} \bar{y}_j (1 - \bar{y}_j)\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The right-hand panel shows the refinement data. The extreme ranges of the probability estimates are very pure. Just outside of the extremes, the values spike around 0.20 and 0.85.</p>
<p>If we can construct them, these diagnostic plots can help us determine where the model may lack correctness. We’ll see related plots in <span class="quarto-unresolved-ref">?sec-calibration</span>.</p>
<p>Finally, compared to cross-entropy, the Brier score has a slight advantage in how extreme probabilities influence the overall statistic. Recall that the logarithm of the probabilities might be problematic when estimates are close to zero. The Brier score is less affected due to its use of squared terms. At most, an extreme probability estimate has a definitive limit to how much it can impact the overall score.</p>
</section>
<section id="sec-roc" class="level3" data-number="15.7.3">
<h3 data-number="15.7.3" class="anchored" data-anchor-id="sec-roc"><span class="header-section-number">15.7.3</span> Reciever Operating Characteristic Curves</h3>
<p>Earlier, we found that our naive Bayes model had a sensitivity of 77.7% and a specificity of 79.4%. Since a false negative might end up poisoning someone, we might want to increase our sensitivity to identify every poisonous mushroom we can. One way to do this is to lower the weight of evidence required to make a class prediction of “poisonous.” The sensitivity and specificity quoted above correspond to a rule where a 50% probability or more indicates poison. What if we were to loosen that threshold and make it 10% so that we classify as an event if <span class="math inline">\(Pr[poisonous] \ge 0.10\)</span>?</p>
<p>In doing this, we will designate many more mushrooms as poisonous and, as a result, the sensitivity increases to 97.4%. Now, we’re identifying almost all of the poisonous mushrooms. The consequence is that many of these newly classified mushrooms are actually edible. The specificity falls to 35.2%. That might be acceptable; we would probably want to discard perfectly edible mushrooms to avoid being poisoned.</p>
<p>If our model’s application was more focused on specificity, we could raise the threshold about 50%. In this case, we increase specificity but do harm to sensitivity (by designating fewer samples to be events).</p>
<div class="important-box">
<p>The point of this exercise is to demonstrate that the default 50% cutoff for binary classification might not be satisfactory for our model’s application.</p>
</div>
<p>This presents some difficulty, though. We probably want to know the sensitivity and specificity to determine whether the model is good enough or compare it to others. Should we optimize the probability cutoff for each model as it is being developed? How <em>would</em> we pick an appropriate threshold?</p>
<p>The receiver operating characteristic (ROC) curve was designed to do just that. The points on the curve are created by looking at many cutoff points<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> and track how the sensitivity and specificity change. The curve is created by plotting the true positive rate (i.e., sensitivity) on the y-axis and the false positive rate (one minus specificity) on the x-axis. <a href="#fig-roc-curve" class="quarto-xref">Figure&nbsp;<span>15.6</span></a> shows this curve for our naive Bayes predictions where three cutoffs are highlighted: 10%, 50%, and 90%.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-roc-curve" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-roc-curve-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.6: Validation set ROC curves for the naive Bayes model. The symbols represent different probability thresholds.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The points along the curve help us make tradeoffs between sensitivity and specificity. For example, we might want to increase the sensitivity further. However, there is not much more vertical space above the point that identifies the 10% cutoff. In moving along the curve to the right, we gain a little more sensitivity, but the loss of specificity would result in far more false positives than we gain in finding true positives.</p>
<p>The ROC curve has a few more features that help with model development. The best-case model is one where the distributions of the predicted class probabilities are separated by the true classes. In this case, the ROC curve would start in the lower left corner and go directly to the upper left corner, where the sensitivity and specificity are both at 1.0. The curve would then directly move to the upper right corner. Since the ranges of the two axes are between zero and one, the area under the curve for this perfect model is 1.0.</p>
<p>What if our model was incapable of learning anything from the data? Choosing a threshold for this ineffective model would not change the sensitivity and specificity, and, as a result, the curve would move along the dotted diagonal line shown in the figure<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. In this situation, the area under the curve would be about 0.5. For <a href="#fig-roc-curve" class="quarto-xref">Figure&nbsp;<span>15.6</span></a>, the ROC AUC was 0.871.</p>
<p>Based on these patterns, we can use the area under the ROC curve as an optimization metric to differentiate or rank different models (or tuning parameter candidates). This single metric value encapsulates all possible thresholds. This allows us to avoid finding an optimal cutoff for each model during development; we can wait until we have narrowed our choice of the model down to a few and then examine their ROC curves to determine the best sensitivity and specificity for the situation. We could show the different ROC curves in the same plot, and the curve that moves closer to the upper left corner is better than the others.</p>
<p>Of course, there are disadvantages to summarizing an entire curve with a single number. Some models might have portions of their curves that show superiority to others, but not uniformly. If a particular section of the curve is specifically interesting, a partial area under the ROC curve can be computed <span class="citation" data-cites="McClish">(<a href="#ref-McClish" role="doc-biblioref">McClish 1989</a>)</span> and might offer a more selective metric.</p>
<p>It can also be difficult to conceptualize what the area under the curve means, how important the difference in AUCs is, and so on.</p>
<p>The ROC curve is defined for data sets where the outcome has two classes. However, <span class="citation" data-cites="Hand2001us">Hand and Till (<a href="#ref-Hand2001us" role="doc-biblioref">2001b</a>)</span> derived a method to calculate a single AUC when there are three or more classes. <span class="citation" data-cites="Hanley1982uc">Hanley and McNeil (<a href="#ref-Hanley1982uc" role="doc-biblioref">1982</a>)</span> had previously recognized that the ROC AUC has a form equivalent to the Wilcoxon statistic (and the Gini Index). Given this, Hand and Till derived a method to combine the AUC values of all pairwise ROC curves to find an overall metric. While it is possible to derive a standard error (and confidence intervals) of a single ROC curve, the authors suggest using bootstrap methods to estimate these statistics for multiclass ROC AUCs.</p>
<p>Finally, the most critical information about ROC curves is that they measure how well we can separate the classes. It is very possible to have a high ROC AUC and a very poor Brier score; see <a href="#fig-discrim-vs-cal" class="quarto-xref">Figure&nbsp;<span>15.2</span></a> for a previous example.</p>
</section>
<section id="sec-pr" class="level3" data-number="15.7.4">
<h3 data-number="15.7.4" class="anchored" data-anchor-id="sec-pr"><span class="header-section-number">15.7.4</span> Precision-Recall Curves</h3>
<p>Like the ROC curve, the PR curve plots the two statistics against one another, with recall being on the x-axis for all possible probability thresholds. Unlike the ROC curve, the threshold determines how many of the most likely items should be retrieved. <a href="#fig-pr-curve" class="quarto-xref">Figure&nbsp;<span>15.7</span></a> shows the results for our example data set. We start the curve from the left-hand side where the threshold is zero, meaning that samples where the probability of the event is greater than zero (i.e., all of them) are selected. The recall is perfect since we have selected all of the relevant items. The precision is equal to the event rate since we have all the true and false positives.</p>
<p>As we increase the threshold, we select fewer items, and the recall decreases and eventually becomes zero at the left-hand side of the curve since no documents are selected. In contrast, the precision increases since the ratio of the selected true positives increases.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-pr-curve" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pr-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-pr-curve-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pr-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.7: A precision-recall curve, estimated with the validation set, for the naive Bayes model.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note that neither the precision or recall (Equations <a href="#eq-precision" class="quarto-xref"><span>15.12</span></a> and <a href="#eq-recall" class="quarto-xref"><span>15.13</span></a>) directly use the true negative component of the confusion matrix. This reflects the focus of these two statistics on finding positives.</p>
<p>While that the ROC curve has its optimal point in the top left, the optimal setting for PR curves is in the plot area’s top right. We can also summarize the PR curve using the under the curve, a value of one being the best. Based on <a href="#fig-pr-curve" class="quarto-xref">Figure&nbsp;<span>15.7</span></a>, the PR AUC was 0.899.</p>
<p>However, note that the horizontal baseline value for the curve corresponds to the prevalence. The PR curve baseline, unlike ROC, moves with outcome distribution. This complicates the understanding of the AUC since the baseline section of the curve moves from data set to data set, and it may not be obvious just by looking at the AUC value.</p>
</section>
<section id="sec-compare-roc-pr" class="level3" data-number="15.7.5">
<h3 data-number="15.7.5" class="anchored" data-anchor-id="sec-compare-roc-pr"><span class="header-section-number">15.7.5</span> Comparing ROC and PR Curves</h3>
<p>Many resources both in social media, blog posts, and the literature <span class="citation" data-cites="saito2015precision ozenne2015precision">(<a href="#ref-ozenne2015precision" role="doc-biblioref">Ozenne, Subtil, and Maucort-Boulch 2015</a>; <a href="#ref-saito2015precision" role="doc-biblioref">Saito and Rehmsmeier 2015</a>)</span> make the claim that:</p>
<ul>
<li>PR curves should always be used for models with class imbalances and</li>
<li>ROC curves are ineffectual with rare events.</li>
</ul>
<p>Many of the arguments made about these effects are not correct. One assumption is that when there is a class imbalance, the goal is always to identify events in a population (i.e., information retrieval). Indeed, a curve based on metrics important to information retrieval will always be better at information retrieval than alternatives. However, there are many instances where we are interested in knowing that there is good class separation even though one class occurs infrequently.</p>
<p>Let’s look at a real example. Pharmaceutical companies use machine learning to derisk drug candidates by predicting the probability that the potential drug has different types of toxicity. For example, <span class="citation" data-cites="maglich2014more">Maglich et al. (<a href="#ref-maglich2014more" role="doc-biblioref">2014</a>)</span> considered models using laboratory tests to predict whether some molecules can cause steroidal toxicity. <span class="citation" data-cites="Kuhn2016">Kuhn (<a href="#ref-Kuhn2016" role="doc-biblioref">2016</a>)</span> provides a summary of these types of predictive models.</p>
<p>Suppose we have a sufficiently sized training set of relevant drug candidates and have used laboratory tests and other data to label them as “toxic” (the event of interest) or “non-toxic.” Let’s assume that the toxicity prevalence is around 1% in the population of molecules of interest.</p>
<p>Once deployed, a model would be used as follows: a medicinal chemist designs a potential drug <em>in-silico</em> (i.e., via a computer). They pass the chemical structure to a machine learning model that can predict the probability of being toxic.</p>
<p>In this instance, the key goal of the machine learning model is that it should be able to accurately distinguish toxic from non-toxic and that the predicted probabilities are well-calibrated. If there is <em>any</em> indication of toxicity, the chemist will be keenly interested in knowing how well the model can differentiate events from non-events<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. The estimate of the true negative rate is just as important to the chemist as the false positive rate.</p>
<p>The context here is that the model is being used to screen new items for the event of interest. The model’s users want to be confident in its quality when events or non-events are predicted. The focus is not solely on finding events (individually or within a collection of items).</p>
<p>In this scenario, the quality of the model should be assessed using multiple metrics: the ROC and PR curves, as well as the Brier score. We’ll show a technique for blending metrics in <span class="quarto-unresolved-ref">?sec-multi-objectives</span>.</p>
<p>Let’s also look at a counterexample where a PR curve has more utility than other metrics. Suppose we work for a retailer trying to find customers in their database who will most likely make a large purchase within a year. We can create a machine learning model to predict this event based on their demographics, previous purchases, and other predictors. We can apply this model to new customers. Still, the initial goal is to find the target customers from the existing data, so the company might send a promotional offer to incentivize them to purchase.</p>
<p>We are looking for customers in an existing database to find positives. There is less emphasis on accuracy on the truly negative customers; there isn’t much downside to offering a promotion to someone unlikely to act on it. Good discrimination isn’t very important, and neither is calibration if we will be choosing the top X% of most likely customers. In this instance, <em>the PR curve is clearly more appropriate for the job</em>.</p>
<p>Some technical characteristics that relate the two curves to one another, gathered from <span class="citation" data-cites="davis2006relationship">Davis and Goadrich (<a href="#ref-davis2006relationship" role="doc-biblioref">2006</a>)</span>:</p>
<ul>
<li>For a specific data set, the ROC and PR curves use the same data points but highlight different aspects of them.</li>
<li>When comparing two models, one ROC curve is uniformly better than the other only if this is also true with the PR curves.</li>
<li>If one model has the best ROC AUC, there is no guarantee that the same is true for the PR AUC.</li>
</ul>
<p>When choosing between these two curves, we should consider how aligned our goals are with the goal of the modeling problem.</p>
</section>
<section id="sec-gain" class="level3" data-number="15.7.6">
<h3 data-number="15.7.6" class="anchored" data-anchor-id="sec-gain"><span class="header-section-number">15.7.6</span> Gain Charts</h3>
<p>Gain charts (a.k.a. lift charts, a.k.a. cumulative accuracy profiles) create a curve that modulates the probability threshold <span class="math inline">\(t\)</span> described in <a href="#eq-gain" class="quarto-xref">Equation&nbsp;<span>15.16</span></a>. If we are looking for relevant documents, we can determine how many (or what percentage of) our unknown samples we should retrieve to achieve our goals.</p>
<p><a href="#fig-gain-curve" class="quarto-xref">Figure&nbsp;<span>15.8</span></a> shows the results where the x-axis is the percentage of data points retrieved, and the y-axis is the percentage of relevant retrieved samples. The grey background reflects the feasible region that contains possible curve values<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>.</p>
<p>In this example, the curve is close to the best possible results until roughly 25% of the samples have been retrieved. The dotted lines show that we would need to retrieve about 30% of the data set to acquire half of the truly relevant samples (on average)</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-gain-curve" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gain-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-gain-curve-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gain-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.8: A gain curve based on the validation set. The dotted linear indicates the percentage of samples to query/acquire in order to capture 30% of the relevant samples.
</figcaption>
</figure>
</div>
</div>
</div>
<p>One could summarize this curve by computing how much of the feasible region is covered by the curve.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-metrics.html#cls-metrics-soft"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
</section>
<section id="sec-cls-calibration" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="sec-cls-calibration"><span class="header-section-number">15.8</span> Measuring and Improving Calibration</h2>
<p>We often talk about a <em>model</em> having good calibration, but, as noted by <span class="citation" data-cites="lichtenstein1977calibration">Lichtenstein, Fischhoff, and Phillips (<a href="#ref-lichtenstein1977calibration" role="doc-biblioref">1977</a>)</span>, calibration is really a characteristic of a single prediction. A prediction is well-calibrated if the probability estimate is close to the true value. We have no way of knowing or proving this. We can conceptualize that, if we could repeatedly collect the data point, the rate at which the event occurs should match our prediction.</p>
<p>As seen in a previous section, the only numeric estimates we have for the true value are the binary indicators for each class (<span class="math inline">\(y_{ik}\)</span>). Unfortunately, these values do not facilitate diagnostics to assess how well our model works for an individual data point<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
<p>The primary diagnostic for classification models is the <strong>calibration plot</strong> of the entire data set (a.k.a. the reliability diagram). For two-class outcomes, this plot is usually created by ordering the data by the estimated probability of the event and binning these into groups (usually ten). From these groups, we can compute the event rate from the outcome data and plot that rate against the midpoint of the bins. The points should fall on a 45-degree diagonal line if the model is well-calibrated. Here, we are assessing statistics involving multiple data points and inferring whether the model is calibrated in general (i.e., not for individual points).</p>
<p><a href="#fig-cal-break-curves" class="quarto-xref">Figure&nbsp;<span>15.9</span></a> shows this type of curve for two models. Panel (a) shows the validation results for our naive Bayes model. The points fall on a fairly straight line, demonstrating good overall calibration. Panel (b) has the same visualization for a boosted tree model limited to only three trees in the ensemble. The result is a curve that is not close to the diagonal. The points on the lower left indicate that the predicted probabilities are much larger than they should be. The opposite occurs in the upper right of panel (b); the probabilities are generally underestimated when compared to the rate in the groups.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cal-break-curves" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cal-break-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-cal-break-curves-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cal-break-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.9: The validation set calibration curves for a naive Bayes model (a) and a sabotaged boosted tree (b). The grey marks on the bottom and top frames of the plots are “rugs” and show where the actual values are; the rug on the top corresponds to the poisonous samples, and the marks on the bottom are for the edible mushrooms.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The downside to this binning approach is that, for small data sets, the results can have very large variability, or there may not be enough data within each bin to make accurate judgments regarding calibration. One alternative is to use moving windows. For example, for a fixed bin width of 0.20, we compute the required statistics and then move the bin by a small amount (say 0.02). This would result overlapping groups that would slide across the range of probability values. <a href="#fig-cal-curves" class="quarto-xref">Figure&nbsp;<span>15.10</span></a>(a) shows the results for the naive Bayes model. This visualization is more capable of demonstrating more nuance in the results.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cal-curves" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cal-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-cal-curves-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cal-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.10: Other types of calibration curves demonstrated using the naive Bayes model results.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Another alternative is to fit a model to the data. Logistic regression is a relatively simple classification model for binary outcomes and is discussed in <span class="quarto-unresolved-ref">?sec-logistic-regression</span>. We can fit this model using our observed outcomes and use the estimated class probabilities (<span class="math inline">\(\hat{p}_i\)</span>) as the predictor. If the probabilities are well-calibrated, the model should show a linear relationship. Another feature that can be used in this diagnostic is to model the predictor with a smoothing spline. This enables the logistic model to reflect where in the data range the calibration is very poor. <a href="#fig-cal-curves" class="quarto-xref">Figure&nbsp;<span>15.10</span></a>(b) illustrates the same pattern as the moving window but uses all the data simultaneously.</p>
<p>What can be done with poorly calibrated predictions? There are approaches for postprocessing the predicted probabilities to have higher fidelity to the observed data. The customary method is to use a logistic regression fit, as seen in <a href="#fig-cal-curves" class="quarto-xref">Figure&nbsp;<span>15.10</span></a>(b), to recalibrate the predictions. First a suitable logistic regression model is created in the same manner as the one used for diagnosis. Recall that the original estimates are used as predictors of the model. To recalibrate a set of predictions, the original values are pushed through the model, and the resulting logistic regression values are used to create new probability predictions.</p>
<p><span class="citation" data-cites="platt1999probabilistic">Platt (<a href="#ref-platt1999probabilistic" role="doc-biblioref">1999</a>)</span> proposed using a nonstandard logistic regression to convert any quantitative score into a probability-like value. <em>Platt scaling</em> also adjusts the class indicators from binary 0/1 values to values slightly different from those values.</p>
<p>This raises questions about which data should be used to fit the calibration model and which to evaluate its efficacy. We’ve previously mentioned that using simple repredictions of the training set is a very bad idea, and that is especially true here. If the model was resampled, the out-of-sample predictions can be used to fit the calibration model. In our example, a single validation set was used.</p>
<p>We were able to set aside 6,107 samples for the calibration set, and we will use these to fit the logistic model. <a href="#fig-re-cal-curves" class="quarto-xref">Figure&nbsp;<span>15.11</span></a> shows the results where panel (a) uses a sliding window to assess the smaller data set using a window size of 20% that moves in 5% increments. From these figures, the class probabilities have been improved.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-re-cal-curves" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-re-cal-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-re-cal-curves-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-re-cal-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.11: The validation set calibration curves for naive Bayes model <em>after</em> recalibration using a logistic regression fit to the calibration set. Different data were used to recalibrate the data and to assess how well the procedure worked.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Using this approach to calibration, the new probability estimates have a Brier score of 0.1465. As shown in <a href="#tbl-calibration-results" class="quarto-xref">Table&nbsp;<span>15.3</span></a>, this is slightly less than the uncalibrated score. However, since the uncalibrated and calibrated predictions are so highly correlated, the 90% intervals do not overlap with zero; the difference is real but not large enough to be seen as important.</p>
<div class="columns">
<div class="column" style="width:20%;">

</div><div class="column" style="width:60%;">
<div id="tbl-calibration-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-calibration-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="nondrtlbri" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#nondrtlbri table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#nondrtlbri thead, #nondrtlbri tbody, #nondrtlbri tfoot, #nondrtlbri tr, #nondrtlbri td, #nondrtlbri th {
  border-style: none;
}

#nondrtlbri p {
  margin: 0;
  padding: 0;
}

#nondrtlbri .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#nondrtlbri .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#nondrtlbri .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#nondrtlbri .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#nondrtlbri .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nondrtlbri .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nondrtlbri .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nondrtlbri .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#nondrtlbri .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#nondrtlbri .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#nondrtlbri .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#nondrtlbri .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#nondrtlbri .gt_spanner_row {
  border-bottom-style: hidden;
}

#nondrtlbri .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#nondrtlbri .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#nondrtlbri .gt_from_md > :first-child {
  margin-top: 0;
}

#nondrtlbri .gt_from_md > :last-child {
  margin-bottom: 0;
}

#nondrtlbri .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#nondrtlbri .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#nondrtlbri .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#nondrtlbri .gt_row_group_first td {
  border-top-width: 2px;
}

#nondrtlbri .gt_row_group_first th {
  border-top-width: 2px;
}

#nondrtlbri .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nondrtlbri .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#nondrtlbri .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#nondrtlbri .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nondrtlbri .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nondrtlbri .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#nondrtlbri .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#nondrtlbri .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#nondrtlbri .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nondrtlbri .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nondrtlbri .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#nondrtlbri .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nondrtlbri .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#nondrtlbri .gt_left {
  text-align: left;
}

#nondrtlbri .gt_center {
  text-align: center;
}

#nondrtlbri .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#nondrtlbri .gt_font_normal {
  font-weight: normal;
}

#nondrtlbri .gt_font_bold {
  font-weight: bold;
}

#nondrtlbri .gt_font_italic {
  font-style: italic;
}

#nondrtlbri .gt_super {
  font-size: 65%;
}

#nondrtlbri .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#nondrtlbri .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#nondrtlbri .gt_indent_1 {
  text-indent: 5px;
}

#nondrtlbri .gt_indent_2 {
  text-indent: 10px;
}

#nondrtlbri .gt_indent_3 {
  text-indent: 15px;
}

#nondrtlbri .gt_indent_4 {
  text-indent: 20px;
}

#nondrtlbri .gt_indent_5 {
  text-indent: 25px;
}

#nondrtlbri .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#nondrtlbri div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 20%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="gt_col_headings header">
<th id="term" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col"></th>
<th id="a.estimate" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col">Brier Score</th>
<th id="a.diff" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col">Difference</th>
<th id="a.diff_int" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col">90% Interval</th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="term">Uncalibrated</td>
<td class="gt_row gt_center" headers=".estimate">0.1502</td>
<td class="gt_row gt_center" headers=".diff">—</td>
<td class="gt_row gt_center" headers=".diff_int">—</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="term">Logistic Calibration</td>
<td class="gt_row gt_center" headers=".estimate">0.1465</td>
<td class="gt_row gt_center" headers=".diff">0.00234</td>
<td class="gt_row gt_center" headers=".diff_int">(0.00234, 0.00493)</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="term">Isotonic Boot. Calibration</td>
<td class="gt_row gt_center" headers=".estimate">0.1474</td>
<td class="gt_row gt_center" headers=".diff">0.00170</td>
<td class="gt_row gt_center" headers=".diff_int">(0.0017, 0.00391)</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-calibration-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.3: Validation set Brier scores for the original model predictions and calibrated versions. The differences are the uncalibrated values minus the calibrated values.
</figcaption>
</figure>
</div>
</div><div class="column" style="width:20%;">

</div>
</div>
<p>There are other approaches for improving calibrating. One is isotonic regression <span class="citation" data-cites="isotonic">(<a href="#ref-isotonic" role="doc-biblioref">Zadrozny and Elkan 2002</a>)</span>. This tool fits a model that will generate monotonic, non-decreasing, predictions: <span class="math inline">\(\hat{y}_i \le \hat{y}_{i+ 1}\)</span> when <span class="math inline">\(x_i \le x_{i+ 1}\)</span>. Multiple methods exist to create such models but the most straightforward approach is the pooled-adjacent violators algorithm (PAVA) of <span class="citation" data-cites="pava">Miriam et al. (<a href="#ref-pava" role="doc-biblioref">1955</a>)</span>. This method adjusts a sequence of data points that are not monotonically increasing to be so. For example, <a href="#tbl-isotonic" class="quarto-xref">Table&nbsp;<span>15.4</span></a> shows an example where the rows are ordered by their model’s predicted probabilities. One column shows a binary indicator for class <code>A</code>. The first three rows have the same outcome (<code>B</code>) but the fourth row is actually class <code>A</code>. The PAVA algorithm then takes the averages of the binary indicators that bracket the discordant value, and all of these probability estimates are given the average value (1/3). From the table, it is clear that there are three possible probability estimates after isotonic regression is used.</p>
<div class="columns">
<div class="column" style="width:25%;">

</div><div class="column" style="width:50%;">
<div id="tbl-isotonic" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-isotonic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th colspan="2" data-quarto-table-cell-role="th" style="text-align: left; empty-cells: hide; border-bottom: hidden;"></th>
<th colspan="2" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Estimated Probability of Class A
</div></th>
</tr>
<tr class="even">
<th style="text-align: left;" data-quarto-table-cell-role="th">Truth</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Binary</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Model</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Isotonic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">B</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">B</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">A</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">0.33</td>
</tr>
<tr class="odd">
<td style="text-align: left;">B</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.40</td>
<td style="text-align: right;">0.33</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.50</td>
<td style="text-align: right;">0.33</td>
</tr>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.60</td>
<td style="text-align: right;">1.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">A</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.70</td>
<td style="text-align: right;">1.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.80</td>
<td style="text-align: right;">1.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">A</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.90</td>
<td style="text-align: right;">1.00</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-isotonic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.4: An example of calculations for calibration via isotonic regression.
</figcaption>
</figure>
</div>
</div><div class="column" style="width:25%;">

</div>
</div>
<p>The nice feature of this tool is that it is nonparametric and makes almost no distributional assumptions. However, one downside is that there are few unique recalibrated probability estimates. For this reason, isotonic regression is advised for large data sets. For example, for the naive Bayes data used for calibration, there were originally 6,100 unique probability estimates while, after isotonic regression is applied, there are only 34 possible values. <a href="#fig-recal-iso-curves" class="quarto-xref">Figure&nbsp;<span>15.12</span></a>(a) shows the calibration curve after isotonic regression is applied.</p>
<p>We can improve the isotonic regression results using <em>bagging</em>. As discussed later in <span class="quarto-unresolved-ref">?sec-cls-bagging</span>, bagging creates an ensemble of models by taking <span class="math inline">\(B\)</span> bootstrap samples to build <span class="math inline">\(B\)</span> discinct models. The final model prediction is the average of the <span class="math inline">\(B\)</span> model predictions. For isotonic regression, this reduces the variability in the predicted (i.e., recalibrated) probabilities and also increases the number of unique probability values. Using 1,000 bootstrap samples, we have 940 unique probability estimates.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-recal-iso-curves" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-recal-iso-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-recal-iso-curves-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-recal-iso-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.12: Validation set calibration curves for naive Bayes model <em>after</em> recalibration using isotonic regression from a calibration set. Both panels show moving 20% windows.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-recal-iso-curves" class="quarto-xref">Figure&nbsp;<span>15.12</span></a>(b) shows the ensemble results and <a href="#tbl-calibration-results" class="quarto-xref">Table&nbsp;<span>15.3</span></a> contains the Brier scores. The curve is slightly closer to the line, and there is far less “wiggle” around the values. The bootstrapping method reduces the Brier score from 0.15 to 0.1474. Like logistic calibration, the different is statistically significant but trival.</p>
<p>There are many other tools in the calibration toolbox, such as beta calibration <span class="citation" data-cites="betacal">(<a href="#ref-betacal" role="doc-biblioref">Kull, Filho, and Flach 2017</a>)</span>. There are also methods for calibrating classification models with three or more classes <span class="citation" data-cites="NIPS2001abdbeb4d johansson21a NEURIPS2021bbc92a64">(<a href="#ref-NIPS2001abdbeb4d" role="doc-biblioref">Zadrozny 2001</a>; <a href="#ref-NEURIPS2021bbc92a64" role="doc-biblioref">Zhao et al. 2021</a>; <a href="#ref-johansson21a" role="doc-biblioref">Johansson, Lofstrom, and Bostrom 2021</a>)</span>.</p>
<p>It is important to recompute <em>all</em> metrics using recalibrated predictions. Also note that since the class probabilities have been altered, any hard predictions should be recomputed since some values may have moved across the chosen probability threshold. Using the set of 6,107 data points that have been recalibrated, the reestimated sensitivity and specificity were 77.7% and 79.4%, respectively and the area under the ROC curve was 0.87.</p>
<div class="note-box">
<p>The user should not set expectations very high; recalibration methods often have limited ability to correct predictions. In our example, a large amount of data was set aside for calibration, and we were able to fix a minor issue with the original model’s predictions. In most cases, recalibration might <em>only</em> be able to fix gross miscalibration trends.</p>
</div>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-metrics.html#sec-cls-calibration"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-ordered-categories" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="sec-ordered-categories"><span class="header-section-number">15.9</span> Ordered Categories</h2>
<p>As previously mentioned, our outcome classes can have an inherent ordering. Since there is an ordering, it might be reasonable to think that misclassification severity relates to how “far” a predicted class is from its true class. For example, risk analysis systems, such as <em>Failure Mode, Effects, and Criticality Analysis</em> <span class="citation" data-cites="fmea">(<a href="#ref-fmea" role="doc-biblioref">US Department of Defense 1980</a>)</span>, involve enumerating different ways a thing can fail and the corresponding consequences. Suppose that we have an ordinal scale for risk consisting of “low,” “moderate,” “severe,” and “catastrophic.” It is reasonable to think that predicting the risk is moderate when it is truly catastrophic is very bad. Predicting a moderate risk to be low may not be as dire.</p>
<p>To demonstrate, let’s return to the Wordle data. We can treat the outcome as ordinal since the number of tries has a natural ordering. Using the data associated with <a href="#fig-wordle-prior" class="quarto-xref">Figure&nbsp;<span>15.1</span></a>, we assembled data from a single user for 467 days. These data were modeled with different sets of predictors computed from the letters: counts of each letter (in the word, first letter, last letter, etc), indicators for double letters, and common bi-grams. A set of nine popular starter words was also used to compute a similarity score for each daily word. They are used to predict how uncommon the word might be and would probably influence the number of tries. The distribution of the outcome was: two tries (<span class="math inline">\(n\)</span> = 34), three tries (<span class="math inline">\(n\)</span> = 137), four tries (<span class="math inline">\(n\)</span> = 181), five tries (<span class="math inline">\(n\)</span> = 85), six tries (<span class="math inline">\(n\)</span> = 25), and unsuccessful (<span class="math inline">\(n\)</span> = 5). A 3:1 training/testing split was used to partition these data, and different models were optimized for predicting the number of tries. The left-hand side of <a href="#tbl-confusion-matrix-costs" class="quarto-xref">Table&nbsp;<span>15.5</span></a> shows a test set confusion matrix for a Wordle model for a RuleFit model (described in <span class="quarto-unresolved-ref">?sec-cls-rule-fit</span>). Note that no unsolved words (i.e., class “X”) were in the test set partition.</p>
<div class="columns">
<div class="column" style="width:15%;">

</div><div class="column" style="width:70%;">
<div id="tbl-confusion-matrix-costs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-confusion-matrix-costs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<colgroup>
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th colspan="2" data-quarto-table-cell-role="th" style="text-align: left; empty-cells: hide; border-bottom: hidden;"></th>
<th colspan="6" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Truth
</div></th>
<th data-quarto-table-cell-role="th" style="text-align: left; empty-cells: hide; border-bottom: hidden;"></th>
<th colspan="6" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Linear Weight 'Errors'
</div></th>
</tr>
<tr class="even">
<th style="text-align: left;" data-quarto-table-cell-role="th">Prediction</th>
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: left;" data-quarto-table-cell-role="th">2</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">3</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">4</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">5</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">6</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">X</th>
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: left;" data-quarto-table-cell-role="th">2</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">3</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">4</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">5</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">6</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">X</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">2</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: green !important;">10</span></td>
<td style="text-align: left;"><span style="     ">2</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(238, 99, 99, 255) !important;">2</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(205, 85, 85, 255) !important;">3</span></td>
<td style="text-align: left;"><span style="     color: rgba(139, 58, 58, 255) !important;">4</span></td>
<td style="text-align: left;"><span style="     color: rgba(139, 58, 58, 255) !important;">5</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: green !important;">28</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     ">4</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(238, 99, 99, 255) !important;">2</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(205, 85, 85, 255) !important;">3</span></td>
<td style="text-align: left;"><span style="     color: rgba(139, 58, 58, 255) !important;">4</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     ">7</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: green !important;">43</span></td>
<td style="text-align: left;"><span style="     ">2</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(238, 99, 99, 255) !important;">2</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(238, 99, 99, 255) !important;">2</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(205, 85, 85, 255) !important;">3</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">5</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     ">2</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: green !important;">13</span></td>
<td style="text-align: left;"><span style="     ">3</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(205, 85, 85, 255) !important;">3</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(238, 99, 99, 255) !important;">2</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(238, 99, 99, 255) !important;">2</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">6</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     ">3</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: green !important;">1</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style="     color: rgba(139, 58, 58, 255) !important;">4</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(205, 85, 85, 255) !important;">3</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(238, 99, 99, 255) !important;">2</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">X</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: green !important;">0</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span style="     color: rgba(139, 58, 58, 255) !important;">5</span></td>
<td style="text-align: left;"><span style="     color: rgba(139, 58, 58, 255) !important;">4</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(205, 85, 85, 255) !important;">3</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(238, 99, 99, 255) !important;">2</span></td>
<td style="text-align: left;"><span style=" font-weight: bold;    color: rgba(255, 106, 106, 255) !important;">1</span></td>
<td style="text-align: left;"><span style="     color: rgba(191, 191, 191, 255) !important;">0</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-confusion-matrix-costs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.5: A six-class ordinal outcome with a confusion matrix (left) and a linear set of weights for each potential error (right).
</figcaption>
</figure>
</div>
</div><div class="column" style="width:15%;">

</div>
</div>
<p>About 81% of the class predictions mostly fall along the diagonal line of the confusion matrix. The classes corresponding to three, four, and five tries had appreciable errors.</p>
<p>There are metrics that use hard classification predictions from ordinal models. We previously mentioned the Kappa statistic. To penalize misclassification errors based on their distance to the true classification, <span class="citation" data-cites="cohen1968weighted">Cohen (<a href="#ref-cohen1968weighted" role="doc-biblioref">1968</a>)</span> developed the <em>weighted Kappa</em> statistic, where the errors’ weights depend on their positional differences.</p>
<p>First, let’s rewrite the Kappa statistic definition from <a href="#eq-kappa" class="quarto-xref">Equation&nbsp;<span>15.3</span></a> in terms of classification error (i.e., the opposite of accuracy). We can compute the error as the number of off-diagonal counts divided by the total count. For expected errors, a similar approach to <a href="#eq-exp-agree" class="quarto-xref">Equation&nbsp;<span>15.4</span></a> can be used to find the amount of chance error. With these two types of error in hand, Kappa is:</p>
<p><span id="eq-kappa-error"><span class="math display">\[
Kappa = 1 - \frac{error}{expected\:error}
\tag{15.26}\]</span></span></p>
<p>Weighted Kappa assigns larger costs (i.e., errors) to cells in the confusion matrix that are “farther away” from the true category. A linear weighting for the Kappa statistic is shown in <a href="#tbl-confusion-matrix-costs" class="quarto-xref">Table&nbsp;<span>15.5</span></a>. There are bands of diagonal integer weights that are 0, 1, 2, <span class="math inline">\(\ldots\)</span>, 5. A quadratic weighting scheme would square each of these weights (e.g., 0, 1, 2<sup>2</sup>, 3<sup>2</sup>, etc.).</p>
<p>For this test set, the estimate of the unweighted Kappa statistic was 0.722, which is outstanding. If we increase the penalty for more disparate errors, the performance values for linear and quadratic weights were 0.75 and 0.791, respectively. Notice that the Kappa values <em>increase</em> as the weights become more severe. In these data, the “cost” of the errors increases faster in the expected agreement than in the observed agreement. This is not often the case.</p>
<p><span class="citation" data-cites="nakov2019semeval">Nakov et al. (<a href="#ref-nakov2019semeval" role="doc-biblioref">2019</a>)</span> suggested ordinal versions of common regression metrics, such as the mean squared error (MSE). These use the positional “errors” between the true and predicted outcomes. For example, if a word requires four tries, but we predict the puzzle will be solved with two tries, the error is -2. These metrics should be fairly understandable since they mimic common statistics used to evaluate regression models. However, there has been scant evidence that they offer a benefit over existing metrics that have been studied and evaluated for decades. The mean squared error for these data was 0.611. How bad is this? If we repeatedly permute the outcome classes and measure the RMSE we find that the worst-case scenario is about 1.34.</p>
<p>For probability predictions, there is an extension to the Brier score called the <em>ranked probability score</em> (RPS). See <span class="citation" data-cites="epstein1969scoring">Epstein (<a href="#ref-epstein1969scoring" role="doc-biblioref">1969</a>)</span> and <span class="citation" data-cites="constantinou2012solving">Constantinou and Fenton (<a href="#ref-constantinou2012solving" role="doc-biblioref">2012</a>)</span>. The computations are very similar to <a href="#eq-brier" class="quarto-xref">Equation&nbsp;<span>15.25</span></a> but use different data:</p>
<p><span id="eq-rps"><span class="math display">\[
RPS = \frac{1}{n(C-1)}\sum_{i=1}^n\sum_{k=1}^C (\tilde{y}_{ik} - \tilde{p}_{ik})^2
\tag{15.27}\]</span></span></p>
<p>Instead of using a binary value <span class="math inline">\(y_{ik}\)</span> corresponding to class <span class="math inline">\(k\)</span> only, the RPS assigns a value of 1 to <span class="math inline">\(\tilde{y}_{ik}\)</span> if the position of the true class is <span class="math inline">\(\le k\)</span>. Similarly, the value of <span class="math inline">\(\tilde{p}_{ik}\)</span> is the cumulative sum of the estimated probabilities corresponding to classes <span class="math inline">\(\le k\)</span>. For example, with the Wordle data, <span class="math inline">\(k = 3\)</span> corresponds to four tries and <span class="math inline">\(\tilde{y}_{i3}\)</span> would have a value of 1.0 if the i<sup>th</sup> sample was solved in two, three, or four tries (and is zero otherwise). The corresponding cumulative probability is the sum of the individual probabilities of solving the puzzle in two, three, or four tries. The RPS inherits the same benefits as the Brier score since they are both proper scoring rules. See <span class="citation" data-cites="stael1970family">Stael von Holstein (<a href="#ref-stael1970family" role="doc-biblioref">1970</a>)</span> and <span class="citation" data-cites="gneiting2007strictly">Gneiting and Raftery (<a href="#ref-gneiting2007strictly" role="doc-biblioref">2007</a>)</span>.</p>
<p>The denominator is normalized by dividing by <span class="math inline">\(C-1\)</span>. The minus one compensates for the fact that all predictions are perfect for the largest class level (since they are cumulative).</p>
<p>The ranked probability score measures the correctness (and calibration) of the question: “How well am I predicting that the event occurs in <span class="math inline">\(k\)</span> or fewer classes?” RPS can be more forgiving of inaccuracies in the individual probability estimates as long as their sum is accurate. See <span class="citation" data-cites="murphy1970ranked">Murphy (<a href="#ref-murphy1970ranked" role="doc-biblioref">1970</a>)</span> for a comparison of these two flavors of the Brier score.</p>
<p>The test set probabilities associated with the test set were used to compute the Brier score estimate and the ranked probability score value. The RPS estimate was 0.0458 and its corresponding permuted “no information” estimate was 0.149. This indicates that the model functions effectively when evaluating the cumulative probability estimates. For comparison, the unordered Brier score for these data was 0.162 (with permuted value 0.476).</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-metrics.html#sec-ordered-categories"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-cls-multi-objectives" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="sec-cls-multi-objectives"><span class="header-section-number">15.10</span> Multi-Objective Assessments</h2>
<p>Choosing a single metric that captures all of the model’s goals may be challenging. When hard class predictions are important, the choice of the probability cutoff should be driven by measures such as sensitivity, specificity, or precision. We might want to optimize a function of multiple metrics (e.g., the ratio of sensitivity and specificity, etc.). Additionally, these metrics don’t consider model calibration and other aspects of the model.</p>
<p>Sometimes, a joint metric like the F-score can be used if it aligns with the project goals. Similarly, the J-index <span class="citation" data-cites="Youdencs">(<a href="#ref-Youdencs" role="doc-biblioref">Youden 1950</a>)</span> might also suffice:</p>
<p><span class="math display">\[
J = Sensitivity + Specificity - 1
\]</span></p>
<p>Additionally, we might want to factor in the characteristics of our trained model. For example, we’ve previously seen that the number of active features in the model can help us better choose which candidate to select. As another example, if we choose between different tuning parameters in a neural network, we might be interested in seeing how the total number of model parameters affects the primary performance metric. There might be an advantageous trade-off between model complexity and performance.</p>
<p>A more advanced <em>multiparameter optimization</em> may be required to differentiate models. Many different methods for accomplishing this are well summarized in <span class="citation" data-cites="karl2023multi">Karl et al. (<a href="#ref-karl2023multi" role="doc-biblioref">2023</a>)</span>. If we have <span class="math inline">\(s\)</span> candidate models, some approaches require the full set of metrics and characteristics to decide (e.g., Pareto optimality). Alternatively, we might be more interested in developing a single objective function that combines metrics in a meaningful way that can measure a single candidate. Iterative optimization (or racing methods) would require a combined performance metric in real time.</p>
<p>We’ll focus on one method to blend multiple criteria into a single composite value called “desirability functions.” Originally conceived by <span class="citation" data-cites="harrington1965desirability">Harrington (<a href="#ref-harrington1965desirability" role="doc-biblioref">1965</a>)</span>, the versions of desirability functions defined by <span class="citation" data-cites="derringer1980simultaneous">Derringer and Suich (<a href="#ref-derringer1980simultaneous" role="doc-biblioref">1980</a>)</span> are more commonly used. They are extensively used in statistical experimental design but also in a wide range of applications, including designing cancer combination therapy <span class="citation" data-cites="kuhn2000incorporating">(<a href="#ref-kuhn2000incorporating" role="doc-biblioref">Kuhn, Carter, and Myers 2000</a>)</span>, medicinal chemistry <span class="citation" data-cites="wager2010moving">(<a href="#ref-wager2010moving" role="doc-biblioref">Wager et al. 2010</a>)</span>, wastewater processing <span class="citation" data-cites="corral2019coagulation">(<a href="#ref-corral2019coagulation" role="doc-biblioref">Bobadilla et al. 2019</a>)</span>, and others. These tools are knowledge-based; users must state specific values of their criteria that are desirable or unacceptable.</p>
<p>To optimize ML models, desirability functions can be defined for any important facet of the model result. For example, we could articulate that we should <em>minimize the Brier score</em> such that</p>
<ol type="1">
<li>Fewer than 20 active predictors are used.</li>
<li>Specificity is at least 80%</li>
<li>Sensitivity is maximized</li>
</ol>
<p>In each of these cases, we can define ranges of acceptable results (e.g., Brier <span class="math inline">\(\in [0, .1)\)</span>, sensitivity <span class="math inline">\(\in (0.8, 1.0]\)</span>, etc.)</p>
<p>Desirability functions map values to a unit range where a value of 1.0 is the most desirable situation while a value of zero is a dealbreaker. <a href="#fig-desirability" class="quarto-xref">Figure&nbsp;<span>15.13</span></a> shows an example mapping for each of the four hypothetical characteristics given above. Three of the four mappings are linear between the best and worst cases, and the other is a “box” constraint that indicates that any value in the range is acceptable.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-desirability" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-desirability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="cls-metrics_files/figure-html/fig-desirability-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-desirability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.13: Examples of desirability functions for four characteristics.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The Brier score and active features use a minimization strategy that is encoded as</p>
<p><span id="eq-d-min"><span class="math display">\[
d_i=
\begin{cases}
    0       &amp;\text{if $x&gt; B$}   \\
    \left(\frac{x-B}{A - B}\right)^{\gamma}     
            &amp;\text{if $A \leq x \leq B$}    \\
    1       &amp;\text{if $x&lt;A$}
\end{cases}
\tag{15.28}\]</span></span></p>
<p>where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are the low and high values, respectively. The <span class="math inline">\(\gamma\)</span> parameter is a scaling factor that modulates of easy (or difficult) it should be to satisfy the goal. The curves in <a href="#fig-desirability" class="quarto-xref">Figure&nbsp;<span>15.13</span></a> use the default value of <span class="math inline">\(\gamma = 1\)</span>, producing straight lines between the limits. Values greater than one make desirability more easily satisfied, and smaller values will result in individual desirability having more impact on the overall optimization.</p>
<p>The sensitivity curve in <a href="#fig-desirability" class="quarto-xref">Figure&nbsp;<span>15.13</span></a> is meant to maximize the characteristic using the equations:</p>
<p><span id="eq-d-max"><span class="math display">\[
d_i=
\begin{cases}
    0       &amp;\text{if $x &lt; A$}  \\
    \left(\frac{x-A}{B-A}\right)^{\gamma}       
            &amp;\text{if $A \leq x \leq B$}    \\
    1       &amp;\text{if $x&gt;B$}
\end{cases}
\tag{15.29}\]</span></span></p>
<p>Different types of desirability functions exist for different goals (box constraints, qualitative values, etc.).</p>
<p>Once we have defined a function for each of the <span class="math inline">\(q\)</span> important characteristics, the <em>overall desirability</em> is a combination of each using a geometric mean:</p>
<p><span id="eq-d-overall"><span class="math display">\[
D = \left(\prod_{i=1}^q d_j\right)^{1/q}
\tag{15.30}\]</span></span></p>
<p>After these computations, the candidates can be ranked by <span class="math inline">\(D\)</span>. Note that the geometric mean has the effect that if any single characteristic has an unacceptable value (i.e., zero), the overall combination is unacceptable.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/cls-metrics.html#sec-cls-multi-objectives"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="chapter-references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="chapter-references">Chapter References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bella2013effect" class="csl-entry" role="listitem">
Bella, A, C Ferri, J Hernandez-Orallo, and Maria J Ramirez-Quintana. 2013. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=On+the+effect+of+calibration+in+classifier+combination&amp;as_ylo=2013&amp;as_yhi=2013&amp;btnG=">On the Effect of Calibration in Classifier Combination</a>.”</span> <em>Applied Intelligence</em> 38 (4): 566–85.
</div>
<div id="ref-corral2019coagulation" class="csl-entry" role="listitem">
Bobadilla, C, R Lorza, R García, F Gómez, and E González. 2019. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Coagulation+determination+of+key+operating+parameters+by+multi+response+surface+methodology+using+desirability+functions&amp;as_ylo=2019&amp;as_yhi=2019&amp;btnG=">Coagulation: Determination of Key Operating Parameters by Multi-Response Surface Methodology Using Desirability Functions</a>.”</span> <em>Water</em> 11 (2): 398.
</div>
<div id="ref-Bridle1990" class="csl-entry" role="listitem">
Bridle, J. 1990. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Probabilistic+Interpretation+of+Feedforward+Classification+Network+Outputs+with+Relationships+to+Statistical+Pattern+Recognition&amp;as_ylo=1990&amp;as_yhi=1990&amp;btnG=">Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition</a>.”</span> In <em>Neurocomputing: Algorithms, Architectures and Applications</em>, 227–36. Springer-Verlag.
</div>
<div id="ref-brier1950verification" class="csl-entry" role="listitem">
Brier, G. 1950. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Verification+of+forecasts+expressed+in+terms+of+probability&amp;as_ylo=1950&amp;as_yhi=1950&amp;btnG=">Verification of Forecasts Expressed in Terms of Probability</a>.”</span> <em>Monthly Weather Review</em> 78 (1): 1–3.
</div>
<div id="ref-clopper1934use" class="csl-entry" role="listitem">
Clopper, C, and E Pearson. 1934. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+use+of+confidence+or+fiducial+limits+illustrated+in+the+case+of+the+binomial&amp;as_ylo=1934&amp;as_yhi=1934&amp;btnG=">The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial</a>.”</span> <em>Biometrika</em> 26 (4): 404–13.
</div>
<div id="ref-Cohen1960" class="csl-entry" role="listitem">
Cohen, J. 1960. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+Coefficient+of+Agreement+for+Nominal+Data&amp;as_ylo=1960&amp;as_yhi=1960&amp;btnG=">A Coefficient of Agreement for Nominal Data</a>.”</span> <em>Educational and Psychological Measurement</em> 20: 37–46.
</div>
<div id="ref-cohen1968weighted" class="csl-entry" role="listitem">
Cohen, J. 1968. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Weighted+Kappa+Nominal+scale+agreement+provision+for+scaled+disagreement+or+partial+credit+&amp;as_ylo=1968&amp;as_yhi=1968&amp;btnG=">Weighted <span>Kappa</span>: Nominal Scale Agreement Provision for Scaled Disagreement or Partial Credit.</a>”</span> <em>Psychological Bulletin</em> 70 (4): 213.
</div>
<div id="ref-constantinou2012solving" class="csl-entry" role="listitem">
Constantinou, A, and N Fenton. 2012. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Solving+the+problem+of+inadequate+scoring+rules+for+assessing+probabilistic+football+forecast+models&amp;as_ylo=2012&amp;as_yhi=2012&amp;btnG=">Solving the Problem of Inadequate Scoring Rules for Assessing Probabilistic Football Forecast Models</a>.”</span> <em>Journal of Quantitative Analysis in Sports</em> 8 (1).
</div>
<div id="ref-davis2006relationship" class="csl-entry" role="listitem">
Davis, J, and M Goadrich. 2006. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+relationship+between+Precision+Recall+and+ROC+curves&amp;as_ylo=2006&amp;as_yhi=2006&amp;btnG=">The Relationship Between Precision-Recall and ROC Curves</a>.”</span> In <em>Proceedings of the 23rd International Conference on Machine Learning</em>, 233–40.
</div>
<div id="ref-delgado2019cohen" class="csl-entry" role="listitem">
Delgado, R, and XA Tibau. 2019. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Why+Cohen+s+Kappa+should+be+avoided+as+performance+measure+in+classification&amp;as_ylo=2019&amp;as_yhi=2019&amp;btnG=">Why Cohen’s Kappa Should Be Avoided as Performance Measure in Classification</a>.”</span> <em>PloS One</em> 14 (9): e0222916.
</div>
<div id="ref-derringer1980simultaneous" class="csl-entry" role="listitem">
Derringer, G, and R Suich. 1980. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Simultaneous+optimization+of+several+response+variables&amp;as_ylo=1980&amp;as_yhi=1980&amp;btnG=">Simultaneous Optimization of Several Response Variables</a>.”</span> <em>Journal of Quality Technology</em> 12 (4): 214–19.
</div>
<div id="ref-ekstrom2011phi" class="csl-entry" role="listitem">
Ekstrom, J. 2011. <span>“The Phi-Coefficient, the Tetrachoric Correlation Coefficient, and the Pearson-Yule Debate.”</span> UCLA Department of Statistics. <a href="https://escholarship.org/uc/item/7qp4604r">https://escholarship.org/uc/item/7qp4604r</a>.
</div>
<div id="ref-epstein1969scoring" class="csl-entry" role="listitem">
Epstein, E. 1969. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+scoring+system+for+probability+forecasts+of+ranked+categories&amp;as_ylo=1969&amp;as_yhi=1969&amp;btnG=">A Scoring System for Probability Forecasts of Ranked Categories</a>.”</span> <em>Journal of Applied Meteorology (1962-1982)</em> 8 (6): 985–87.
</div>
<div id="ref-gneiting2007strictly" class="csl-entry" role="listitem">
Gneiting, T, and A Raftery. 2007. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Strictly+proper+scoring+rules+prediction+and+estimation&amp;as_ylo=2007&amp;as_yhi=2007&amp;btnG=">Strictly Proper Scoring Rules, Prediction, and Estimation</a>.”</span> <em>Journal of the American Statistical Association</em> 102 (477): 359–78.
</div>
<div id="ref-guggenmoos1996meaning" class="csl-entry" role="listitem">
Guggenmoos-Holzmann, I. 1996. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+meaning+of+kappa+probabilistic+concepts+of+reliability+and+validity+revisited&amp;as_ylo=1996&amp;as_yhi=1996&amp;btnG=">The Meaning of Kappa: Probabilistic Concepts of Reliability and Validity Revisited</a>.”</span> <em>Journal of Clinical Epidemiology</em> 49 (7): 775–82.
</div>
<div id="ref-Hand2001us" class="csl-entry" role="listitem">
Hand, D, and R Till. 2001b. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+Simple+Generalisation+of+the+Area+Under+the+ROC+Curve+for+Multiple+Class+Classification+Problems&amp;as_ylo=2001&amp;as_yhi=2001&amp;btnG=">A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems</a>.”</span> <em>Machine Learning</em> 45 (2): 171–86.
</div>
<div id="ref-hand2001simple" class="csl-entry" role="listitem">
Hand, D, and R Till. 2001a. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+simple+generalisation+of+the+area+under+the+ROC+curve+for+multiple+class+classification+problems&amp;as_ylo=2001&amp;as_yhi=2001&amp;btnG=">A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems</a>.”</span> <em>Machine Learning</em> 45: 171–86.
</div>
<div id="ref-Hanley1982uc" class="csl-entry" role="listitem">
Hanley, J, and B McNeil. 1982. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+Meaning+and+Use+of+the+Area+under+a+Receiver+Operating+ROC+Curvel+Characteristic&amp;as_ylo=1982&amp;as_yhi=1982&amp;btnG=">The Meaning and Use of the Area Under a Receiver Operating (ROC) Curvel Characteristic</a>.”</span> <em>Radiology</em> 143 (1): 29–36.
</div>
<div id="ref-harrington1965desirability" class="csl-entry" role="listitem">
Harrington, E. 1965. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+desirability+function&amp;as_ylo=1965&amp;as_yhi=1965&amp;btnG=">The Desirability Function</a>.”</span> <em>Industrial Quality Control</em> 21 (10): 494–98.
</div>
<div id="ref-johansson21a" class="csl-entry" role="listitem">
Johansson, U, T Lofstrom, and H Bostrom. 2021. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Calibrating+multi+class+models&amp;as_ylo=2021&amp;as_yhi=2021&amp;btnG=">Calibrating Multi-Class Models</a>.”</span> In <em>Proceedings of the Tenth Symposium on Conformal and Probabilistic Prediction and Applications</em>, edited by L Carlsson, Z Luo, G Cherubin, and K An Nguyen, 152:111–30. Proceedings of Machine Learning Research.
</div>
<div id="ref-karl2023multi" class="csl-entry" role="listitem">
Karl, F, T Pielok, J Moosbauer, F Pfisterer, S Coors, M Binder, L Schneider, et al. 2023. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Multi+objective+hyperparameter+optimization+in+machine+learning+An+overview&amp;as_ylo=2023&amp;as_yhi=2023&amp;btnG=">Multi-Objective Hyperparameter Optimization in Machine Learning—an Overview</a>.”</span> <em>ACM Transactions on Evolutionary Learning and Optimization</em> 3 (4): 1–50.
</div>
<div id="ref-kruppa2014probability" class="csl-entry" role="listitem">
Kruppa, J, Y Liu, HC Diener, T Holste, R Weimar, I Konig, and A Ziegler. 2014. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Probability+estimation+with+machine+learning+methods+for+dichotomous+and+multicategory+outcome+Applications&amp;as_ylo=2014&amp;as_yhi=2014&amp;btnG=">Probability Estimation with Machine Learning Methods for Dichotomous and Multicategory Outcome: Applications</a>.”</span> <em>Biometrical Journal</em> 56 (4): 564–83.
</div>
<div id="ref-Kuhn2016" class="csl-entry" role="listitem">
Kuhn, M. 2016. <span>“Nonclinical Statistics for Pharmaceutical and Biotechnology Industries.”</span> In, edited by L Zhang, 141–55. Springer International Publishing.
</div>
<div id="ref-kuhn2000incorporating" class="csl-entry" role="listitem">
Kuhn, M, WH Carter, and R Myers. 2000. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Incorporating+noise+factors+into+experiments+with+censored+data&amp;as_ylo=2000&amp;as_yhi=2000&amp;btnG=">Incorporating Noise Factors into Experiments with Censored Data</a>.”</span> <em>Technometrics</em> 42 (4): 376–83.
</div>
<div id="ref-betacal" class="csl-entry" role="listitem">
Kull, M, T Filho, and P Flach. 2017. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Beta+calibration+a+well+founded+and+easily+implemented+improvement+on+logistic+calibration+for+binary+classifiers&amp;as_ylo=2017&amp;as_yhi=2017&amp;btnG=">Beta Calibration: A Well-Founded and Easily Implemented Improvement on Logistic Calibration for Binary Classifiers</a>.”</span> In <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</em>, edited by A Singh and J Zhu, 54:623–31. Proceedings of Machine Learning Research.
</div>
<div id="ref-lewis1991evaluating" class="csl-entry" role="listitem">
Lewis, D. 1991. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Evaluating+text+categorization&amp;as_ylo=1991&amp;as_yhi=1991&amp;btnG=">Evaluating Text Categorization</a>.”</span> In <em>Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California</em>.
</div>
<div id="ref-lichtenstein1977calibration" class="csl-entry" role="listitem">
Lichtenstein, S, B Fischhoff, and L Phillips. 1977. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Calibration+of+probabilities+The+state+of+the+art&amp;as_ylo=1977&amp;as_yhi=1977&amp;btnG=">Calibration of Probabilities: The State of the Art</a>.”</span> In <em>Decision Making and Change in Human Affairs: Proceedings of the Fifth Research Conference on Subjective Probability, Utility, and Decision Making</em>, 275–324. Springer.
</div>
<div id="ref-maglich2014more" class="csl-entry" role="listitem">
Maglich, J, M Kuhn, R Chapin, and M Pletcher. 2014. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=More+than+just+hormones+H295R+cells+as+predictors+of+reproductive+toxicity&amp;as_ylo=2014&amp;as_yhi=2014&amp;btnG=">More Than Just Hormones: H295R Cells as Predictors of Reproductive Toxicity</a>.”</span> <em>Reproductive Toxicology</em> 45: 77–86.
</div>
<div id="ref-MATTHEWS1975442" class="csl-entry" role="listitem">
Matthews, B. W. 1975. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Comparison+of+the+predicted+and+observed+secondary+structure+of+T4+phage+lysozyme&amp;as_ylo=1975&amp;as_yhi=1975&amp;btnG=">Comparison of the Predicted and Observed Secondary Structure of T4 Phage Lysozyme</a>.”</span> <em>Biochimica Et Biophysica Acta (BBA) - Protein Structure</em> 405 (2): 442–51.
</div>
<div id="ref-McClish" class="csl-entry" role="listitem">
McClish, D. 1989. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Analyzing+a+Portion+of+the+ROC+Curve&amp;as_ylo=1989&amp;as_yhi=1989&amp;btnG=">Analyzing a Portion of the ROC Curve</a>.”</span> <em>Medical Decision Making</em> 9: 190–95.
</div>
<div id="ref-meeker2017statistical" class="csl-entry" role="listitem">
Meeker, W, G Hahn, and L Escobar. 2017. <em>Statistical Intervals: A Guide for Practitioners and Researchers</em>. Vol. 541. John Wiley &amp; Sons.
</div>
<div id="ref-pava" class="csl-entry" role="listitem">
Miriam, A, H Brunk, G Ewing, W Reid, and E Silverman. 1955. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=An+Empirical+Distribution+Function+for+Sampling+with+Incomplete+Information&amp;as_ylo=1955&amp;as_yhi=1955&amp;btnG=">An Empirical Distribution Function for Sampling with Incomplete Information</a>.”</span> <em>The Annals of Mathematical Statistics</em> 26 (4): 641–47.
</div>
<div id="ref-murphy1970ranked" class="csl-entry" role="listitem">
Murphy, A. 1970. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+ranked+probability+score+and+the+probability+score+A+comparison&amp;as_ylo=1970&amp;as_yhi=1970&amp;btnG=">The Ranked Probability Score and the Probability Score: A Comparison</a>.”</span> <em>Monthly Weather Review</em> 98 (12): 917–24.
</div>
<div id="ref-murphy1973new" class="csl-entry" role="listitem">
Murphy, A. 1973. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+new+vector+partition+of+the+probability+score&amp;as_ylo=1973&amp;as_yhi=1973&amp;btnG=">A New Vector Partition of the Probability Score</a>.”</span> <em>Journal of Applied Meteorology and Climatology</em> 12 (4): 595–600.
</div>
<div id="ref-nakov2019semeval" class="csl-entry" role="listitem">
Nakov, P, A Ritter, S Rosenthal, F Sebastiani, and V Stoyanov. 2019. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=+SemEval+2016+task+4+Sentiment+analysis+in+Twitter+&amp;as_ylo=2019&amp;as_yhi=2019&amp;btnG="><span>SemEval-2016</span> Task 4: Sentiment Analysis in <span>Twitter</span></a>.”</span> <em>arXiv</em>.
</div>
<div id="ref-newcombe1998two" class="csl-entry" role="listitem">
Newcombe, R. 1998. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Two+sided+confidence+intervals+for+the+single+proportion+comparison+of+seven+methods&amp;as_ylo=1998&amp;as_yhi=1998&amp;btnG=">Two-Sided Confidence Intervals for the Single Proportion: Comparison of Seven Methods</a>.”</span> <em>Statistics in Medicine</em> 17 (8): 857–72.
</div>
<div id="ref-ozenne2015precision" class="csl-entry" role="listitem">
Ozenne, B, F Subtil, and D Maucort-Boulch. 2015. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+precision+recall+curve+overcame+the+optimism+of+the+receiver+operating+characteristic+curve+in+rare+diseases&amp;as_ylo=2015&amp;as_yhi=2015&amp;btnG=">The Precision–Recall Curve Overcame the Optimism of the Receiver Operating Characteristic Curve in Rare Diseases</a>.”</span> <em>Journal of Clinical Epidemiology</em> 68 (8): 855–59.
</div>
<div id="ref-platt1999probabilistic" class="csl-entry" role="listitem">
Platt, J. 1999. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Probabilistic+outputs+for+support+vector+machines+and+comparisons+to+regularized+likelihood+methods&amp;as_ylo=1999&amp;as_yhi=1999&amp;btnG=">Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods</a>.”</span> <em>Advances in Large Margin Classifiers</em> 10 (3): 61–74.
</div>
<div id="ref-saito2015precision" class="csl-entry" role="listitem">
Saito, T, and M Rehmsmeier. 2015. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+precision+recall+plot+is+more+informative+than+the+ROC+plot+when+evaluating+binary+classifiers+on+imbalanced+datasets&amp;as_ylo=2015&amp;as_yhi=2015&amp;btnG=">The Precision-Recall Plot Is More Informative Than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets</a>.”</span> <em>PloS One</em> 10 (3): e0118432.
</div>
<div id="ref-sanders1963subjective" class="csl-entry" role="listitem">
Sanders, F. 1963. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=On+subjective+probability+forecasting&amp;as_ylo=1963&amp;as_yhi=1963&amp;btnG=">On Subjective Probability Forecasting</a>.”</span> <em>Journal of Applied Meteorology and Climatology</em> 2 (2): 191–201.
</div>
<div id="ref-stael1970family" class="csl-entry" role="listitem">
Stael von Holstein, CA. 1970. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+family+of+strictly+proper+scoring+rules+which+are+sensitive+to+distance&amp;as_ylo=1970&amp;as_yhi=1970&amp;btnG=">A Family of Strictly Proper Scoring Rules Which Are Sensitive to Distance</a>.”</span> <em>Journal of Applied Meteorology and Climatology</em> 9 (3): 360–64.
</div>
<div id="ref-stanski1989survey" class="csl-entry" role="listitem">
Stanski, H, L Wilson, and W Burrows. 1989. <span>“Survey of Common Verification Methods in Meteorology.”</span> World Meteorological Organization.
</div>
<div id="ref-thompson1988reappraisal" class="csl-entry" role="listitem">
Thompson, D, and S Walter. 1988. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+reappraisal+of+the+kappa+coefficient&amp;as_ylo=1988&amp;as_yhi=1988&amp;btnG=">A Reappraisal of the Kappa Coefficient</a>.”</span> <em>Journal of Clinical Epidemiology</em> 41 (10): 949–58.
</div>
<div id="ref-fmea" class="csl-entry" role="listitem">
US Department of Defense. 1980. <em>Military Standard: Procedures for Performing a Failure Mode, Effects, and Criticality Analysis (<span>MIL-STD-1629A</span>)</em>.
</div>
<div id="ref-wager2010moving" class="csl-entry" role="listitem">
Wager, T, X Hou, P Verhoest, and A Villalobos. 2010. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Moving+beyond+rules+the+development+of+a+central+nervous+system+multiparameter+optimization+CNS+MPO+approach+to+enable+alignment+of+druglike+properties&amp;as_ylo=2010&amp;as_yhi=2010&amp;btnG=">Moving Beyond Rules: The Development of a Central Nervous System Multiparameter Optimization (CNS MPO) Approach to Enable Alignment of Druglike Properties</a>.”</span> <em>ACS Chemical Neuroscience</em> 1 (6): 435–49.
</div>
<div id="ref-wagner2021mushroom" class="csl-entry" role="listitem">
Wagner, D, D Heider, and G Hattab. 2021. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Mushroom+data+creation+curation+and+simulation+to+support+classification+tasks&amp;as_ylo=2021&amp;as_yhi=2021&amp;btnG=">Mushroom Data Creation, Curation, and Simulation to Support Classification Tasks</a>.”</span> <em>Scientific Reports</em> 11 (1): 8134.
</div>
<div id="ref-yates1982external" class="csl-entry" role="listitem">
Yates, F. 1982. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=External+correspondence+Decompositions+of+the+mean+probability+score&amp;as_ylo=1982&amp;as_yhi=1982&amp;btnG=">External Correspondence: Decompositions of the Mean Probability Score</a>.”</span> <em>Organizational Behavior and Human Performance</em> 30 (1): 132–56.
</div>
<div id="ref-Youdencs" class="csl-entry" role="listitem">
Youden, W. 1950. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Index+for+Rating+Diagnostic+Tests&amp;as_ylo=1950&amp;as_yhi=1950&amp;btnG=">Index for Rating Diagnostic Tests</a>.”</span> <em>Cancer</em> 3 (1): 32–35.
</div>
<div id="ref-NIPS2001abdbeb4d" class="csl-entry" role="listitem">
Zadrozny, B. 2001. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Reducing+multiclass+to+binary+by+coupling+probability+estimates&amp;as_ylo=2001&amp;as_yhi=2001&amp;btnG=">Reducing Multiclass to Binary by Coupling Probability Estimates</a>.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by T Dietterich, S Becker, and Z Ghahramani. Vol. 14. MIT Press.
</div>
<div id="ref-isotonic" class="csl-entry" role="listitem">
Zadrozny, B, and C Elkan. 2002. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Transforming+Classifier+Scores+into+Accurate+Multiclass+Probability+Estimates&amp;as_ylo=2002&amp;as_yhi=2002&amp;btnG=">Transforming Classifier Scores into Accurate Multiclass Probability Estimates</a>.”</span> In <em>Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 694–99. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-NEURIPS2021bbc92a64" class="csl-entry" role="listitem">
Zhao, S, M Kim, R Sahoo, T Ma, and S Ermon. 2021. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Calibrating+Predictions+to+Decisions+A+Novel+Approach+to+Multi+Class+Calibration&amp;as_ylo=2021&amp;as_yhi=2021&amp;btnG=">Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration</a>.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by M Ranzato, A Beygelzimer, Y Dauphin, P Liang, and J Wortman Vaughan, 34:22313–24. Curran Associates, Inc.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://www.nytimes.com/games/wordle/index.html"><code>https://www.nytimes.com/games/wordle/index.html</code></a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This author’s starter word is <code>caret</code> - it contains two common vowels and three important consonants.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>If first guesses were random, this rate is 6-fold higher than is expected by chance. Even though initial guesses are <em>not</em> random, the initial guess rate is implausibly high.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>This is the same as Shannon entropy and “information content”.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Class imbalances can be difficult to overcome. <span class="quarto-unresolved-ref">?sec-imbalances</span> will discuss different approaches to alleviating their effects.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This data set is very easy to predict. Other models, even simple decision trees, can produce nearly perfect results. We’ll use it here with a less powerful model for illustration purposes. The gravity of incorrectly predicting a poisonous mushroom to be edible makes discussing certain errors more compelling.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>This does lead to confusion since the loss function in question, the likelihood, is different for different data types (e.g., real numbers, counts, etc.). In linear regression, ”log loss” also corresponds to the sum of squared errors.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Also called “mean probability score.”<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>There is also a direct connection between refinement and the area under the ROC curve. See <span class="citation" data-cites="hand2001simple">Hand and Till (<a href="#ref-hand2001simple" role="doc-biblioref">2001a</a>)</span>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Usually <em>all</em> possible cutoffs.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Due to randomness, it might have some area under the diagonal line.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>It is not ideal to be in the position of having to justify a model that just gave someone bad news about their favorite thing. This situation makes one appreciate the crucial importance of very good documentation about the model.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>The curve can fall slightly lower than the lower diagonal boundary by chance.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>This will not be the case when assessing calibration for regression models because our outcome is in the same numerical format as the prediction.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/aml4td\.org");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/comparing-models.html" class="pagination-link" aria-label="Comparing Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Comparing Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/cls-linear.html" class="pagination-link" aria-label="Generalized Linear and Additive Classifiers">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Generalized Linear and Additive Classifiers</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js" type="text/javascript"></script>
<script type="text/javascript">
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>




</body></html>