---
knitr:
  opts_chunk:
    cache.path: "../_cache/overfitting/"
---

# Overfitting {#sec-overfitting}

```{r}
#| label: overfitting-setup
#| include: false
#| message: false
#| warning: false

source("../R/_common.R")

# ------------------------------------------------------------------------------
# Required packages

library(tidymodels)
library(bonsai)
library(patchwork)
library(rlang)
library(kernlab)
library(splines2)
library(future)

# ------------------------------------------------------------------------------
# Set options

plan("multisession")
tidymodels_prefer()
theme_set(theme_transparent())
set_options()

# ------------------------------------------------------------------------------
# Load other data

load("../RData/bayes_opt_calcs.RData")
```

This chapter describes the most crucial concept in predictive modeling: overfitting. When a model fails, it is almost always due to overfitting of some sort. The problem is that you may only realize that there is an issue once you finish the development phase of modeling and are exposed to completely new data.

To get started, we'll discuss model complexity and how it can be both good and bad. 

## Model Complexity and Overfitting {#sec-complexity}

Many predictive models have parameters. For example, the linear regression model initially shown in @eq-poly-linear-reg contains slopes and intercepts (the $\beta$ values) estimated during model training using the predictor and outcome data. In other words, it is possible to write the equations to solve to estimate these values. 

However, in that same equation, a series of polynomial terms were used. The number of additional features is also a parameter but, unlike the $\beta$ coefficients, it cannot be directly estimated from the data. @eq-poly-linear-reg arbitrarily used three terms for the predictor $x$. 

Two other models were discussed in @sec-whole-game. First, the tree-based model shown in @fig-reg-tree had five splits of the data. This led to a model fit that resulted in six regions of the $x$ space, each with its specific predicted value. We could have continued to split the training set into finer and finer partitions so that the fitted curve more effectively modeled the observed data. As such, the depth of the tree was a parameter that needed to be set before the model fit could be estimated. 

Similarly, the neural network required the number of hidden units to be defined (among other values). We weren't sure what that should be, so a set of candidate values were assessed, and the one with the best results was used for the final model fit (see @fig-delivery-nnet-tune). We've also seen specific tuning parameters in @sec-feature-hashing, @sec-combining-categories, and also for the embedding methods in @sec-embeddings. 

In each of these cases, there was a **tuning parameter**  (a.k.a., _hyperparameter_) that defined the structure of the model and could not be directly estimated from the data. Increasing the parameter's value for each model made the model's potential structure more elaborate. The effect of this increase in the model's _complexity_ is additional capability to adapt to any pattern seen in the training data. The straight line fit associated with a linear regression model without polynomial terms would have done an abysmal job on such a nonlinear data set. Adding additional complexity allowed it to better conform to the training set data. 

The problem is that added complexity can also add risk. Being more capable of adapting to subtle trends in the training data is only a good thing when such trends _generalize_ to other data. Recall in @sec-effect-encodings where we were estimating the effect of an agent on the price of a hotel room. One agent had a single booking. The naive estimate of the ADR wouldn't generalize well. As they accrue more and more bookings, their mean would undoubtedly change to a more realistic value. Using the raw ADR value would "overfit to the training set data."

For many sophisticated models, the complexity can be modulated via one or more tuning parameters that detect inconsequential patterns in the training data. The problem is that they may go too far and over-optimize the model fit. 

As an example, @fig-two-class-overfit shows a scenario where there are two predictors (A and B) and each point belongs to one of two classes (vermilion circles or blue triangles). These data were simulated and panel (a) shows a thick grey line representing the true class boundary where there is a 50% chance that a data point belongs to either class. Points to the left of the grey curve are more likely to be circles than points to the right. The curve itself has a smooth, parabolic shape. 

We can fit a model to these data and estimate the class boundary. The model we used^[A support vector machine. See @sec-cls-svm.] has a tuning parameter called the _cost value_. For now, you can think of this parameter as one that discourages the model from making an incorrect prediction (i.e., placing it on the wrong side of the estimated boundary). In other words, as the cost parameter increases, the model is more and more incentivized to be accurate on the training data. 

When the cost is low, the results are often boundaries with low complexity. @fig-two-class-overfit(b) shows the result where the fitted boundary is in black; points on the left of the boundary would be classified as circles. There is some curvature and, while correctly classifying the points in the middle of the training set, it could do a better job of emulating the true boundary. This model is slightly _underfit_ since more complexity would make it better. 

::: {#fig-two-class-overfit}

::: {.figure-content}

```{r}
#| renderings: [light, dark]
#| echo: false
#| out-width: 100%
#| fig-width: 10.5
#| fig-height: 3.5

f <- expr(-1 - 4 * A - 2 * B - 0.2 * A^2 + 1 * B^2)

x_seq <- seq(-4, 4, length.out = 100)
grid <- crossing(A = seq(-3, 3, length.out = 100), B = x_seq) %>%
  mutate(
    lp = rlang::eval_tidy(f, data = .)
  )

set.seed(943)
sim_tr <- sim_logistic(200, f)
sim_new <- sim_logistic(1000, f)
sim_rs <- vfold_cv(sim_tr)

sim_cols <- c("#D55E00", "#56B4E9")

# A function directly using ksvm() for the decision values instead of the
# probabilities; the latter are not reproducible with ksvm()
svm_grid_vals <- function(cost = 1) {
  set.seed(323)
  res <- ksvm(
    as.matrix(sim_tr[, 1:2]),
    y = sim_tr$class,
    C = cost,
    kpar = list(sigma = 10^-1)
  ) %>%
    predict(as.matrix(grid[, 1:2]), type = "decision")
  colnames(res) <- "value"
  as_tibble(res) %>% bind_cols(grid)
}

high_cost <- svm_grid_vals(2^20)
mid_cost <- svm_grid_vals(1)
low_cost <- svm_grid_vals(1 / 100)

# ------------------------------------------------------------------------------

base_plot <- sim_tr %>%
  ggplot(aes(A, B)) +
  geom_point(
    aes(col = class, pch = class),
    alpha = 3 / 4,
    cex = 2,
    show.legend = FALSE
  ) +
  coord_equal() +
  lims(x = c(-3, 3), y = c(-4, 4)) +
  ggtitle("(a) True boundary") +
  labs(x = "Predictor A", y = "Predictor B") +
  scale_color_manual(values = sim_cols)

base_plot_lt <- 
  base_plot +
  geom_contour(
    data = grid,
    aes(z = lp),
    breaks = 0,
    col = "black", 
    linewidth = 2,
    alpha = 1 / 10
  ) +
  theme_transparent()

base_plot_dk <- 
  base_plot +
  geom_contour(
    data = grid,
    aes(z = lp),
    breaks = 0,
    col = dark_gold, 
    linewidth = 2,
    alpha = 1 / 10
  ) +
  dk_thm

# ------------------------------------------------------------------------------

pointers <- tibble(
  A = c(-2.0, 2.00, 2.00),
  B = c(2.4, 1.5, -3.5),
  C = c(-1.3, 1.25, 1.25),
  D = c(2.4, 1.5, -3.5)
)

pointers_blowback <- tibble(A = c(2.20), B = c(-3.5), C = c(1.25), D = c(-3.5))

high_plot <- 
  base_plot +
  geom_segment(
    data = pointers,
    aes(xend = C, yend = D),
    arrow = arrow(length = unit(0.03, "npc"))
  ) +
  geom_segment(
    data = pointers_blowback,
    aes(xend = C, yend = D),
    color = "red",
    arrow = arrow(length = unit(0.03, "npc"))
  ) +
  ggtitle("(d) High complexity") +
  labs(x = "Predictor A", y = NULL)

high_plot_lt <- 
  high_plot +
  geom_contour(
    data = high_cost,
    aes(z = value),
    breaks = 0,
    col = "black", # diff
    linewidth = 1 / 2,
    alpha = 3 / 4
  ) +
  theme_transparent()

high_plot_dk <- 
  high_plot +
  geom_contour(
    data = high_cost,
    aes(z = value),
    breaks = 0,
    col = dark_gold, 
    linewidth = 1 / 2,
    alpha = 3 / 4
  )  +
  dk_thm

# ------------------------------------------------------------------------------

mid_plot_lt <-
  base_plot +
  geom_contour(
    data = mid_cost,
    aes(z = value),
    breaks = 0,
    col = "black", 
    linewidth = 1 / 2,
    alpha = 3 / 4
  ) +
  ggtitle("(c) Medium complexity") +
  labs(x = "Predictor A", y = NULL) +
  theme_transparent()

mid_plot_dk <-
  base_plot +
  geom_contour(
    data = mid_cost,
    aes(z = value),
    breaks = 0,
    col = dark_gold, 
    linewidth = 1 / 2,
    alpha = 3 / 4
  ) +
  ggtitle("(c) Medium complexity") +
  labs(x = "Predictor A", y = NULL) +
  dk_thm

# ------------------------------------------------------------------------------

low_plot_lt <- base_plot +
  geom_contour(
    data = low_cost,
    aes(z = value),
    breaks = 0,
    col = "black", 
    linewidth = 1 / 2,
    alpha = 3 / 4
  ) +
  ggtitle("(b) Low complexity") +
  labs(x = "Predictor A", y = NULL)

low_plot_dk <- base_plot +
  geom_contour(
    data = low_cost,
    aes(z = value),
    breaks = 0,
    col = dark_gold,  
    linewidth = 1 / 2,
    alpha = 3 / 4
  ) +
  ggtitle("(b) Low complexity") +
  labs(x = "Predictor A", y = NULL) +
  dk_thm

# ------------------------------------------------------------------------------

base_plot_lt +
  low_plot_lt +
  mid_plot_lt +
  high_plot_lt +
  plot_layout(nrow = 1, byrow = FALSE)

base_plot_dk +
  low_plot_dk +
  mid_plot_dk +
  high_plot_dk +
  plot_layout(nrow = 1, byrow = FALSE)
```

:::

Examples of how a model can overfit the training data across different levels of complexity.

:::

When the cost is increased, the complexity also increases (@fig-two-class-overfit(c)). The boundary is more parabolic, and a few more training set points are on the correct side of the estimated line. 

@fig-two-class-overfit(d) shows what occurs when the model is instructed that it is incredibly bad when incorrectly predicting a data point in the training set. Its boundary contorts in a manner that attempts to gain a few more points of accuracy. Two specific points, identified by black arrows, appear to drive much of the additional structure in the black curve. Since we know the actual boundary, it's easy to see that these contortions will not reproduce with new data. For example, it is implausible that a small island of vermilion circles exists in the mainstream of blue triangles. 

Another sign that a model probably has too much complexity is the additional component of the decision boundary at the bottom of @fig-two-class-overfit(d) (red arrow). Points inside this part of the boundary would be classified as triangles even though no triangles are near it. This is a sort of "blowback" of an overly complex model where choices the model makes in one part of the predictor space add complexity in a completely different part of the space. This blowback is often seen in areas where there are no data. It’s unnecessary and points to the idea that the model is trying to do too much. 

This boundary demonstrates how added complexity can be a bad thing. A test set of data was simulated and is shown in @fig-two-class-new. The performance for this overly complex model is far from optimal since the extra areas induced by the high cost value do not contain the right class of points. 

::: {#fig-two-class-new}

::: {.figure-content}

```{r}
#| renderings: [light, dark]
#| echo: false
#| warning: false
#| out-width: 35%
#| fig-width: 4
#| fig-height: 6

new_p <- 
  sim_new %>%
  ggplot(aes(A, B)) +
  geom_point(
    aes(col = class, pch = class),
    alpha = 3 / 4,
    cex = 2,
    show.legend = FALSE
  ) +
  coord_equal() +
  lims(x = c(-3, 3), y = c(-4, 4)) +
  labs(x = "Predictor A", y = NULL) +
  scale_color_manual(values = sim_cols)

new_p +
  geom_contour(
    data = grid,
    aes(z = lp),
    breaks = 0,
    col = "black",
    linewidth = 2,
    alpha = 1 / 12
  ) +
  geom_contour(
    data = high_cost,
    aes(z = value),
    breaks = 0,
    col = "black",
    linewidth = 1,
    alpha = 3 / 4
  ) +
  theme_transparent()

new_p +
  geom_contour(
    data = grid,
    aes(z = lp),
    breaks = 0,
    col = dark_gold,
    linewidth = 2,
    alpha = 1 / 12
  ) +
  geom_contour(
    data = high_cost,
    aes(z = value),
    breaks = 0,
    col = dark_gold,
    linewidth = 1,
    alpha = 3 / 4
  ) +
  dk_thm
```

:::

The high complexity fit from @fig-two-class-overfit(d) superimposed over a test set of data.

:::
    

Our goal is to find tuning parameter values that are _just right_: complex enough to accurately represent the data but not complex enough to over-interpret it. 	
	
## The Ways Models Overfit {#sec-how-to-overfit}

This leads us to a formal definition of overfitting: 

::: {.important-box}

Overfitting is the situation where a model over-interprets trends and patterns in the training set that do not generalize to new data. 

These irreproducible data trends result in models that **do very well on the training set** but greatly underperform on the validation set. 

:::

We can overinterpret the training set data in a few different ways. For example, @sec-imbalances will discuss class imbalance problems that occur when one or more outcome classes have a very low probability of occurrence. As models are allowed more complexity, they will often maximize accuracy (or some proxy for it) by simply declaring that all data points belong to the majority class. If there is a 1% event rate, 99% accuracy is easily achievable by overfitting to the majority class. 

Another example is overfitting the predictor set. The "low $N$, high $P$" problem [@Johnstone2009] occurs when data points are expensive to obtain, but each comes with abundant information. For example, in high-dimensional biology data, it is common to have dozens of data points and hundreds of thousands of predictors. Many models cannot be estimated with this imbalance of dimensions, and often, the first step is to filter out uninformative predictors. 

Suppose some statistical inferential method, such as a t-test, is used to determine if a predictor can differentiate between two classes. Small sample sizes make the chance of uninformative predictors falsely passing the filter large. If the same data are then used to build and evaluate the model, there is a significant probability that an ineffective model will appear to be very predictive [@Ambroise2002p1493;@fes]. Again, using a data set external to the model training and development process shows you when this occurs. 

Finally, it is possible to overfit a model via sample filtering. For example, in some machine learning competitions, participants might sub-sample the training set to be as similar to the unlabelled test set as possible. This will most likely improve their test set performance, but it generally handicaps a model when predicting a new yet-to-be-seen data set. 

## External Data to Measure Effectiveness {#sec-external-validation}

By now, it should seem clear that the best way of avoiding overfitting is to quantify the model performance using a separate data set. How do we do that? We've said that most modeling projects create an initial partition of the data into training and testing sets. Can the testing set detect overfitting? 

We’ll again use the classification data from @fig-two-class-overfit to demonstrate, this time with a different model. A boosted tree^[These are described in more detail in @sec-cls-boosting] is an ensemble model. The model creates a sequence of decision trees, each depending on the previous, to create a large ensemble of individual trees. For a new sample, each tree is used to predict the outcome, and a weighted average is used to create the predicted class probabilities. 

 The model has tuning parameters related to the tree (e.g., tree depth) and ones related to the process of building the ensemble (e.g., the number of trees in the ensemble). We’ll use 100 trees to create our ensemble. However, we don’t know how deep the trees should be. One method of controlling the number of splits in a tree is to control how much data should be in a partition to continue splitting. For example, in @fig-reg-tree, the left-most terminal node in the tree contained $n_{tr}$ = 971 data points after all splits are applied. If we keep splitting with no constraint, at some point, the amount of data in the node is too small to continue. 

This tuning parameter, we'll call it $n_{min}$, is used to moderate the tree-growing process. Small values increase the risk of overfitting; it’s possible to have a single data point left in the terminal node. Large values prevent the tree from accurately modeling the data, and underfitting occurs. What does the trade-off between complexity and performance look like for these data? 

```{r}
#| label: min-n-overfitting-example
#| cache: true
#| message: false
#| warning: false

lgb_spec <- boost_tree(trees = 100, learn_rate = 0.1, min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("lightgbm")

min_vals <- tibble(min_n = 3:80)

brier <- metric_set(brier_class)

set.seed(888)
lgb_cv_res <- lgb_spec %>%
  tune_grid(
    class ~ .,
    resamples = sim_rs,
    grid = min_vals,
    metrics = brier
  )

# ------------------------------------------------------------------------------

indices <- list(
  list(
    analysis = 1:nrow(sim_tr),
    assessment = (nrow(sim_tr) + 1):(nrow(sim_tr) + nrow(sim_new))
  )
)
test_split <- lapply(indices, make_splits, data = bind_rows(sim_tr, sim_new))
test_rs <- manual_rset(test_split, c("Split 1"))

get_resub <- function(x) {
  x
}

ctrl <- control_grid(extract = get_resub)

set.seed(888)
lgb_test_res <- lgb_spec %>%
  tune_grid(
    class ~ .,
    resamples = test_rs,
    grid = min_vals,
    control = ctrl,
    metrics = brier
  )

# ------------------------------------------------------------------------------

brier_cv <- collect_metrics(lgb_cv_res) %>%
  select(min_n, brier = mean) %>%
  mutate(Method = "Resampling")

brier_test <- collect_metrics(lgb_test_res) %>%
  select(min_n, brier = mean) %>%
  mutate(Method = "Test Set")

brier_resub <- collect_extracts(lgb_test_res) %>%
  mutate(
    train_pred = map(.extracts, ~ augment(.x, new_data = sim_tr)),
    brier = map_dbl(train_pred, ~ brier_class(.x, class, .pred_one)$.estimate)
  ) %>%
  select(min_n, brier) %>%
  mutate(Method = "Resubstitution")

brier_all <- bind_rows(brier_resub, brier_cv, brier_test) %>%
  mutate(
    Method = factor(
      Method,
      levels = c("Resubstitution", "Test Set", "Resampling")
    )
  )

method_cols <- c(
  Resubstitution = "#f6c85f",
  "Test Set" = "#6f4e7c",
  Resampling = "#0b84a5"
)

min_n_df <- brier_all %>%
  slice_min(brier, n = 1, by = Method)
```

We took the training data and fit models using values $n_{min}$ = 3, 4, ..., 80. To understand performance, the Brier score [@brier1950verification;@steyerberg2009] is used. It is an error metric for classification models (similar to RMSE) and measures how close the predicted probabilities are to their binary (i.e., 0/1) values. A Brier score of zero is a perfect model, and ineffectual models tend to have values $\ge$ 0.25. 

@fig-complexity-error shows the results. The yellow curve illustrates the Brier score when the training set is re-predicted. There is a sharp increase in error as the tuning parameter values increase. This line suggests that we use very small values. The smallest Brier score is associated with $n_{min}$ = `r min_n_df$min_n[min_n_df$Method == "Resubstitution"]` (producing an effectively zero Brier score).

::: {#fig-complexity-error}

::: {.figure-content}

```{r}
#| renderings: [light, dark]
#| echo: false
#| out-width: 60%
#| fig-width: 6
#| fig-height: 4.25

complexity_error_p <-
  brier_all %>%
  filter(Method != "Resampling") %>%
  ggplot(aes(min_n, brier, col = Method, pch = Method)) +
  geom_point(cex = 1.2) +
  geom_line() +
  labs(x = "Minimal Node Size", y = "Brier Score") +
  scale_color_manual(values = method_cols)

complexity_error_p + theme_transparent()

complexity_error_p + dk_thm
```

:::

An example to show how complexity and model error, measured two ways, interact. The tuning parameter on the x-axis is $n_{min}$ used for moderating tree growth.

:::

The purple line shows the Brier score trend on the test set. This tells a different story where small $n_{min}$ values overfit, and the model error improves as larger values are investigated. The Brier score is lowest around $n_{min}$ = `r min_n_df$min_n[min_n_df$Method == "Test Set"]` with a corresponding Brier score estimate of `r signif(min_n_df$brier[min_n_df$Method == "Test Set"], 3)`. From here, increasing the tuning parameter results in shallow trees that underfit. As a result the Brier score slowly increases. 

Contrasting these two curves, data that are _external_ to the training set gives a much more accurate picture of performance. However:

::: {.important-box}
**Do not use the test set to develop your model.**
:::

We've only used it here to show that an external data set is needed to effectively optimize model complexity. If we only have training and testing data, what should we do?  

There are two ways to solve this problem: 

 1. Use a validation set.
 2. Resample the training set [@sec-resampling]. 

Both of these options are described in detail in the next chapter. 

As previously discussed, validation sets are a third split of the data, created at the same time as the training and test sets. They can be used as an external data set during model development (as in @sec-whole-game). This approach is most applicable when the project starts with abundant data. 

Resampling methods take the training set and make multiple variations of it, usually by subsampling the rows of the data. For example, 10-fold cross-validation would make ten versions of the training set, each with a different 90% of the training data. We would say that, for this method, there are ten "resamples" or "folds." To measure performance, we would fit ten models on the majority of the data in each fold, then predict the separate 10% that was held out. This would generate ten different Brier scores, which are averaged to produce the final resampling estimate of the model. If we are trying to optimize complexity, we will apply this process for every candidate tuning parameter value. 

@fig-complexity-resampling shows the results when 10-fold cross-validation is used to pick $n_{min}$. The points on the additional curve are the averages of 10 Brier scores from each fold for each value of $n_{min}$. The resampling curve doesn't precisely mimic the test set results but does have the same general pattern: there is a real improvement in model error as $n_{min}$ is increased but it begins to worsen due to underfitting. In this case, resampling finds that  $n_{min}$ = `r min_n_df$min_n[min_n_df$Method == "Resampling"]` is numerically best and the resampling estimate of the Brier score was `r signif(min_n_df$brier[min_n_df$Method == "Resampling"], 3)`, slightly more pessimistic when compared to the test set result.

::: {#fig-complexity-resampling}

::: {.figure-content}

```{r}
#| renderings: [light, dark]
#| echo: false
#| out-width: 60%
#| fig-width: 6
#| fig-height: 4.25

complexity_resampling_p <-
  brier_all %>%
  ggplot(aes(min_n, brier, col = Method, pch = Method)) +
  geom_point(cex = 1.2) +
  geom_line() +
  labs(x = "Minimal Node Size", y = "Brier Score") +
  scale_color_manual(values = method_cols)

complexity_resampling_p + theme_transparent()

complexity_resampling_p + dk_thm
```
:::

Model complexity versus model error with external validation via resampling.

:::

We suggest using either of these two approaches (validation set or resampling) to measure performance during model development. Before going to the next chapter, let's give a preview of [Chapters -@sec-grid-search] and [-@sec-iterative-search].

## How Should We Optimize Complexity? {#sec-optimizing-complexity}

Now that we know how to appropriately estimate metrics for evaluating effectiveness, how do we choose tuning parameter values? Do we pick them at random? 

Operationally, there are two main classes of search routines for optimizing model complexity via tuning parameters. The oldest is _grid search_. For each tuning parameter in the model, we define a reasonable set of specific candidate values to investigate. When there is more than one tuning parameter, there are various ways to create a multidimensional grid (these are discussed in @sec-grid-search). Each candidate in the grid is evaluated and the "best" combination of parameter values is chosen as the one to use (if we think the model works well enough). 

One tuning parameter used in tree-based ensembles and neural networks is the _learning rate_. This parameter typically governs how quickly a model adapts during iterations of model fitting (i.e., training). In gradient descent, the learning rate specifies how far we proceed in the optimal direction. For machine learning models, the parameter must be greater than zero and usually has a maximum value around 0.1 to 1.0. When evaluating different learning rates, it is common to think of them in logarithmic units (typically, base 10). This is because the effect of the parameter on model performance is often nonlinear; a per-unit change of 0.05 has different effects on where in the total range that it occurs. 

Suppose that the performance goal is to minimize some notion of error (e.g., RMSE, the Brier score, etc.). @fig-learn-rate-grid shows an example of how the learning rate can affect the model error^[This was taken from a real example.]. At low rates, the error is high. Increasing the rate results in a drop in error that reaches a trough of optimal performance. However, increasing the learning rate at some point causes a different type of underfitting^[A very high learning rate for tree-based ensembles can result in models that produce near-constant predicted values (as opposed to many unique predicted values that are inaccurate).], increasing the error. 

The solid points illustrate a very simple five-point grid of errors whose y-axis value is the resampled error. While this grid does not pick the absolute best value, it does result in a model with good performance (relatively speaking) very quickly. If a larger grid were used, we would have placed a grid point much closer to the optimal value.

::: {#fig-learn-rate-grid}

::: {.figure-content}

```{r}
#| renderings: [light, dark]
#| echo: false
#| out-width: 60%
#| fig-width: 6
#| fig-height: 4.25

set.seed(1)
grid_points <- grid_points %>%
  mutate(
    smoothed = stats::filter(mean, rep(1 / 3, 3)),
    smoothed = if_else(is.na(smoothed), mean, smoothed),
    sampled = smoothed + rnorm(n(), sd = 1 / 2)
  )

grid_p <- 
  grid_points %>%
  ggplot(aes(learn_rate)) +
  geom_line(aes(y = smoothed), linewidth = 1, alpha = 1 / 4) +
  geom_point(
    data = grid_points %>% slice(c(1, 25, 50, 75, 100)),
    aes(y = sampled),
    cex = 3
  ) +
  scale_x_log10() +
  labs(x = "Learning Rate", y = "Error")

grid_p_lt <- 
  grid_p +
  geom_line(aes(y = smoothed), linewidth = 1, alpha = 1 / 4) +
  geom_point(
    data = grid_points %>% slice(c(1, 25, 50, 75, 100)),
    aes(y = sampled),
    cex = 3
  ) +
  theme_transparent()

grid_p_dk <- 
  grid_p +
  geom_line(aes(y = smoothed), linewidth = 1, alpha = 1 / 4, col = dark_data) +
  geom_point(
    data = grid_points %>% slice(c(1, 25, 50, 75, 100)),
    aes(y = sampled),
    cex = 3, col = dark_data
  ) +
  dk_thm

grid_p_lt

grid_p_dk
```
:::

An example of grid search containing five candidates for a learning rate parameter.

:::

Grid search pre-defines the grid points and all of them are tested before the results can be analyzed. There are good and bad aspects of this method, and it has been (unjustly) criticized for being inefficient. For most models, it can be very efficient and there are additional tools and tricks to make it even faster. See @sec-grid-search for more details. 

The other type of search method is _iterative_. These tools start with one or more initial candidate points and conduct analyses that predict which tuning parameter value(s) should be evaluated next. The most widely used iterative tuning method is **Bayesian optimization** [@MockusBO;@gramacy2020surrogates;@garnett2023]. After each candidate is evaluated, a Bayesian model is used to suggest the next value and this process repeats until a pre-defined number of iterations is reached. Any search method could be used, such as simulated annealing, genetic algorithms, and others. 

@fig-learn-rate-seq has an animation to demonstrate. First, three initial points were sampled (shown as open circles). A Bayesian model is fit to these data points and is used to predict the _probability distribution_ of the metric (e.g. RMSE, Brier score, etc.). A tool called an acquisition function is utilized to choose which learning rate value to evaluate on the next search iteration based on the mean and variance of the predicted distribution. In @fig-learn-rate-seq, this process repeats for a total of `r max(bayes_points$.iter)` iterations. 

::: {#fig-learn-rate-seq}

::: {.figure-content}

```{r}
#| renderings: [light, dark]
#| fig-height: 6.1
#| out-width: 60%
#| message: false
#| warning: false

knitr::include_graphics("../premade/anime_learn_rate.gif")

knitr::include_graphics("../premade/anime_learn_rate_dark.gif")
```

:::

An example of iterative optimization. The open circles represent the three initial points and the solid circles show the progress of the Bayesian optimization of the learning rate.

:::

The animation shows that the search evaluated very disparate values, including the lower and upper limits that were defined. However, after a few iterations, the search focuses on points near the optimal value. Again, this method discussed in more detail in @sec-iterative-search.

All of our optimization methods assume that a reasonable range of tuning parameters is known. The range is naturally bounded in some cases, such as the number of PCA components. Otherwise, you (and/or the ML community) will develop a sense of appropriate ranges of these parameters. We will suggest basic ranges here. For example, for the learning rate, we know it has to be greater than zero but the fuzzy upper bound of about 0.1 is a convention learned by trial and error. 

## Chapter References {.unnumbered}

