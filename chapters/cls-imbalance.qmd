---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-imbalance/"
---

# Class Imbalances {#sec-cls-imbalance}

```{r}
#| label: cls-imbalance-setup
#| include: false

source("../R/_common.R")
# source("../R/_themes_ggplot.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(mirai)
library(gt)
library(probably)
library(patchwork)
library(bonsai)
library(rules)
library(themis)
library(ggforce)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_bw())
set_options()
daemons(parallel::detectCores())

# ------------------------------------------------------------------------------

closest_point <- function(x, what = ".threshold", target = 0.5) {
  x$delta <- abs(x[[what]] - target)
  dplyr::slice_min(x, delta, with_ties = FALSE)
}
```


```{r}
#| label: load-data
#| include: false

data(myopia, package = "aplore3")
myopia_df <-
  myopia |>
  as_tibble() |>
  select(-id) |>
  rename(
    year = studyyear,
    class = myopic,
    age = age,
    female = gender,
    spherical_equivalent_refraction = spheq,
    axial_length = al,
    anterior_chamber_depth = acd,
    lens_thickness = lt,
    vitreous_chamber_depth = vcd,
    hours_sports = sporthr,
    hours_reading = readhr,
    hours_gaming = comphr,
    hours_studying = studyhr,
    hours_tv = tvhr,
    near_work_activities = diopterhr,
    mother_myopic = mommy,
    father_myopic = dadmy
  ) |>
  mutate(
    class = factor(tolower(class), levels = c("yes", "no")),
    female = if_else(female == "Female", 1, 0),
    mother_myopic = if_else(mother_myopic == "Yes", 1, 0),
    father_myopic = if_else(father_myopic == "Yes", 1, 0)
  ) |>
  relocate(class)

# ------------------------------------------------------------------------------

set.seed(3571)
myopia_split <- initial_split(myopia_df, strata = class)
myopia_train <- training(myopia_split)
myopia_test <- testing(myopia_split)
myopia_rs <- vfold_cv(myopia_train, repeats = 5)
```

Examples: churn, click through, diseases (trip test, Parkinsonâ€™s), blood specimen mixups

Focus on two class problems, but an issue for all classes

Prior versus likelihood, posterior

note on time slices to make events

## Underlying mathematical issue

## Imbalances should make us think about what we want from our model

Finding events vs accurate predictions

discussion or relative risk vs absolute risk

This should drive our choice of optimization metric

don't confuse dealing with extreme frequencies with finding important events

## Example: Predicting Myopia


### Exploring the Data


```{r}
#| label: fig-myopia-splom
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 7
#| fig-cap: "Scatterplot matrix of eye measurements."

myopia_df |>
  select(class, spherical_equivalent_refraction:vitreous_chamber_depth) |>
  rename_with(~ tolower(gsub("_", "\n", .x))) |>
  ggplot(aes(x = .panel_x, y = .panel_y, col = class)) +
  geom_point(alpha = 0.5, shape = 16, size = 1) +
  geom_autodensity(aes(fill = class, col = class), alpha = 3 / 4) +
  facet_matrix(vars(-class), layer.diag = 2, grid.y.diag = FALSE) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "top")
```

## Accepting the State of Nature 

problem of never predicting the minory class with poor probabilities

Cutoff adjustments

```{r}
#| label: model-setup
#| include: false

# add classification costs
cls_mtr <- metric_set(
  brier_class,
  roc_auc,
  pr_auc,
  kap,
  mn_log_loss,
  accuracy,
  sensitivity,
  specificity
)
ctrl_grid <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)
ctrl_rs <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
```

```{r}
#| label: recipes
#| include: false
bare_rec <-
  recipe(class ~ ., data = myopia_train)

norm_rec <-
  bare_rec |>
  step_normalize(all_numeric_predictors())

down_rec <-
  bare_rec |>
  step_downsample(class)

```


```{r}
#| label: myopia-glmnet-plain
#| include: false
#| cache: true

path <- 10^seq(-5, -1, length.out = 20)

glmn_plain_spec <-
  logistic_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet", path_values = !!path)

glmn_plain_wflow <- workflow(norm_rec, glmn_plain_spec)

glmn_plain_grid <-
  crossing(mixture = c(0.05, (1:4) / 4), penalty = path)

glmn_plain_res <-
  glmn_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = glmn_plain_grid
  )

glmn_plain_best <- select_best(glmn_plain_res, metric = "brier_class")
glmn_plain_best_mtr <- collect_metrics(glmn_plain_res) |>
  inner_join(glmn_plain_best |> select(.config), by = ".config")
glmn_plain_best_pred <- collect_predictions(
  glmn_plain_res,
  parameters = glmn_plain_best
)

glmn_plain_roc_curve <- 
  glmn_plain_best_pred |>
  roc_curve(class, .pred_yes)

glmn_pr_roc_curve <- 
  glmn_plain_best_pred |>
  pr_curve(class, .pred_yes)

glmn_plain_roc_default <- closest_point(glmn_plain_roc_curve)
glmn_plain_pr_default <- closest_point(glmn_pr_roc_curve)
```

```{r}
#| label: myopia-mlp-plain
#| include: false
#| cache: true
mlp_spec <-
  mlp(
    hidden_units = tune(),
    penalty = tune(),
    learn_rate = tune(),
    epochs = 25,
    activation = tune()
  ) |>
  set_engine(
    "brulee",
    stop_iter = 5,
    optimizer = "ADAMw",
    verbose = FALSE,
    rate_schedule = tune(),
    batch_size = tune(),
    momentum = tune()
  ) |>
  set_mode("classification")

mlp_rec <- 
  bare_rec |> 
  step_dummy(all_factor_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

mlp_plain_wflow <- workflow(mlp_rec, mlp_spec)
mlp_plain_param <- 
  mlp_plain_wflow |>
  extract_parameter_set_dials() |> 
  update(
    momentum = momentum(c(0.8, 0.99)),
    penalty = penalty(c(-10, -1)),
    learn_rate = learn_rate(c(-4, -1)),
    activation = activation(c("elu", "relu", "tanh", "tanhshrink")),
    rate_schedule = rate_schedule(c("cyclic", "decay_time", "none"))
  )
set.seed(820)
mlp_plain_res <-
  mlp_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    param_info = mlp_plain_param,
    grid = 25
  )

mlp_plain_best <- select_best(mlp_plain_res, metric = "brier_class")
mlp_plain_best_mtr <- collect_metrics(mlp_plain_res) |>
  inner_join(mlp_plain_best |> select(.config), by = ".config")
mlp_plain_best_pred <- collect_predictions(
  mlp_plain_res,
  parameters = mlp_plain_best
)

mlp_roc_curve <- 
  mlp_plain_best_pred |>
  roc_curve(class, .pred_yes)

mlp_default <- closest_point(mlp_roc_curve)
mlp_sensitive <- closest_point(mlp_roc_curve, "sensitivity", 0.95)
```

```{r}
#| label: myopia-lgb-plain
#| include: false
#| cache: true

lgb_plain_spec <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    mtry = tune(),
    learn_rate = tune(),
    stop_iter = tune()
  ) |>
  set_mode("classification") |>
  set_engine("lightgbm")


lgb_plain_wflow <- workflow(class ~ ., lgb_plain_spec)

set.seed(820)
lgb_plain_res <-
  lgb_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25
  )

lgb_plain_best <- select_best(lgb_plain_res, metric = "brier_class")
lgb_plain_best_mtr <- collect_metrics(lgb_plain_res) |>
  inner_join(lgb_plain_best |> select(.config), by = ".config")
lgb_plain_best_pred <- collect_predictions(
  lgb_plain_res,
  parameters = lgb_plain_best
)

lgb_roc_curve <- 
  lgb_plain_best_pred |>
  roc_curve(class, .pred_yes)

lgb_default <- closest_point(lgb_roc_curve)
lgb_sensitive <- closest_point(lgb_roc_curve, "sensitivity", 0.95)

```

```{r}
#| label: myopia-ranger-plain
#| include: false
#| cache: true

rf_plain_spec <-
  rand_forest(
    trees = 2000,
    min_n = tune(),
    mtry = tune()
  ) |>
  set_mode("classification")

rf_plain_wflow <- workflow(class ~ ., rf_plain_spec)

set.seed(820)
rf_plain_res <-
  rf_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25
  )

rf_plain_best <- select_best(rf_plain_res, metric = "brier_class")
rf_plain_best_mtr <- collect_metrics(rf_plain_res) |>
  inner_join(rf_plain_best |> select(.config), by = ".config")
rf_plain_best_pred <- collect_predictions(
  rf_plain_res,
  parameters = rf_plain_best
)

rf_plain_roc_curve <- 
  rf_plain_best_pred |>
  roc_curve(class, .pred_yes)

rf_plain_default <- closest_point(rf_plain_roc_curve)
rf_plain_sensitive <- closest_point(rf_plain_roc_curve, "sensitivity", 0.95)
```

```{r}
#| label: myopia-xrf-plain
#| include: false
#| cache: true

xrf_plain_spec <-
  rule_fit(
    mtry = tune(),
    trees = tune(),
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    sample_size = tune(),
    penalty = tune()
  ) |>
  set_engine('xrf', counts = FALSE) |>
  set_mode('classification')

xrf_rec <- 
  bare_rec |> 
  step_dummy(all_factor_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

xrf_plain_wflow <- workflow(xrf_rec, xrf_plain_spec)

set.seed(820)
xrf_plain_res <-
  xrf_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25
  )

xrf_plain_best <- select_best(xrf_plain_res, metric = "brier_class")
xrf_plain_best_mtr <- collect_metrics(xrf_plain_res) |>
  inner_join(xrf_plain_best |> select(.config), by = ".config")
xrf_plain_best_pred <- collect_predictions(
  xrf_plain_res,
  parameters = xrf_plain_best
)

xrf_roc_curve <- 
  xrf_plain_best_pred |>
  roc_curve(class, .pred_yes)

xrf_default <- closest_point(xrf_roc_curve)
xrf_sensitive <- closest_point(xrf_roc_curve, "sensitivity", 0.95)
```


```{r}
#| label: myopia-plain-roundup
#| echo: false
#| fig-width: 3
#| fig-height: 6
#| out-width: 40%

plain_res <- as_workflow_set(
  glmnet = glmn_plain_res,
  nnet = mlp_plain_res,
  boosting = lgb_plain_res,
  rand_forest = rf_plain_res,
  rule_fit = xrf_plain_res
)

rank_plain <-
  rank_results(plain_res, rank_metric = "brier_class") |>
  filter(.metric == "brier_class")

rank_plain_best <-
  rank_plain |>
  slice_min(mean, by = c(wflow_id)) |>
  mutate(percent_worse = (rank - 1) / nrow(rank_plain) * 100)

rank_plain |>
  ggplot(aes(rank, mean, col = wflow_id)) +
  geom_point() +
  geom_vline(
    data = rank_plain_best,
    aes(col = wflow_id, xintercept = rank),
    lty = 2
  ) +
  theme(legend.position = "top")
```


```{r}
#| label: fig-myopia-rf-plots
#| echo: false
#| out-width: 90%
#| fig-width: 8
#| fig-height: 4
#| fig-cap: "Calibration and ROC curves for the best random forest model without adjustments for the class imbalance. The solid blue point on the ROC curve corresponds to the default cutoff, while the red square indicates the threshold used to achieve 95% sensitivity. The validation set predictions were used to compute these visualizations."

rf_plain_cal <- 
  rf_plain_best_pred |>
  cal_plot_windowed(class, .pred_yes, step_size = 0.02, window_size = 0.2) +
  labs(title = "Calibration Curve")

rf_plain_roc <- 
  autoplot(rf_plain_roc_curve) + 
  geom_point(
    data = rf_plain_default, 
    aes(x = 1 - specificity, y = sensitivity),
    cex = 4, 
    col = "blue"
  ) + 
  geom_point(
    data = rf_plain_sensitive, 
    aes(x = 1 - specificity, y = sensitivity),
    cex = 4, 
    col = "red",
    pch = 15
  ) +
  labs(title = "ROC Curve", y = "Sensitivity", x = "1 - Specificity")

rf_plain_cal + rf_plain_roc
```

```{r}
#| label: plain-default

rf_plain_default
```


## Adjustments for Balance

could be balancing the data, weights, metric contributions, or costs

(Better wording)

### Sampling and Synthesis Techniques

Subsampling

```{r}
#| label: myopia-lgb-down
#| include: false
#| cache: true

lgb_down_wflow <- workflow(down_rec, lgb_plain_spec)

set.seed(820)
lgb_down_res <-
  lgb_down_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 50
  )
```

### Cost-sensitive learning


```{r}
#| label: myopia-lgb-cls-wts
#| include: false
#| cache: true

lgb_cls_wts_spec <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    mtry = tune(),
    learn_rate = tune(),
    stop_iter = tune()
  ) |>
  set_mode("classification") |>
  set_engine("lightgbm", scale_pos_weight = tune())


lgb_cls_wts_wflow <- workflow(bare_rec, lgb_cls_wts_spec)

lgb_cls_wts_param <-
  lgb_cls_wts_wflow |>
  extract_parameter_set_dials() |>
  update(
    mtry = mtry(c(1, ncol(myopia_train) - 1)),
    scale_pos_weight = class_weights(c(1, 15)),
    learn_rate = learn_rate(c(-2, 1))
  )

set.seed(820)
lgb_cls_wts_res <-
  lgb_cls_wts_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    param_info = lgb_cls_wts_param,
    grid = 50
  )
```

### Other Methods

case weights

Unrealistic priors

Logistic regression with altered intercept


## Chapter References {.unnumbered}

