---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-imbalance/"
---

# Class Imbalances {#sec-cls-imbalances}

```{r}
#| label: cls-imbalance-setup
#| include: false

source("../R/_common.R")
# source("../R/_themes_ggplot.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(mirai)
library(gt)
library(probably)
library(patchwork)
library(bonsai)
library(rules)
library(themis)
library(ggforce)
library(discrim)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_bw())
set_options()
daemons(parallel::detectCores())

# ------------------------------------------------------------------------------

closest_point <- function(x, what = ".threshold", target = 0.5) {
  x$delta <- abs(x[[what]] - target)
  dplyr::slice_min(x, delta, with_ties = FALSE)
}
```

```{r}
#| label: load-data
#| include: false

data(myopia, package = "aplore3")
myopia_df <-
  myopia |>
  as_tibble() |>
  select(-id) |>
  rename(
    year = studyyear,
    class = myopic,
    age = age,
    female = gender,
    spherical_equivalent_refraction = spheq,
    axial_length = al,
    anterior_chamber_depth = acd,
    lens_thickness = lt,
    vitreous_chamber_depth = vcd,
    hours_sports = sporthr,
    hours_reading = readhr,
    hours_gaming = comphr,
    hours_studying = studyhr,
    hours_tv = tvhr,
    near_work_activities = diopterhr,
    mother_myopic = mommy,
    father_myopic = dadmy
  ) |>
  mutate(
    class = factor(tolower(class), levels = c("yes", "no")),
    female = if_else(female == "Female", 1, 0),
    mother_myopic = if_else(mother_myopic == "Yes", 1, 0),
    father_myopic = if_else(father_myopic == "Yes", 1, 0)
  ) |>
  relocate(class)

# ------------------------------------------------------------------------------

set.seed(3571)
myopia_split <- initial_split(myopia_df, strata = class)
myopia_train <- training(myopia_split)
myopia_test <- testing(myopia_split)

set.seed(538)
myopia_rs <- vfold_cv(myopia_train, repeats = 5)

# ------------------------------------------------------------------------------

num_pred <- ncol(myopia_train) - 1
max_num_terms <- num_pred + choose(num_pred, 2)
```

Examples: churn, click through, diseases (trip test, Parkinson’s), blood specimen mixups

Focus on two class problems, but an issue for all classes

Prior versus likelihood, posterior

note on time slices to make events

## Example: Predicting Myopia {#sec-yyopia}

For demonstration, we'll use data from the myopia study as described in Section 1.6.6 of @hosmer2013applied. Myopia (i.e., nearsightedness) can be caused by the shape of one's eye, genetics, and environmental conditions, including the heavy use of computer or television screens. The data are from a medical study running from `r min(myopia_df$year)` to `r max(myopia_df$year)` and contained `r sum(myopia_df$female)` girls and `r sum(myopia_df$female == 0)` boys ranging from `r min(myopia_df$age)` to `r max(myopia_df$age)` years old. These data are from the initial exam and include demographic factors, several physiological measurements of the eye from an ocular examination, and whether either parent is also myopic. The outcome is determined after a five-year follow-up visit. In this study, `r sum(myopia_df$class == "yes")` (`r round(mean(myopia_df$class == "yes") * 100, 1)`%) of children were nearsighted. 

To begin, a 3:1 stratified split was used to partition the data into training and testing sets. Stratification was based on the outcome. This results in a training set of `r nrow(myopia_train)` children, only `r sum(myopia_train$class == "yes")` of whom are myopic. For the test set, `r sum(myopia_test$class == "yes")` children out of `r nrow(myopia_test)` were myopic. Since the training set and the number of events are small, five repeats of 10-fold cross-validation were used with the same stratification strategy. The repeats will help increase the precision of our resampling statistics. 

::: {.important-box}
It is critical that the test set reflects the true event rate in the population of interest. While we may change the event rate in the training set during model development, the test set data should be as consistent as possible with reality. 
:::

We'll conduct some quick exploratory data analysis, then proceed to creating models based on traditional modeling methods, i.e., without making adjustments for class imbalance during model optimization. 

### Exploring the Data {#sec-myopia-eda}

```{r}
#| label: myopia-age-sex
#| include: false
rates_by_age_sex <- 
  myopia_train |>
  count(class, age, female) |>
  pivot_wider(id_cols = c(female, age), names_from = class, values_from = n) |>
  mutate(
    yes = if_else(is.na(yes), 0, yes),
    n = yes + no,
    stats = map2(
      yes,
      n,
      ~ tidy(binom.test(x = .x, n = .y, conf.level = .9))
    ),
    rate = map_dbl(stats, ~ .x$estimate),
    lower = map_dbl(stats, ~ .x$conf.low),
    upper = map_dbl(stats, ~ .x$conf.high),
    Sex = if_else(female == 1, "Female", "Male")
  ) 

rates_by_parent <-
  myopia_train |>
  count(class, mother_myopic, father_myopic) |>
  pivot_wider(
    id_cols = c(mother_myopic, father_myopic),
    names_from = class,
    values_from = n
  ) |>
  mutate(
    yes = if_else(is.na(yes), 0, yes),
    n = yes + no,
    stats = map2(
      yes,
      n,
      ~ tidy(binom.test(x = .x, n = .y, conf.level = .9))
    ),
    rate = round(map_dbl(stats, ~ .x$estimate) * 100, 1),
    lower = round(map_dbl(stats, ~ .x$conf.low) * 100, 1),
    upper = round(map_dbl(stats, ~ .x$conf.high) * 100, 1)
  ) |> 
  arrange(mother_myopic, father_myopic)
```

First, let's examine the age and sex of the participants. @fig-myopia-age-sex shows the class percentages for 10 subgroups of the data. The size of the point is based on the number of children in the subgroup, indicating that many of them are 6 years old, while very few are 9 years old. There were no nine-year-old myopic girls in the training set. 

```{r}
#| label: fig-myopia-age-sex
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: "Training set rates of myopia by age and sex of the children. The error bars are based on 90% confidence intervals, and the size of the point indicates the number of children in each subgroup."

rates_by_age_sex |>
  ggplot(aes(age, col = Sex)) +
  geom_point(aes(y = rate, size = n), position = position_dodge(width = 0.25)) +
  geom_errorbar(
    aes(ymin = lower, ymax = upper),
    position = position_dodge(width = 0.25),
    width = 0.2
  )+
  scale_color_manual(values = c("#417839FF", "#E18727FF")) +
  scale_y_continuous(labels = label_percent(), limits = c(0, 1)) +
  labs(x = "Age", y = "Rate of Myopia") +
  theme(legend.position = "top") 
```

The points suggest a slight increase in myopia rates among girls, but this is well within the error bars defined by the 90% confidence intervals. Due to the concentration of points in a single age group, it is difficult to determine whether there is an age trend. However, since the slight rate increase for girls is constant from ages six to eight, there is little indication of an interaction between these factors. 

Second, do genetics matter? If neither parent is myopic, the rates for their children are low: `r rates_by_parent$rate[1]`% with 90% confidence interval (`r rates_by_parent$lower[1]`%, `r rates_by_parent$upper[1]`%). When _both_ parents have the condition, the rate is  `r rates_by_parent$rate[4]`%  (CI: `r rates_by_parent$lower[4]`%, `r rates_by_parent$upper[4]`%), indicating a strong signal. 
The effects of single paternal and maternal genetics are in between, with rates `r rates_by_parent$rate[2]`% and `r rates_by_parent$rate[3]`%, respectively, with similar confidence intervals.    

The ocular exam yielded five different measurements of the eye. @fig-myopia-splom shows a scatterplot matrix of pairwise relationships, colored by the outcome class. With one exception,  there is little to no correlation between predictors. However, the axial length and the vitreous chamber depth have a high positive correlation. Additionally, most predictors have fairly symmetric distributions, although the spherical equivalent refraction data exhibit a moderately strong right skew. This predictor also shows some raw separation between the classes. 

```{r}
#| label: fig-myopia-splom
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 7
#| fig-cap: "Scatterplot matrix of eye measurements."

myopia_train |>
  select(class, spherical_equivalent_refraction:vitreous_chamber_depth) |>
  rename_with(~ tolower(gsub("_", "\n", .x))) |>
  ggplot(aes(x = .panel_x, y = .panel_y, col = class)) +
  geom_point(alpha = 0.5, shape = 16, size = 1) +
  geom_autodensity(aes(fill = class, col = class), alpha = 3 / 4) +
  facet_matrix(vars(-class), layer.diag = 2, grid.y.diag = FALSE) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "top")
```

## Underlying Mathematical Issue {#sec-imbalance-math}

talk about specific literature on logistic models. 

We’ll use two predictors in the myopia data to visualize the effects of different techniques. In the upper-left panel of the lower diagonal of @fig-myopia-splom we see the spherical equivalent refraction and vitreous chamber depth predictor data from the training set, colored by the outcome class. A flexible discriminant analysis model using MARS basis functions was used to model these data. Interactions between hinge functions were allowed, and generalized cross-validation was used to automatically determine the optimal level of model complexity. The model was fit to the training set, and @fig-myopia-fda shows the class boundary, defined using the 50% probability cutoff. The events (in red) occupy the space to the left of the main data stream. The class boundary does its best to find the optimal partition of the data, but we can see that less than half of the training set events are not to the left of the boundary. 

```{r}
#| label: myopia-plain-fda
#| include: false

myopia_formatted <- 
  myopia_train |> 
  mutate(
    father_myopic = factor(ifelse(father_myopic == 1, "myopic", "normal")),
    mother_myopic = factor(ifelse(mother_myopic == 1, "myopic", "normal")),
    sex = factor(ifelse(female == 1, "Female", "Male"))
    ) |> 
  rename(
    `paternal genetics` = father_myopic,
    `maternal genetics` = mother_myopic
  ) |> 
  rename_with(~ paste0(.x, ")"), contains("hours")) |> 
  rename_with(~ gsub("hours_", "hours (", .x), contains("hours"))|> 
  rename_with(~ gsub("_", " ", .x), everything()) |> 
  select(-female)

smol_grid <- 
  crossing(
    `spherical equivalent refraction` = seq(-1, 5, length.out = 100),
    `axial length` = seq(20, 25, length.out = 100)
  )

ends <- tribble(
  ~`spherical equivalent refraction`, ~`axial length`, ~yes, ~no,
  NA_real_, NA_real_, 0.0, 1.0,
  NA_real_, NA_real_, 1.0, 0.0
)

myopia_smol_formatted <- 
  myopia_formatted |> 
  select(`spherical equivalent refraction`, `axial length`, class)

fda_plain_fit <- discrim_flexible(prod_degree = 2) |> 
  fit(class ~ ., data = myopia_smol_formatted)
fda_plain_pred <- augment(fda_plain_fit, new_data = smol_grid)
fda_plain_capture <- 
  augment(fda_plain_fit, new_data = myopia_smol_formatted) |> 
  filter(.pred_yes >= 0.5 & class == "yes") |> 
  nrow()
```

:::: {#fig-myopia-fda}

```{r}
#| label: myopia-fda
#| echo: false
#| fig-width: 4.8
#| fig-height: 5.2
#| warning: false
#| out-width: 50%
myopia_smol_formatted |> 
  ggplot(aes(`spherical equivalent refraction`, `axial length`)) +
  geom_point(aes(color = class), alpha = 0.5, shape = 16, size = 2, show.legend = FALSE) + 
  geom_tile(data = fda_plain_pred, aes(fill = .pred_yes), alpha = .1) +
  geom_contour(data = fda_plain_pred, aes(z = .pred_yes), breaks = 1/2, col = "black") +
  scale_fill_gradient2(low = "#377EB8", mid = "white", high = "#E41A1C", midpoint = 0.5) +
  scale_color_brewer(palette = "Set1") + 
  labs(fill = "Probability") +
  theme(legend.position = "top")
```

A flexible discriminant fit to the myopia training set. Using the default probability cutoff of 50%, shown as the black contour, `r fda_plain_capture` of the `r sum(myopia_smol_formatted$class == "yes")` events are correctly predicted as events. 
::::

Trees provide another example that demonstrates how models can overly focus on the majority class. Since most trees employ a greedy search, the first split is often one that captures the majority of the majority class. As the tree-growing process continues, the minority class becomes less likely to be predicted in the nodes, since $n_{min}$ limits will restrict fine-grained splits. Also, many pruning methods will remove nodes with higher uncertainty, which is often a function of sample size; smaller nodes are more likely to be pruned. When a standard CART model was fit to the data, the rules to each terminal node were:  

 1. if spherical equivalent refraction $\ge 0.28$ then rate = 6% ($n$ = 382)
 2. if spherical equivalent refraction $\in [0.23, 0.28)$ then rate = 60% ($n$ = 15) 
 3. if spherical equivalent refraction $\in [-0.05, 0.23)$ then rate = 33% ($n$ = 46) 
 4. if spherical equivalent refraction $\le  -0.05$ then rate = 74% ($n$ = 19)
 
Split 1 was the initial split, and we can see that it captures most of the non-myopic children. The second and third rules attempt to identify spaces where myopic children can be identified, but these two equations are likely the result of overfitting. Regardless, the splits demonstrate how the majority class can dominate the objective function, making the majority class data more likely to be correctly predicted.  

This also brings up a pattern that can occur when dealing with class imbalances that is somewhat counterintuitive: models that are _less_ complex tend to work better than highly complex models. In essence, the more models can adapt to the data, the more likely they are to overfit to the majority class. The rule set shown above is a good example. Tree-based models employ a greedy optimization approach to train the model and can carve out small subspaces of predictors to make local predictions. It would be very difficult for a logistic regression to do the same, at least without enormously complex feature engineering. As a result, these simpler models can have sufficient predictive power to be useful, but not so much as to discern minute patterns in the data, thereby correctly predicting the few events and risking overfitting.  We'll see this pattern in the example data, which is admittedly small, but it has also repeated in much larger datasets with class imbalances. 

Before moving on to specific techniques, let's discuss a potential fallacy encountered when there are rare events: correctly predicting the events to the detriment of the overall model goals is often inappropriate. 

## Imbalances Make Us Focus on Our Goals {#sec-imbalance-goals}

Before starting the model development process, it is essential to consider how class balance impacts the project goals. It is often the case that the fact that events rarely occur means that the goal of the model is to identify the precious events in a large dataset (such as in information retrieval), even if it is at the cost of more false positives than we would ordinarily tolerate. In other words, finding the events is of paramount importance, and other considerations, such as interpretability, inference, and explanation of predictions, are not terribly important. This is often the case where the hard class prediction is used in an automated system that only takes the qualitative prediction as input. For example, @apm discusses a model used for a high-performance computing grid to route computational jobs to the correct queue based on their execution time. The system only needs to know where to send the job, and measures of uncertainty, such as estimated probabilities for how long the job will take, play no role in the process. 

However, it can also be the case that we need to understand why the model works or why a data point was predicted to belong to a specific class. In @sec-drug-interactions, we’ll discuss a project from drug development where a model is used to estimate the risk of a potential drug causing patient harm. We definitely want to find the events (e.g., problematic drugs), but we also need a good understanding of how certain we are that there is an issue. 

::: {.important-box}
In other words, we should not confuse what we will do to deal with extreme frequencies with the idea that we should just care about finding important events. 
:::

These issues are important to consider because, for class imbalances, several approaches can be taken to mitigate the disparity in class frequencies. Some of them can subvert the accuracy of the predicted class probabilities (i.e., result in poor calibration). If we require a model that predicts useful probabilistic outputs, we would avoid some modeling techniques. 

Additionally, we should settle on what performance metrics are most important. We may need multiple metrics, including some to measure calibration (Brier scores), overall class separation (ROC curves), and/or the ability to detect events (PR curves). Additionally, knowing how well the hard class predictions work may require attention to sensitivity, specificity, recall, and other metrics. 

These are all issues that should be thought about for every modeling project. We emphasize it here because imbalances, especially extreme ones, substantially raise the level of difficulty.

The next two sections will outline two general philosophies when modeling imbalanced data. The first is to accept that the imbalance and model it as-is. We can utilize all the tools and methods previously discussed to create a model driven by performance statistics based on probabilistic predictions, such as log-loss or the area under the receiver operator characteristic curve. Once we have an appropriate model for these criteria, we can focus on how to postprocess the predictions to produce the optimal hard class predictions. 

The second philosophy is to circumvent the issue of the imbalance by either removing it or by making the performance metric aware that some training set points (e.g., the events) are more important than others. Both approaches bias the model to be less focused on the majority class.

Both philosophies have advantages and disadvantages. Generally, if finding the events is the overriding goal, the second philosophy will likely be the best choice.   

## Strategy 1: Accepting the State of Nature {#sec-imbalance-acceptance}

In this case, we'll try to build ML models while generally ignoring the imbalance. The idea is that we should initially focus on optimizing metrics based on class probability estimates, such as the Brier score, cross-entropy, or the areas under the ROC or PR curves. Once we have a model with good calibration and/or discrimination, we can then focus on making the best hard class predictions. We'll show the advantages and disadvantages of using this strategy. 

To get started, we'll fit a subset of models to these data and generally follow the process outlined in the previous chapters. There is little to no preprocessing required for these models. There is a single pair of highly correlated predictors, but that will be handled by the model in situations where multicorrelation is a concern. 

```{r}
#| label: model-setup
#| include: false

# add classification costs
cls_mtr <- metric_set(
  brier_class,
  roc_auc,
  pr_auc,
  kap,
  mn_log_loss,
  accuracy,
  sensitivity,
  specificity
)
ctrl_grid <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)
ctrl_rs <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
```

```{r}
#| label: recipes
#| include: false
bare_rec <-
  recipe(class ~ ., data = myopia_train)

norm_rec <-
  bare_rec |>
  step_normalize(all_numeric_predictors())

glmn_rec <-
  bare_rec |>
  step_interact(~ all_predictors():all_predictors()) |> 
  step_normalize(all_numeric_predictors())

down_rec <-
  bare_rec |>
  step_downsample(class)

smote_rec <-
  bare_rec |>
  step_smote(class)

rose_rec <-
  bare_rec |>
  step_rose(class)

asa_rec <-
  bare_rec |>
  step_adasyn(class)

nm_rec <-
  bare_rec |>
  step_nearmiss(class)
```

```{r}
#| label: myopia-glmnet-plain
#| include: false
#| cache: true

path <- 10^seq(-4, -0.6, by = 0.2)

glmn_plain_spec <-
  logistic_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet", path_values = !!path)

glmn_plain_wflow <- workflow(glmn_rec, glmn_plain_spec)

glmn_plain_grid <- crossing(mixture = round((0:3) / 3, 2), penalty = path)

glmn_plain_res <-
  glmn_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = glmn_plain_grid
  )

### 

glmn_plain_best <- select_best(glmn_plain_res, metric = "brier_class")
glmn_plain_fit <- fit_best(glmn_plain_res)
glmn_plain_main <- 
  glmn_plain_fit |> 
  tidy() |> 
  filter(estimate != 0 & !grepl("(_x_)|(Intercept)", term))
glmn_plain_int<- 
  glmn_plain_fit |> 
  tidy() |> 
  filter(estimate != 0 & grepl("_x_", term))

###

glmn_plain_alt_brier <- 
  show_best(glmn_plain_res, metric = "brier_class", n = 50) |> 
  filter(mixture < 0.1) |> 
  slice_min(mean)

glmn_plain_alt_best <- 
  glmn_plain_alt_brier |> 
  select(mixture, penalty, .config)

glmn_plain_alt_fit <- 
  glmn_plain_wflow |> 
  finalize_workflow(glmn_plain_alt_best) |> 
  fit(myopia_train)

glmn_plain_alt_main <- 
  glmn_plain_alt_fit |> 
  tidy() |> 
  filter(estimate != 0 & !grepl("(_x_)|(Intercept)", term))
glmn_plain_alt_int<- 
  glmn_plain_alt_fit |> 
  tidy() |> 
  filter(estimate != 0 & grepl("_x_", term))

### 

glmn_plain_best_mtr <- collect_metrics(glmn_plain_res) |>
  inner_join(glmn_plain_alt_best |> select(.config), by = ".config")
glmn_plain_best_pred <- collect_predictions(
  glmn_plain_res,
  parameters = glmn_plain_alt_best,
  summarize = TRUE
)

glmn_plain_roc_curve <- 
  glmn_plain_best_pred |>
  roc_curve(class, .pred_yes)

glmn_pr_roc_curve <- 
  glmn_plain_best_pred |>
  pr_curve(class, .pred_yes)

glmn_plain_roc_default <- closest_point(glmn_plain_roc_curve)
glmn_plain_pr_default <- closest_point(glmn_pr_roc_curve)
glmn_plain_roc_sensitive <- closest_point(glmn_plain_roc_curve, "sensitivity", 0.80)

glmn_plain_ranges <- 
  glmn_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(glmn_plain_res))) |>
  map(range)

glmn_brier_init <- 
  show_best(glmn_plain_res, metric = "brier_class") |> 
  slice(1) |> 
  pluck("mean")

glmn_brier_final <- 
  glmn_plain_best_mtr$mean[glmn_plain_best_mtr$.metric == "brier_class"]

glmn_brier_drop <- round((glmn_brier_final - glmn_brier_init) / glmn_brier_init * 100, 1)
```

```{r}
#| label: myopia-mlp-plain
#| include: false
#| cache: true
mlp_spec <-
  mlp(
    hidden_units = tune(),
    penalty = tune(),
    learn_rate = tune(),
    epochs = 100,
    activation = "relu"
  ) |>
  set_engine(
    "brulee",
    stop_iter = 5,
    optimizer = "ADAMw",
    verbose = FALSE,
    batch_size = tune(),
    momentum = tune()
  ) |>
  set_mode("classification")

mlp_rec <- 
  bare_rec |> 
  step_dummy(all_factor_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

mlp_plain_wflow <- workflow(mlp_rec, mlp_spec)
mlp_plain_param <- 
  mlp_plain_wflow |>
  extract_parameter_set_dials() |> 
  update(
    momentum = momentum(c(0.8, 0.99)),
    hidden_units = hidden_units(c(2, 50)),
    penalty = penalty(c(-10, -1)),
    learn_rate = learn_rate(c(-4, -1))
  )
set.seed(820)
mlp_plain_res <-
  mlp_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    param_info = mlp_plain_param,
    grid = 25
  )

mlp_plain_best <- select_best(mlp_plain_res, metric = "brier_class")
mlp_plain_best_mtr <- collect_metrics(mlp_plain_res) |>
  inner_join(mlp_plain_best |> select(.config), by = ".config")
mlp_plain_best_pred <- collect_predictions(
  mlp_plain_res,
  parameters = mlp_plain_best,
  summarize = TRUE
)

mlp_roc_curve <- 
  mlp_plain_best_pred |>
  roc_curve(class, .pred_yes)

mlp_plain_default <- closest_point(mlp_roc_curve)
mlp_plain_sensitive <- closest_point(mlp_roc_curve, "sensitivity", 0.95)

mlp_plain_ranges <- 
  mlp_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(mlp_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-lgb-plain
#| include: false
#| cache: true

lgb_plain_spec <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    mtry = tune(),
    learn_rate = tune(),
    stop_iter = 5
  ) |>
  set_mode("classification") |>
  set_engine("lightgbm")

lgb_plain_wflow <- workflow(class ~ ., lgb_plain_spec)

set.seed(820)
lgb_plain_res <-
  lgb_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25
  )

lgb_plain_best <- select_best(lgb_plain_res, metric = "brier_class")
lgb_plain_best_mtr <- collect_metrics(lgb_plain_res) |>
  inner_join(lgb_plain_best |> select(.config), by = ".config")
lgb_plain_best_pred <- collect_predictions(
  lgb_plain_res,
  parameters = lgb_plain_best,
  summarize = TRUE
)

lgb_roc_curve <- 
  lgb_plain_best_pred |>
  roc_curve(class, .pred_yes)

lgb_plain_default <- closest_point(lgb_roc_curve)
lgb_plain_sensitive <- closest_point(lgb_roc_curve, "sensitivity", 0.95)

lgb_plain_ranges <- 
  lgb_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(lgb_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-ranger-plain
#| include: false
#| cache: true

rf_plain_spec <-
  rand_forest(
    trees = 2000,
    min_n = tune(),
    mtry = tune()
  ) |>
  set_mode("classification")

rf_plain_wflow <- workflow(class ~ ., rf_plain_spec)

set.seed(820)
rf_plain_res <-
  rf_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25
  )

rf_plain_best <- select_best(rf_plain_res, metric = "brier_class")
rf_plain_best_mtr <- collect_metrics(rf_plain_res) |>
  inner_join(rf_plain_best |> select(.config), by = ".config")
rf_plain_best_pred <- collect_predictions(
  rf_plain_res,
  parameters = rf_plain_best,
  summarize = TRUE
)

rf_plain_roc_curve <- 
  rf_plain_best_pred |>
  roc_curve(class, .pred_yes)

rf_plain_default <- closest_point(rf_plain_roc_curve)
rf_plain_sensitive <- closest_point(rf_plain_roc_curve, "sensitivity", 0.95)

rf_plain_ranges <- 
  rf_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(rf_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-xrf-plain
#| include: false
#| cache: true

xrf_plain_spec <-
  rule_fit(
    mtry = tune(),
    trees = tune(),
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    penalty = tune()
  ) |>
  set_engine('xrf', counts = FALSE) |>
  set_mode('classification')

xrf_rec <- 
  bare_rec |> 
  step_dummy(all_factor_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

xrf_plain_wflow <- workflow(xrf_rec, xrf_plain_spec)
xrf_plain_param <- 
  xrf_plain_wflow |> 
  extract_parameter_set_dials() |> 
  update(
    tree_depth = tree_depth(c(2,4)),
    min_n = min_n(c(2, 10))
    )

set.seed(820)
xrf_plain_res <-
  xrf_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25,
    param_info = xrf_plain_param
  )

xrf_plain_best <- select_best(xrf_plain_res, metric = "brier_class")
xrf_plain_best_mtr <- collect_metrics(xrf_plain_res) |>
  inner_join(xrf_plain_best |> select(.config), by = ".config")
xrf_plain_best_pred <- collect_predictions(
  xrf_plain_res,
  parameters = xrf_plain_best,
  summarize = TRUE
)

xrf_plain_roc_curve <- 
  xrf_plain_best_pred |>
  roc_curve(class, .pred_yes)

xrf_plain_roc_default <- closest_point(xrf_plain_roc_curve)
xrf_plain_roc_sensitive <- closest_point(xrf_plain_roc_curve, "sensitivity", 0.95)

xrf_plain_ranges <- 
  xrf_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(xrf_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-xrf-plain-best
set.seed(926)
xrf_plain_fit <-
  xrf_plain_wflow |>
  finalize_workflow(xrf_plain_best) |>
  fit(myopia_train)
xrf_plain_rules <- xrf_plain_fit |> tidy(penalty = xrf_plain_best$penalty)

xrf_plain_best_pred_ind <- collect_predictions(
    xrf_plain_res,
    parameters = xrf_plain_best,
    summarize = FALSE
)
xrf_max_prob <- xrf_plain_best_pred_ind |> pluck(".pred_yes") |> max()
```

A set of five models was tested. First, a regularized logistic regression was used with glmnet penalties. The predictors included main effects and all two-factor interactions. Penalty values ranging from 10<sup>`r min(log10(glmn_plain_ranges$penalty))`</sup> to 10<sup>`r max(log10(glmn_plain_ranges$penalty))`</sup> were evaluated with four mixtures that ranged from `r min(glmn_plain_ranges$mixture* 100)`% to `r max(glmn_plain_ranges$mixture* 100)`% Lasso. A regular grid of `r nrow(glmn_plain_grid)` candidates was used to optimize these parameters^[We use the original implementation of the glmnet model, which features a capability where, for a specific mixture value, a single model fit contains results for multiple penalty values. For this reason, we can evaluate a regular grid with  `r nrow(glmn_plain_grid)` candidates with only `r vctrs::vec_unique_count(glmn_plain_grid$mixture)` model fits, one for each mixture value. A regular grid exploits this feature, but a space-filling design would nullify this advantage.].

Single-layer neural networks were evaluated and were tuned over: 

:::: {.columns}

::: {.column width="50%"}
- The number of hidden units (`r min(mlp_plain_ranges$hidden_units)` to `r max(mlp_plain_ranges$hidden_units)`). 
- The L<sub>2</sub> penalty (10<sup>`r min(log10(mlp_plain_ranges$penalty))`</sup> to 10<sup>`r max(log10(mlp_plain_ranges$penalty))`</sup>).
- The learning rate (10<sup>`r min(log10(mlp_plain_ranges$learn_rate))`</sup> to 10<sup>`r max(log10(mlp_plain_ranges$learn_rate))`</sup>).
:::

::: {.column width="50%"}
- The batch size (`r min(mlp_plain_ranges$batch_size)` to `r max(mlp_plain_ranges$batch_size)` samples).
- AdamW momentum (`r min(mlp_plain_ranges$momentum)` to `r max(mlp_plain_ranges$momentum)`).
:::

::::

SGD via AdamW was used to optimize the model for up to 100 epochs, with early stopping after 5 consecutive poor results. ReLU activation was used. 

Boosted trees (via light GBM) were also trained for up to 1,000 iterations with early stopping after 5 bad iterations. The model was tuned over: 

:::: {.columns}

::: {.column width="50%"}
- Tree depth (`r min(lgb_plain_ranges$tree_depth)` to `r max(lgb_plain_ranges$tree_depth)` splits). 
- Amount of data needed for further splitting (`r min(lgb_plain_ranges$min_n)` to `r max(lgb_plain_ranges$min_n)` samples).
:::

::: {.column width="50%"}
- The learning rate (10<sup>`r min(log10(lgb_plain_ranges$learn_rate))`</sup> to 10<sup>`r max(log10(lgb_plain_ranges$learn_rate))`</sup>).
- $m_{try}$ (`r min(lgb_plain_ranges$mtry)` to `r max(lgb_plain_ranges$mtry)` predictors).
:::

::::

Additionally, random forests with 2,000 classification trees were tuned over: 

- Amount of data needed for further splitting (`r min(rf_plain_ranges$min_n)` to `r max(rf_plain_ranges$min_n)` samples).
- $m_{try}$ (`r min(rf_plain_ranges$mtry)` to `r max(rf_plain_ranges$mtry)` predictors)..

Finally, a RuleFit model was optimized over: 

:::: {.columns}

::: {.column width="50%"}
- The number of boosting iterations (`r min(xrf_plain_ranges$trees)` to `r max(xrf_plain_ranges$trees)`)
- Tree depth (`r min(xrf_plain_ranges$tree_depth)` to `r max(xrf_plain_ranges$tree_depth)` splits). 
- Amount of data needed for further splitting (`r min(xrf_plain_ranges$min_n)` to `r max(xrf_plain_ranges$min_n)` samples).

:::

::: {.column width="50%"}
- The learning rate (10<sup>`r min(log10(xrf_plain_ranges$learn_rate))`</sup> to 10<sup>`r max(log10(xrf_plain_ranges$learn_rate))`</sup>).
-  Proportional $m_{try}$ (`r min(round(xrf_plain_ranges$mtry * 100, 1))`% to `r max(round(xrf_plain_ranges$mtry * 100, 1))`%).
- The L<sub>1</sub> penalty (10<sup>`r min(log10(xrf_plain_ranges$penalty))`</sup> to 10<sup>`r max(log10(xrf_plain_ranges$penalty))`</sup>).
:::

::::

Apart from the logistic regression, a space-filling design of 25 candidates was used for optimization. 

@fig-myopia-plain-roundup visualizes the results across and within models. Each point represents a candidate within a model. These are ranked by their resampled Brier score and are ordered accordingly. The vertical lines help highlight where the best candidate was located in the ranking. Interestingly, the logistic regression and RuleFit models had the best results and were very similar (see discussion below). The other three models had both good and bad performance across their candidates, but, in terms of model calibration, did not do as well. This may be due to the fact that this data set is better served by simpler models; the added complexity generated by large tree ensembles and neural networks is likely overfitting to some degree. 

```{r}
#| label: fig-myopia-plain-roundup
#| echo: false
#| fig-width: 6
#| fig-height: 3
#| out-width: 80%
#| fig-cap: "Cross-validated Brier scores for different modeling approaches without adjustments for class imbalance. The dashed vertical lines indicate the highest-ranked candidate for each model."

plain_res <- as_workflow_set(
  glmnet = glmn_plain_res,
  nnet = mlp_plain_res,
  boosting = lgb_plain_res,
  rand_forest = rf_plain_res,
  rule_fit = xrf_plain_res
)

rank_plain <-
  rank_results(plain_res, rank_metric = "brier_class") |>
  filter(.metric == "brier_class")

rank_plain_best <-
  rank_plain |>
  slice_min(mean, by = c(wflow_id)) |>
  mutate(percent_worse = (rank - 1) / nrow(rank_plain) * 100)

rank_plain |>
  ggplot(aes(rank, mean, col = wflow_id)) +
  geom_point(cex = 1/2) +
  geom_vline(
    data = rank_plain_best,
    aes(col = wflow_id, xintercept = rank),
    lty = 2
  ) +
  labs(x = "Rank", y = "Brier Score", col = "Model") +
  theme(legend.position = "top")
```

While the Rulefit and logistic regressions had the smallest Brier scores, their respective best candidates have issues. For the RuleFit model, the maximum estimated probability of a patient being myopic is `r round(xrf_max_prob * 100, 1)`% across all of the assessment sets. While the values are well-calibrated within the observed data range, this upper limit of probability is probably too small to be the final model. We would expect at least one of the held-out predictions to be closer to one when the patient is truly myopic. We also do not have a clear sense of how well-calibrated the predictions are when values are closer to one. 

TODO: problem of never predicting the minor class with poor probabilities

For the regularized logistic model, @fig-myopia-logistic illustrates the relationship between the tuning parameters and the Brier score. We can see that while the numerically best results are to use `r round(glmn_plain_best$mixture * 100, 0)`% Lasso regularization with a penalty of 10<sup>`r min(log10(glmn_plain_best$penalty))`</sup>, there are several other combinations with nearly equivalent metrics. The model fitted on the entire training set could have a maximum of `r max_num_terms` model coefficients (apart from the intercept) since there are `r num_pred` main effects and `r choose(num_pred, 2)` possible two-way interactions. However, the Lasso model can remove predictors, and the fitted model contains only `r nrow(glmn_plain_main)` main effect and `r nrow(glmn_plain_int)` interactions. For a model used purely for prediction, this is not a terrible situation. However, the lack of main effects is troubling since there are interaction terms in the model with no corresponding main effects. This is inconsistent with the heredity principle discussed in @sec-interactions-principles and suggests that these interactions are unlikely to be real. 

To sidestep this issue, we can choose a different tuning parameter candidate. If we use _no_ Lasso regularization, the model will contain all parameters, but regularizes some of them to be close to zero. This would ensure that all of the interaction terms have corresponding main effects in the model. Taking this approach, the penalty associated with the smallest Brier score is 10<sup>`r min(log10(glmn_plain_alt_best$penalty))`</sup>. For this model, the resampling estimate of the Brier score is `r signif(glmn_plain_alt_brier$mean, 3)`%, which is `r glmn_brier_drop` worse than the original choice, but still very good when compared to the other models. We'll use this candidate going forward. 

Additionally, @fig-myopia-logistic shows some diagnostics for the logistic regression. These plots are constructed by obtaining the assessment set predictions across the `r nrow(myopia_rs)` resamples. Note that since repeated cross-validation was used, each training set point has five replicates in the held-out data, totaling `r format(nrow(myopia_train) * nrow(myopia_rs), big.mark = ",")` rows across all resamples. To create the calibration and ROC curves, the five predictions for each training set point are averaged so that we have a set of `r nrow(myopia_train)` rows of predictions. Please note that these are approximate and may differ from the performance statistics generated by resampling. For example, @fig-myopia-logistic shows a single curve while the resampled ROC AUC is the average of `r nrow(myopia_rs)` ROC curves created with 10% of the training set. Despite this, the visualizations can help us characterize the quality of the fit and detect any significant issues in the predictions. 

```{r}
#| label: fig-myopia-logistic
#| echo: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: "Results of the logistic regression model optimization (without adjustments for the class imbalance). The top plot shows the performance profile for the two tuning parameters. Underneath are the calibration and ROC curves. The solid blue point on the ROC curve corresponds to the default cutoff. The pooled averaged holdout predictions were used to compute these visualizations."

p <- 
  autoplot(glmn_plain_res, metric = "brier_class") + 
  theme(legend.position = "top") + 
  labs(y = "Resampled Brier Score")

glmn_plain_cal <- 
 glmn_plain_best_pred |>
  cal_plot_windowed(class, .pred_yes, step_size = 0.05, window_size = 0.2) +
  labs(title = "Calibration Curve")

glmn_plain_roc <- 
  autoplot(glmn_plain_roc_curve) + 
  geom_point(
    data = glmn_plain_roc_default, 
    aes(x = 1 - specificity, y = sensitivity),
    cex = 4, 
    col = "blue"
  ) + 
  labs(title = "ROC Curve", y = "Sensitivity", x = "1 - Specificity")

p / (glmn_plain_cal + glmn_plain_roc)
```

The calibration curve is generally good, except for the upper end, where it indicates that the model underpredicts the probability of myopia. There is also a region around an x-axis point where events occur at a rate of 60%, and the probabilities are slightly overpredicted. All in all, though, the calibration curve is impressive. 

The ROC curve is symmetric and fairly unremarkable. The visualization shows that the default 50% threshold for converting the probabilities to hard "yes/no" predictions is somewhat problematic. The sensitivity for this cutpoint is very poor (`r round(glmn_plain_best_mtr$mean[xrf_plain_best_mtr$.metric == "sensitivity"] * 100, 1)`%) while the specificity is exquisite (`r round(glmn_plain_best_mtr$mean[xrf_plain_best_mtr$.metric == "specificity"] * 100, 1)`%). This is fairly common for class imbalances. The model performs significantly better where the data are more abundant, and since most participants in the study are not myopic, high specificity is more easily achieved. 

We quantified performance for these models using a variety of methods. @tbl-plain-metrics shows several statistics for both hard and soft prediction types. When assessing class probability estimates, the Brier scores for each model are all acceptably low, and most areas under the ROC curves have objectively large values. The areas under the precision-recall curves are mediocre, with the largest possible value being 1.0 and the worst value is `r signif(mean(myopia_train$class == "yes"), digits = 3)`. 

:::: {.columns}

::: {.column width="5%"}
:::

::: {.column width="90%"}

::: {#tbl-plain-metrics}

```{r}
#| label: plain-metrics
#| echo: false
mtr_labs <- c("Brier Score",  "ROC AUC", "PR AUC", "Binomial Log Loss",
              "Accuracy", "Kappa", "Sensitivity", "Specificity")

plain_metric_table <- 
  bind_rows(
    glmn_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Logistic (glmnet)"), 
    mlp_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Neural Network"),
    rf_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Random Forest"),
    lgb_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Boosted Tree"),
    xrf_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "RuleFit")
  ) |> 
  filter(Metric != "mn_log_loss") |> 
  mutate(
    Metric = 
      case_when(
        Metric == "accuracy" ~ "Accuracy",
        Metric == "kap" ~ "Kappa",
        Metric == "roc_auc" ~ "ROC AUC",
        Metric == "brier_class" ~ "Brier Score",
        Metric == "mn_log_loss" ~ "Binomial Log Loss",
        Metric == "pr_auc" ~ "PR AUC",
        Metric == "sensitivity" ~ "Sensitivity",
        Metric == "specificity" ~ "Specificity",
        TRUE ~ Metric
      ),
    Metric = factor(Metric, levels = mtr_labs)
  ) |> 
  pivot_wider(id_cols = Model, names_from = Metric, values_from = Estimate)

gt(plain_metric_table) |>
  fmt_number(
    columns = c(`ROC AUC`, `PR AUC`, `Brier Score`, Kappa),
    n_sigfig = 3
  ) |>
  fmt_percent(columns = c(Accuracy, Sensitivity, Specificity), decimals = 1) |>
  cols_move_to_start(`Brier Score`) |> 
  tab_spanner(
    "Hard Class Prediction Metrics",
    columns = c(Accuracy, Sensitivity, Specificity, Kappa)
  ) |> 
    tab_spanner(
    "Probability Prediction Metrics",
    columns = c(`ROC AUC`, `PR AUC`, `Brier Score`)
  )
```

Resampled performance metrics for the best candidates across different models. The models were optimized based on the Brier score. 
 
:::

:::

::: {.column width="5%"}
:::

:::: 

The hard class predictions in @tbl-plain-metrics use the default 50% probability cutoff. A few of the models have accuracies that are larger than the rate that the majority class occurs in the training set (`r round(mean(myopia_train$class == "no") * 100, 1)`%). The Kappa statistics indicate a significant signal in the models for predicting qualitative class membership. However, with this cutoff, the model is effectively useless at predicting which people are myopic, as the best possible sensitivity is `r round(max(plain_metric_table$Sensitivity) * 100, 1)`%. 

If we are satisfied with the model's ability to discriminate between classes (as evidenced by the ROC curve) and that the probabilities are sufficiently calibrated, we can address the sensitivity problem by selecting an appropriate probability threshold. As with most optimization problems, we require an objective function to make the best decision regarding this tuning parameter. There are a few ways we can approach the problem: 

 1. Given what we know about how the model will be used, we could define specific costs for false negatives and false positives (with correctly predicted classes having zero cost). Using these values, we can choose a threshold that minimizes the cost of the decision. This is similar to a weighted Kappa approach, except the weights would not be symmetric. 
 2. Seek a constrained optimization. For example, we can try to maximize the sensitivity under the constraint that the specificity is at an acceptable value. The ROC curve provides the substrate for these calculations, as it includes sensitivity and specificity for various thresholds in the data. 
 3. Optimize an existing composite score that captures the right balance of sensitivity and specificity. The Matthews correlation coefficient and F-statistics are examples of such metrics. 
 4. Use a multiparameter optimization method, such as desirability functions from @sec-cls-multi-objectives, to define a jointly acceptable result (i.e., overall desirability). This would enable us to consider multiple performance metrics, as well as other variables, in the decision-making process. 
 
We'll focus on the first two options. 

To choose the threshold using a custom cost metric (the first option in the list above), we would have to determine an appropriate cost structure for false positives and false negatives, usually based on the consequences of either error. For our myopia example, this is a somewhat subjective choice that depends on some context-specific rationale. In other cases, the costs might be analytically calculated as a function of monetary costs. @fig-class-cost shows examples of cost curves if the false positive cost is fixed at 1.0 and the cost of a false negative varies in severity. The y-axis is the mean of the cost values averaged across the `r nrow(myopia_rs)` assessment sets. 
 
```{r}
#| label: myopia-glmnet-threshold
#| include: false
#| cache: true
thresholds <- seq(0.0, 0.40, by = 0.02)[-1]

glmn_tlr <-
  tailor() |>
  adjust_probability_threshold(threshold = tune())

# Using a single penalty causes an error. We'll use two and pick the appropriate
# value afterwards

glmn_plain_mod_spec <-
  logistic_reg(penalty = tune(), mixture = !!glmn_plain_alt_best$mixture) |>
  set_engine("glmnet", path_values = !!path)

glmn_thr_wflow <- workflow(glmn_rec, glmn_plain_mod_spec, glmn_tlr) 

glmn_thr_res <-
  glmn_thr_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = crossing(
      penalty = c(0.001, glmn_plain_alt_best$penalty), 
      threshold = thresholds)
  )

glmn_thr_pred <- 
  glmn_thr_res |> 
  collect_predictions(summarize = FALSE) |> 
  filter(penalty == glmn_plain_alt_best$penalty)
```

```{r}
#| label: fig-class-cost
#| echo: false
#| out-width: 60%
#| fig-width: 6
#| fig-height: 4
#| fig-cap: "Cost curves when the cost of a correct prediction is zero, the cost of a false positive is 1.0, and the cost of a false negative varies."
costs <- c(2, 5, 10, 15)

for (i in seq_along(costs)) {
  tmp <- 
    glmn_thr_pred |> 
    mutate(
      cost_val = costs[i],
      .pred_class = if_else(.pred_yes >= threshold, "yes", "no"),
      .pred_class = factor(.pred_class, levels = c("yes", "no")),
      cost = 
        case_when(
          class == "yes" & .pred_class == "no"  ~ cost_val,
          class == "no"  & .pred_class == "yes" ~ 1.0,
          TRUE ~ 0.0)
    ) |> 
    summarize(
      cost = mean(cost), 
      .by = c(id, id2, threshold, cost_val)
    )
  
  if (i == 1) {
    cost_mtr <- tmp
  } else {
    cost_mtr <- bind_rows(cost_mtr, tmp)
  }
}

cost_mean_mtr <- 
  cost_mtr |> 
  summarize(
    num = sum(!is.na(cost)),
    cls_cost = mean(cost),
    std_err = sd(cost),
    .by = c(threshold, cost_val)
  )

cost_best_mtr <- 
  cost_mean_mtr |> slice_min(cls_cost, by = c(cost_val))

cost_mean_mtr |>
  ggplot(aes(threshold, col = format(cost_val))) +
  geom_point(aes(y = cls_cost), alpha = 1) +
  geom_line(aes(y = cls_cost), alpha = 1) +
  geom_point(data = cost_best_mtr, aes(y = cls_cost), cex = 3) +
  labs(x = "Probability Threshold", y = "Total Cost", col = "Cost of FN") +
  scale_x_continuous(labels = label_percent()) +
  scale_color_manual(
    values = c("#c3cf88", "#ABDDA4FF", "#66C2A5FF", "#3288BDFF")
  ) +
  theme(legend.position = "top")
```

The figure shows that increasing the cost of incorrectly predicting the events yields increasingly smaller thresholds. This is due to classifying more points as events, regardless of their actual class. The cost metric is punished less by incorrectly predicted non-events, so the overall cost statistic decreases substantially in the curves with higher false negative costs. 

::: {.important-box}
It is important to realize that this strategy only optimizes the probability threshold. The model fit (e.g., slope and intercept parameter estimates) has not been affected by the cost function.  In the next section, we will consider costs to the objective function that the model uses during training. 
:::

Moving on to constrained optimization, when choosing a threshold using an ROC curve, an immediate question arises: "Which data should be used to compute the ROC curve?" _Preferably_, the cutoff selection should use different data than the data used to fit the model. For example, diagnostic tests that require approval from a government agency (e.g., the Food and Drug Administration in the US) require a different data set to set the threshold. However, this may not be feasible if there is not an abundance of data or the resources to acquire such data. If we can't hold out another data set, we can use the assessment sets generated during resampling^[Or a validation set if multiple resamples are not used.]. There are at least two different approaches. 

 - **Option 2A - _post hoc_ pooling**: After we have resampled our model configurations, we can pool the assessment set predictions for the best candidate. This ROC curve is computed based on the overall existing class probability estimate values. This approach generated the visualizations in @fig-myopia-logistic. 

- **Option 2B - threshold tuning**: During model tuning, we treat the probability threshold as a tuning parameter and include it in the grid search. For each threshold in the grid, conditional on any other tuning parameters, we have resampling estimates for sensitivity and specificity. These points are then used to create an ROC curve. 

The primary difference between these approaches lies in data usage: do we compute one large ROC curve from pooled data or an ROC curve from resampled statistics? Statistically, the latter is more appropriate, but either might produce acceptable results.

Note that there are a few options when we treat the threshold as a tuning parameter. A one-stage approach would include the threshold along with any other tuning parameters and use a single grid for all^[When adding the threshold to the optimization grid, it might be helpful to start with a space-filling design and _crossing_ it with a vector of thresholds. This allows us to have data on the thresholds for each of the candidate points. There is minimal overhead associated with this approach, as the thresholding computations occur after preprocessing and model fitting. For example, if an initial grid with 25 points is crossed with 10 threshold values,  there are still only 25 model configurations to train, which takes the longest time to compute.]. However, recall that the choice of this parameter does not affect any probability-based metrics, such as the Brier score, log-loss, or the area under the ROC curve. Instead, we could use a two-stage approach where the non-threshold parameters are optimized to find their best results, and then we can use a one-dimensional grid to find the best threshold (based on metrics appropriate for hard class predictions). We took this approach here: `r length(thresholds)` threshold values ranging from `r round(min(thresholds) * 100, 1)`% to `r round(max(thresholds) * 100, 1)`% defined the second-stage grid. 

```{r}
#| label: fig-myopia-two-roc
#| echo: false
#| out-width: 50%
#| fig-width: 4.5
#| fig-height: 4.7
#| fig-cap: "Two approaches for optimizing the probability threshold via the ROC curve. Both curves use the regularized logistic regression's resampling results."

glmn_thr_roc <- 
  glmn_thr_res |> 
  collect_metrics(type = "wide") |> 
  filter(penalty == glmn_plain_alt_best$penalty) |> 
  select(threshold, sensitivity, specificity) |> 
  bind_rows(
    tribble(
      ~threshold , ~sensitivity , ~specificity ,
      0 ,            1 ,            0 ,
      1 ,            0 ,            1
    )
  )

roc_tradeoff <- 
  glmn_thr_roc |>
  filter(specificity >= 0.8) |>
  slice_max(sensitivity) |>
  mutate(
    prevalence = mean(myopia_df$class == "yes"),
    ppv = (sensitivity * prevalence) / ((sensitivity * prevalence) +
        ((1 - specificity) * (1 - prevalence))),
    npv = (specificity * (1 - prevalence)) / (((1 - sensitivity) * prevalence) +
        ((specificity) * (1 - prevalence)))
  )

glmn_thr_roc |>
  mutate(Method = "Threshold Tuning") |> 
  bind_rows(
    glmn_plain_roc_curve |> 
      select(.threshold, sensitivity, specificity) |> 
      mutate(Method = "post hoc Pooling")
  ) |> 
  ggplot(aes(1 - specificity, sensitivity, col = Method)) +
  geom_step(direction = "vh", linewidth = 0.8) +
  coord_obs_pred() +
  theme(legend.position = "top") + 
  labs(color = NULL) +
  scale_color_manual(values = c("#8785B2FF", "#DABD61FF"))
```
 
For the logistic regression model, @fig-myopia-two-roc shows the ROC curves for both schemes. The curves significantly overlap for these data. Note that the curve associated with a second-stage grid contains `r nrow(glmn_thr_roc) - 2` unique points since it is based on the values used in our second-stage grid. The curve for the _post hoc_ pooling method has `r nrow(glmn_plain_roc_curve) - 2` points, which represents how many unique predicted probabilities are contained in the _post hoc_ data pool. 
 
Using either method for composing the ROC tool provides us with a tool to make trade-offs between sensitivity and specificity. For example, if we want to maximize sensitivity on the condition that specificity is at least 80%, the curve produced by threshold tuning indicates that the cutoff should be `r round(roc_tradeoff$threshold * 100, 1)`%. The estimated statistics associated with this value are a sensitivity of `r round(roc_tradeoff$sensitivity * 100, 1)`% and a specificity of `r round(roc_tradeoff$specificity * 100, 1)`%. 

This overall approach involves finding a suitable ML model that does not allow class imbalance to alter our methodology, while deferring cutoff selection until the end of the process. This approach can lead to a well-calibrated model that appropriately classifies new samples. The downside becomes apparent when trying to explain individual results to the model's consumers. 

For our myopia example, suppose we make a prediction for a patient where the probability of myopia is estimated to be 20%. The conversation with the patient might sound something like

> "We estimate that your child has a 20% chance of being severely nearsighted and, based on this, we would give them a diagnosis of myopia." 

It is natural for someone to find the diagnosis counterintuitive since they are probably comparing the probability to the natural 50% threshold. It might lead the parent to ask:  

> "That probability seems pretty low to make that diagnosis. Is that a mistake?"

The non-technical response could be:

> "We think that the diagnosis is appropriate because the overall chances of being myopic are low, and we have determined that our cutoff for making the diagnosis (`r round(roc_tradeoff$threshold * 100, 1)`%) gives us the most effective diagnoses."

Of course, this is paraphrasing, but the hypothetical conversation does illustrate the potential confusion. 

This seeming disconnect can become worse as the prevalence becomes smaller. For very rare events (i.e., < 5%), the cutoff associated with the sensitivity-specificity trade-off can be very small, perhaps less than 1%. This is the price paid for having probability estimates that are consistent with the rate at which the event occurs in the wild. 
 
## Strategy 2: Correcting the Imbalance {#sec-imblance-correction}

In the previous section, the only operation used to account for the small minority class was to optimize the probability threshold _after_ the model had been chosen. In this section, we'll consider several methods for actively addressing the imbalance during model optimization. The two primary classes of techniques are: 

- Changing the composition of the training set data.
- Upweighting the minority class's effect on the objective function.

As we'll see, these can have a profound effect on the class probability estimates (both good and bad). These techniques are also most effective when the primary goal of the model is to identify events in new data. 

### Sampling and Synthesis Techniques {#sec-sampling-synthesis}

One strategy for addressing a class imbalance is to eliminate it in the training set by adding or removing rows, depending on the class. A "sampling" method would remove existing rows from the data (likely from the majority class) to even the class frequencies. The removal may be random or based on characteristics of the existing data. Other sampling methods can oversample existing data. "Synthesis" methods invent new rows of the data in a way that makes them similar to or consistent with existing examples from the same class. 

Regardless of the method used, note that rebalancing occurs only on the data being used to train the model. If we are within a resampling loop, the analysis sets are affected, but the assessment sets are not. If we are going to fit a model to the entire data set, the training set is rebalanced, but the test set^[And validation set, if any] are unaffected. The reason for this is that the data used to measure how well the models work should be drawn from the same population as the data the model will eventually be used to predict. Rebalancing tools deliberately bias the sample to be unlike the population being predicted. 

Note that all the techniques mentioned here should be considered as yet another preprocessing method. As such, it is in the interest of the modeler to resample the models and include these steps within the resampling process. It would be inappropriate to apply a technique such as downsampling and then resample the data. Since these methods involve randomness, we need to account for it in our performance statistics. 

#### Downsampling {#sec-downsampling}

This approach is straightforward: every class with more data than the minority class is randomly sampled, ensuring that all classes have the same number of training set points. The minority class stays the same. For our myopia data, the majority class is randomly reduced from `r nrow(myopia_train)` to `r sum(myopia_df$class == "yes")` patients. @fig-myopia-subsampling show examples for the two predictor subset including how the FDA fit changes with the reduced data set. We can see that the class boundary is slightly more simplistic than @fig-myopia-fda but has shifted to the right, capturing more events with the default 50% cutoff (at the expense of increased false positives). 

```{r}
#| label: myopia-down-fda
#| include: false
myopia_down_formatted <- 
  recipe(class ~ ., data = myopia_train) |> 
  step_downsample(class) |>
  prep() |> 
  bake(new_data = NULL)

set.seed(616)
myopia_down_smol_formatted <- 
  recipe(class ~ ., data = myopia_smol_formatted) |> 
  step_downsample(class) |>
  prep() |> 
  bake(new_data = NULL)

fda_down_fit <- discrim_flexible(prod_degree = 2) |> 
  fit(class ~ ., data = myopia_down_smol_formatted)
fda_down_pred <- augment(fda_down_fit, new_data = smol_grid)
fda_down_capture <- 
  augment(fda_down_fit, new_data = myopia_smol_formatted) |> 
  filter(.pred_yes >= 0.5 & class == "yes") |> 
  nrow()
```

::: {#fig-myopia-subsampling}

::: {.figure-content}

```{shinylive-r}
#| label: fig-subsampling
#| out-width: "80%"
#| viewerHeight: 600
#| standalone: true

library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(munsell)
library(scales)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
# source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-sa.R")

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-1, 10, -1)),
    column(
      width = 10,
      selectInput(
        inputId = "method",
        label = " ",
        choices = c(
          "Downsampled",
          "SMOTE",
          "ROSE",
          "Near Miss",
          "Adaptive Synthetic Algorithm"
        ),
        selected = c("Downsampled")
      )
    )
  ),
  as_fill_carrier(plotOutput("plot")),
  br(),
  textOutput("text")
)

server <- function(input, output) {
  load(url(
    "https://raw.githubusercontent.com/aml4td/website/main/RData/imbalanced_sampled.RData"
  ))

  xrng <- range(imbalanced_sampled$spherical_equivalent_refraction)
  yrng <- range(imbalanced_sampled$vitreous_chamber_depth)

  orig <- imbalanced_sampled %>%
    dplyr::filter(Data == "Original")

  output$plot <-
    renderPlot(
      {
        dat <- imbalanced_sampled %>%
          dplyr::filter(Data == input$method) |>
          dplyr::bind_rows(orig) |>
          dplyr::mutate(
            Data = factor(Data, levels = c("Original", input$method))
          )

        grid <- imbalanced_grid %>%
          dplyr::filter(Data == input$method) |>
          dplyr::select(-Data) |>
          filter(!is.na(.pred_yes))
        p <-
          dat |>
          ggplot(
            aes(
              spherical_equivalent_refraction,
              vitreous_chamber_depth
            )
          ) +
          geom_point(
            aes(
              col = class,
              pch = class
            ),
            cex = 1.6,
            alpha = 2 / 3
          ) +
          facet_wrap(~Data) +
          coord_fixed(ratio = 1.4, xlim = xrng, ylim = yrng)

        p <- p +
          theme(legend.position = "top") +
          scale_color_brewer(drop = FALSE, palette = "Set1", direction = 01) +
          theme_light_bl()

        p <-
          p +
          geom_tile(data = grid, aes(fill = .pred_yes), alpha = .1) +
          geom_contour(
            data = grid,
            aes(z = .pred_yes),
            breaks = 1 / 2,
            col = "black"
          ) +
          scale_fill_gradient2(
            low = "#377EB8",
            mid = "white",
            high = "#E41A1C",
            midpoint = 0.5
          ) +
          labs(
            x = "spherical equivalent refraction",
            y = "vitreous chamber depth",
            fill = "Probability"
          )
        print(p)
      },
      res = 100
    )
  
  output$text <-
    renderText(
      paste(
        "Using the default 50% threshold, this method correctly identified", 
        imbalanced_hits[input$method], 
        "truly myopic subjects (out of 60).") 
    )
}

app <- shinyApp(ui = ui, server = server)
app
```

:::

The effect of a few sampling and synthesis techniques on two of the predictors in the myopia training set. The class boundary is based on a 50% probability cutoff.

:::

Besides rebalancing, the advantage of downsampling is that models can be trained faster. The downside is that important information might be lost in the majority class(es) due to random sampling. We are diminishing the influence of the non-minority classes and creating a smaller training set. Another effect of this is that our models will tend to be more simplistic, as the data cannot support excessive complexity. 

```{r}
#| label: myopia-ridge
#| include: false

glmn_ridge_spec <-
  logistic_reg(penalty = tune(), mixture = 0) |>
  set_engine("glmnet", path_values = !!path)

glmn_ridge_wflow <- workflow(glmn_rec, glmn_ridge_spec)

glmn_ridge_grid <- tibble(penalty = path)
```

```{r}
#| label: myopia-sampling
#| include: false
#| cache: true

sampling_rec_list <-
  list(
    Downsample = down_rec,
    SMOTE = smote_rec,
    ROSE = rose_rec,
    `Near Miss` = nm_rec
  )

lgb_plain_grid <-
  lgb_plain_spec |>
  extract_parameter_set_dials() |>
  update(mtry = mtry(c(1, 17))) |>
  grid_space_filling(size = 25)

sampling_set <-
  bind_rows(
    workflow_set(
      preproc = sampling_rec_list,
      model = list(glmn = glmn_ridge_spec)
    ) |>
      option_add(grid = glmn_ridge_grid),
    workflow_set(
      preproc = sampling_rec_list,
      model = list(lgb = lgb_plain_spec)
    ) |>
      option_add(grid = lgb_plain_grid)
  ) |>
  workflow_map(
    resamples = myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    seed = 837
  )
```

#### Upsamping {#sec-upsampling}

Upsampling will create random replicates of the majority class(es) so that all classes have the same frequency in the training set. This increases the training set, sometimes to orders of magnitude larger, so training time can become excessive for some models. There is also the issue of having a potentially large number of replicates for each non-majority class value in the original training set. For example, for models such as random forest that use bootstrap samples to fit models, we could have even more replicates of these data points. Another complication occurs when a model, such as _K_-nearest neighbors, is used; all of the nearest neighbors might be the same data point. 

Upsampling is probably the least used of these techniques do to these issues. 

#### SMOTE {#sec-smote}

The Synthetic Minority Oversampling Technique (SMOTE) is a method that combines random sampling with the creation of novel data points, which are fusions and/or perturbations of existing points. There are a few flavors of this method, but we'll focus on the original technique by REF where all predictors are required to be numeric. 

For each data point in the minority class, compute its K-nearest neighbors. The create a new artificial data point, we select a minority class point at random ($\boldsymbol{x}_i$) as well as one of its neighbors. For each predictor, we compute the difference between the original data point and its chosen neighbor (call this $\delta_j$ for predictor $j$). For each predictor, we compute a uniform random number $u_j \sim U(0, 1)$ and modify the original point as $x^*_{ij} = x_{ij} + u_j\delta_j$. Essentially, SMOTE generates a hyperrectangle between the selected point and its chosen neighbor, and then randomly places the new sample within the hyperrectangle. 

#### ROSE {#sec-rose}

#### Boarderline Methods {#sec-borderline}

#### Comparisons Using the Myopia Data {#sec-myopia-sampling}

The top panel of @fig-myopia-sampling shows the model configuration statistics for different sampling and synthesis methods. The visualization emulates an ROC curve by plotting the sensitivity versus the false positive rate (i.e., one minus specificity) to demonstrate the trade-offs between the two types of errors. The downsampling panel shows a tight cluster of model results for both types of models, located in the region where sensitivity and specificity are approximately equal. Unsurprisingly, this is due to the equal frequencies of the two classes in the training data (post-sampling). However, this will not always be the case for other data sets. Near-miss sampling has a similar pattern with tighter clustering of model configuration results. 

```{r}
#| label: sampling-metrics
#| include: false

sampling_mtr <-
  sampling_set |>
  collect_metrics() |>
  select(-.estimator, -n, -std_err, -preproc) |>
  mutate(
    model = if_else(model == "boost_tree", "Boosted Tree", "Logistic Regression"),
    model = factor(model, levels = c("Logistic Regression", "Boosted Tree")),
    sampling = map_chr(wflow_id, ~ strsplit(.x, "_")[[1]][1])
    ) |>
  pivot_wider(
    id_cols = c(wflow_id, .config, model, sampling),
    names_from = .metric,
    values_from = mean
  )
```

```{r}
#| label: fig-myopia-sampling
#| echo: false
#| fig-width: 7
#| fig-height: 7
#| out-width: 80%
#| fig-cap: "Cross-validated statistics for the subsampling and synthesis methods."

sampling_sens_spec <- 
  sampling_mtr |>
  ggplot(aes(1 - specificity, sensitivity, col = model, pch = model)) +
  geom_abline(lty = 3) + 
  geom_point(cex = 2, alpha = 2 / 3) +
  facet_wrap(~sampling, ncol = 4) +
  coord_fixed(xlim = 0:1, ylim = 0:1) + 
  theme(legend.position = "bottom") +
  labs(color = "Model", pch = "Model") +
  scale_shape_manual(values = c(1:0)) +
  scale_color_manual(values = c("#EF7C12FF", "#172869FF"))

sampling_sens_brier <- 
  sampling_mtr |>
  ggplot(aes(sensitivity, brier_class, col = model, pch = model)) +
  geom_point(cex = 2, alpha = 2 / 3, show.legend = FALSE) +
  facet_wrap(~model) +
  coord_fixed(ratio = 4) +
  labs(y = "Brier Score")+
  scale_shape_manual(values = c(1:0)) +
  scale_color_manual(values = c("#EF7C12FF", "#172869FF"))

sampling_sens_spec / sampling_sens_brier
```

The two synthetic methods showed slightly different patterns. ROSE and SMOTE with logistic regression yielded another tight cluster of results, with somewhat smaller statistics than those obtained through downsampling. For boosting, both SMOTE and ROSE demonstrated greater versatility in the tuning parameter results, enabling trade-offs between sensitivity and specificity. However, the pattern of the configurations favored specificity over sensitivity, especially for SMOTE. 

The bottom panel of @fig-myopia-sampling shows that, when sensitivity is improved by these tools, there can be a corresponding increase in the Brier scores. The boosting results show that a high sensitivity raises the Brier to values that would indicate a complete lack of calibration in the predicted probability estimates. This is deliberate; the goal of rebalancing is to have parity in the class frequencies. This enables the model to increase the likelihood of identifying events in the data. The consequence of doing so is that our class probability distributions are biased towards overestimating the probability of an event. However, when assessing the model, we use unbalanced data, and the increased probability of an event is not consistent with the true event rate (as seen outside of the balanced training set).

It is worth noting that we could also optimize the threshold to further fine-tune the sensitivity and specificity values. Unlike the analysis in the previous section, the custom thresholds are much less likely to be excessively small (or large). However, if this is required, we are probably better off using unsampled training sets or using cost-sensitive learning (shown in the next section).

```{r}
#| label: sampling-comparisons
#| include: false
#| cache: true

glmn_plain_pred <-
  glmn_thr_res |>
  collect_predictions(summarize = TRUE) |>
  filter(
    penalty == glmn_plain_alt_best$penalty & 
      threshold == roc_tradeoff$threshold
  ) |>
  select(.row, plain = .pred_yes) |>
  mutate(model = "Logistic Regression")

plain_pred <-
  bind_rows(
    glmn_plain_pred,
    lgb_plain_best_pred |>
      select(.row, plain = .pred_yes) |>
      mutate(model = "Boosted Tree")
  )

best_sampling <-
  sampling_mtr |>
  slice_max(roc_auc, by = c(sampling, model), with_ties = FALSE) |>
  select(wflow_id, .config, model, sampling, roc_auc)

sampling_pred <- NULL
for (i in seq_along(best_sampling$wflow_id)) {
  tmp_preds <-
    sampling_set |>
    extract_workflow_set_result(id = best_sampling$wflow_id[i]) |>
    collect_predictions(summarize = TRUE) |>
    filter(.config == best_sampling$.config[i]) |>
    select(class, .pred_yes, .row) |>
    mutate(
      model = best_sampling$model[i],
      sampling = best_sampling$sampling[i]
    )

  sampling_pred <- bind_rows(sampling_pred, tmp_preds)
}

sampling_pred <-
  sampling_pred |>
  pivot_wider(
    id_cols = c(class, .row, model),
    names_from = sampling,
    values_from = .pred_yes
  )

plain_sampling_pred <-
  sampling_pred |>
  full_join(plain_pred, by = c(".row", "model"))

# ------------------------------------------------------------------------------

sampling_roc_auc <- function(split) {
  grps <- c("Downsample", "SMOTE", "ROSE", "Near Miss")
  rocs <- rep(NA, length(grps))
  dat <- analysis(split) |> dplyr::select(-.row)
  plain_roc <- dat |> roc_auc(class, plain) |> pluck(".estimate")
  for (j in seq_along(grps)) {
    rocs[j] <- roc_auc_vec(dat$class, dat[[grps[j]]])
  }
  tibble(term = grps, estimate = plain_roc - rocs)
}

set.seed(518)
sampling_glmn_roc_reps <-
  plain_sampling_pred |>
  filter(model == "Logistic Regression") |> 
  bootstraps(times = 2000) |>
  mutate(stats = map(splits, sampling_roc_auc)) |> 
  int_pctl(stats, alpha = 0.1) |> 
  mutate(model = "Logistic Regression")

set.seed(518)
sampling_lgb_roc_reps <-
  plain_sampling_pred |>
  filter(model == "Boosted Tree") |> 
  bootstraps(times = 2000) |>
  mutate(stats = map(splits, sampling_roc_auc)) |> 
  int_pctl(stats, alpha = 0.1) |> 
  mutate(model = "Boosted Tree")

sampling_roc_reps <- 
  bind_rows(
    sampling_glmn_roc_reps,
    sampling_lgb_roc_reps
  ) |> 
  mutate(
    model = factor(model, levels = c("Logistic Regression", "Boosted Tree"))
  )

glmn_samp_roc_rng <- range(best_sampling$roc_auc[best_sampling$model == "Logistic Regression"])
lgb_samp_roc_rng <- range(best_sampling$roc_auc[best_sampling$model == "Boosted Tree"])
```

We can see that these tools have an effect on the hard class prediction metrics, such as sensitivity and specificity. Although @fig-myopia-sampling indicates that the calibration properties are not good, meaning that probability estimates do not reflect the rate at which events occur. However, it is possible that the probabilities can help discriminate between the classes for some threshold. For the four sampling methods used on these data, their ROC AUC values for the best logistic regression models ranged from `r round(glmn_samp_roc_rng[1], 3)` to `r round(glmn_samp_roc_rng[2], 3)` The range for the boosted tree models was `r round(lgb_samp_roc_rng[1], 3)` to `r round(lgb_samp_roc_rng[2], 3)`. Are these any better or worse than the models that were developed in @sec-imbalance-acceptance? Using the bootstrap techniques in @sec-compare-holdout, we linked the predictions by row for all of these models, created 2,000 bootstrap samples, and then computed the difference in the area under the ROC curves. @fig-myopia-sampling-comparison shows the results. For these data, there doesn’t appear to be any improvement in the ROC AUCs when subsampling and synthesis tools are used. Note that the x-axis scale is fairly small; these AUC values are very similar to one another. One comparison did not cover zero, indicating that using logistic regression with SMOTE had a slightly worse AUC than simply using the training set as is. 

```{r}
#| label: fig-myopia-sampling-comparison
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| out-width: 70%
#| fig-cap: "Bootstrap 90% confidence intervals to assess whether subsampling and synthesis methods improve the area under the ROC curve. Negative values indicate that the AUC values were higher for models developed in @sec-imbalance-acceptance."

sampling_roc_reps |>
  ggplot(aes(y = term, col = model)) +
  geom_vline(xintercept = 0, lty = 3, col = "black") +
  geom_point(aes(x = .estimate), position = position_dodge(width = 0.5)) +
  geom_errorbar(
    aes(xmin = .lower, xmax = .upper),
    width = 1 / 2,
    position = position_dodge(width = 0.5),
    alpha = 3 / 4
  ) +
  scale_color_manual(values = c("#EF7C12FF", "#172869FF")) +
  labs(
    x = "Difference in ROC AUC\n(sampling - basic)",
    y = NULL,
    color = "Model"
  ) +
  theme(legend.position = "top")
```

These results highlights the idea that rebalancing tools can help a model do better at finding events and that the user should only focus on hard class predictions (and perhaps avoid exposing the users to the probability estimates). 

### Cost-Sensitive Learning {#sec-cost-sensitive-learning}

We’ve previously talked about the consequences of making an incorrect prediction. For example, @sec-mushrooms showed a model that predicts whether mushrooms are poisonous or not. We could think about this in terms of the general cost of a prediction (not too dissimilar to the discussion related to SVM models). 

If we have $C$ classes, we could write down a cost matrix that defines the specific costs $\mathcal{c}_{j|k}$ of incorrectly predicting a data point to be class $j$ when it is truly class $k$. In many instances, the diagonal terms of this matrix are zero since there is no real "cost" to making the correct prediction. There are cases where different types of benefits are associated with different types of correct predictions. See REF for an example. However, for the purposes of our discussion here, we will assume that the diagonal is zero. 

If we have this cost structure, we can use the predicted class probabilities to estimate the expected costs for a collection of data by averaging a set of individual costs. 

Let’s say that $C=3$ and the cost matrix is 

$$
\mathcal{C} = 
\begin{bmatrix}
0.00 & 1.00 & 3.00 \\
0.50 & 0.00 & 2.00 \\ 
0.25 & 0.75 & 0.00
\end{bmatrix}
$$

where the true values are in columns, and the predicted class levels are rows. For example, the cost of predicting a data point that is Class 3 as Class 1 is $\mathcal{c}_{1|3} = 3.00$. Here are some examples of probabilities and their individual costs:

```{r}
#| label: cost-examples
#| echo: false
ex_costs <- 
  tribble(
    ~truth, ~estimate, ~cost,
    "A", "B", 0.50,
    "A", "C", 0.25,
    "B", "A", 1.00,
    "B", "C", 0.75,
    "C", "A", 3.00,
    "C", "B", 2.00
  )

ex_cost_dat <- 
  tribble(
    ~.prob_A, ~.prob_B, ~.prob_C, ~truth,
    1 / 3, 1 / 3, 1 / 3,  1L,
    1 / 3, 1 / 3, 1 / 3,  2L,
    1 / 3, 1 / 3, 1 / 3,  3L,
    0.1, 0.5, 0.4,  1L,
    0.4, 0.5, 0.1,  3L,
  ) |> 
  mutate(truth = factor(LETTERS[truth]))

cost_res <- NULL
for (i in 1:nrow(ex_cost_dat)) {
  tmp <- 
    ex_cost_dat |> 
    slice(i) |> 
    classification_cost(truth, .prob_A:.prob_C, costs = ex_costs) 
  cost_res <- bind_rows(cost_res, tmp)
}

cost_tbl <- 
  bind_cols(ex_cost_dat, cost_res) |> 
  select(Truth = truth, A = .prob_A, B = .prob_B, C = .prob_C, Cost = .estimate)

cost_tbl |> 
  gt() |> 
  fmt_number(c(-Truth)) |> 
  tab_spanner(columns= A:C, label = "Probability Estimates")
```

If our ML model can utilize custom objective functions, minimizing the expected cost is a fairly rational way to ensure your model is fit for purpose. If not, we can use the expected cost to help optimize the tuning parameters of our model (as we did in @fig-class-cost). 

Another way some researchers have parameterized cost is to collapse the matrix, resulting in a general cost for misclassifying a sample, regardless of what it was misclassified as. In this case, we sum the cost matrix over columns, resulting in one cost per class. XXX describes these as class weights. In our example matrix shown above, the class weights are $\mathcal{C}_1 = 0.75$,  $\mathcal{C}_2 = 1.75$,  and  $\mathcal{C}_3 = 5.00$. For a two-class outcome (e.g., myopia or not), the class weights are the same as the two off-diagonal entries in the cost matrix. 

If we consider class-specific costs as weights, we can generalize many performance metrics to be cost-sensitive. For example, we can make the Brier score cost-sensitive using a modified version of @eq-brier

$$
Brier = \frac{1}{WC}\sum_{i=1}^n\sum_{k=1}^C w_k(y_{ik} - \hat{p}_{ik})^2
$$ {#eq-brier-cost}

where $w_k$ is the cost or weight for class $k$ and $W$ is the sum of the $n$ weights. If we have two classes and $w_1 = 5$ and $w_2 = 1$, every data point whose true outcome is the first class will have five-fold more influence over the objective function. In this way, we can train the model to ensure that it performs better in predicting specific classes than others. This is the basis for _cost-sensitive learning_^[Note that our previous use of costs in @fig-class-cost was _after_ the model was trained. In this section, we will use weights/costs so that the model adapts to the cost structure during training.]. 

In practice, our implementations of ML model metrics may or may not enable cost-sensitive learning. If not, they may have the capacity to handle _case weights_. These are numeric values associated with each row of the data set that specify how it should affect the computations. There are many types of case weights for different purposes. Here are a few examples: 

- Frequency weights are integers that specify how often the specific predictor pattern occurs in the data. If there were two categorical predictions with a total of 25 combinations, a very large data set can be compressed by using a case weight that reflects how often each of the 25 patterns occurs. Frequency weights would be assigned to both the training and testing sets. 

- Importance weights are numbers (decimal and non-negative) that reflect how important the row is to the model fit. This is the type of weight that we’ve discussed above. Importance weights should only be assigned to the training set; the test set should not use them, as it should accurately reflect the state of the data in the real world. 

If the metric implementation allows for case weights, we can use importance weights to make them sensitive to different classes. 

::: {.note-box}
While not related to class imbalances, importance weights are also useful for time-series data, where we would like more recent data to have a higher priority in the model fit. It can also be useful for dealing with problematic/anomalous data in a model. For example, data collected during the global COVID-19 pandemic can have a negative impact on fits, but it may still need to be included. Assigning these row small case weights may solve those issues. 
:::

Note that the scale of class weights may differ across metrics and/or models (as we’ll see below). For one type of model, minority class weights ranging from 1 to 50 might have the same performance as another model/metric that ranges from 1 to 5. There is some trial and error associated with tuning the weights. 

If your problem has a well-known cost structure, likely expressed in units of currency, there is probably little ambiguity about it. Otherwise, we might use class weights to either decrease the likelihood of tragic errors from occurring in the model or to help the model overcome a severe class imbalance. In other words, the "importance" in importance weights could mean many different things. 

CART book, resampling weights versus case weights

To visualize an example, @fig-myopia-cost-fda shows the same data as @fig-myopia-fda, but in this case, case weights were used so that the myopic data (in red) had a 15-fold larger impact on the objective function than the non-myopic children. The same FDA model was trained on the data, but it used the case weights. The result is a class boundary that bends in the middle, allowing it to encompass a greater portion of the minority class than the original fit. 

```{r}
#| label: myopia-cost-fda
#| include: false
myopia_smol_wts_formatted <- 
  myopia_smol_formatted |> 
  mutate(
    cls_wts = if_else(class == "yes", 15.0, 1.0),
    cls_wts = importance_weights(cls_wts)
  )

fda_wts_wflow <- 
  workflow(class ~ ., discrim_flexible(prod_degree = 2)) |> 
  add_case_weights(cls_wts)

fda_cost_fit <- fit(fda_wts_wflow, data = myopia_smol_wts_formatted)
fda_cost_pred <- augment(fda_cost_fit, new_data = smol_grid)
fda_cost_capture <- 
  augment(fda_cost_fit, new_data = myopia_smol_formatted) |> 
  filter(.pred_yes >= 0.5 & class == "yes") |> 
  nrow()
```

:::: {#fig-myopia-cost-fda}

```{r}
#| label: myopia-cost-grid
#| echo: false
#| fig-width: 4.8
#| fig-height: 5.2
#| warning: false
#| out-width: 50%
myopia_smol_formatted |> 
  ggplot(aes(`spherical equivalent refraction`, `axial length`)) +
  geom_point(aes(color = class), alpha = 0.5, shape = 16, size = 2, show.legend = FALSE) + 
  geom_tile(data = fda_cost_pred, aes(fill = .pred_yes), alpha = .1) +
  geom_contour(data = fda_cost_pred, aes(z = .pred_yes), breaks = 1/2, col = "black") +
  scale_fill_gradient2(low = "#377EB8", mid = "white", high = "#E41A1C", midpoint = 0.5) +
  scale_color_brewer(palette = "Set1") + 
  labs(fill = "Probability") +
  theme(legend.position = "top")
```

A flexible discriminant fit to the myopia training set using cost-sensitive learning. The myopic subjects had a 15-fold higher cost in the objective function and `r fda_cost_capture` of the `r sum(myopia_smol_formatted$class == "yes")` events (in red) are identified using the default 50% threshold. This figure is comparable with @fig-myopia-fda and @fig-myopia-down-fda.

::::

Case weights based on the class can also approximate some of the subsampling methods shown in the previous section. For example, instead of dropping training set rows from the data, we can adjust the importance weights so that the sum of the weights in the majority class equals the sum of the weights in the minority class. By doing this, we may be able to obtain a better model since the majority of the majority class is not lost.  Similarly, an approximation to upsampling would only increase the minority class weights so that the class weight totals are in equilibrium. This would also result in faster training times than basic oversampling.

It is worth noting that some stochastic ensemble implementations (e.g., boosting and random forests) often confuse the terms "case weights" and "sampling weights." In these instances, the weights are used to preferentially sample the training set while fitting the models (usually trees) without directly impacting the objective function. This process is more akin to downsampling and upsampling than to cost-sensitive learning. 

To demonstrate with the full myopia data set, boosted trees and regularized logistic regression were used once again. The lightgbm implementation was used and, while their documentation is unclear about how the weights are used, it appears that the weights are applied to the objective function and not just the sampling mechanism^[A GitHub issue ([`https://github.com/microsoft/LightGBM/issues/1299`](https://github.com/microsoft/LightGBM/issues/1299)) has evidence that the weights are a "multiplication applied to every positive label weight" and shows some C++ code to that effect.].

```{r}
#| label: myopia-lgb-cls-wts
#| include: false
#| cache: true

lgb_cost_spec <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    mtry = tune(),
    learn_rate = tune(),
    stop_iter = 5
  ) |>
  set_mode("classification") |>
  set_engine("lightgbm", scale_pos_weight = tune())

lgb_cost_wflow <- workflow(class ~ ., lgb_cost_spec)

lgb_cost_grid <-
  lgb_plain_grid |>
  crossing(scale_pos_weight = 2^(-10:0))

set.seed(820)
lgb_cost_res <-
  lgb_cost_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = lgb_cost_grid
  )
```

```{r}
#| label: myopia-glmn-cls-wts
#| include: false
#| cache: true

all_wts <- 2^(seq(0, 4, by = 0.5))
all_glmn_wts <- vector(mode = "list", length = length(all_wts))

for (i in seq_along(all_wts)) {
  myopia_wts_train <-
    myopia_train |>
    mutate(
      cls_wts = if_else(class == "yes", all_wts[i], 1.0),
      cls_wts = importance_weights(cls_wts)
    )

  set.seed(538)
  myopia_wts_rs <- vfold_cv(myopia_wts_train, repeats = 5)

  glmn_wts_wflow <-
    workflow(class ~ ., glmn_ridge_spec) |>
    add_case_weights(cls_wts)

  all_glmn_wts[[i]] <-
    glmn_wts_wflow |>
    tune_grid(
      myopia_wts_rs,
      metrics = cls_mtr,
      control = ctrl_grid,
      grid = glmn_ridge_grid
    )
}
```

```{r}
#| label: cost-metrics
#| include: false

lgb_cost_mtr <-
  lgb_cost_res |>
  collect_metrics(type = "wide") |>
  mutate(class_weight = 1 / scale_pos_weight)

glmn_wts_mtr <- 
  all_glmn_wts |> 
  map2_dfr(
    all_wts, 
    ~ collect_metrics(.x, type = "wide") |> 
      mutate(class_weight = .y)
  )

cost_mtr <- 
  bind_rows(
  lgb_cost_mtr |> 
  select(.config, sensitivity, specificity, brier_class, class_weight) |> 
  mutate(
    Model = "Boosted Tree"),
  glmn_wts_mtr |> 
    select(.config, sensitivity, specificity, brier_class, class_weight) |> 
    mutate(Model = "Logistic Regression")
  )

cost_rng <-
  cost_mtr |>
  summarize(
    max_wt = max(class_weight),
    min_wt = min(class_weight),
    .by = c(Model)
  )

cost_mtr <- 
  cost_mtr |> 
  full_join(cost_rng, by = "Model") |> 
  mutate(
    Model = factor(Model, levels = c("Logistic Regression", "Boosted Tree")),
    class_weight = log2(class_weight),
    min_wt = log2(min_wt),
    max_wt = log2(max_wt),
    scaled_weight = (class_weight - min_wt) / (max_wt - min_wt)
  )
```

The same grids were used as in the previous section for both models. For the case weights ranges for each model, some investigation was required to find appropriate changes in the performance metrics. While both parameter ranges were explored in log<sub>2</sub> units, starting at 1.0, the boosted model used a maximum weight of 2<sup>10</sup>, whereas logistic regression used maximum weights of 2<sup>4</sup>. The majority class weights were kept at a value of 1.0 for both models. For each model, a sequence of weights was created in a 1D grid, and these were crossed with the original grids.

@fig-myopia-cost-sensitive shows the results where the ranges of the weights have been standardized across the two models to be within [0, 1]. In the top panel, another ROC-like visualization is used to demonstrate the effect of using higher weights for the myopic samples. We can see that both models tend to increase sensitivity as the weights increase^[As with the previous section, these statistics use the default 50% threshold.]. This is not always the case, as some tuning parameters for the model were suboptimal on their own, and adjusting the class weights did not yield a significant change. For the most part, the increase in sensitivity comes with the price of reduced specificity. The different class weights enable us to select the optimal trade-off between the two. 

As with subsampling and synthesis methods, the class probabilities generated by this model become increasingly inconsistent with reality as the cost of the minority class increases. The bottom panel shows the degradation in the Brier score tracks with increased sensitivity. Once again, we caution against using these probability estimates with users, as they may not accurately emulate the true likelihood of the event.

```{r}
#| label: fig-myopia-cost-sensitive
#| echo: false
#| fig-width: 7
#| fig-height: 7
#| out-width: 80%
#| fig-cap: "Cross-validated statistics for the cost-sensitive models."

cost_roc <- 
  cost_mtr |>
  ggplot(aes(1 - specificity, sensitivity, col = scaled_weight)) +
  geom_point(cex = 2, alpha = 1 / 3) +
  coord_fixed(xlim = 0:1, ylim = 0:1) +
  facet_wrap(~ Model) + 
  labs(color = "Class Weight (scaled)") +
  theme(legend.position = "bottom") +
  scale_color_viridis_c(option = "A", direction = -1)

cost_sens_brier <- 
  cost_mtr |>
  ggplot(aes(sensitivity, brier_class, col = scaled_weight)) +
  geom_point(cex = 2, alpha = 1 / 3, show.legend = FALSE) +
  lims(x = 0:1) +
  facet_wrap(~ Model) +
  coord_fixed(ratio = 4.5, xlim = 0:1, ylim = c(0.08, 0.30)) +
  scale_color_viridis_c(option = "A", direction = -1)

cost_roc / cost_sens_brier
```

When comparing subsampling/synthetic methods to cost-sensitive learning, the latter appears to be more versatile than the former. It enables the user to have more direct control over the trade-off between false-negative and false-positive rates. The downside to cost-sensitive learning is that it may be dependent on the implementation of the method, whereas subsampling/synthetic tools can be used anywhere, as they are specialized preprocessors. 

## Revenge of the Prevalence {#sec-prevalence-again}

It is worth revisiting the discussion from @sec-cls-two-classes on how the event rate (i.e., prevalence) affects important statistics.  We'll continue to use the myopia data for discussion.  

If we assume that the rate of myopia in the population of interest is consistent with our data set (`r round(roc_tradeoff$prevalence * 100, 1)`%), we can gain a deeper understanding of how well the model performs.  Given our estimates of sensitivity and specificity, the unconditional performance statistics are: 

- positive predictive value (PPV): `r round(roc_tradeoff$ppv * 100, 1)`%
- negative predictive value (NPV): `r round(roc_tradeoff$npv * 100, 1)`%

Even though our sensitivity and specificity ended up being fairly balanced, the statistics that convey to true ability to predict are not.  
 
This could exacerbate the potential confusion that can occur when explaining the results of a prediction. Continuing the hypothetical conversation from @sec-imbalance-acceptance, the explanation of the PPV might be: 

> "We understand that this rule of thumb might seem overly stringent. 
> That said, from our data, we know that when we predict that a child is myopic, the probability that they are actually myopic is roughly `r round(roc_tradeoff$ppv * 100, 0)`%.
> This is because _most children are not myopic_ but we do find some evidence to make the diagnosis." 

Conversely, if the child had a very small predicted probability and a myopia diagnosis was not used, the explanation would be: 

> "Even though the chances that your child is myopic are greater than zero, from our data we know that when we predict that a child is not myopic, the probability that they are actually not myopic is overwhelmingly high (roughly `r round(roc_tradeoff$npv * 100, 0)`%).
> This is because _most children are not myopic_ and there is not enough evidence to disprove this for your child." 

This is much less ambiguous than the previous hypothetical conversation. 

We can try to optimize the model for PPV and NPV in the same way as we did for sensitivity and specificity. However, to do this, the prevalence must be well known for the specific population that will be predicted by the model and should be stable over time. 

For example, @covidcdc calculated COVID-19 cases and deaths, and estimated cumulative incidence for March and April 2020. The mortality rate varied geographically. For example, during that period, New York City had an estimated mortality rate of 5.3%, while the remainder of the state of New York had an estimated rate of 2.2%. Thankfully, in 2025, these rates are much lower. 

This illustrates that computations that depend on the event rate can be challenging, as the values can systematically change under different conditions and over time. 

## Chapter References {.unnumbered}
 
