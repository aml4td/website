---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-imbalance/"
---

# Class Imbalances {#sec-cls-imbalance}

```{r}
#| label: cls-imbalance-setup
#| include: false

source("../R/_common.R")
# source("../R/_themes_ggplot.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(mirai)
library(gt)
library(probably)
library(patchwork)
library(bonsai)
library(rules)
library(themis)
library(ggforce)
library(discrim)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_bw())
set_options()
daemons(parallel::detectCores())

# ------------------------------------------------------------------------------

closest_point <- function(x, what = ".threshold", target = 0.5) {
  x$delta <- abs(x[[what]] - target)
  dplyr::slice_min(x, delta, with_ties = FALSE)
}
```


```{r}
#| label: load-data
#| include: false

data(myopia, package = "aplore3")
myopia_df <-
  myopia |>
  as_tibble() |>
  select(-id) |>
  rename(
    year = studyyear,
    class = myopic,
    age = age,
    female = gender,
    spherical_equivalent_refraction = spheq,
    axial_length = al,
    anterior_chamber_depth = acd,
    lens_thickness = lt,
    vitreous_chamber_depth = vcd,
    hours_sports = sporthr,
    hours_reading = readhr,
    hours_gaming = comphr,
    hours_studying = studyhr,
    hours_tv = tvhr,
    near_work_activities = diopterhr,
    mother_myopic = mommy,
    father_myopic = dadmy
  ) |>
  mutate(
    class = factor(tolower(class), levels = c("yes", "no")),
    female = if_else(female == "Female", 1, 0),
    mother_myopic = if_else(mother_myopic == "Yes", 1, 0),
    father_myopic = if_else(father_myopic == "Yes", 1, 0)
  ) |>
  relocate(class)

# ------------------------------------------------------------------------------

set.seed(3571)
myopia_split <- initial_split(myopia_df, strata = class)
myopia_train <- training(myopia_split)
myopia_test <- testing(myopia_split)

set.seed(538)
myopia_rs <- vfold_cv(myopia_train, repeats = 5)

# ------------------------------------------------------------------------------

num_pred <- ncol(myopia_train) - 1
max_num_terms <- num_pred + choose(num_pred, 2)
```

Examples: churn, click through, diseases (trip test, Parkinsonâ€™s), blood specimen mixups

Focus on two class problems, but an issue for all classes

Prior versus likelihood, posterior

note on time slices to make events


## Example: Predicting Myopia

For demonstration, we'll use data from the myopia study as described in Section 1.6.6 of @hosmer2013applied. Myopia (i.e., nearsightedness) can be caused by the shape of one's eye, genetics, and environmental conditions, including the heavy use of computer or television screens. The data are from a medical study running from `r min(myopia_df$year)` to `r max(myopia_df$year)` and contained `r sum(myopia_df$female)` girls and `r sum(myopia_df$female == 0)` ranging from `r min(myopia_df$age)` to `r max(myopia_df$age)` years old. These data are from the initial exam and include demographic factors, several physiological measurements of the eye from an ocular examination, and whether either parent is also myopic. The outcome is determined after a five-year follow-up visit. In this study, `r sum(myopia_df$class == "yes")` (`r round(mean(myopia_df$class == "yes") * 100, 1)`%) of children were nearsighted. 

To begin, a 3:1 stratified split was used to partition the data into training and testing sets. Stratification was based on the outcome. This results in a training set of `r nrow(myopia_train)` children, only `r sum(myopia_train$class == "yes")` of whom are myopic. For the test set, `r sum(myopia_test$class == "yes")` children out of `r nrow(myopia_test)` were myopic. Since the training set and the number of events are small, five repeats of 10-fold cross-validation were used with the same stratification strategy. The repeats will help increase the precision of our resampling statistics. 

::: {.important-box}
It is critical that the test set reflects the true event rate in the population of interest. While we may change the event rate in the training set during model development, the test set data should be as consistent as possible with reality. 
:::

We'll conduct some quick exploratory data analysis, then proceed to creating models based on traditional modeling methods, i.e., without making adjustments for class imbalance during model optimization. 

### Exploring the Data

```{r}
#| label: myopia-age-sex
#| include: false
rates_by_age_sex <- 
  myopia_train |>
  count(class, age, female) |>
  pivot_wider(id_cols = c(female, age), names_from = class, values_from = n) |>
  mutate(
    yes = if_else(is.na(yes), 0, yes),
    n = yes + no,
    stats = map2(
      yes,
      n,
      ~ tidy(binom.test(x = .x, n = .y, conf.level = .9))
    ),
    rate = map_dbl(stats, ~ .x$estimate),
    lower = map_dbl(stats, ~ .x$conf.low),
    upper = map_dbl(stats, ~ .x$conf.high),
    Sex = if_else(female == 1, "Female", "Male")
  ) 

rates_by_parent <-
  myopia_train |>
  count(class, mother_myopic, father_myopic) |>
  pivot_wider(
    id_cols = c(mother_myopic, father_myopic),
    names_from = class,
    values_from = n
  ) |>
  mutate(
    yes = if_else(is.na(yes), 0, yes),
    n = yes + no,
    stats = map2(
      yes,
      n,
      ~ tidy(binom.test(x = .x, n = .y, conf.level = .9))
    ),
    rate = round(map_dbl(stats, ~ .x$estimate) * 100, 1),
    lower = round(map_dbl(stats, ~ .x$conf.low) * 100, 1),
    upper = round(map_dbl(stats, ~ .x$conf.high) * 100, 1)
  ) |> 
  arrange(mother_myopic, father_myopic)
```

First, let's examine the age and sex of the participants. @fig-myopia-age-sex shows the class percentages for 10 subgroups of the data. The size of the point is based on the number of children in the subgroup, indicating that many of them are 6 years old, while very few are 9 years old. There were no nine-year-old myopic girls in the training set. 

```{r}
#| label: fig-myopia-age-sex
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: "Training set rates of myopia by age and sex of the children. The error bars are based on 90% confidence intervals, and the size of the point indicates the number of children in each subgroup."

rates_by_age_sex |>
  ggplot(aes(age, col = Sex)) +
  geom_point(aes(y = rate, size = n), position = position_dodge(width = 0.25)) +
  geom_errorbar(
    aes(ymin = lower, ymax = upper),
    position = position_dodge(width = 0.25),
    width = 0.2
  )+
  scale_color_manual(values = c("#417839FF", "#E18727FF")) +
  scale_y_continuous(labels = label_percent(), limits = c(0, 1)) +
  labs(x = "Age", y = "Rate of Myopia") +
  theme(legend.position = "top") 
```

The points suggest a slight increase in myopia rates among girls, but this is well within the error bars defined by the 90% confidence intervals. Due to the concentration of points in a single age group, it is difficult to determine whether there is an age trend. However, since the slight rate increase for girls is constant from ages six to eight, there is little indication of an interaction between these factors. 

Second, do genetics matter? If neither parent is myopic, the rates for their children are low: `r rates_by_parent$rate[1]`% with 90% confidence interval (`r rates_by_parent$lower[1]`%, `r rates_by_parent$upper[1]`%). When _both_ parents have the condition, the rate is  `r rates_by_parent$rate[4]`%  (CI: `r rates_by_parent$lower[4]`%, `r rates_by_parent$upper[4]`%), indicating a strong signal. 
The effects of single paternal and maternal genetics are in between, with rates `r rates_by_parent$rate[2]`% and `r rates_by_parent$rate[3]`%, respectively, with similar confidence intervals.    

The ocular exam yielded five different measurements of the eye. @fig-myopia-splom shows a scatterplot matrix of pairwise relationships, colored by the outcome class. With one exception,  there is little to no correlation between predictors. However, the axial length and the vitreous chamber depth have a high positive correlation. Additionally, most predictors have fairly symmetric distributions, although the spherical equivalent refraction data exhibit a moderately strong right skew. This predictor also shows some raw separation between the classes. 

```{r}
#| label: fig-myopia-splom
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 7
#| fig-cap: "Scatterplot matrix of eye measurements."

myopia_train |>
  select(class, spherical_equivalent_refraction:vitreous_chamber_depth) |>
  rename_with(~ tolower(gsub("_", "\n", .x))) |>
  ggplot(aes(x = .panel_x, y = .panel_y, col = class)) +
  geom_point(alpha = 0.5, shape = 16, size = 1) +
  geom_autodensity(aes(fill = class, col = class), alpha = 3 / 4) +
  facet_matrix(vars(-class), layer.diag = 2, grid.y.diag = FALSE) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "top")
```

## Underlying mathematical issue

```{r}
#| label: myopia-plain-fda
#| include: false

myopia_formatted <- 
  myopia_train |> 
  mutate(
    father_myopic = factor(ifelse(father_myopic == 1, "myopic", "normal")),
    mother_myopic = factor(ifelse(mother_myopic == 1, "myopic", "normal")),
    sex = factor(ifelse(female == 1, "Female", "Male"))
    ) |> 
  rename(
    `paternal genetics` = father_myopic,
    `maternal genetics` = mother_myopic
  ) |> 
  rename_with(~ paste0(.x, ")"), contains("hours")) |> 
  rename_with(~ gsub("hours_", "hours (", .x), contains("hours"))|> 
  rename_with(~ gsub("_", " ", .x), everything()) |> 
  select(-female)

smol_grid <- 
  crossing(
    `spherical equivalent refraction` = seq(-1, 5, length.out = 100),
    `axial length` = seq(20, 25, length.out = 100)
  )

ends <- tribble(
  ~`spherical equivalent refraction`, ~`axial length`, ~yes, ~no,
  NA_real_, NA_real_, 0.0, 1.0,
  NA_real_, NA_real_, 1.0, 0.0
)

myopia_smol_formatted <- 
  myopia_formatted |> 
  select(`spherical equivalent refraction`, `axial length`, class)

fda_plain_fit <- discrim_flexible(prod_degree = 2) |> 
  fit(class ~ ., data = myopia_smol_formatted)
fda_plain_pred <- augment(fda_plain_fit, new_data = smol_grid)
fda_plain_capture <- 
  augment(fda_plain_fit, new_data = myopia_smol_formatted) |> 
  filter(.pred_yes >= 0.5 & class == "yes") |> 
  nrow()
```


:::: {#fig-myopia-fda}

```{r}
#| label: myopia-fda
#| echo: false
#| fig-width: 4.8
#| fig-height: 5.2
#| warning: false
#| out-width: 50%
myopia_smol_formatted |> 
  ggplot(aes(`spherical equivalent refraction`, `axial length`)) +
  geom_point(aes(color = class), alpha = 0.5, shape = 16, size = 2, show.legend = FALSE) + 
  geom_tile(data = fda_plain_pred, aes(fill = .pred_yes), alpha = .1) +
  geom_contour(data = fda_plain_pred, aes(z = .pred_yes), breaks = 1/2, col = "black") +
  scale_fill_gradient2(low = "#377EB8", mid = "white", high = "#E41A1C", midpoint = 0.5) +
  scale_color_brewer(palette = "Set1") + 
  labs(fill = "Probability") +
  theme(legend.position = "top")
```

A flexible discriminant fit to the myopia training set. Using the default probability cutoff of 50%, shown as the black contour, `r fda_plain_capture` of the `r sum(myopia_smol_formatted$class == "yes")` events are correctly predicted as events. 
::::


## Imbalances should make us think about what we want from our model

Finding events vs accurate predictions

discussion or relative risk vs absolute risk

This should drive our choice of optimization metric

don't confuse dealing with extreme frequencies with finding important events




Using these data, the next two sections will discuss the two primary strategies for addressing class imbalances.  

## Strategy 1: Accepting the State of Nature {#sec-imbalance-acceptance}

In this case, we'll try to build ML models while generally ignoring the imbalance. The idea is that we should initially focus on optimizing metrics based on class probability estimates, such as the Brier score, cross-entropy, or the areas under the ROC or PR curves. Once we have a model with good calibration and/or discrimination, we can then focus on making the best hard class predictions. We'll show the advantages and disadvantages of using this strategy. 

To get started, we'll fit a subset of models to these data and generally follow the process outlined in the previous chapters. There is little to no preprocessing required for these models. There is a single pair of highly correlated predictors, but that will be handled by the model in situations where multicorrelation is a concern. 

```{r}
#| label: model-setup
#| include: false

# add classification costs
cls_mtr <- metric_set(
  brier_class,
  roc_auc,
  pr_auc,
  kap,
  mn_log_loss,
  accuracy,
  sensitivity,
  specificity
)
ctrl_grid <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)
ctrl_rs <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
```

```{r}
#| label: recipes
#| include: false
bare_rec <-
  recipe(class ~ ., data = myopia_train)

norm_rec <-
  bare_rec |>
  step_normalize(all_numeric_predictors())

glmn_rec <-
  bare_rec |>
  step_interact(~ all_predictors():all_predictors()) |> 
  step_normalize(all_numeric_predictors())

down_rec <-
  bare_rec |>
  step_downsample(class)

smote_rec <-
  bare_rec |>
  step_smote(class)

rose_rec <-
  bare_rec |>
  step_rose(class)

asa_rec <-
  bare_rec |>
  step_adasyn(class)

nm_rec <-
  bare_rec |>
  step_nearmiss(class)
```


```{r}
#| label: myopia-glmnet-plain
#| include: false
#| cache: true

path <- 10^seq(-4, -0.6, by = 0.2)

glmn_plain_spec <-
  logistic_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet", path_values = !!path)

glmn_plain_wflow <- workflow(glmn_rec, glmn_plain_spec)

glmn_plain_grid <-crossing(mixture = round((0:3) / 3, 2), penalty = path)

glmn_plain_res <-
  glmn_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = glmn_plain_grid
  )

### 

glmn_plain_best <- select_best(glmn_plain_res, metric = "brier_class")
glmn_plain_fit <- fit_best(glmn_plain_res)
glmn_plain_main <- 
  glmn_plain_fit |> 
  tidy() |> 
  filter(estimate != 0 & !grepl("(_x_)|(Intercept)", term))
glmn_plain_int<- 
  glmn_plain_fit |> 
  tidy() |> 
  filter(estimate != 0 & grepl("_x_", term))

###

glmn_plain_alt_brier <- 
  show_best(glmn_plain_res, metric = "brier_class", n = 50) |> 
  filter(mixture < 0.1) |> 
  slice_min(mean)

glmn_plain_alt_best <- 
  glmn_plain_alt_brier |> 
  select(mixture, penalty, .config)

glmn_plain_alt_fit <- 
  glmn_plain_wflow |> 
  finalize_workflow(glmn_plain_alt_best) |> 
  fit(myopia_train)

glmn_plain_alt_main <- 
  glmn_plain_alt_fit |> 
  tidy() |> 
  filter(estimate != 0 & !grepl("(_x_)|(Intercept)", term))
glmn_plain_alt_int<- 
  glmn_plain_alt_fit |> 
  tidy() |> 
  filter(estimate != 0 & grepl("_x_", term))

### 

glmn_plain_best_mtr <- collect_metrics(glmn_plain_res) |>
  inner_join(glmn_plain_alt_best |> select(.config), by = ".config")
glmn_plain_best_pred <- collect_predictions(
  glmn_plain_res,
  parameters = glmn_plain_alt_best,
  summarize = TRUE
)

glmn_plain_roc_curve <- 
  glmn_plain_best_pred |>
  roc_curve(class, .pred_yes)

glmn_pr_roc_curve <- 
  glmn_plain_best_pred |>
  pr_curve(class, .pred_yes)

glmn_plain_roc_default <- closest_point(glmn_plain_roc_curve)
glmn_plain_pr_default <- closest_point(glmn_pr_roc_curve)
glmn_plain_roc_sensitive <- closest_point(glmn_plain_roc_curve, "sensitivity", 0.80)

glmn_plain_ranges <- 
  glmn_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(glmn_plain_res))) |>
  map(range)

glmn_brier_init <- 
  show_best(glmn_plain_res, metric = "brier_class") |> 
  slice(1) |> 
  pluck("mean")

glmn_brier_final <- 
  glmn_plain_best_mtr$mean[glmn_plain_best_mtr$.metric == "brier_class"]

glmn_brier_drop <- round((glmn_brier_final - glmn_brier_init) / glmn_brier_init * 100, 1)
```

```{r}
#| label: myopia-mlp-plain
#| include: false
#| cache: true
mlp_spec <-
  mlp(
    hidden_units = tune(),
    penalty = tune(),
    learn_rate = tune(),
    epochs = 100,
    activation = "relu"
  ) |>
  set_engine(
    "brulee",
    stop_iter = 5,
    optimizer = "ADAMw",
    verbose = FALSE,
    batch_size = tune(),
    momentum = tune()
  ) |>
  set_mode("classification")

mlp_rec <- 
  bare_rec |> 
  step_dummy(all_factor_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

mlp_plain_wflow <- workflow(mlp_rec, mlp_spec)
mlp_plain_param <- 
  mlp_plain_wflow |>
  extract_parameter_set_dials() |> 
  update(
    momentum = momentum(c(0.8, 0.99)),
    hidden_units = hidden_units(c(2, 50)),
    penalty = penalty(c(-10, -1)),
    learn_rate = learn_rate(c(-4, -1))
  )
set.seed(820)
mlp_plain_res <-
  mlp_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    param_info = mlp_plain_param,
    grid = 25
  )

mlp_plain_best <- select_best(mlp_plain_res, metric = "brier_class")
mlp_plain_best_mtr <- collect_metrics(mlp_plain_res) |>
  inner_join(mlp_plain_best |> select(.config), by = ".config")
mlp_plain_best_pred <- collect_predictions(
  mlp_plain_res,
  parameters = mlp_plain_best,
  summarize = TRUE
)

mlp_roc_curve <- 
  mlp_plain_best_pred |>
  roc_curve(class, .pred_yes)

mlp_plain_default <- closest_point(mlp_roc_curve)
mlp_plain_sensitive <- closest_point(mlp_roc_curve, "sensitivity", 0.95)

mlp_plain_ranges <- 
  mlp_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(mlp_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-lgb-plain
#| include: false
#| cache: true

lgb_plain_spec <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    mtry = tune(),
    learn_rate = tune(),
    stop_iter = 5
  ) |>
  set_mode("classification") |>
  set_engine("lightgbm")


lgb_plain_wflow <- workflow(class ~ ., lgb_plain_spec)

set.seed(820)
lgb_plain_res <-
  lgb_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25
  )

lgb_plain_best <- select_best(lgb_plain_res, metric = "brier_class")
lgb_plain_best_mtr <- collect_metrics(lgb_plain_res) |>
  inner_join(lgb_plain_best |> select(.config), by = ".config")
lgb_plain_best_pred <- collect_predictions(
  lgb_plain_res,
  parameters = lgb_plain_best,
  summarize = TRUE
)

lgb_roc_curve <- 
  lgb_plain_best_pred |>
  roc_curve(class, .pred_yes)

lgb_plain_default <- closest_point(lgb_roc_curve)
lgb_plain_sensitive <- closest_point(lgb_roc_curve, "sensitivity", 0.95)

lgb_plain_ranges <- 
  lgb_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(lgb_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-ranger-plain
#| include: false
#| cache: true

rf_plain_spec <-
  rand_forest(
    trees = 2000,
    min_n = tune(),
    mtry = tune()
  ) |>
  set_mode("classification")

rf_plain_wflow <- workflow(class ~ ., rf_plain_spec)

set.seed(820)
rf_plain_res <-
  rf_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25
  )

rf_plain_best <- select_best(rf_plain_res, metric = "brier_class")
rf_plain_best_mtr <- collect_metrics(rf_plain_res) |>
  inner_join(rf_plain_best |> select(.config), by = ".config")
rf_plain_best_pred <- collect_predictions(
  rf_plain_res,
  parameters = rf_plain_best,
  summarize = TRUE
)

rf_plain_roc_curve <- 
  rf_plain_best_pred |>
  roc_curve(class, .pred_yes)

rf_plain_default <- closest_point(rf_plain_roc_curve)
rf_plain_sensitive <- closest_point(rf_plain_roc_curve, "sensitivity", 0.95)

rf_plain_ranges <- 
  rf_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(rf_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-xrf-plain
#| include: false
#| cache: true

xrf_plain_spec <-
  rule_fit(
    mtry = tune(),
    trees = tune(),
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    penalty = tune()
  ) |>
  set_engine('xrf', counts = FALSE) |>
  set_mode('classification')

xrf_rec <- 
  bare_rec |> 
  step_dummy(all_factor_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

xrf_plain_wflow <- workflow(xrf_rec, xrf_plain_spec)
xrf_plain_param <- 
  xrf_plain_wflow |> 
  extract_parameter_set_dials() |> 
  update(
    tree_depth = tree_depth(c(2,4)),
    min_n = min_n(c(2, 10))
    )

set.seed(820)
xrf_plain_res <-
  xrf_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25,
    param_info = xrf_plain_param
  )

xrf_plain_best <- select_best(xrf_plain_res, metric = "brier_class")
xrf_plain_best_mtr <- collect_metrics(xrf_plain_res) |>
  inner_join(xrf_plain_best |> select(.config), by = ".config")
xrf_plain_best_pred <- collect_predictions(
  xrf_plain_res,
  parameters = xrf_plain_best,
  summarize = TRUE
)

xrf_plain_roc_curve <- 
  xrf_plain_best_pred |>
  roc_curve(class, .pred_yes)

xrf_plain_roc_default <- closest_point(xrf_plain_roc_curve)
xrf_plain_roc_sensitive <- closest_point(xrf_plain_roc_curve, "sensitivity", 0.95)

xrf_plain_ranges <- 
  xrf_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(xrf_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-xrf-plain-best
set.seed(926)
xrf_plain_fit <-
  xrf_plain_wflow |>
  finalize_workflow(xrf_plain_best) |>
  fit(myopia_train)
xrf_plain_rules <- xrf_plain_fit |> tidy(penalty = xrf_plain_best$penalty)

xrf_plain_best_pred_ind <- collect_predictions(
    xrf_plain_res,
    parameters = xrf_plain_best,
    summarize = FALSE
)
xrf_max_prob <- xrf_plain_best_pred_ind |> pluck(".pred_yes") |> max()
```

A set of five models was tested. First, a regularized logistic regression was used with glmnet penalties. The predictors included main effects and all two-factor interactions. Penalty values ranging from 10<sup>`r min(log10(glmn_plain_ranges$penalty))`</sup> to 10<sup>`r max(log10(glmn_plain_ranges$penalty))`</sup> were evaluated with four mixtures that ranged from `r min(glmn_plain_ranges$mixture* 100)`% to `r max(glmn_plain_ranges$mixture* 100)`% Lasso. A regular grid of `r nrow(glmn_plain_grid)` candidates was used to optimize these parameters^[We use the original implementation of the glmnet model, which features a capability where, for a specific mixture value, a single model fit contains results for multiple penalty values. For this reason, we can evaluate a regular grid with  `r nrow(glmn_plain_grid)` candidates with only `r vctrs::vec_unique_count(glmn_plain_grid$mixture)` model fits, one for each mixture value. A regular grid exploits this feature, but a space-filling design would nullify this advantage.].

Single-layer neural networks were evaluated and were tuned over: 

:::: {.columns}

::: {.column width="50%"}
- The number of hidden units (`r min(mlp_plain_ranges$hidden_units)` to `r max(mlp_plain_ranges$hidden_units)`). 
- The L<sub>2</sub> penalty (10<sup>`r min(log10(mlp_plain_ranges$penalty))`</sup> to 10<sup>`r max(log10(mlp_plain_ranges$penalty))`</sup>).
- The learning rate (10<sup>`r min(log10(mlp_plain_ranges$learn_rate))`</sup> to 10<sup>`r max(log10(mlp_plain_ranges$learn_rate))`</sup>).
:::

::: {.column width="50%"}
- The batch size (`r min(mlp_plain_ranges$batch_size)` to `r max(mlp_plain_ranges$batch_size)` samples).
- AdamW momentum (`r min(mlp_plain_ranges$momentum)` to `r max(mlp_plain_ranges$momentum)`).
:::

::::

SGD via AdamW was used to optimize the model for up to 100 epochs, with early stopping after 5 consecutive poor results. ReLU activation was used. 

Boosted trees (via light GBM) were also trained for up to 1,000 iterations with early stopping after 5 bad iterations. The model was tuned over: 

:::: {.columns}

::: {.column width="50%"}
- Tree depth (`r min(lgb_plain_ranges$tree_depth)` to `r max(lgb_plain_ranges$tree_depth)` splits). 
- Amount of data needed for further splitting (`r min(lgb_plain_ranges$min_n)` to `r max(lgb_plain_ranges$min_n)` samples).
:::

::: {.column width="50%"}
- The learning rate (10<sup>`r min(log10(lgb_plain_ranges$learn_rate))`</sup> to 10<sup>`r max(log10(lgb_plain_ranges$learn_rate))`</sup>).
- $m_{try}$ (`r min(lgb_plain_ranges$mtry)` to `r max(lgb_plain_ranges$mtry)` predictors).
:::

::::

Additionally, random forests with 2,000 classification trees were tuned over: 

- Amount of data needed for further splitting (`r min(rf_plain_ranges$min_n)` to `r max(rf_plain_ranges$min_n)` samples).
- $m_{try}$ (`r min(rf_plain_ranges$mtry)` to `r max(rf_plain_ranges$mtry)` predictors)..

Finally, a RuleFit model was optimized over: 

:::: {.columns}

::: {.column width="50%"}
- The number of boosting iterations (`r min(xrf_plain_ranges$trees)` to `r max(xrf_plain_ranges$trees)`)
- Tree depth (`r min(xrf_plain_ranges$tree_depth)` to `r max(xrf_plain_ranges$tree_depth)` splits). 
- Amount of data needed for further splitting (`r min(xrf_plain_ranges$min_n)` to `r max(xrf_plain_ranges$min_n)` samples).

:::

::: {.column width="50%"}
- The learning rate (10<sup>`r min(log10(xrf_plain_ranges$learn_rate))`</sup> to 10<sup>`r max(log10(xrf_plain_ranges$learn_rate))`</sup>).
-  Proportional $m_{try}$ (`r min(round(xrf_plain_ranges$mtry * 100, 1))`% to `r max(round(xrf_plain_ranges$mtry * 100, 1))`%).
- The L<sub>1</sub> penalty (10<sup>`r min(log10(xrf_plain_ranges$penalty))`</sup> to 10<sup>`r max(log10(xrf_plain_ranges$penalty))`</sup>).
:::

::::

Apart from the logistic regression, a space-filling design of 25 candidates was used for optimization. 

@fig-myopia-plain-roundup visualizes the results across and within models. Each point represents a candidate within a model. These are ranked by their resampled Brier score and are ordered accordingly. The vertical lines help highlight where the best candidate was located in the ranking. Interestingly, the logistic regression and RuleFit models had the best results and were very similar (see discussion below). The other three models had both good and bad performance across their candidates, but, in terms of model calibration, did not do as well. This may be due to the fact that this data set is better served by simpler models; the added complexity generated by large tree ensembles and neural networks is likely overfitting to some degree. 

```{r}
#| label: fig-myopia-plain-roundup
#| echo: false
#| fig-width: 6
#| fig-height: 3
#| out-width: 80%
#| fig-cap: "Cross-validated Brier scores for different modeling approaches without adjustments for class imbalance. The dashed vertical lines indicate the highest-ranked candidate for each model."

plain_res <- as_workflow_set(
  glmnet = glmn_plain_res,
  nnet = mlp_plain_res,
  boosting = lgb_plain_res,
  rand_forest = rf_plain_res,
  rule_fit = xrf_plain_res
)

rank_plain <-
  rank_results(plain_res, rank_metric = "brier_class") |>
  filter(.metric == "brier_class")

rank_plain_best <-
  rank_plain |>
  slice_min(mean, by = c(wflow_id)) |>
  mutate(percent_worse = (rank - 1) / nrow(rank_plain) * 100)

rank_plain |>
  ggplot(aes(rank, mean, col = wflow_id)) +
  geom_point(cex = 1/2) +
  geom_vline(
    data = rank_plain_best,
    aes(col = wflow_id, xintercept = rank),
    lty = 2
  ) +
  labs(x = "Rank", y = "Brier Score", col = "Model") +
  theme(legend.position = "top")
```


While the Rulefit and logistic regressions had the smallest Brier scores, their respective best candidates have issues. For the RuleFit model, the maximum estimated probability of a patient being myopic is `r round(xrf_max_prob * 100, 1)`% across all of the assessment sets. While the values are well-calibrated within the observed data range, this upper limit of probability is probably too small to be the final model. We would expect at least one of the held-out predictions to be closer to one when the patient is truly myopic. We also do not have a clear sense of how well-calibrated the predictions are when values are closer to one. 

TODO: problem of never predicting the minor class with poor probabilities

For the regularized logistic model, @fig-myopia-logistic illustrates the relationship between the tuning parameters and the Brier score. We can see that while the numerically best results are to use `r round(glmn_plain_best$mixture * 100, 0)`% Lasso regularization with a penalty of 10<sup>`r min(log10(glmn_plain_best$penalty))`</sup>, there are several other combinations with nearly equivalent metrics. The model fitted on the entire training set could have a maximum of `r max_num_terms` model coefficients (apart from the intercept) since there are `r num_pred` main effects and `r choose(num_pred, 2)` possible two-way interactions. However, the Lasso model can remove predictors, and the fitted model contains only `r nrow(glmn_plain_main)` main effect and `r nrow(glmn_plain_int)` interactions. For a model used purely for prediction, this is not a terrible situation. However, the lack of main effects is troubling since there are interaction terms in the model with no corresponding main effects. This is inconsistent with the heredity principle discussed in @sec-interactions-principles and suggests that these interactions are unlikely to be real. 

To sidestep this issue, we can choose a different tuning parameter candidate. If we use _no_ Lasso regularization, the model will contain all parameters, but regularizes some of them to be close to zero. This would ensure that all of the interaction terms have corresponding main effects in the model. Taking this approach, the penalty associated with the smallest Brier score is 10<sup>`r min(log10(glmn_plain_alt_best$penalty))`</sup>. For this model, the resampling estimate of the Brier score is `r signif(glmn_plain_alt_brier$mean, 3)`%, which is `r glmn_brier_drop` worse than the original choice, but still very good when compared to the other models. We'll use this candidate going forward. 

Additionally, @fig-myopia-logistic shows some diagnostics for the logistic regression. These plots are constructed by obtaining the assessment set predictions across the `r nrow(myopia_rs)` resamples. Note that since repeated cross-validation was used, each training set point has five replicates in the held-out data, totaling `r format(nrow(myopia_train) * nrow(myopia_rs), big.mark = ",")` rows across all resamples. To create the calibration and ROC curves, the five predictions for each training set point are averaged so that we have a set of `r nrow(myopia_train)` rows of predictions. Please note that these are approximate and may differ from the performance statistics generated by resampling. For example, @fig-myopia-logistic shows a single curve while the resampled ROC AUC is the average of `r nrow(myopia_rs)` ROC curves created with 10% of the training set. Despite this, the visualizations can help us characterize the quality of the fit and detect any significant issues in the predictions. 

```{r}
#| label: fig-myopia-logistic
#| echo: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: "Results of the logistic regression model optimization (without adjustments for the class imbalance). The top plot shows the performance profile for the two tuning parameters. Underneath are the calibration and ROC curves. The solid blue point on the ROC curve corresponds to the default cutoff. The pooled averaged holdout predictions were used to compute these visualizations."

p <- 
  autoplot(glmn_plain_res, metric = "brier_class") + 
  theme(legend.position = "top") + 
  labs(y = "Resampled Brier Score")

glmn_plain_cal <- 
 glmn_plain_best_pred |>
  cal_plot_windowed(class, .pred_yes, step_size = 0.05, window_size = 0.2) +
  labs(title = "Calibration Curve")

glmn_plain_roc <- 
  autoplot(glmn_plain_roc_curve) + 
  geom_point(
    data = glmn_plain_roc_default, 
    aes(x = 1 - specificity, y = sensitivity),
    cex = 4, 
    col = "blue"
  ) + 
  labs(title = "ROC Curve", y = "Sensitivity", x = "1 - Specificity")

p / (glmn_plain_cal + glmn_plain_roc)
```

The calibration curve is generally good, except for the upper end, where it indicates that the model underpredicts the probability of myopia. There is also a region around an x-axis point where events occur at a rate of 60%, and the probabilities are slightly overpredicted. All in all, though, the calibration curve is impressive. 

The ROC curve is symmetric and fairly unremarkable. The visualization shows that the default 50% threshold for converting the probabilities to hard "yes/no" predictions is somewhat problematic. The sensitivity for this cutpoint is very poor (`r round(glmn_plain_best_mtr$mean[xrf_plain_best_mtr$.metric == "sensitivity"] * 100, 1)`%) while the specificity is exquisite (`r round(glmn_plain_best_mtr$mean[xrf_plain_best_mtr$.metric == "specificity"] * 100, 1)`%). This is fairly common for class imbalances. The model performs significantly better where the data are more abundant, and since most participants in the study are not myopic, high specificity is more easily achieved. 

We quantified performance for these models using a variety of methods. @tbl-plain-metrics shows several statistics for both hard and soft prediction types. When assessing class probability estimates, the Brier scores for each model are all acceptably low, and most areas under the ROC curves have objectively large values. The areas under the precision-recall curves are mediocre, with the largest possible value being 1.0 and the worst value is `r signif(mean(myopia_train$class == "yes"), digits = 3)`. 

:::: {.columns}

::: {.column width="5%"}
:::

::: {.column width="90%"}

::: {#tbl-plain-metrics}

```{r}
#| label: plain-metrics
#| echo: false
mtr_labs <- c("Brier Score",  "ROC AUC", "PR AUC", "Binomial Log Loss",
              "Accuracy", "Kappa", "Sensitivity", "Specificity")

plain_metric_table <- 
  bind_rows(
    glmn_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Logistic (glmnet)"), 
    mlp_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Neural Network"),
    rf_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Random Forest"),
    lgb_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Boosted Tree"),
    xrf_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "RuleFit")
  ) |> 
  filter(Metric != "mn_log_loss") |> 
  mutate(
    Metric = 
      case_when(
        Metric == "accuracy" ~ "Accuracy",
        Metric == "kap" ~ "Kappa",
        Metric == "roc_auc" ~ "ROC AUC",
        Metric == "brier_class" ~ "Brier Score",
        Metric == "mn_log_loss" ~ "Binomial Log Loss",
        Metric == "pr_auc" ~ "PR AUC",
        Metric == "sensitivity" ~ "Sensitivity",
        Metric == "specificity" ~ "Specificity",
        TRUE ~ Metric
      ),
    Metric = factor(Metric, levels = mtr_labs)
  ) |> 
  pivot_wider(id_cols = Model, names_from = Metric, values_from = Estimate)

gt(plain_metric_table) |>
  fmt_number(
    columns = c(`ROC AUC`, `PR AUC`, `Brier Score`, Kappa),
    n_sigfig = 3
  ) |>
  fmt_percent(columns = c(Accuracy, Sensitivity, Specificity), decimals = 1) |>
  cols_move_to_start(`Brier Score`) |> 
  tab_spanner(
    "Hard Class Prediction Metrics",
    columns = c(Accuracy, Sensitivity, Specificity, Kappa)
  ) |> 
    tab_spanner(
    "Probability Prediction Metrics",
    columns = c(`ROC AUC`, `PR AUC`, `Brier Score`)
  )
```

Resampled performance metrics for the best candidates across different models. The models were optimized based on the Brier score. 
 
:::

:::

::: {.column width="5%"}
:::

:::: 

The hard class predictions in @tbl-plain-metrics use the default 50% probability cutoff. A few of the models have accuracies that are larger than the rate that the majority class occurs in the training set (`r round(mean(myopia_train$class == "no") * 100, 1)`%). The Kappa statistics suggest that there is a signal in the models for predicting qualitative class membership. However, with this cutoff, the model is effectively useless at predicting which people are myopic, as the best possible sensitivity is `r round(max(plain_metric_table$Sensitivity) * 100, 1)`%. 

If we are satisfied with the model's ability to discriminate between classes (as evidenced by the ROC curve) and that the probabilities are sufficiently calibrated, we can address the sensitivity problem by selecting an appropriate probability threshold. As with most optimization problems, we require an objective function to make the best decision regarding this tuning parameter. There are a few ways we can approach the problem: 

 1. Given what we know about how the model will be used, we could define specific costs for false negatives and false positives (with correctly predicted classes having zero cost). Using these values, we can choose a threshold that minimizes the cost of the decision. This is similar to a weighted Kappa approach, except the weights would not be symmetric. 
 2. Seek a constrained optimization. For example, we can try to maximize the sensitivity under the constraint that the specificity is at an acceptable value. The ROC curve provides the substrate to make these calculations since it includes sensitivity and specificity for many thresholds in the data. 
 3. Optimize an existing composite score that captures the right balance of sensitivity and specificity. The Matthews correlation coefficient and F-statistics are examples of such metrics. 
 4. Use a multiparameter optimization method, such as desirability functions from @sec-cls-multi-objectives, to define a jointly acceptable result (i.e., overall desirability). This would enable us to consider multiple performance metrics, as well as other variables, in the decision-making process. 
 
We'll focus on the first two options. 

To choose the threshold using a custom cost metric (the first option in the list above), we would have to determine an appropriate cost structure for false positives and false negatives, usually based on the consequences of either error. For our myopia example, this is a somewhat subjective choice that depends on some context-specific rationale. In other cases, the costs might be analytically calculated as a function of monetary costs. @fig-class-cost shows examples of cost curves if the false positive cost is fixed at 1.0 and the cost of a false negative varies in severity. The y-axis is the mean of the cost values averaged across the `r nrow(myopia_rs)` assessment sets. 
 
```{r}
#| label: myopia-glmnet-threshold
#| include: false
#| cache: true
thresholds <- seq(0.0, 0.40, by = 0.02)[-1]

glmn_tlr <-
  tailor() |>
  adjust_probability_threshold(threshold = tune())

# Using a single penalty causes an error. We'll use two and pick the appropriate
# value afterwards

glmn_plain_mod_spec <-
  logistic_reg(penalty = tune(), mixture = !!glmn_plain_alt_best$mixture) |>
  set_engine("glmnet", path_values = !!path)

glmn_thr_wflow <- workflow(glmn_rec, glmn_plain_mod_spec, glmn_tlr) 

glmn_thr_res <-
  glmn_thr_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = crossing(
      penalty = c(0.001, glmn_plain_alt_best$penalty), 
      threshold = thresholds)
  )

glmn_thr_pred <- 
  glmn_thr_res |> 
  collect_predictions(summarize = FALSE) |> 
  filter(penalty == glmn_plain_alt_best$penalty)
```

```{r}
#| label: fig-class-cost
#| echo: false
#| out-width: 60%
#| fig-width: 6
#| fig-height: 4
#| fig-cap: "Cost curves when the cost of a correct prediction is zero, the cost of a false positive is 1.0, and the cost of a false negative varies."
costs <- c(2, 5, 10, 15)

for (i in seq_along(costs)) {
  tmp <- 
    glmn_thr_pred |> 
    mutate(
      cost_val = costs[i],
      .pred_class = if_else(.pred_yes >= threshold, "yes", "no"),
      .pred_class = factor(.pred_class, levels = c("yes", "no")),
      cost = 
        case_when(
          class == "yes" & .pred_class == "no"  ~ cost_val,
          class == "no"  & .pred_class == "yes" ~ 1.0,
          TRUE ~ 0.0)
    ) |> 
    summarize(
      cost = mean(cost), 
      .by = c(id, id2, threshold, cost_val)
    )
  
  if (i == 1) {
    cost_mtr <- tmp
  } else {
    cost_mtr <- bind_rows(cost_mtr, tmp)
  }
}

cost_mean_mtr <- 
  cost_mtr |> 
  summarize(
    num = sum(!is.na(cost)),
    cls_cost = mean(cost),
    std_err = sd(cost),
    .by = c(threshold, cost_val)
  )

cost_best_mtr <- 
  cost_mean_mtr |> slice_min(cls_cost, by = c(cost_val))

cost_mean_mtr |>
  ggplot(aes(threshold, col = format(cost_val))) +
  geom_point(aes(y = cls_cost), alpha = 3 / 4) +
  geom_line(aes(y = cls_cost), alpha = 3 / 4) +
  geom_point(data = cost_best_mtr, aes(y = cls_cost), cex = 3) +
  labs(x = "Probability Threshold", y = "Total Cost", col = "Cost of FN") +
  scale_x_continuous(labels = label_percent()) +
  theme(legend.position = "top")
```

The figure shows that increasing the cost of incorrectly predicting the events yields increasingly smaller thresholds. This is due to classifying more points as events, regardless of their actual class. The cost metric is punished less by incorrectly predicted non-events, so the overall cost statistic decreases substantially in the curves with higher false negative costs. 

::: {.important-box}
It is important to realize that this strategy only optimizes the probability threshold. The model fit (e.g., slope and intercept parameter estimates) has not been affected by the cost function.  In the next section, we will consider costs to the objective function that the model uses during training. 
:::

Moving on to constrained optimization, when choosing a threshold using an ROC curve, an immediate question arises: "Which data should be used to compute the ROC curve?" _Preferably_, the cutoff selection should use different data than the data used to fit the model. For example, diagnostic tests that require approval from a government agency (e.g., the Food and Drug Administration in the US) require a different data set to set the threshold. However, this may not be feasible if there is not an abundance of data or the resources to acquire such data. If we can't hold out another data set, we can use the assessment sets generated during resampling^[Or a validation set if multiple resamples are not used.]. There are at least two different approaches. 

 - **Option A - _post hoc_ pooling**: After we have resampled our model configurations, we can pool the assessment set predictions for the best candidate. This ROC curve is computed based on the overall existing class probability estimate values. This approach generated the visualizations in @fig-myopia-logistic. 

- **Option B - threshold tuning**: During model tuning, we treat the probability threshold as a tuning parameter and include it in the grid search. For each threshold in the grid, conditional on any other tuning parameters, we have resampling estimates for sensitivity and specificity. These points are then used to create an ROC curve. 

The primary difference between these approaches lies in data usage: do we compute one large ROC curve from pooled data or an ROC curve from resampled statistics? Statistically, the latter is more appropriate, but either might produce acceptable results.

Note that there are a few options when we treat the threshold as a tuning parameter. A one-stage approach would include the threshold along with any other tuning parameters and use a single grid for all^[When adding the threshold to the optimization grid, it might be helpful to start with a space-filling design and _crossing_ it with a vector of thresholds. This allows us to have data on the thresholds for each of the candidate points. There is minimal overhead associated with this approach, as the thresholding computations occur after preprocessing and model fitting. For example, if an initial grid with 25 points is crossed with 10 threshold values,  there are still only 25 model configurations to train, which takes the longest time to compute.]. However, recall that the choice of this parameter does not affect any probability-based metrics, such as the Brier score, log-loss, or the area under the ROC curve. Instead, we could use a two-stage approach where the non-threshold parameters are optimized to find their best results, and then we can use a one-dimensional grid to find the best threshold (based on metrics appropriate for hard class predictions). We took this approach here: `r length(thresholds)` threshold values ranging from `r round(min(thresholds) * 100, 1)`% to `r round(max(thresholds) * 100, 1)`% defined the second-stage grid. 

```{r}
#| label: fig-myopia-two-roc
#| echo: false
#| out-width: 50%
#| fig-width: 5
#| fig-height: 5.1
#| fig-cap: "Two approaches for optimizing the probability threshold via the ROC curve. Both curves use the regularized logistic regression's resampling results."

glmn_thr_roc <- 
  glmn_thr_res |> 
  collect_metrics(type = "wide") |> 
  filter(penalty == glmn_plain_alt_best$penalty) |> 
  select(threshold, sensitivity, specificity) |> 
  bind_rows(
    tribble(
      ~threshold , ~sensitivity , ~specificity ,
      0 ,            1 ,            0 ,
      1 ,            0 ,            1
    )
  )

roc_tradeoff <- 
  glmn_thr_roc |>
  filter(specificity >= 0.8) |>
  slice_max(sensitivity) |>
  mutate(
    prevalence = mean(myopia_df$class == "yes"),
    ppv = (sensitivity * prevalence) / ((sensitivity * prevalence) +
        ((1 - specificity) * (1 - prevalence))),
    npv = (specificity * (1 - prevalence)) / (((1 - sensitivity) * prevalence) +
        ((specificity) * (1 - prevalence)))
  )

glmn_thr_roc |>
  mutate(Method = "Threshold Tuning") |> 
  bind_rows(
    glmn_plain_roc_curve |> 
      select(.threshold, sensitivity, specificity) |> 
      mutate(Method = "post hoc Pooling")
  ) |> 
  ggplot(aes(1 - specificity, sensitivity, col = Method)) +
  geom_step(direction = "vh", linewidth = 0.8) +
  coord_obs_pred() +
  theme(legend.position = "top") + 
  labs(color = NULL) +
  scale_color_manual(values = c("#8785B2FF", "#DABD61FF"))
```
 
For the logistic regression model, @fig-myopia-two-roc shows the ROC curves for both schemes. The curves significantly overlap for these data. Note that the curve associated with a second-stage grid contains `r nrow(glmn_thr_roc) - 2` unique points since it is based on the values used in our second-stage grid. The curve for the _post hoc_ pooling method has `r nrow(glmn_plain_roc_curve) - 2` points, which represents how many unique predicted probabilities are contained in the _post hoc_ data pool. 
 
Using either method for composing the ROC tool provides us with a tool to make trade-offs between sensitivity and specificity. For example, if we want to maximize sensitivity on the condition that specificity is at least 80%, the curve produced by threshold tuning indicates that the cutoff should be `r round(roc_tradeoff$threshold * 100, 1)`%. The estimated statistics associated with this value are a sensitivity of `r round(roc_tradeoff$sensitivity * 100, 1)`% and a specificity of `r round(roc_tradeoff$specificity * 100, 1)`%. 

This overall approach involves finding a suitable ML model that does not allow class imbalance to alter our methodology, while deferring cutoff selection until the end of the process. This approach can lead to a well-calibrated model that appropriately classifies new samples. The downside becomes apparent when trying to explain individual results to the model's consumers. 

For our myopia example, suppose we make a prediction for a patient where the probability of myopia is estimated to be 20%. The conversation with the patient might sound something like

> "We estimate that your child has a 20% chance of being severely nearsighted and, based on this, we would give them a diagnosis of myopia." 

It is natural for someone to find the diagnosis counterintuitive since they are probably comparing the probability to the natural 50% threshold. It might lead the parent to ask:  

> "That probability seems pretty low to make that diagnosis. Is that a mistake?"

The non-technical response could be:

> "We think that the diagnosis is appropriate because the overall chances of being myopic are low, and we have determined that our cutoff for making the diagnosis (`r round(roc_tradeoff$threshold * 100, 1)`%) gives us the most effective diagnoses."

Of course, this is paraphrasing, but the hypothetical conversation does illustrate the potential confusion. 

This seeming disconnect can become worse as the prevalence becomes smaller. For very rare events (i.e., < 5%), the cutoff associated with the sensitivity-specificity trade-off can be very small, perhaps less than 1%. This is the price paid for having probability estimates that are consistent with the rate at which the event occurs in the wild. 
 
 
## Strategy 2: Corrections for the Imbalance

In the previous section, the only operation used to account for the small minority class was to optimize the probability threshold _after_ the model had been chosen. In this section, we'll consider several methods for actively addressing the imbalance during model optimization. The two primary classes of techniques are: 

- Changing the composition of the training set data.
- Upweighting the minority class's effect on the objective function.

As we'll see, these can have a profound effect on the class probability estimates (both good and bad). These techniques are also most effective when the primary goal of the model is to identify events in new data. 

### Sampling and Synthesis Techniques


#### Downsampling

```{r}
#| label: myopia-down-fda
#| include: false
myopia_down_formatted <- 
  recipe(class ~ ., data = myopia_train) |> 
  step_downsample(class) |>
  prep() |> 
  bake(new_data = NULL)

set.seed(616)
myopia_down_smol_formatted <- 
  recipe(class ~ ., data = myopia_smol_formatted) |> 
  step_downsample(class) |>
  prep() |> 
  bake(new_data = NULL)

fda_down_fit <- discrim_flexible(prod_degree = 2) |> 
  fit(class ~ ., data = myopia_down_smol_formatted)
fda_down_pred <- augment(fda_down_fit, new_data = smol_grid)
fda_down_capture <- 
  augment(fda_down_fit, new_data = myopia_smol_formatted) |> 
  filter(.pred_yes >= 0.5 & class == "yes") |> 
  nrow()
```


:::: {#fig-myopia-down-fda}

```{r}
#| label: myopia-down-fda-plot
#| echo: false
#| fig-width: 4.8
#| fig-height: 5.2
#| warning: false
#| out-width: 50%
myopia_smol_formatted |> 
  ggplot(aes(`spherical equivalent refraction`, `axial length`)) +
  geom_point(aes(color = class), alpha = 0.5, shape = 16, size = 2, show.legend = FALSE) + 
  geom_tile(data = fda_down_pred, aes(fill = .pred_yes), alpha = .1) +
  geom_contour(data = fda_down_pred, aes(z = .pred_yes), breaks = 1/2, col = "black") +
  scale_fill_gradient2(low = "#377EB8", mid = "white", high = "#E41A1C", midpoint = 0.5) +
  scale_color_brewer(palette = "Set1") + 
  labs(fill = "Probability") +
  theme(legend.position = "top")
```

A flexible discriminant fit to the downsampled myopia training set. The subsampling process increased the number of events found using the 50% threshold (`r fda_down_capture` out of `r sum(myopia_smol_formatted$class == "yes")`). This figure is comparable with @fig-myopia-fda.

::::



::: {#fig-subsampling}

::: {.figure-content}

```{shinylive-r}
#| label: fig-subsampling
#| out-width: "80%"
#| viewerHeight: 600
#| standalone: true

library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
# source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-sa.R")


ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-1, 10, -1)),
    column(
      width = 10,
      selectInput(
        inputId = "method",
        label = " ",
        choices = c(
          "Downsampled",
          "SMOTE",
          "ROSE",
          "Near Miss",
          "Adaptive Synthetic Algorithm",
          "Tomek"
        ),
        selected = c("Downsampled")
      )
    )
  ),
  as_fill_carrier(plotOutput("plot"))
)

server <- function(input, output) {
  load(url(
    "https://raw.githubusercontent.com/aml4td/website/main/RData/imbalanced_sampled.RData"
  ))

  # load("RData/imbalanced_sampled.RData")
  xrng <- extendrange(imbalanced_sampled$spherical_equivalent_refraction)
  yrng <- extendrange(imbalanced_sampled$vitreous_chamber_depth)

  orig <- imbalanced_sampled %>%
    dplyr::filter(Data == "Original")

  output$plot <-
    renderPlot(
      {
        # browser()
        dat <- imbalanced_sampled %>%
          dplyr::filter(Data == input$method) |>
          dplyr::bind_rows(orig) |>
          dplyr::mutate(
            Data = factor(Data, levels = c("Original", input$method))
          )
        p <-
          dat |>
          ggplot(aes(
            spherical_equivalent_refraction,
            vitreous_chamber_depth,
            col = class,
            pch = class,
            alpha = class
          )) +
          geom_point(alpha = 0.5, cex = 1.6) +
          facet_wrap(~Data) +
          coord_fixed(xlim = xrng, ylim = yrng)

        p <- p +
          theme(legend.position = "top") +
          labs(
            x = "spherical equivalent refraction",
            y = "vitreous chamber depth"
          ) +
          scale_color_brewer(drop = FALSE, palette = "Set1", direction = 01) +
          theme_light_bl()
        print(p)
      },
      res = 100
    )
}

app <- shinyApp(ui = ui, server = server)
app
```

:::

TODO

:::


Subsampling

```{r}
#| label: myopia-ridge
#| include: false

glmn_ridge_spec <-
  logistic_reg(penalty = tune(), mixture = 0) |>
  set_engine("glmnet", path_values = !!path)

glmn_ridge_wflow <- workflow(glmn_rec, glmn_ridge_spec)

glmn_ridge_grid <- tibble(penalty = path)
```

```{r}
#| label: myopia-sampling
#| include: false
#| cache: true

sampling_rec_list <-
  list(
    Downsample = down_rec,
    SMOTE = smote_rec,
    ROSE = rose_rec,
    `Near Miss` = nm_rec
  )

lgb_plain_grid <-
  lgb_plain_spec |>
  extract_parameter_set_dials() |>
  update(mtry = mtry(c(1, 17))) |>
  grid_space_filling(size = 25)

sampling_set <-
  bind_rows(
    workflow_set(
      preproc = sampling_rec_list,
      model = list(glmn = glmn_ridge_spec)
    ) |>
      option_add(grid = glmn_ridge_grid),
    workflow_set(
      preproc = sampling_rec_list,
      model = list(lgb = lgb_plain_spec)
    ) |>
      option_add(grid = lgb_plain_grid)
  ) |>
  workflow_map(
    resamples = myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    seed = 837
  )
```

#### Upsamping

#### SMOTE

#### ROSE

#### Boarderline Methods

#### Comparisons

```{r}
#| label: sampling-metrics
#| include: false

sampling_mtr <-
  sampling_set |>
  collect_metrics() |>
  select(-.estimator, -n, -std_err, -preproc) |>
  mutate(
    model = if_else(model == "boost_tree", "Boosted Tree", "Logistic Regression"),
    model = factor(model, levels = c("Logistic Regression", "Boosted Tree")),
    sampling = map_chr(wflow_id, ~ strsplit(.x, "_")[[1]][1])
    ) |>
  pivot_wider(
    id_cols = c(wflow_id, .config, model, sampling),
    names_from = .metric,
    values_from = mean
  )
```


```{r}
#| label: fig-myopia-sampling
#| echo: false
#| fig-width: 7
#| fig-height: 7
#| out-width: 80%
#| fig-cap: "Cross-validated statistics for the subsampling and synthesis methods."

sampling_sens_spec <- 
  sampling_mtr |>
  ggplot(aes(1 - specificity, sensitivity, col = model, pch = model)) +
  geom_abline(lty = 3) + 
  geom_point(cex = 2, alpha = 2 / 3) +
  facet_wrap(~sampling, ncol = 4) +
  coord_fixed(xlim = 0:1, ylim = 0:1) + 
  theme(legend.position = "bottom") +
  labs(color = "Model", pch = "Model") +
  scale_shape_manual(values = c(1:0)) +
  scale_color_manual(values = c("#EF7C12FF", "#172869FF"))

sampling_sens_brier <- 
  sampling_mtr |>
  ggplot(aes(sensitivity, brier_class, col = model, pch = model)) +
  geom_point(cex = 2, alpha = 2 / 3, show.legend = FALSE) +
  facet_wrap(~model) +
  coord_fixed(ratio = 4.5, xlim = 0:1, ylim = c(0.1, 0.28)) +
  labs(y = "Brier Score")+
  scale_shape_manual(values = c(1:0)) +
  scale_color_manual(values = c("#EF7C12FF", "#172869FF"))

sampling_sens_spec / sampling_sens_brier
```

### Cost-sensitive learning

Weâ€™ve previously talked about the consequences of making an incorrect prediction. For example, @sec-mushrooms showed a model that predicts whether mushrooms are poisonous or not. We could think about this in terms of the general cost of a prediction (not too dissimilar to the discussion related to SVM models). 

If we have $C$ classes, we could write down a cost matrix that defines the specific costs $\mathcal{c}_{j|k}$ of incorrectly predicting a data point to be class $j$ when it is truly class $k$. In many instances, the diagonal terms of this matrix are zero since there is no real "cost" to making the correct prediction. There are cases where different types of benefits are associated with different types of correct predictions. See REF for an example. However, for the purposes of our discussion here, we will assume that the diagonal is zero. 

If we have this cost structure, we can use the predicted class probabilities to estimate the expected costs for a collection of data by averaging a set of individual costs. 

Letâ€™s say that $C=3$ and the cost matrix is 


$$
\mathcal{C} = 
\begin{pmatrix}
0.00 & 1.00 & 3.00 \\
0.50 & 0.00 & 2.00 \\ 
0.25 & 0.75 & 0.00
\end{pmatrix}
$$

where the true values are in columns and the predicted class levels are rows. For example, the cost of predicting a data point that is Class 3 as Class 1 is $\mathcal{c}_{3|1} = 3.00$. 

Here are some examples of probabilities and their individual costs. 

If our ML model can utilize custom objective functions, minimizing the expected cost is a fairly rational way to ensure your model is fit for purpose. If not, we can use the expected cost to help optimize the tuning parameters of our model (as we did in @fig-class-cost). 

Another way some researchers have parameterized cost is to collapse the matrix, resulting in a general cost for misclassifying a sample, regardless of what it was misclassified as. In this case, we sum the cost matrix over rows (COLUMNS?) so that there is one cost per class. XXX describes these as class weights. 

For a two-class outcome (e.g., myopia or not), the class weights are the same as the two off-diagonal entries in the cost matrix. 

If we consider class-specific costs as weights, we can generalize many performance metrics to be cost-sensitive. For example, we can make the Brier score cost-sensitive using a modified version of @eq-brier

$$
Brier = \frac{1}{WC}\sum_{i=1}^n\sum_{k=1}^C w_k(y_{ik} - \hat{p}_{ik})^2
$$ {#eq-brier-cost}

where $w_k$ is the cost or weight for class $k$ and $W$ is the sum of the $n$ weights. If we have two classes and $w_1 = 5$ and $w_2 = 1$, every data point whose true outcome is the first class will have five-fold more influence over the objective function. In this way, we can train the model to ensure that it performs better in predicting specific classes than others. This is the basis for _cost-sensitive learning_^[Note that our previous use of costs in @fig-class-cost was _after_ the model was trained. In this section, we will use weights/costs so that the model adapts to the cost structure during training.]. 

In practice, our implementations of ML model metrics may or may not enable cost-sensitive learning. If not, they may have the capacity to handle _case weights_. These are numeric values associated with each row of the data set that specify how it should affect the computations. There are many types of case weights for different purposes. Here are a few examples: 

- Frequency weights are integers that specify how often the specific predictor pattern occurs in the data. If there were two categorical predictions with a total of 25 combinations, a very large data set can be compressed by using a case weight that reflects how often each of the 25 patterns occurs. Frequency weights would be assigned to both the training and testing sets. 

- Importance weights are numbers (decimal and non-negative) that reflect how important the row is to the model fit. This is the type of weight that weâ€™ve discussed above. Importance weights should only be assigned to the training set; the test set should not use them, as it should accurately reflect the state of the data in the real world. 

If the metric implementation allows for case weights, we can use importance weights to make them sensitive to different classes. 

::: {.note-box}
While not related to class imbalances, importance weights are also useful for time-series data, where we would like more recent data to have a higher priority in the model fit. It can also be useful for dealing with problematic/anomalous data in a model. For example, data collected during the global COVID-19 pandemic can have a negative impact on fits, but it may still need to be included. Assigning these row small case weights may solve those issues. 
:::

Note that the scale of class weights may differ across metrics and/or models (as weâ€™ll see below). For one type of model, minority class weights ranging from 1 to 50 might have the same performance as another model/metric that ranges from 1 to 5. There is some trial and error associated with tuning the weights. 

If your problem has a well-known cost structure, likely expressed in units of currency, there is probably little ambiguity about it. Otherwise, we might use class weights to either decrease the likelihood of tragic errors from occurring in the model or to help the model overcome a severe class imbalance. In other words, the "importance" in importance weights could mean many different things. 

CART book, resampling weights versus case weights

To visualize an example, @fig-myopia-cost-fda shows the same data as @fig-myopia-fda but in this case, case weights were used so that the myopic data (in red) had a 15-fold larger impact on the objective function than the non-myopic children. The same FDA model was trained on the data, but it used the case weights. The result is a class boundary that bends in the middle, allowing it to encompass a greater portion of the minority class than the original fit. 

```{r}
#| label: myopia-cost-fda
#| include: false
myopia_smol_wts_formatted <- 
  myopia_smol_formatted |> 
  mutate(
    cls_wts = if_else(class == "yes", 15.0, 1.0),
    cls_wts = importance_weights(cls_wts)
  )

fda_wts_wflow <- 
  workflow(class ~ ., discrim_flexible(prod_degree = 2)) |> 
  add_case_weights(cls_wts)

fda_cost_fit <- fit(fda_wts_wflow, data = myopia_smol_wts_formatted)
fda_cost_pred <- augment(fda_cost_fit, new_data = smol_grid)
fda_cost_capture <- 
  augment(fda_cost_fit, new_data = myopia_smol_formatted) |> 
  filter(.pred_yes >= 0.5 & class == "yes") |> 
  nrow()
```

:::: {#fig-myopia-cost-fda}

```{r}
#| label: myopia-cost-grid
#| echo: false
#| fig-width: 4.8
#| fig-height: 5.2
#| warning: false
#| out-width: 50%
myopia_smol_formatted |> 
  ggplot(aes(`spherical equivalent refraction`, `axial length`)) +
  geom_point(aes(color = class), alpha = 0.5, shape = 16, size = 2, show.legend = FALSE) + 
  geom_tile(data = fda_cost_pred, aes(fill = .pred_yes), alpha = .1) +
  geom_contour(data = fda_cost_pred, aes(z = .pred_yes), breaks = 1/2, col = "black") +
  scale_fill_gradient2(low = "#377EB8", mid = "white", high = "#E41A1C", midpoint = 0.5) +
  scale_color_brewer(palette = "Set1") + 
  labs(fill = "Probability") +
  theme(legend.position = "top")
```

A flexible discriminant fit to the myopia training set using cost-sensitive learning. The myopic subjects had a 15-fold higher cost in the objective function and `r fda_cost_capture` of the `r sum(myopia_smol_formatted$class == "yes")` events (in red) are identified using the default 50% threshold. This figure is comparable with @fig-myopia-fda and @fig-myopia-down-fda.

::::

Case weights based on the class can also approximate some of the subsampling methods shown in the previous section. For example, instead of dropping training set rows from the data, we can adjust the importance weights so that the sum of the weights in the majority class equals the sum of the weights in the minority class. By doing this, we may be able to obtain a better model since the majority of the majority class is not lost.  Similarly, an approximation to upsampling would only increase the minority class weights so that the class weight totals are in equilibrium. This would also result in faster training times than basic oversampling.

It should be noted that some stochastic ensemble implementations (e.g., boosting and random forests) confuse the terms "case weights" and "sampling weights." In these instances, the weights are used to preferentially sample the training set while fitting the models (usually trees) without directly impacting the objective function. This process is more akin to downsampling and upsampling than to cost-sensitive learning. 

To demonstrate with the full myopia data set, boosted trees and regularized logistic regression were used once again. The lightgbm implementation was used and, while their documentation is unclear about how the weights are used, it appears that the weights are applied to the objective function and not just the sampling mechanism^[A GitHub issue ([`https://github.com/microsoft/LightGBM/issues/1299`](https://github.com/microsoft/LightGBM/issues/1299)) has evidence that the weights are a â€œmultiplication applied to every positive label weightâ€ and shows some C++ code to that effect.].

```{r}
#| label: myopia-lgb-cls-wts
#| include: false
#| cache: true

lgb_cost_spec <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    mtry = tune(),
    learn_rate = tune(),
    stop_iter = 5
  ) |>
  set_mode("classification") |>
  set_engine("lightgbm", scale_pos_weight = tune())

lgb_cost_wflow <- workflow(class ~ ., lgb_cost_spec)

lgb_cost_grid <-
  lgb_plain_grid |>
  crossing(scale_pos_weight = 2^(-10:0))

set.seed(820)
lgb_cost_res <-
  lgb_cost_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = lgb_cost_grid
  )
```


```{r}
#| label: myopia-glmn-cls-wts
#| include: false
#| cache: true

all_wts <- 2^(seq(0, 4, by = 0.5))
all_glmn_wts <- vector(mode = "list", length = length(all_wts))

for (i in seq_along(all_wts)) {
  myopia_wts_train <-
    myopia_train |>
    mutate(
      cls_wts = if_else(class == "yes", all_wts[i], 1.0),
      cls_wts = importance_weights(cls_wts)
    )

  set.seed(538)
  myopia_wts_rs <- vfold_cv(myopia_wts_train, repeats = 5)

  glmn_wts_wflow <-
    workflow(class ~ ., glmn_ridge_spec) |>
    add_case_weights(cls_wts)

  all_glmn_wts[[i]] <-
    glmn_wts_wflow |>
    tune_grid(
      myopia_wts_rs,
      metrics = cls_mtr,
      control = ctrl_grid,
      grid = glmn_ridge_grid
    )
}
```

```{r}
#| label: cost-metrics
#| include: false

lgb_cost_mtr <-
  lgb_cost_res |>
  collect_metrics(type = "wide") |>
  mutate(class_weight = 1 / scale_pos_weight)

glmn_wts_mtr <- 
  all_glmn_wts |> 
  map2_dfr(
    all_wts, 
    ~ collect_metrics(.x, type = "wide") |> 
      mutate(class_weight = .y)
  )

cost_mtr <- 
  bind_rows(
  lgb_cost_mtr |> 
  select(.config, sensitivity, specificity, brier_class, class_weight) |> 
  mutate(
    Model = "Boosted Tree"),
  glmn_wts_mtr |> 
    select(.config, sensitivity, specificity, brier_class, class_weight) |> 
    mutate(Model = "Logistic Regression")
  )

cost_rng <-
  cost_mtr |>
  summarize(
    max_wt = max(class_weight),
    min_wt = min(class_weight),
    .by = c(Model)
  )

cost_mtr <- 
  cost_mtr |> 
  full_join(cost_rng, by = "Model") |> 
  mutate(
    Model = factor(Model, levels = c("Logistic Regression", "Boosted Tree")),
    class_weight = log2(class_weight),
    min_wt = log2(min_wt),
    max_wt = log2(max_wt),
    scaled_weight = (class_weight - min_wt) / (max_wt - min_wt)
  )
```

The same grids were used as in the previous section for both models. For the case weights ranges for each model, some investigation was required to find appropriate changes in the performance metrics. While both parameter ranges were explored in log<sub>2</sub> units, starting at 1.0, the boosted model used a maximum weight of 2<sup>10</sup>, whereas logistic regression used maximum weights of 2<sup>4</sup>. The majority class weights were kept at a value of 1.0 for both models. For each model, a sequence of weights was created in a 1D grid, and these were crossed with the original grids.

@fig-myopia-cost-sensitive shows the results where the ranges of the weights have been standardized across the two models to be within [0, 1]. In the top panel, another ROC-like visualization is used to demonstrate the effect of using higher weights for the myopic samples. We can see that both models tend to increase sensitivity as the weights increase^[As with the previous section, these statistics use the default 50% threshold.]. This is not always the case, as some tuning parameters for the model were suboptimal on their own, and adjusting the class weights did not yield a significant change. For the most part, the increase in sensitivity comes with the price of reduced specificity. The different class weights enable us to select the optimal trade-off between the two. 

As with subsampling and synthesis methods, the class probabilities generated by this model become increasingly inconsistent with reality as the cost of the minority class increases. The bottom panel shows the degradation in the Brier score tracks with increased sensitivity. Once again, we caution against using these probability estimates with users, as they may not accurately emulate the true likelihood of the event.

```{r}
#| label: fig-myopia-cost-sensitive
#| echo: false
#| fig-width: 7
#| fig-height: 7
#| out-width: 80%
#| fig-cap: "Cross-validated statistics for the cost-sensitive models."

cost_roc <- 
  cost_mtr |>
  ggplot(aes(1 - specificity, sensitivity, col = scaled_weight)) +
  geom_point(cex = 2, alpha = 1 / 3) +
  coord_fixed(xlim = 0:1, ylim = 0:1) +
  facet_wrap(~ Model) + 
  labs(color = "Class Weight (scaled)") +
  theme(legend.position = "bottom") +
  scale_color_viridis_c(option = "A", direction = -1)

cost_sens_brier <- 
  cost_mtr |>
  ggplot(aes(sensitivity, brier_class, col = scaled_weight)) +
  geom_point(cex = 2, alpha = 1 / 3, show.legend = FALSE) +
  lims(x = 0:1) +
  facet_wrap(~ Model) +
  coord_fixed(ratio = 4.5, xlim = 0:1, ylim = c(0.08, 0.30)) +
  scale_color_viridis_c(option = "A", direction = -1)

cost_roc / cost_sens_brier
```


### Other Methods

case weights

Unrealistic priors

Logistic regression with altered intercept


## Revenge of the Prevalence


It is worth revisiting the discussion from @sec-cls-two-classes on how the event rate (i.e., prevalence) affects important statistics.  We'll continue to use the myopia data for discussion.  

If we assume that the rate of myopia in the population of interest is consistent with our data set (`r round(roc_tradeoff$prevalence * 100, 1)`%), we can gain a deeper understanding of how well the model performs.  Given our estimates of sensitivity and specificity, the unconditional performance statistics are: 

- positive predictive value (PPV): `r round(roc_tradeoff$ppv * 100, 1)`%
- negative predictive value (NPV): `r round(roc_tradeoff$npv * 100, 1)`%

Even though our sensitivity and specificity ended up being fairly balanced, the statistics that convey to true ability to predict are not.  
 
This could exacerbate the potential confusion that can occur when explaining the results of a prediction. Continuing the hypothetical conversation from @sec-imbalance-acceptance, the explanation of the PPV might be: 

> "We understand that this rule of thumb might seem overly stringent. 
> That said, from our data, we know that when we predict that a child is myopic, the probability that they are actually myopic is roughly `r round(roc_tradeoff$ppv * 100, 0)`%.
> This is because _most children are not myopic_ but we do find some evidence to make the diagnosis." 

Conversely, if the child had a very small predicted probability and a myopia diagnosis was not used, the explanation would be: 

> "Even though the chances that your child is myopic are greater than zero, from our data we know that when we predict that a child is not myopic, the probability that they are actually not myopic is overwhelmingly high (roughly `r round(roc_tradeoff$npv * 100, 0)`%).
> This is because _most children are not myopic_ and there is not enough evidence to disprove this for your child." 

This is much less ambiguous than the previous hypothetical conversation. 

We can try to optimize the model for PPV and NPV in the same way as we did for sensitivity and specificity. However, to do this, the prevalence must be well known for the specific population that will be predicted by the model and should be stable over time. 

For example, @covidcdc calculated COVID-19 cases and deaths, and estimated cumulative incidence for March and April 2020. The mortality rate varied geographically. For example, during that period, New York City had an estimated mortality rate of 5.3%, while the remainder of the state of New York had an estimated rate of 2.2%. Thankfully, in 2025, these rates are much lower. 

This illustrates that computations that depend on the event rate can be challenging, as the values can systematically change under different conditions and over time. 

## Chapter References {.unnumbered}

 