---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-imbalance/"
---

# Class Imbalances {#sec-cls-imbalance}

```{r}
#| label: cls-imbalance-setup
#| include: false

source("../R/_common.R")
# source("../R/_themes_ggplot.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(mirai)
library(gt)
library(probably)
library(patchwork)
library(bonsai)
library(rules)
library(themis)
library(ggforce)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_bw())
set_options()
daemons(parallel::detectCores())

# ------------------------------------------------------------------------------

closest_point <- function(x, what = ".threshold", target = 0.5) {
  x$delta <- abs(x[[what]] - target)
  dplyr::slice_min(x, delta, with_ties = FALSE)
}
```


```{r}
#| label: load-data
#| include: false

data(myopia, package = "aplore3")
myopia_df <-
  myopia |>
  as_tibble() |>
  select(-id) |>
  rename(
    year = studyyear,
    class = myopic,
    age = age,
    female = gender,
    spherical_equivalent_refraction = spheq,
    axial_length = al,
    anterior_chamber_depth = acd,
    lens_thickness = lt,
    vitreous_chamber_depth = vcd,
    hours_sports = sporthr,
    hours_reading = readhr,
    hours_gaming = comphr,
    hours_studying = studyhr,
    hours_tv = tvhr,
    near_work_activities = diopterhr,
    mother_myopic = mommy,
    father_myopic = dadmy
  ) |>
  mutate(
    class = factor(tolower(class), levels = c("yes", "no")),
    female = if_else(female == "Female", 1, 0),
    mother_myopic = if_else(mother_myopic == "Yes", 1, 0),
    father_myopic = if_else(father_myopic == "Yes", 1, 0)
  ) |>
  relocate(class)

# ------------------------------------------------------------------------------

set.seed(3571)
myopia_split <- initial_split(myopia_df, strata = class)
myopia_train <- training(myopia_split)
myopia_test <- testing(myopia_split)
myopia_rs <- vfold_cv(myopia_train, repeats = 5)

# ------------------------------------------------------------------------------

num_pred <- ncol(myopia_train) - 1
max_num_terms <- num_pred + choose(num_pred, 2)
```

Examples: churn, click through, diseases (trip test, Parkinsonâ€™s), blood specimen mixups

Focus on two class problems, but an issue for all classes

Prior versus likelihood, posterior

note on time slices to make events

## Underlying mathematical issue

## Imbalances should make us think about what we want from our model

Finding events vs accurate predictions

discussion or relative risk vs absolute risk

This should drive our choice of optimization metric

don't confuse dealing with extreme frequencies with finding important events

## Example: Predicting Myopia

For demonstration, we'll use data from the Myopia Study, as described in Section 1.6.6 of @hosmer2013applied. Myopia (i.e., nearsightedness) can be caused by the shape of one's eye, genetics, and environmental conditions, including the heavy use of computer or television screens.  

The data are from a medical study running from `r min(myopia_df$year)` to `r max(myopia_df$year)` and contained `r sum(myopia_df$female)` girls and `r sum(myopia_df$female == 0)` ranging from `r min(myopia_df$age)` to `r max(myopia_df$age)` years old. These data are from the initial exam and include demographic factors, several physiological measurements of the eye from an ocular examination, and whether either parent is also myopic. The outcome is determined after a five-year follow-up visit. In this study, `r sum(myopia_df$class == "yes")` (`r round(mean(myopia_df$class == "yes") * 100, 1)`%) of children were nearsighted. 

To begin, a 75%:25% stratified split was used to partition the data into training and testing sets. Stratification was based on the outcome. This results in a training set of `r nrow(myopia_train)` children, only `r sum(myopia_train$class == "yes")` of whom are myopic. For testing, `r sum(myopia_test$class == "yes")` children of `r nrow(myopia_test)` were myopic. Since the training set and the number of events are small, five repeats of 10-fold cross-validation were used with the same stratification strategy.  

We'll conduct some quick exploratory data analysis, then proceed to creating models based on traditional modeling methods, i.e., without making adjustments for class imbalance during model optimization. 

### Exploring the Data

```{r}
#| label: myopia-age-sex
#| inlcude: false
rates_by_age_sex <- 
  myopia_train |>
  count(class, age, female) |>
  pivot_wider(id_cols = c(female, age), names_from = class, values_from = n) |>
  mutate(
    yes = if_else(is.na(yes), 0, yes),
    n = yes + no,
    stats = map2(
      yes,
      n,
      ~ tidy(binom.test(x = .x, n = .y, conf.level = .9))
    ),
    rate = map_dbl(stats, ~ .x$estimate),
    lower = map_dbl(stats, ~ .x$conf.low),
    upper = map_dbl(stats, ~ .x$conf.high),
    Sex = if_else(female == 1, "Female", "Male")
  ) 

rates_by_parent <-
  myopia_train |>
  count(class, mother_myopic, father_myopic) |>
  pivot_wider(
    id_cols = c(mother_myopic, father_myopic),
    names_from = class,
    values_from = n
  ) |>
  mutate(
    yes = if_else(is.na(yes), 0, yes),
    n = yes + no,
    stats = map2(
      yes,
      n,
      ~ tidy(binom.test(x = .x, n = .y, conf.level = .9))
    ),
    rate = round(map_dbl(stats, ~ .x$estimate) * 100, 1),
    lower = round(map_dbl(stats, ~ .x$conf.low) * 100, 1),
    upper = round(map_dbl(stats, ~ .x$conf.high) * 100, 1)
  ) |> 
  arrange(mother_myopic, father_myopic)
```

First, let's examine the age and sex of the participants. @fig-myopia-age-sex shows the class percentages for 10 subgroups of the data. The size of the point is based on the number of children in the subgroup, indicating that many of them are 6 years old, while very few are 1 year old. There were no nine-year-old myopic girls in the training set. 

```{r}
#| label: fig-myopia-age-sex
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: "Training set rates of myopia by age and sex of the children. The errors bars are based on 90% confidence intervals and the size of the point indicates the number of children in each subgroup."

rates_by_age_sex |>
  ggplot(aes(age, col = Sex)) +
  geom_point(aes(y = rate, size = n), position = position_dodge(width = 0.25)) +
  geom_errorbar(
    aes(ymin = lower, ymax = upper),
    position = position_dodge(width = 0.25),
    width = 0.2
  ) +
  scale_y_continuous(labels = label_percent()) +
  labs(x = "Age", y = "Rate of Myopia") +
  theme(legend.position = "top") +
  scale_color_manual(values = c("#417839FF", "#E18727FF"))
```

The points suggest a slight increase in myopia rates among girls, but this is well within the error bars defined by the 90% confidence intervals. Due to the concentration of points in a single age group, it is difficult to determine whether there is an age trend. Since the slight rate increase for girls is constant from ages six to eight, there is little indication of an interaction between these factors. 

Second, do genetics matter? If neither parent is myopic, the rates for their children are low: `r rates_by_parent$rate[1]`% with 90% confidence interval (`r rates_by_parent$lower[1]`%, `r rates_by_parent$upper[1]`%). When _both_ parents have the condition, the rate is  `r rates_by_parent$rate[4]`%  (CI: `r rates_by_parent$lower[4]`%, `r rates_by_parent$upper[4]`%), indicating a string signal. 
The effects of paternal and material genetics are in between, with rates `r rates_by_parent$rate[2]`% and `r rates_by_parent$rate[3]`%, respectively, with similar confidence intervals.    

The ocular exam yielded five different measurements of the eye. @fig-myopia-splom shows a scatterplot matrix of pairwise relationships, colored by the outcome class. With one exception,  there is little to no correlation between predictors. However, the axial length and the vitreous chamber depth have a high positive correlation. Additionally, most predictors have fairly symmetric distributions, although the spherical equivalent refraction exhibits a moderately strong right skew. This predictor also shows some raw separation between the classes. 

```{r}
#| label: fig-myopia-splom
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 7
#| fig-cap: "Scatterplot matrix of eye measurements."

myopia_train |>
  select(class, spherical_equivalent_refraction:vitreous_chamber_depth) |>
  rename_with(~ tolower(gsub("_", "\n", .x))) |>
  ggplot(aes(x = .panel_x, y = .panel_y, col = class)) +
  geom_point(alpha = 0.5, shape = 16, size = 1) +
  geom_autodensity(aes(fill = class, col = class), alpha = 3 / 4) +
  facet_matrix(vars(-class), layer.diag = 2, grid.y.diag = FALSE) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "top")
```

## Accepting the State of Nature {#sec-imbalance-acceptance}

problem of never predicting the minory class with poor probabilities

```{r}
#| label: model-setup
#| include: false

# add classification costs
cls_mtr <- metric_set(
  brier_class,
  roc_auc,
  pr_auc,
  kap,
  mn_log_loss,
  accuracy,
  sensitivity,
  specificity
)
ctrl_grid <- control_grid(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)
ctrl_rs <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
```

```{r}
#| label: recipes
#| include: false
bare_rec <-
  recipe(class ~ ., data = myopia_train)

norm_rec <-
  bare_rec |>
  step_normalize(all_numeric_predictors())

glmn_rec <-
  bare_rec |>
  step_interact(~ all_predictors():all_predictors()) |> 
  step_normalize(all_numeric_predictors())

down_rec <-
  bare_rec |>
  step_downsample(class)
```


```{r}
#| label: myopia-glmnet-plain
#| include: false
#| cache: true

path <- 10^seq(-4, -0.6, by = 0.2)

glmn_plain_spec <-
  logistic_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet", path_values = !!path)

glmn_plain_wflow <- workflow(glmn_rec, glmn_plain_spec)

glmn_plain_grid <-crossing(mixture = round((0:3) / 3, 2), penalty = path)

glmn_plain_res <-
  glmn_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = glmn_plain_grid
  )

### 

glmn_plain_best <- select_best(glmn_plain_res, metric = "brier_class")
glmn_plain_fit <- fit_best(glmn_plain_res)
glmn_plain_main <- 
  glmn_plain_fit |> 
  tidy() |> 
  filter(estimate != 0 & !grepl("(_x_)|(Intercept)", term))
glmn_plain_int<- 
  glmn_plain_fit |> 
  tidy() |> 
  filter(estimate != 0 & grepl("_x_", term))

###

glmn_plain_alt_brier <- 
  show_best(glmn_plain_res, metric = "brier_class", n = 50) |> 
  filter(mixture < 0.1) |> 
  slice_min(mean)

glmn_plain_alt_best <- 
  glmn_plain_alt_brier |> 
  select(mixture, penalty, .config)

glmn_plain_alt_fit <- 
  glmn_plain_wflow |> 
  finalize_workflow(glmn_plain_alt_best) |> 
  fit(myopia_train)

glmn_plain_alt_main <- 
  glmn_plain_alt_fit |> 
  tidy() |> 
  filter(estimate != 0 & !grepl("(_x_)|(Intercept)", term))
glmn_plain_alt_int<- 
  glmn_plain_alt_fit |> 
  tidy() |> 
  filter(estimate != 0 & grepl("_x_", term))

### 

glmn_plain_best_mtr <- collect_metrics(glmn_plain_res) |>
  inner_join(glmn_plain_alt_best |> select(.config), by = ".config")
glmn_plain_best_pred <- collect_predictions(
  glmn_plain_res,
  parameters = glmn_plain_alt_best,
  summarize = TRUE
)

glmn_plain_roc_curve <- 
  glmn_plain_best_pred |>
  roc_curve(class, .pred_yes)

glmn_pr_roc_curve <- 
  glmn_plain_best_pred |>
  pr_curve(class, .pred_yes)

glmn_plain_roc_default <- closest_point(glmn_plain_roc_curve)
glmn_plain_pr_default <- closest_point(glmn_pr_roc_curve)
glmn_plain_roc_sensitive <- closest_point(glmn_plain_roc_curve, "sensitivity", 0.80)

glmn_plain_ranges <- 
  glmn_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(glmn_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-mlp-plain
#| include: false
#| cache: true
mlp_spec <-
  mlp(
    hidden_units = tune(),
    penalty = tune(),
    learn_rate = tune(),
    epochs = 100,
    activation = "relu"
  ) |>
  set_engine(
    "brulee",
    stop_iter = 5,
    optimizer = "ADAMw",
    verbose = FALSE,
    batch_size = tune(),
    momentum = tune()
  ) |>
  set_mode("classification")

mlp_rec <- 
  bare_rec |> 
  step_dummy(all_factor_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

mlp_plain_wflow <- workflow(mlp_rec, mlp_spec)
mlp_plain_param <- 
  mlp_plain_wflow |>
  extract_parameter_set_dials() |> 
  update(
    momentum = momentum(c(0.8, 0.99)),
    hidden_units = hidden_units(c(2, 50)),
    penalty = penalty(c(-10, -1)),
    learn_rate = learn_rate(c(-4, -1))
  )
set.seed(820)
mlp_plain_res <-
  mlp_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    param_info = mlp_plain_param,
    grid = 25
  )

mlp_plain_best <- select_best(mlp_plain_res, metric = "brier_class")
mlp_plain_best_mtr <- collect_metrics(mlp_plain_res) |>
  inner_join(mlp_plain_best |> select(.config), by = ".config")
mlp_plain_best_pred <- collect_predictions(
  mlp_plain_res,
  parameters = mlp_plain_best,
  summarize = TRUE
)

mlp_roc_curve <- 
  mlp_plain_best_pred |>
  roc_curve(class, .pred_yes)

mlp_plain_default <- closest_point(mlp_roc_curve)
mlp_plain_sensitive <- closest_point(mlp_roc_curve, "sensitivity", 0.95)

mlp_plain_ranges <- 
  mlp_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(mlp_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-lgb-plain
#| include: false
#| cache: true

lgb_plain_spec <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    mtry = tune(),
    learn_rate = tune(),
    stop_iter = 5
  ) |>
  set_mode("classification") |>
  set_engine("lightgbm")


lgb_plain_wflow <- workflow(class ~ ., lgb_plain_spec)

set.seed(820)
lgb_plain_res <-
  lgb_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25
  )

lgb_plain_best <- select_best(lgb_plain_res, metric = "brier_class")
lgb_plain_best_mtr <- collect_metrics(lgb_plain_res) |>
  inner_join(lgb_plain_best |> select(.config), by = ".config")
lgb_plain_best_pred <- collect_predictions(
  lgb_plain_res,
  parameters = lgb_plain_best,
  summarize = TRUE
)

lgb_roc_curve <- 
  lgb_plain_best_pred |>
  roc_curve(class, .pred_yes)

lgb_plain_default <- closest_point(lgb_roc_curve)
lgb_plain_sensitive <- closest_point(lgb_roc_curve, "sensitivity", 0.95)

lgb_plain_ranges <- 
  lgb_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(lgb_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-ranger-plain
#| include: false
#| cache: true

rf_plain_spec <-
  rand_forest(
    trees = 2000,
    min_n = tune(),
    mtry = tune()
  ) |>
  set_mode("classification")

rf_plain_wflow <- workflow(class ~ ., rf_plain_spec)

set.seed(820)
rf_plain_res <-
  rf_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25
  )

rf_plain_best <- select_best(rf_plain_res, metric = "brier_class")
rf_plain_best_mtr <- collect_metrics(rf_plain_res) |>
  inner_join(rf_plain_best |> select(.config), by = ".config")
rf_plain_best_pred <- collect_predictions(
  rf_plain_res,
  parameters = rf_plain_best,
  summarize = TRUE
)

rf_plain_roc_curve <- 
  rf_plain_best_pred |>
  roc_curve(class, .pred_yes)

rf_plain_default <- closest_point(rf_plain_roc_curve)
rf_plain_sensitive <- closest_point(rf_plain_roc_curve, "sensitivity", 0.95)

rf_plain_ranges <- 
  rf_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(rf_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-xrf-plain
#| include: false
#| cache: true

xrf_plain_spec <-
  rule_fit(
    mtry = tune(),
    trees = tune(),
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    penalty = tune()
  ) |>
  set_engine('xrf', counts = FALSE) |>
  set_mode('classification')

xrf_rec <- 
  bare_rec |> 
  step_dummy(all_factor_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

xrf_plain_wflow <- workflow(xrf_rec, xrf_plain_spec)
xrf_plain_param <- 
  xrf_plain_wflow |> 
  extract_parameter_set_dials() |> 
  update(
    tree_depth = tree_depth(c(2,4)),
    min_n = min_n(c(2, 10))
    )

set.seed(820)
xrf_plain_res <-
  xrf_plain_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 25,
    param_info = xrf_plain_param
  )

xrf_plain_best <- select_best(xrf_plain_res, metric = "brier_class")
xrf_plain_best_mtr <- collect_metrics(xrf_plain_res) |>
  inner_join(xrf_plain_best |> select(.config), by = ".config")
xrf_plain_best_pred <- collect_predictions(
  xrf_plain_res,
  parameters = xrf_plain_best,
  summarize = TRUE
)

xrf_plain_roc_curve <- 
  xrf_plain_best_pred |>
  roc_curve(class, .pred_yes)

xrf_plain_roc_default <- closest_point(xrf_plain_roc_curve)
xrf_plain_roc_sensitive <- closest_point(xrf_plain_roc_curve, "sensitivity", 0.95)

xrf_plain_ranges <- 
  xrf_plain_res |> 
  collect_metrics() |> 
  select(all_of(.get_tune_parameter_names(xrf_plain_res))) |>
  map(range)
```

```{r}
#| label: myopia-xrf-plain-best
set.seed(926)
xrf_plain_fit <-
  xrf_plain_wflow |>
  finalize_workflow(xrf_plain_best) |>
  fit(myopia_train)
xrf_plain_rules <- xrf_plain_fit |> tidy(penalty = xrf_plain_best$penalty)

xrf_plain_best_pred_ind <- collect_predictions(
    xrf_plain_res,
    parameters = xrf_plain_best,
    summarize = FALSE
)
xrf_max_prob <- xrf_plain_best_pred_ind |> pluck(".pred_yes") |> max()
```

A regularized logistic regression was used with glmnet penalties. The predictors included main effects and all two-factor interactions. Penalty values ranging from 10<sup>`r min(log10(glmn_plain_ranges$penalty))`</sup> to 10<sup>`r max(log10(glmn_plain_ranges$penalty))`</sup> were evaluated with four mixtures that ranged from `r min(glmn_plain_ranges$mixture* 100)`% to `r max(glmn_plain_ranges$mixture* 100)`% Lasso. A regular grid of `r nrow(glmn_plain_grid)` candidates was used. 

A neural network was tuned over: 

:::: {.columns}

::: {.column width="50%"}
- The number of hidden units (`r min(mlp_plain_ranges$hidden_units)` to `r max(mlp_plain_ranges$hidden_units)`). 
- The L<sub>2</sub> penalty (10<sup>`r min(log10(mlp_plain_ranges$penalty))`</sup> to 10<sup>`r max(log10(mlp_plain_ranges$penalty))`</sup>).
- The learning rate (10<sup>`r min(log10(mlp_plain_ranges$learn_rate))`</sup> to 10<sup>`r max(log10(mlp_plain_ranges$learn_rate))`</sup>).
:::

::: {.column width="50%"}
- The batch size (`r min(mlp_plain_ranges$batch_size)` to `r max(mlp_plain_ranges$batch_size)` samples).
- AdamW momentum (`r min(mlp_plain_ranges$momentum)` to `r max(mlp_plain_ranges$momentum)`).
:::

::::

SGD via AdamW was used to optimize the model for up to 100 epochs, with early stopping after 5 consecutive poor results. ReLU activation was used. 

Boosted trees (via light GBM) were trained for up to 1,000 iterations with early stopping after 5 bad iterations. The model was tuned over: 

:::: {.columns}

::: {.column width="50%"}
- Tree depth (`r min(lgb_plain_ranges$tree_depth)` to `r max(mlp_plain_ranges$tree_depth)` splits). 
- Amount of data needed for further splitting (`r min(lgb_plain_ranges$min_n)` to `r max(mlp_plain_ranges$min_n)` samples).
:::

::: {.column width="50%"}
- The learning rate (10<sup>`r min(log10(lgb_plain_ranges$learn_rate))`</sup> to 10<sup>`r max(log10(lgb_plain_ranges$learn_rate))`</sup>).
- $m_{try}$ (`r min(lgb_plain_ranges$mtry)` to `r max(lgb_plain_ranges$mtry)` predictors).
:::

::::

Random forests with 2,000 classification trees were tuned over: 

- Amount of data needed for further splitting (`r min(rf_plain_ranges$min_n)` to `r max(rf_plain_ranges$min_n)` samples).
- $m_{try}$ (`r min(rf_plain_ranges$mtry)` to `r max(rf_plain_ranges$mtry)` predictors)..

Finally, a RuleFit model was optimized over: 

:::: {.columns}

::: {.column width="50%"}
- The number of boosting iterations (`r min(xrf_plain_ranges$trees)` to `r max(xrf_plain_ranges$trees)`)
- Tree depth (`r min(xrf_plain_ranges$tree_depth)` to `r max(xrf_plain_ranges$tree_depth)` splits). 
- Amount of data needed for further splitting (`r min(xrf_plain_ranges$min_n)` to `r max(xrf_plain_ranges$min_n)` samples).

:::

::: {.column width="50%"}
- The learning rate (10<sup>`r min(log10(xrf_plain_ranges$learn_rate))`</sup> to 10<sup>`r max(log10(xrf_plain_ranges$learn_rate))`</sup>).
-  Proportional $m_{try}$ (`r min(round(xrf_plain_ranges$mtry * 100, 1))`% to `r max(round(xrf_plain_ranges$mtry * 100, 1))`%).
- The L<sub>1</sub> penalty (10<sup>`r min(log10(xrf_plain_ranges$penalty))`</sup> to 10<sup>`r max(log10(xrf_plain_ranges$penalty))`</sup>).
:::

::::

Apart from the logistic regression, a space-filling design of 25 candidates was used for optimization. 

@fig-myopia-plain-roundup visualizes the results across and within models. Each point represents a candidate within a model. These are ranked by their Brier score and are ordered accordingly. The vertical lines help highlight where the best candidate was located in the ranking. Interestingly, the logistic regression and RuleFit models had the best results and very similar (see discussion below). The other three models had both good and bad performance across their candidates, but, in terms of model calibration, did not do as well. This may be due to this data set being better served by simplistic models; the added complexity generated by large tree ensembles and neural networks are probably overfitting to some degree. 

```{r}
#| label: fig-myopia-plain-roundup
#| echo: false
#| fig-width: 6
#| fig-height: 3
#| out-width: 80%
#| fig-cap: "Cross-validated Brier scores for different modeling approaches without adjustments for class imbalance. The dashed vertical lines indicate the highest ranked candidate for each model."

plain_res <- as_workflow_set(
  glmnet = glmn_plain_res,
  nnet = mlp_plain_res,
  boosting = lgb_plain_res,
  rand_forest = rf_plain_res,
  rule_fit = xrf_plain_res
)

rank_plain <-
  rank_results(plain_res, rank_metric = "brier_class") |>
  filter(.metric == "brier_class")

rank_plain_best <-
  rank_plain |>
  slice_min(mean, by = c(wflow_id)) |>
  mutate(percent_worse = (rank - 1) / nrow(rank_plain) * 100)

rank_plain |>
  ggplot(aes(rank, mean, col = wflow_id)) +
  geom_point(cex = 1) +
  geom_vline(
    data = rank_plain_best,
    aes(col = wflow_id, xintercept = rank),
    lty = 2
  ) +
  labs(x = "Rank", y = "Brier Score", col = "Model") +
  theme(legend.position = "top")
```


While the Rulefit and logistic regressions had the smallest Brier scores, their respective best candidates have issues. For the RuleFit model, the maximum estimated probability of a patient being myopic is `r round(xrf_max_prob * 100, 1)`% across all of the assessment sets. While the values are well-calibrated, this upper limit of probability is probably too small to be the final model. We would expect at least one of the held-out predictions to be closer to one when the patient is truly myopic. 

For the regularized logistic model, @fig-myopia-logistic shows the relationship of the tuning parameters with the Brier score. We can see that while the numerically best results are to use `r round(glmn_plain_best$mixture * 100, 0)`% Lasso regularization with a penalty of 10<sup>`r min(log10(glmn_plain_best$penalty))`</sup>, there are several other combinations with nearly equivalent metrics. The model fitted on the entire training set could have a maximum of `r max_num_terms` model coefficients (apart from the intercept) since there are `r num_pred` main effects and `r choose(num_pred, 2)` possible two-way interactions. However, the Lasso model can remove predictors, and the fitted model contains only `r nrow(glmn_plain_main)` main effect and `r nrow(glmn_plain_int)` interactions. 

For a model used purely for prediction, this is not a terrible situation. However, the lack of main effects is troubling since there are interaction terms in the model with no corresponding main effects. This is inconsistent with the heredity principle discussed in @sec-interactions-principles and suggests that these interactions are unlikely to be real. 

To sidestep this issue, we can choose a different tuning parameter candidate. If we use _no_ Lasso regularization, the model will contain all parameters, but regularizes some them to be close to zero. This would ensure that all of the interaction terms have corresponding main effects in the model. Taking this approach, the penalty associated with the smallest Brier score is 10<sup>`r min(log10(glmn_plain_alt_best$penalty))`</sup>. For this model, the resampling estimate of the Brier score is `r signif(glmn_plain_alt_brier$mean, 3)`, which is still very good when compared to the other models. We'll use this candidate going forward. 

Additionally, @fig-myopia-logistic shows some diagnostics for the logistic regression. These plots are constructed by obtaining the assessment set predictions across the `r nrow(myopia_rs)` resamples. Note that since repeated cross-validation was used, each training set point has five replicates in the held-out data. To create the calibration and ROC curves, the five predictions for each training set point are averaged so that we have a set of `r nrow(myopia_train)` prediction rows. Please note that these are approximate and may differ from the performance statistics generated by resampling. For example, @fig-myopia-logistic shows a single curve while the resampled ROC AUC is the average of `r nrow(myopia_rs)` ROC curves created with 10% of the training set. Despite this, the visualizations can help us characterize the quality of the fit and detect any significant issues in the predictions. 

```{r}
#| label: fig-myopia-logistic
#| echo: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: "Results of the logistic regression model optimization (without adjustments for the class imbalance). The top plot shows the performance profile for the two tuning parameters. Underneath are the calibration and ROC curves. The solid blue point on the ROC curve corresponds to the default cutoff. The pooled averaged holdout predictions were used to compute these visualizations."

p <- 
  autoplot(glmn_plain_res, metric = "brier_class") + 
  theme(legend.position = "top") + 
  labs(y = "Resampled Brier Score")

glmn_plain_cal <- 
 glmn_plain_best_pred |>
  cal_plot_windowed(class, .pred_yes, step_size = 0.05, window_size = 0.2) +
  labs(title = "Calibration Curve")

glmn_plain_roc <- 
  autoplot(glmn_plain_roc_curve) + 
  geom_point(
    data = glmn_plain_roc_default, 
    aes(x = 1 - specificity, y = sensitivity),
    cex = 4, 
    col = "blue"
  ) + 
  labs(title = "ROC Curve", y = "Sensitivity", x = "1 - Specificity")

p / (glmn_plain_cal + glmn_plain_roc)
```

The calibration curve is generally good, except for the upper end, where it indicates that the model underpredicts the probability of myopia. There is also a region around an x-axis point where events occur at a rate of 60%, and the probabilities are slightly overpredicted. All in all, though, the calibration curve is impressive. 

The ROC curve is symmetric and fairly unremarkable. The visualization shows that the default 50% threshold for converting the probabilities to hard "yes/no" predictions is somewhat problematic. The sensitivity for this cutpoint is very poor (`r round(glmn_plain_best_mtr$mean[xrf_plain_best_mtr$.metric == "sensitivity"] * 100, 1)`%) while the specificity is exquisite (`r round(glmn_plain_best_mtr$mean[xrf_plain_best_mtr$.metric == "specificity"] * 100, 1)`%). This is fairly common for class imbalances. The model performs significantly better where the data are more abundant, and since most participants in the study are not myopic, high specificity is more easily achieved. 

We quantified performance for these models using a variety of methods. @tbl-plain-metrics shows several statistics for both hard and soft prediction types. When assessing class probability estimates, the Brier scores for each model are all acceptably low, and most areas under the ROC and PR curves have objectively large values. 

:::: {.columns}

::: {.column width="5%"}
:::

::: {.column width="90%"}

::: {#tbl-plain-metrics}

```{r}
#| label: plain-metrics
#| echo: false
mtr_labs <- c("Brier Score",  "ROC AUC", "PR AUC", "Binomial Log Loss",
              "Accuracy", "Kappa", "Sensitivity", "Specificity")

plain_metric_table <- 
  bind_rows(
    glmn_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Logistic (glmnet)"), 
    mlp_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Neural Network"),
    rf_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Random Forest"),
    lgb_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "Boosted Tree"),
    xrf_plain_best_mtr |> 
      select(Metric = .metric, Estimate = mean) |> 
      mutate(Model = "RuleFit")
  ) |> 
  filter(Metric != "mn_log_loss") |> 
  mutate(
    Metric = 
      case_when(
        Metric == "accuracy" ~ "Accuracy",
        Metric == "kap" ~ "Kappa",
        Metric == "roc_auc" ~ "ROC AUC",
        Metric == "brier_class" ~ "Brier Score",
        Metric == "mn_log_loss" ~ "Binomial Log Loss",
        Metric == "pr_auc" ~ "PR AUC",
        Metric == "sensitivity" ~ "Sensitivity",
        Metric == "specificity" ~ "Specificity",
        TRUE ~ Metric
      ),
    Metric = factor(Metric, levels = mtr_labs)
  ) |> 
  pivot_wider(id_cols = Model, names_from = Metric, values_from = Estimate)

gt(plain_metric_table) |>
  fmt_number(
    columns = c(`ROC AUC`, `PR AUC`, `Brier Score`, Kappa),
    n_sigfig = 3
  ) |>
  fmt_percent(columns = c(Accuracy, Sensitivity, Specificity), decimals = 1) |>
  cols_move_to_start(`Brier Score`) |> 
  tab_spanner(
    "Hard Class Estimates",
    columns = c(Accuracy, Sensitivity, Specificity, Kappa)
  ) |> 
    tab_spanner(
    "Probability Estimates",
    columns = c(`ROC AUC`, `PR AUC`, `Brier Score`)
  )
```

Resampled performance metrics for best candidates across different models. The models were optimized on the basis of the Brier score. 
 
:::

:::

::: {.column width="5%"}
:::

:::: 

The hard class prediction uses the default 50% probability cutoff. A few of the models have accuracies that are larger than the rate that the majority class occurs in the training set (`r round(mean(myopia_train$class == "no") * 100, 1)`%). The Kappa statistics indicate that there is some signal in the models when it comes to predicting qualitative class membership. However, with this cutoff, the model is effectively useless at predicting which people are myopic, as the best possible sensitivity is `r round(max(plain_metric_table$Sensitivity) * 100, 1)`%. 

If we are satisfied with the model's ability to discriminate between classes (as evidenced by the ROC curve) and that the probabilities are sufficiently calibrated, we can address the sensitivity problem by selecting an appropriate probability threshold. As with most optimization problems, we require an objective function to make the best decision regarding this tuning parameter. There are a few ways we can approach the problem: 

 1. Given what we know about how the model will be used, we could define specific costs for false negatives and false positives (with correctly predicted classes having zero cost). Using these values, we can choose a threshold that minimizes the cost of the decision. This is similar to a weighted Kappa approach except the weights would not be symmetric. 
 2. Seek a constrained optimization. For example, we can try to maximize the sensitivity under the constraint that the specificity is at an acceptable value. The ROC curve provides the substrate to make these calculations since it includes sensitivity and specificity for each possible threshold in the data. 
 3. Optimize an existing composite score that captures the right balance of sensitivity and specificity. The Matthews correlation coefficientis an example of such metrics. 
 4. Use a multiparameter optimization method, such as desirability functions from @sec-cls-multi-objectives, to define a jointly acceptable result (i.e., overall desirability). This would allow us to factor in multiple performance metrics, as well as other variables, in the decision process. 
 
We'll focus on the first two options. 

To choose the threshold using a custom cost metric (the first bullet point above), we would have to determine an appropriate cost structure for false positives and false negatives, usually based on the consequences of either error. For our myopia example, this is a somewhat subjective choice that depends on some context-specific rationale. In other cases, the costs might be analytically calculated as a function of monetary costs. @fig-class-cost shows examples of cost curves if the false positive cost is fixed at 1.0 and the cost of a false negative varies in severity. The y-axis is the mean of the cost values averaged across the `r nrow(myopia_rs)` assessment sets. 
 
```{r}
#| label: myopia-glmnet-threshold
#| include: false
#| cache: true
thresholds <- seq(0.0, 0.40, by = 0.02)[-1]

glmn_tlr <-
  tailor() |>
  adjust_probability_threshold(threshold = tune())

# Using a single penalty causes an error. We'll use two and pick the appropriate
# value afterwards

glmn_plain_mod_spec <-
  logistic_reg(penalty = tune(), mixture = !!glmn_plain_alt_best$mixture) |>
  set_engine("glmnet", path_values = !!path)

glmn_thr_wflow <- workflow(glmn_rec, glmn_plain_mod_spec, glmn_tlr) 

glmn_thr_res <-
  glmn_thr_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = crossing(
      penalty = c(0.001, glmn_plain_alt_best$penalty), 
      threshold = thresholds)
  )

glmn_thr_pred <- 
  glmn_thr_res |> 
  collect_predictions(summarize = FALSE) |> 
  filter(penalty == glmn_plain_alt_best$penalty)
```

```{r}
#| label: fig-class-cost
#| echo: false
#| out-width: 60%
#| fig-width: 6
#| fig-height: 4
#| fig-cap: "Cost curves when the cost of a correct prediction is zero, the cost of a false positive is 1.0, and the cost of a false negative varies."
costs <- c(2, 5, 10, 15)

for (i in seq_along(costs)) {
  tmp <- 
    glmn_thr_pred |> 
    mutate(
      cost_val = costs[i],
      .pred_class = if_else(.pred_yes >= threshold, "yes", "no"),
      .pred_class = factor(.pred_class, levels = c("yes", "no")),
      cost = 
        case_when(
          class == "yes" & .pred_class == "no"  ~ cost_val,
          class == "no"  & .pred_class == "yes" ~ 1.0,
          TRUE ~ 0.0)
    ) |> 
    summarize(
      cost = mean(cost), 
      .by = c(id, id2, threshold, cost_val)
    )
  
  if (i == 1) {
    cost_mtr <- tmp
  } else {
    cost_mtr <- bind_rows(cost_mtr, tmp)
  }
}

cost_mean_mtr <- 
  cost_mtr |> 
  summarize(
    num = sum(!is.na(cost)),
    cls_cost = mean(cost),
    std_err = sd(cost),
    .by = c(threshold, cost_val)
  )

cost_best_mtr <- 
  cost_mean_mtr |> slice_min(cls_cost, by = c(cost_val))

cost_mean_mtr |>
  ggplot(aes(threshold, col = format(cost_val))) +
  geom_point(aes(y = cls_cost), alpha = 3 / 4) +
  geom_line(aes(y = cls_cost), alpha = 3 / 4) +
  geom_point(data = cost_best_mtr, aes(y = cls_cost), cex = 3) +
  labs(x = "Probability Threshold", y = "Total Cost", col = "Cost of FN") +
  scale_x_continuous(labels = label_percent()) +
  theme(legend.position = "top")
```

::: {.important-box}
It is important to realize that this strategy only optimizes the probability threshold. The model fit (e.g., slope and intercept parameter estimates) has not been affected by the cost function.  In the next section, we will consider costs to the objective function that the model uses during training. 
:::

Moving on to Option 2, when choosing a threshold using an ROC curve, there is an immediate question: "Which data should be used to compute the ROC curve?" _Preferably_, the cutoff selection should use different data than the data used to fit the model. For example, diagnostic tests that require approval from a government agency (e.g., the Food and Drug Administration in the US) require a different data set to set the threshold. However, this may not be feasible if there is not an abundance of data or the resources to acquire such data. If we can't hold out another data set, we can use the assessment sets generated during resampling^[Or a validation set if multiple resamples are not used.]. There are at least two different approaches. 

 - **Option A - _post hoc_ pooling**: After we have resampled our model configurations, we can pool the assessment set predictions for the best candidate. This ROC curve is computed overall existing class probability estimate values. This was the approach that generated the visualizations in @fig-myopia-logistic. 

- **Option B - threshold tuning**: During model tuning we treat the probability threshold as a tuning parameter and include it in the grid search. For each threshold in the grid, conditional on any other tuning parameters, we have resampling estimates for sensitivity and specificity. These points are then used to create an ROC curve. 

The primary difference between these approaches lies in data usage: do we compute one large ROC curve from pooled data or or an ROC curves from resampled statistics? Statistically, the latter is more appropriate, but either might produce acceptable results.

Note that there are a few options when we treat the threshold as a tuning parameter. A one-stage approach would include the threshold along with any other tuning parameters and use a single grid for all. However, recall that the choice of this parameter does not affect any probability-based metrics, such as the Brier score, log-loss, or the area under the ROC curve. We could use a two-stage approach where the non-threshold parameters are optimized to find their best results and then we can use a one-dimensional grid to find the best threshold (based on metrics appropriate for hard class predictions). We took this approach here: `r length(thresholds)` threshold values ranging from `r round(min(thresholds) * 100, 1)`% to `r round(max(thresholds) * 100, 1)`% defined the second-stage grid. 

```{r}
#| label: fig-myopia-two-roc
#| echo: false
#| out-width: 50%
#| fig-width: 5
#| fig-height: 5.1
#| fig-cap: "Two approaches for optimizing the probability threshold via the ROC curve. Both curves use the regularized logistic regression's resampling results."

glmn_thr_roc <- 
  glmn_thr_res |> 
  collect_metrics(type = "wide") |> 
  filter(penalty == glmn_plain_alt_best$penalty) |> 
  select(threshold, sensitivity, specificity) |> 
  bind_rows(
    tribble(
      ~threshold , ~sensitivity , ~specificity ,
      0 ,            1 ,            0 ,
      1 ,            0 ,            1
    )
  )

roc_tradeoff <- 
  glmn_thr_roc |>
  filter(specificity >= 0.8) |>
  slice_max(sensitivity) |>
  mutate(
    prevalence = mean(myopia_df$class == "yes"),
    ppv = (sensitivity * prevalence) / ((sensitivity * prevalence) +
        ((1 - specificity) * (1 - prevalence))),
    npv = (specificity * (1 - prevalence)) / (((1 - sensitivity) * prevalence) +
        ((specificity) * (1 - prevalence)))
  )

glmn_thr_roc |>
  mutate(Method = "Threshold Tuning") |> 
  bind_rows(
    glmn_plain_roc_curve |> 
      select(.threshold, sensitivity, specificity) |> 
      mutate(Method = "post hoc Pooling")
  ) |> 
  ggplot(aes(1 - specificity, sensitivity, col = Method)) +
  geom_step(direction = "vh", linewidth = 0.8) +
  coord_obs_pred() +
  theme(legend.position = "top") + 
  labs(color = NULL) +
  scale_color_manual(values = c("#8785B2FF", "#DABD61FF"))
```
 
For the logistic regression model, @fig-myopia-two-roc shows the ROC curves for both schemes. The curves are almost identical for these data. Note that the curve associated with a second-stage grid has `r nrow(glmn_thr_roc) - 2` points on the grid, which is based on our grid. The curve for the _post hoc_ pooling method has `r nrow(glmn_plain_roc_curve) - 2` points, which represents how many unique predicted probabilities are contained in the data pool. Despite this, there is significant overlap in the two curves. 
 
Using either method for composing the ROC tool provides us with a tool to make trade-offs between sensitivity and specificity. For example, if we want to maximize sensitivity on the condition that specificity is at least 80%, the curve produced by threshold tuning indicates that the cutoff should be `r round(roc_tradeoff$threshold * 100, 1)`%. The estimated statistics associated with this value are a sensitivity of `r round(roc_tradeoff$sensitivity * 100, 1)`% and a specificity of `r round(roc_tradeoff$specificity * 100, 1)`%. 

This overall approach involves finding a suitable ML model that does not allow class imbalance to alter our methodology, while deferring cutoff selection until the end of the process. This approach can lead to a well-calibrated model that appropriately classifies new samples. The downside becomes apparent when trying to explain individual results to the model's consumers. 

For our myopia example, suppose we make a prediction for a patient where the probability of myopia is estimated to be 20%. The conversation with the patient might sound something like

> "We estimate that your child has a 20% chance of being severely nearsighted and, based on this, we would give them a diagnosis of myopia." 

It is natural for someone to find the diagnosis counterintuitive and lead the parent to ask: 

> "That probability seems pretty low to make that diagnosis. Is that a mistake?"

The non-technical response could be:

> "We think that the diagnosis is appropriate because the overall chances of being myopic are low, and we have determined that our cutoff for making the diagnosis (`r round(roc_tradeoff$threshold * 100, 1)`%) gives us the most effective diagnoses."

Of course, this is paraphrasing, but the hypothetical conversation does illustrate the potential confusion. 

This seeming disconnect can become worse as the prevalence becomes smaller. For very rare events (i.e., < 5%), the cutoff associated with the sensitivity-specificity tradeoff can be very small, such as 0.05%. This is the price that is paid for having probability estimates that are consistent with the rate the event occurs in the wild. 
 
 
## Adjustments for Balance

could be balancing the data, weights, metric contributions, or costs

(Better wording)

### Sampling and Synthesis Techniques

Subsampling

```{r}
#| label: myopia-lgb-down
#| include: false
#| cache: true

lgb_down_wflow <- workflow(down_rec, lgb_plain_spec)

set.seed(820)
lgb_down_res <-
  lgb_down_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    grid = 50
  )
```

### Cost-sensitive learning


```{r}
#| label: myopia-lgb-cls-wts
#| include: false
#| cache: true

lgb_cls_wts_spec <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    mtry = tune(),
    learn_rate = tune(),
    stop_iter = tune()
  ) |>
  set_mode("classification") |>
  set_engine("lightgbm", scale_pos_weight = tune())


lgb_cls_wts_wflow <- workflow(bare_rec, lgb_cls_wts_spec)

lgb_cls_wts_param <-
  lgb_cls_wts_wflow |>
  extract_parameter_set_dials() |>
  update(
    mtry = mtry(c(1, ncol(myopia_train) - 1)),
    scale_pos_weight = class_weights(c(1, 15)),
    learn_rate = learn_rate(c(-2, 1))
  )

set.seed(820)
lgb_cls_wts_res <-
  lgb_cls_wts_wflow |>
  tune_grid(
    myopia_rs,
    metrics = cls_mtr,
    control = ctrl_grid,
    param_info = lgb_cls_wts_param,
    grid = 50
  )
```

### Other Methods

case weights

Unrealistic priors

Logistic regression with altered intercept


## Revenge of the Prevlance


It is worth revisiting the discussion from @sec-cls-two-classes on how the event rate (i.e., prevalence) affects important probabilities. We'll continue to use the myopia data for discussion.  

If we assume that the rate of myopia in the population of interest is consistent with our data set (`r round(roc_tradeoff$prevalence * 100, 1)`%), we can gain a deeper understanding of how well the model performs.  Given our estimates of sensitivity and specificity, the unconditional probabilities are: 

- positive predictive value (PPV): `r round(roc_tradeoff$ppv * 100, 1)`%
- negative predictive value (NPV): `r round(roc_tradeoff$npv * 100, 1)`%

Even though our sensitivity and specificity ended up being fairly balanced, the statistics that convey to true ability to predict are not.  
 
This potentially exacerbates the potential confusion that can occur when explaining the results of a prediction. Continuing the hypothetical conversation from @sec-imbalance-acceptance, the explanation of the PPV might be: 

> "We understand that this rule of thumb might seem overly stringent. 
> That said, from our data, we know that when we predict that a child is myopic, the probability that they are actually myopic is roughly `r round(roc_tradeoff$ppv * 100, 0)`%.
> This is because _most children are not myopic_ but we do find some evidence to make the diagnosis." 

Conversely, if the child had a very small predicted probability and a myopia diagnosis was not used, the explanation would be: 

> "Even though the chances that your child is myopic are greater than zero, from our data we know that when we predict that a child is not myopic, the probability that they are actually not myopic is overwhelmingly high (roughly `r round(roc_tradeoff$npv * 100, 0)`%).
> This is because _most children are not myopic_ and there is not enough evidence to disprove this for your child." 

This is much less ambiguous than the previous hypothetical conversation. 

We can try to optimize the model for PPV and NPV in the same way as we did for sensitivity and specificity. However, to do this, the prevalence must be well known for the specific population that will be predicted by the model and should be stable over time. 

For example, @covidcdc calculated COVID-19 cases and deaths, and estimated cumulative incidence for March and April 2020. The mortality rate varied geographically. For example, during that period, New York City had an estimated mortality rate of 5.3%, while the remainder of the state of New York had an estimated rate of 2.2%. Thankfully, in 2025, these rates are much lower. 

This illustrates that computations that depend on the event rate can be challenging, as the values can systematically change under different conditions as well as over time. 

## Chapter References {.unnumbered}

