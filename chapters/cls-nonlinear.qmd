---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-nonlinear/"
---

# Complex Nonlinear Boundaries {#sec-cls-nonlinear}

```{r}
#| label: cls-nonlinear-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------

library(kernlab)
library(tidymodels)
library(torch)
library(mirai)
library(spatialsample)
library(discrim)
library(important)
library(embed)
library(bestNormalize)
library(gt)
library(probably)
library(patchwork)
library(important)
library(paletteer)
library(brulee)
library(paletteer)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_bw())
set_options()
theme_set(theme_bw())
daemons(parallel::detectCores())

cls_mtr <- metric_set(brier_class, roc_auc, pr_auc, mn_log_loss)

# ------------------------------------------------------------------------------
# load data 

load("../RData/forested_data.RData")
load("../RData/optimizers.RData")

# ------------------------------------------------------------------------------
# Gradient descent helpers

source("../R/setup_2D_GD.R")
```

This chapter focuses on several classes of models that naturally produce nonlinear boundaries, such as neural networks, K-nearest neighbors, and support vector machines. A separate class of models, which use tree structures, also has inherently nonlinear classification boundaries. We'll discuss those models in @sec-cls-trees and @sec-cls-ensembles. 

In this chapter, several recurring motifs are present across different types of models. First, this involves the use of an operation that utilizes the positive part of a function, known as "hinge" or "rectifier" functions. We'll see these in @sec-fda and @sec-nnet-activation. Another element that will be encountered across different models is the kernel function, previously seen in @sec-gp. These functions are used to quantify the distance or similarity between data points (or, more specifically, vectors). Both the K-nearest neighbor and the support vector machine (SVM) models rely on these functions. Finally, the dot product between vectors plays a critical role in the important _attention_ mechanism in neural networks (@sec-attention) as well as in SVM models (@sec-cls-svm). 

This chapter contains _two_ sections on neural networks. The first describes the most basic version of these models, called the _multilayer perceptron_. The fundamentals of these models are discussed in this section. The second section explores various methodologies to enhance the performance of neural network models for tabular data, similar to their effectiveness in predicting images and text. At the time of this writing, substantial research on this topic is underway in the field. This section will also discuss one of the most effective tools for most deep learning models: attention-based transformers.

To get started, the first chapter summarizes an old, traditional model for classification, linear discriminant analysis (LDA), and shows how it can be effectively modified to produce nonlinear class boundaries.   

## Nonlinear Discriminants  {#sec-nonlinear-da}

Discriminant analysis is a traditional statistical method for classification. Its relevance has waned over the years, but we'll discuss it because it has led to some interesting and highly functional extensions. Our initial focus is on linear discriminant analysis (LDA).

Linear discriminant analysis [@mclachlan2005discriminant;@murphy2012machine] produces linear class boundaries and can be motivated via Bayes' Rule. If our training set predictors for class $k$ are contained in $\boldsymbol{X}_k$, we assume that these data have a multivariate Gaussian distribution with a class-specific mean vector and a common covariance matrix: $\boldsymbol{X}_k \sim N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma})$. In terms of Bayes' Rule, this is $Pr[Y = k|\boldsymbol{x}]$ where $\boldsymbol{x}$ is a vector of $p$ numeric predictors (for a single data point). If $Pr[Y = k]$ is our prior probability of the outcome being class $k$, then our prediction equation is:

$$
Pr[Y = k |\boldsymbol{x}] =\frac{Pr[Y = k]Pr[\boldsymbol{x}|Y = k]}{\sum\limits_{l=1}^C Pr[Y = l]Pr[\boldsymbol{x}|Y = l]}
$$ {#eq-discrim}

This equation is straightforward to calculate. Using the training data, we can calculate our standard maximum likelihood estimates for the mean vector and covariance matrix, $\bar{\boldsymbol{x}}_k$ and $\boldsymbol{S}$, respectively. Using these, the multivariate normal probability is the most complex part of the equation (but is trivial to compute with modern software). Additionally, we can save time by computing only the numerator of @eq-discrim for each class and then normalizing the probabilities so that they sum to 1.0. When classifying a new data point $\boldsymbol{x}$, we can plug it into @eq-discrim, along with the prior probabilities, and produce a posterior probability for each class. The hard class prediction corresponds ot the class with the largest posterior. Alternatively, we can frame the prediction in terms of the discriminant function: 

$$
D_k(\boldsymbol{x}) = \boldsymbol{x}'\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k + \frac{1}{2}\boldsymbol{\mu}_k'\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k + \log\left(Pr[Y = k]\right)
$$ {#eq-discrim-func}

The discriminant values are not dissimilar from the linear predictor in logistic regression. They take values on the real line, and larger values are indicative of higher (posterior) probabilities. The class associated with the largest discriminant value is taken as the hard class prediction. 

One nice feature of LDA is that $\boldsymbol{\Sigma}$ models the covariance/correlation structure of the predictors. This means that moderate multicollinearity isn't an issue for LDA. Additionally, this equation is applicable to any number of classes. However, if there are linear dependencies in the data or if the number of rows in the training set is smaller than $p$, the covariance matrix is singular and we cannot compute the probability. Indicator columns could be awkwardly used in this model; they are numbers, so the probabilities could be computed. However, variables comprised of two whole numbers are most certainly not Gaussian. It defeats the purpose of using this model. 

As is, LDA is not a versatile or powerful tool. It produces linear boundaries. If we were to induce nonlinear boundaries via feature engineering, logistic regression would be a much better choice. 

There are many variations and extensions of discriminant analysis. The covariance matrix can be class-specific, leading to quadratic discriminant analysis (QDA) [@ghojogh2019linear], where the class boundaries are low-level polynomials. There are also robust versions of these models that are independent of distributional outliers [@hubert2024robust], as well as regularized discriminants that attempt to reduce the complexity of the covariance matrices [@friedman1989regularized;@pang2009shrinkage]. 

There are two important variations of discriminant analysis. One is a useful simplification of the model (naive Bayes), and the other is a generalization that can include automatic feature selection, nonlinearities, and other helpful characteristics (called _flexible_ discriminant analysis (FDA)).

### Naive Bayes {#sec-naive-bayes}

LDA estimates $Pr[\boldsymbol{x}|Y = k]$ via a multivariate Gaussian probability. Naive Bayes [@hand2001idiot], on the other hand, takes a different tack and decomposes that probability calculation into a product of $p$ separate distributions for each predictor: 

$$
Pr[\boldsymbol{x}|Y = k] = \prod_{j=1}^p Pr[x_j|Y = k] 
$$ {#eq-ind-probs}

This makes the inherent assumption that our predictors are all statistically independent of one another. For most datasets, this is unlikely to be true. 

However, it does loosen the distributional assumption of each variable. We don't have to assume normality and, in fact, we don't even have to declare a full parametric probability distribution. Kernel density estimates [@Silverman86;@chen2017tutorial] are a type of smoother that adaptively describes the data. Here, a _kernel_ is a function^[We'll see kernel functions two more times in this chapter.] that produces non-negative values that serve as weights. For example, a uniform kernel assigns equal weights to values between two points, a Gaussian kernel uses a squared exponential function (i.e., $K(x) = \exp(x^2)$), and so on. The density estimate is formed by using a running kernel function across the data. At each point, the density function is computed, generating a weight that is larger for points closest to our observed data. As we move across the observed data, the weights accumulate so that their sum is larger when closer to the actual data. The type of kernel function defines how the weight is distributed near the point. For a uniform kernel, the weight is the same within a neighborhood of the point, and for a Gaussian, the weight is largest at the actual point and then decreases exponentially as we move away. 

For example, @fig-densities displays the estimated density for the minimum vapor variable using a blue line for three subsets of the data. Density estimates using a fully parametric $N(\mu, \sigma)$ distribution are shown in orange. In the left-hand panel, the entire training set is shown. There is a slight right skew to the data. Both estimates do fairly well for the data with vapor values above 200. However, the Gaussian estimates are not flexible enough to accurately model the data in the middle of the range, and both methods perform poorly for maximum vapor values below 200 Pa x 100. The middle panel shows the non-forested locations. Based on the histogram, these data appear to be trimodal with a left skew, and the nonparametric method faithfully represents these peaks. The Gaussian is constrained to a single mode; using this approach would likely result in underfitting. The right-hand panel displays the forested locations and exhibits a large right skew. The lower half of these data are poorly modeled using the fully parametric approach. 

```{r}
#| label: fig-densities
#| echo: false
#| out-width: 60%
#| fig-width: 6.5
#| fig-height: 3
#| fig-cap: Parametric and nonparametric density estimates for the minimum vapor predictor, shown separately for the entire training set and then for each class. The blue line is the nonparametric estimate, and the orange curve is the traditional Gaussian estimate. 

forested_dens <- 
  forested_train |> 
  dplyr::select(vapor_min, county) |> 
  mutate(class = "All") |> 
  bind_rows(
    forested_train |> 
      dplyr::select(vapor_min, county, class) |> 
      mutate(class = as.character(class))
  ) |> 
  mutate(
    class = factor(class, levels = c("All", "No", "Yes"))
  )

forested_dens |> 
  ggplot(aes(vapor_min)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 20, col = "white", alpha = 1 / 4) + 
  geom_density(col = "#088BBEFF", linewidth = 3 / 4) + 
  geom_line(
    aes(
      y = dnorm(vapor_min,
                mean = tapply(vapor_min, class, mean, na.rm = TRUE)[PANEL],
                sd = tapply(vapor_min, class, sd, na.rm = TRUE)[PANEL])),
    color = "#F7AA14FF", linewidth = 3 / 4) +
  geom_rug(alpha = 3 / 4) + 
  facet_wrap(~ class, ncol = 3) + 
  labs(x = "Minimum Vapor (Pa x 100)")
```

There is a potential tuning parameter: the "wigglyness" of the density can be adjusted using a _badnwidth_ multiplier. The bandwidth determines the width of the kernel's effect. A small bandwidth makes the density estimate very peaked near the observed data, whereas a large bandwidth spreads the kernel weight over a wide range and generates a very smooth overall density. There is no standard value for the bandwidth, but effective estimation methods exist that select a value that minimizes the mean squared error [@park1990comparison]. This default method of determining the bandwidth is designed to create an accurate density, but it might not align with the best bandwidth to help us classify the data. To compensate for this, we can tune an adjustment factor that is a multiplier of the default bandwidth. A value greater than 1.0 encourages more smoothing of the data, while a smaller values make very bumpy densities. 

Since naive Bayes does not assume much about $Pr[x_p|Y = k]$, we can also use categorical predictors. Here, the frequency distribution of the data is computed and used. @fig-counties showed the raw probabilities associated with the forested class (i.e., $Pr[y = forested]$). There are some issues, though; some counties have no forested locations or have very few locations at all. Any location without a forested location produces a probability of zero, which, when multiplied in  @eq-ind-probs, causes the entire probability to be zero for that class, regardless of any other predictors. That is a problem. In this case, we can use more complex estimates, such as the Beta-Binomial estimate in @eq-beta-bin-mean. This would pull our estimates at zero or one towards the middle. Suppose a county had ten locations, none of which are forested. The Beta-Binomial method with $\alpha = 1$ and $\beta = 3$ would pull the estimate from zero to `r round((0 + 1) / (10 + 1 + 3) * 100, 1)`%. Alternatively,  Laplace smoothing [@Manning2009, Section 11.3.2] can be used: 

$$
\hat{p}_{k} = \frac {x_k+\alpha }{n_k+\alpha C_{x}}
$$ {#eq-laplace-smooth}

where $C_{x}$ is the number of levels of the qualitative predictor and $x_k$ and $n_k$ are the number of events and locations, respectively. Here, with $C_x = 2$ and $\alpha = 1$, the new estimate is `r round((0 + 1) / (10 + 1 * 2) * 100, 1)`%. 

Despite the potentially unreasonable assumption that the predictors are independent of one another, naive Bayes models can work surprisingly well. In addition to the loosening of restrictions regarding distributional assumptions, they have the advantage of being highly efficient for training and prediction. Since the predictors are all evaluated separately, many of the calculations can be conducted in parallel. Depending on how the conditional densities are computed, they can be easily added to database tables, and the final prediction can be executed using highly efficient SQL.

```{r}
#| label: forested-nb
#| include: false
#| cache: true
#| warning: false
nb_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_predictor_best(
    all_predictors(),
    score = "imp_rf",
    prop_terms = tune()
  )

nb_wflow <- workflow(nb_rec, naive_Bayes(smoothness = tune(), Laplace = 1))

get_filter_info <- function(x) x |> extract_recipe() |> tidy(number = 1)

ctrl_nb <- 
  control_grid(
      save_pred = TRUE,
      save_workflow = TRUE,
      extract = get_filter_info, 
      parallel_over = "resamples"
    )

set.seed(872)
forest_nb_res <-
  nb_wflow |>
  tune_grid(
    resamples = forested_rs,
    grid = crossing(
      smoothness = seq(0.5, 1.5, by = 0.1),
      prop_terms = (1:5 / 5)
    ),
    metrics = cls_mtr,
    control = ctrl_nb
  )

nb_best_res <- show_best(forest_nb_res, metric = "brier_class")
forest_nb_mtr <-
  forest_nb_res |> 
  collect_metrics(summarize = FALSE) |> 
  mutate(wflow_id = "naive Bayes") |> 
  relocate(wflow_id)
forest_nb_best <- select_best(forest_nb_res, metric = "brier_class")

forest_nb_pred <-
  forest_nb_res |> 
  collect_predictions(parameters = forest_nb_best) |> 
  mutate(wflow_id = "naive Bayes") |> 
  relocate(wflow_id)

forest_nb_mtr_ex <- 
  forest_nb_pred |> 
  cls_mtr(class, estimate = .pred_class, .pred_Yes)

metric_corr <- 
  forest_nb_res |> 
  collect_metrics(type = "wide") |> 
  dplyr::select(brier_class, roc_auc, mn_log_loss) |> 
  cor() |> 
  round(2)

```

One potential downside for this model is that there is a tendency for the class probability estimates to be seriously uncalibrated. Since we treat the predictors as independent entities, we end up multiplying many probabilities together. When the predictors _are_ related, this means including redundant terms. When any set of probabilities is multiplied together, there is a tendency for the values to become polar; the products tend to migrate towards zero or one. This may not effect the ability of the model to separate classes, but it will compromise the accuracy of the probability estimates. Many predictions, when incorrect, are _confidently_ incorrect.

@fig-nb-probs shows an example for the forestation data for one model candidate. The visualization on the left shows that the majority of the probabilities are very close to zero or one, with a small percentage of locations falling between these two values. The visualization on the right shows the corresponding calibration curve. Here, the concentration of locations at the poles is not consistent with their observed event rates. For example, locations predicted to have estimated probabilities at or close to zero tend to have a rate of forestation closer to 10% and a similar trend occurs near 1.0 on the x-axis. The probabilities in between are no better. Across many of the bins, their actual event rates hover near 0.5. The Brier score for these data is poor (`r round(forest_nb_mtr_ex$.estimate[forest_nb_mtr_ex$.metric == "brier_class"], 3)`) but the area under the ROC curve (`r round(forest_nb_mtr_ex$.estimate[forest_nb_mtr_ex$.metric == "roc_auc"], 3)`) shows good separation of the classes by the model. 

```{r}
#| label: fig-nb-probs
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: An example of how the probability estimates produced by naive Bayes can have poor calibration properties. 

p_nb_prob <- 
  forest_nb_pred |> 
  ggplot(aes(.pred_Yes)) + 
  geom_histogram(binwidth = 0.025) + 
  facet_wrap(~class, ncol = 1) + 
  geom_rug(alpha = 1 / 4) +
  labs(x = "Probability of forestation")

p_nb_cal <- 
  forest_nb_pred |> 
  cal_plot_windowed(class, .pred_Yes, window_size = 0.2, step_size = 0.025)

p_nb_prob + p_nb_cal
```

To potentially counter this, an initial feature filter may be beneficial. Fewer predictors _may_ have an effect of producing probabilities that are more accurate. For the forestation data, a random forest model was computed (within each resample), and each predictor is ranked by its permutation importance score. We can choose a proportion of parameters to retain and then fit our naive Bayes models. Proportions of 20%, 40%, ..., 100% were used during the tuning process. The importance score results are shown in @fig-forested-rf-imp. The intervals are 90% confidence intervals estimated using the 10 replicate values produced during cross-validation. 

```{r}
#| label: fig-forested-rf-imp
#| echo: false
#| out-width: 80%
#| fig-width: 6.5
#| fig-height: 3
#| fig-cap: Aggregate feature rankings using a random forest importance score on the forestation data. The bands reflect 90% confidence intervals computed using replicates over cross-validation.

forest_nb_res |>
  collect_extracts() |>
  dplyr::select(-.config, -id) |>
  unnest(.extracts) |>
  distinct() |>
  filter(prop_terms == 1 & smoothness == 1) |>
  summarize(
    n = length(score),
    sd = sd(score),
    mean = mean(score),
    .by = c(terms, prop_terms)
  ) |>
  mutate(
    lower = mean - qnorm(0.05) * sd / sqrt(n),
    upper = mean + qnorm(0.05) * sd / sqrt(n)
  ) |>
  rename(variable = terms) |>
  full_join(name_key, by = "variable") |>
  mutate(text = if_else(is.na(text), variable, text)) |> 
  ggplot(aes(mean, reorder(text, mean))) +
  geom_point(cex = 3/4) +
  geom_errorbar(aes(xmin = lower, xmax = upper), width = 1 / 3, alpha = 1 / 2) +
  labs(y = NULL, x = "Random Forest Importance Score")
```

The results show that, according to random forest, the annual precipitation has, by far, the largest effect on the prediction function. After this, the maximum vapor and longitude predictors have nearly equal importance. A cluster of several predictors has middling importance. Several other predictors have very low scores: the county, year, eastness, and westness. Please note that each model operates differently^[A different view of predictor importance will be seem later in @fig-forested-fda-imp.]. These scores are not universal or absolute. We use them as a tool to help remove irrelevant predictors before training the primary supervised model. 

Additionally, we have summarized the results over resamples for visualization. In practice, we execute the random forest within each resample and then fit the model on that resample's listing of which predictors are "important". We _do not_ compute the importance once and use the same ranking across resamples. 

For the forestation data, we tuned the naive Bayes kernel parameter adjustment with values between 0.5 and 1.5. For Laplace smoothing, a constant $\alpha = 1$ was used. Outside of feature selection, no preprocessing was conducted; the categorical counties remained as-is, and no transformations were applied to the data. 

The Brier score and ROC AUC results are in @fig-nb-tune. For each metric, the model performed optimally when all predictors were included, and there was a severe drop-off in performance when only 20% of the predictors were retained. For each curve, the results indicate that the model prefers extremely wiggly kernel densities. However, note that, for each curve, the difference in metrics is not large. While there is a preference here for adjustments that reduce the kernel bandwidth, it does not yield an enormous improvement in our metrics. 

The predictions for the _best_ candidate are those shown in @fig-nb-probs. As noted there, the Brier score was mediocre while the ROC curve results indicate that the model can do a good job at qualitative separating the classes. 

```{r}
#| label: fig-nb-tune
#| echo: false
#| out-width: 55%
#| fig-width: 5
#| fig-height: 4
#| fig-cap: Naive Bayes tuning results for the forestation data. 
autoplot(forest_nb_res, metric = c("brier_class", "roc_auc")) + 
  labs(y = "Brier Score") + 
  scale_colour_paletteer_d("calecopal::eschscholzia") +
  theme(legend.position = "top")
```

It is also worth noting that, for this model, the Brier and ROC AUC values point to similar findings. Across all of the candidates, the correlation between these metrics was `r metric_corr[1,2]`; they tend to agree very often. The correlation between these metrics and cross-entropy was slightly lower: `r cli::format_inline("{metric_corr[3,1:2]}")` for the Brier score and the ROC AUC, respectively. 

Now, let's turn our attention to the other notable variation of LDA: flexible discriminant analysis. 
`r r_comp("cls-nonlinear.html#sec-naive-bayes")`

### Flexible Discriminants {#sec-fda}

Flexible discriminant analysis (FDA) by @hastie1994flexible extends LDA so that more complex discriminant functions can be created. The derivation is fairly complex; you can skip to the "Important" box below to avoid some linear algebra.

@hand1981discrimination, @breiman1984nonlinear, and others had shown that the LDA model can be estimated using a series of basic linear regressions. To do this, we first create an $n_{tr}\times C$ matrix $\boldsymbol{Y}$ of class indicators where there is a column for each class that contains binary indicators for the corresponding class level. We also create a $C\times C-1$ matrix of initial "scores" $\boldsymbol{\Omega}$ using a contrast function (previously described in @sec-indicators). @mda_2024 use a Hemlert-like contrast matrix. For $C=3$ classes, the Helmert contrast is 

$$
Helmert = 
\begin{pmatrix} 
-1 & -1 \\ 
\phantom{-}  1 & -1 \\ 
\phantom{-}  0 &\phantom{-}   2 \\ 
\end{pmatrix}
$$

then normalize it using a function of class proportions in the training set. For example, if there were $C = 3$ classes in the outcome with equal frequencies, the initial score matrix could be:

$$
\boldsymbol{\Omega} = 
\begin{pmatrix} 
\phantom{-} 1.22 & -0.71 \\ 
-1.22 & -0.71 \\ 
\phantom{-} 0.00 & \phantom{-} 1.41 \\ 
\end{pmatrix}
$$

Being a contrast matrix, the column means are zero and have covariances of zero. 

We can create the artificial columns for our linear regressions by computing $\boldsymbol{\Omega}^* = \boldsymbol{Y}\boldsymbol{\Omega}$. Using our predictor set and the columns of $\boldsymbol{\Omega}^*$ as the outcomes, a multivariate linear regression is conducted to produce a matrix of regression parameters $\widehat{\boldsymbol{B}}$ and predictions $\hat{\boldsymbol{\Omega}} = \boldsymbol{X}'\hat{\boldsymbol{B}}$. 

Lastly, we compute an Eigen decomposition of $\boldsymbol{\Omega}'\hat{\boldsymbol{\Omega}}$, yielding the eigenvalues and a matrix of eigenvectors denoted as $\Phi$.  

The end result is the matrix $\Phi$ that can be used to scale the linear regression predictions to approximate the discriminant functions in @eq-discrim-func. For a new sample $\boldsymbol{x}$, the _vector_ of discriminant functions post multiplies the prediction by by $\Phi$:

$$
\boldsymbol{D}(\boldsymbol{x}) \approx \hat{\boldsymbol{\Omega}}\Phi = \boldsymbol{x}'\hat{\boldsymbol{B}}\Phi
$$ {#eq-fda-discrim}

which is $(C-1) \times 1$ and has entries for all but the last class level^[The approximation is related to some scaling factors that are functions of the eigenvalues associated with $\Phi$.]. The estimated class probabilities can be computed from the discriminant functions (often using the softmax function). We compute last probability estimate using $\hat{\pi}_C = 1 - \sum_k^{C-1}\hat{\pi}_k$. 

::: {.important-box}
The magic in @hastie1994flexible is that we could replace linear regression with any other technique for modeling numeric outcomes and still frame the results as discriminant analysis. For example, we could use a ridge or lasso penalty to produce better predictions and convert the results into a classification model by rescaling the predictions with $\Phi$ as we did in @eq-fda-discrim. 
:::

In their original paper, the authors of FDA replace it with multivariate adaptive regression splines (MARS) and a penalized spline technique called Bruto. We find that the best application of FDA uses MARS, which will be discussed in more detail in @sec-mars. 

As we'll see in @sec-reg-nonlinear, MARS is a stepwise method that sequentially adds two new model terms for each predictor into a linear model. These two new features transform the predictor value using a _hinge function_: $h(x) = xI(x > 0)$. At each iteration of fitting, MARS determines the best predictor to add to the current linear regression model as well as a corresponding optimal split point $a$ for that predictor. Two terms are added: $h(x - a)$ and $h(a - x)$. The first term is zero for values of $x$ greater than $a$. and the second term is zero below the split point. @fig-fda-hinge displays examples of these "left" and "right" hinge functions. 

::: {#fig-fda-hinge}

::: {.figure-content}

```{shinylive-r}
#| label: shiny-fda-hinge
#| viewerHeight: 600
#| viewerWidth: "100%"
#| standalone: true
#| fig-alt: Examples of a single set of MARS features. 
library(shiny)
library(bslib)
library(ggplot2)
library(dplyr)
library(tidyr)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")

ui <- page_fillable(
  theme = bs_theme(bg = "#fcfefe", fg = "#595959"),
  padding = "1rem",
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(xs = c(-2, 8, -2), sm = 4),
    sliderInput(
      "beta_1",
      label = "Left slope",
      min = -4.0,
      max = 4.0,
      step = 0.5,
      value = 1
    ),
    sliderInput(
      "beta_2",
      label = "Right slope",
      min = -4.0,
      max = 4.0,
      step = 0.5,
      value = 1
    ),
    sliderInput(
      "split",
      label = "Split",
      min = 0,
      max = 10,
      step = 0.1,
      value = 5
    )
  ),
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(xs = c(-2, 8, -2)),
    as_fill_carrier(plotOutput("plots"))
  )
)

server <- function(input, output) {
  theme_set(theme_bw())
  output$plots <-
    renderPlot({
      dat <- tibble(x = seq(0, 10, by = 0.05))

      h_right <- function(x, a) ifelse(x > a, x - a, 0)
      h_left <- function(x, a) ifelse(x < a, a - x, 0)

      split <- input$split
      beta_0 <- 0
      beta_1 <- input$beta_1
      beta_2 <- input$beta_2

      feat <-
        dat |>
        mutate(
          left = h_left(x, split),
          right = h_right(x, split),
          pred = beta_0 + beta_1 * left + beta_2 * right
        )

      p <-
        feat |>
        pivot_longer(
          cols = c(left, right, pred),
          names_to = "side",
          values_to = "value"
        ) |>
        mutate(
          group = case_when(
            side == "left" ~ "Left Hinge: h(x - a)",
            side == "right" ~ "Right Hinge: h(a - x)",
            TRUE ~ "Regression Equation"
          ),
          group = factor(
            group,
            levels = c("Left Hinge: h(x - a)", "Right Hinge: h(a - x)", "Regression Equation")
          )
        ) |>
        ggplot(aes(x, value)) +
        geom_line() +
        geom_vline(xintercept = split, lty = 3) +
        facet_wrap(~group, ncol = 1, scale = "free_y") +
        labs(x = "Original Predictor", y = "Feature Value") 

      print(p)
    }, res = 100)
}

app <- shinyApp(ui, server)

app
```

:::

An example of how MARS partitions the predictor space into two distinct regions, each with its own regression line that connects at the split point. For the bottom panel, we assume an intercept of $\beta_0 = 0$ for @fig-fda-hinge.

:::

As seen in the figure, the hinge feature $h(x - a)$ isolates its effect of the predictor to values greater than $a$. Likewise, its sibling feature $h(a - x)$ only has an effect on the model in the range $x < a$. If $x$ is the first predictor selected by MARS, this creates a flexible segmented regression:

$$
y_i = \beta_0 + \beta_1 h(x_{i1} - a) + \beta_2 h(a - x_{i1}) + \epsilon_i
$$

and estimated using ordinary linear regression. The figure above shows examples of the segmented regression model that is created for different values of $\beta_1$ and $\beta_2$. 

The "growing" phase of MARS creates new pairs of features for each iteration but adding perspective pairs to the existing regression model and monitoring performance. The pair that improves the model the most is retained, and the process repeats until a predefined number of terms is reached.  

During the training process, MARS can split on the same variable multiple times. This can occur when there is significant curvature in the relationship between a predictor and the outcome. For categorical predictors, it automatically creates $C$ binary indicators that can also be used for splitting. MARS can also choose to add a predictor into the model without hinge functions (i.e., as is) if that is the best representation for the model at that time. We can define the maximum number of model terms that should be included; this is an important tuning parameter. 

So far, we have described what occurs when MARS is constrained to produce an _additive model_ (i.e., there is one predictor per model term). Alternatively, we can also allow the training process to include interaction terms. Once it selects a new pair of terms to add to the model, it can conduct a secondary search to find a different predictor (and split value) that pairs best with the initial two sibling features. This results in four new terms being added to the model, each isolating a two-dimensional region of the predictor space it affects. The choice of an additive or a second degree is a tuning parameter. Higher-level interactions are possible but become computationally infeasible at some point. 

After the growing phase, MARS can conduct a pruning phase where it removes terms from the model using an approximation to cross-validation called _generalized cross-validation_ (GCV) that is possible when ordinary linear regression is used to estimate parameters [@golub1979generalized]. Using GCV, it can compare the error that occurs when a specific model term is removed. One or both hinge pairs can be removed during this process. After the pruning phase, the final MARS model is computed and, in conjunction with the resulting matrix $\Phi$, it can be used to estimate predictions. 

FDA-MARS models can produce class boundaries that appear as segmented or trapezoidal in nature. For example, panels in @fig-cls-boundaries labeled as "Medium Complexity" and "High Complexity" were created using FDA-MARS with interactions and different numbers of retained features. 

In terms of preprocessing, the MARS algorithm will handle the translation of categorical predictors into binary indicators, and we should avoid doing this prior to the model fit. The hinge functions are estimated using ordinary least squares; we might consider some preprocessing related to this technique. For example, there may be a benefit to using a transformation to enhance the symmetry of the predictors. Missing data should be imputed prior to modeling.  Additionally, MARS automatically selects features. If a predictor was never used within a model term or if all terms using that predictor were pruned from the model, the prediction function is independent of that column. This benefit translated to FDA-MARS. As was seen in @fig-irrelevant-predictors, there should be no concern for adding possibly irrelevant predictors to the model during training; no extra feature selection step is required.  

For using MARS with FDA with our forestation data, we initially generated a total of 200 features (apart from the intercept) during the growing phase. To tune the model, we considered additive and interaction models as well as using forward and backward removal strategies for the pruning phase. The number of retained terms was optimized to be between 2 and 50 final model terms. 

```{r}
#| label: forested-fda
#| include: false
#| cache: true
fda_spec <- 
  discrim_flexible(num_terms = tune(), prod_degree = tune(), prune_method = tune()) |> 
  set_engine("earth", nk = 201)

fda_wflow <- workflow(class ~ .,  fda_spec)

fda_param <- 
  fda_wflow |> 
  extract_parameter_set_dials() |> 
  update(num_terms = num_terms(c(2, 100)))

shears <- c("backward", "forward")

set.seed(872)
forest_fda_res <-
  fda_wflow |>
  tune_grid(
    resamples = forested_rs,
    grid = crossing(num_terms = 2:50, prod_degree= 1:2, prune_method = shears),
    param_info = fda_param,
    metrics = cls_mtr,
    control = control_grid(
      save_pred = TRUE,
      save_workflow = TRUE,
      parallel_over = "everything"
    )
  )
```

```{r}
#| label: fda-details
#| include: false
forest_fda_best <- tibble(num_terms = 30, prune_method = "forward", prod_degree = 2)

fda_fit <-
  fda_wflow |>
  finalize_workflow(forest_fda_best) |>
  fit(forested_train)

forest_fda_mtr <-
  forest_fda_res |> 
  collect_metrics(summarize = FALSE) |> 
  mutate(wflow_id = "FDA/MARS") |> 
  relocate(wflow_id)

forest_fda_best_mtr <- 
  forest_fda_res |> 
  collect_metrics() |> 
  inner_join(forest_fda_best, by = join_by(num_terms, prod_degree, prune_method))

forest_fda_pred <-
  forest_fda_res |> 
  collect_predictions(parameters = forest_fda_best) |> 
  mutate(wflow_id = "FDA/MARS") |> 
  relocate(wflow_id)
```

```{r}
#| label: fda-terms
#| include: false

mars_fit <- 
  fda_fit |>
  extract_fit_engine() |>
  pluck("fit")

name_sub <- function(x) {
  x <- ifelse(grepl("county", x), "county", x)
  mtch <- name_list %in% x
  no_mtch <- !(x %in% name_list)
  text_nms <- names(name_list)[mtch]
  res <- c(text_nms, x[no_mtch])
  if (length(x) > 1) {
    res <- paste(sort(c(text_nms, x[no_mtch])), collapse = ", ")
    res <- paste0("{", res, "}")
  } 
  res
}

fda_coefs <- 
  mars_fit |> 
  coef() |> 
  as_tibble(rownames = "term") |> 
  mutate(
    pred_chr = map(term, ~ sort(all.vars(as.formula(paste("~", .x))))),
    text = map_chr(pred_chr, name_sub),
    num_var = map_int(pred_chr, length)
  )

counts <- 
  fda_coefs |> 
  filter(term != "(Intercept)") |>
  count(text, num_var) |> 
  mutate(nums = ifelse(n > 1, paste0(n, " times"), "1 time")) |> 
  arrange(num_var, desc(n), text) |> 
  dplyr::select(text, nums)

fda_imp <-
  mars_fit |>
  earth::evimp() |>
  unclass() |>
  as_tibble(rownames = "variable") |>
  mutate(
    variable = ifelse(
      grepl("county", variable),
      gsub("county", "county: ", variable),
      variable
    )
  ) |> 
  full_join(name_key, by = "variable")

# some are missing from training set
missing_mars_cols <- 
  as_tibble_col(names(forested_train)[-1], column_name = "variable") |> 
  anti_join(fda_imp |> filter(!grepl("county", variable)), by = "variable") |> 
  filter(variable != "county")

fda_imp <- 
  fda_imp |> 
  bind_rows(missing_mars_cols) |> 
  mutate(
    text = if_else(is.na(text), variable, text),
    gcv = ifelse(is.na(gcv), 0.0, gcv)
  )

excluded <- sort(fda_imp$text[is.na(fda_imp$used)])
exluded_text <- cli::format_inline("{excluded}")

for (i in seq_along(name_list)) {
  col_nm <- name_list[[i]]
  new_nm <- names(name_list)[i]
  fda_coefs$term <- gsub(col_nm, new_nm, fda_coefs$term)
}

fda_coefs$term <- gsub("county", "county: ", fda_coefs$term)
fda_coefs$term <- gsub("-", " - ", fda_coefs$term)
fda_coefs$term <- gsub("*", " x ", fda_coefs$term, fixed = TRUE)

fda_coefs <- dplyr::rename(fda_coefs, `Model Term` = term)

num_int <- length(grep(" x ", fda_coefs$`Model Term`))
num_main <- nrow(fda_coefs) - 1 - num_int
num_ind <- length(grep("county", fda_coefs$`Model Term`))
```

We can see how this worked out in @fig-fda-tune. Performance leveled off around 25-35 terms for each pruning method, with forward selection showing a tiny advantage for these data. Models with interaction effects were uniformly more successful than additive models. An interaction model using forward selection and 30 terms appears to be a reasonable choice. This configuration had an estimated Brier score of `r round(forest_fda_best_mtr$mean[forest_fda_best_mtr$.metric == "brier_class"], 3)` and an area under the ROC curve of `r round(forest_fda_best_mtr$mean[forest_fda_best_mtr$.metric == "roc_auc"], 3)`.

```{r}
#| label: fig-fda-tune
#| echo: false
#| warning: false
#| message: false
#| out-width: 90%
#| fig-width: 8
#| fig-height: 4
#| fig-cap: FDA-MDA tuning results for the forestation data.  
forest_fda_res |> 
  collect_metrics() |> 
  filter(.metric == "brier_class") |> 
  mutate(
    `Model Type` = if_else(prod_degree == 1, "Additive", "Interactions"),
    prune_method = tools::toTitleCase(paste(prune_method, "Selection Pruning"))
  ) |> 
  ggplot(aes(num_terms, mean, col = `Model Type`)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~ prune_method) + 
  labs(x = "# Model Terms", y = "Brier Score")+
  theme(legend.position = "top")
```

Once the FDA-MARS model was fit to the training set, the results was `r nrow(fda_coefs)` total coefficients. The implementation of MARS checks several stopping conditions for the growing phase based on the coefficient of determination (a.k.a. R<sup>2</sup>), including: 

- Reached a R<sup>2</sup> of 0.999 or more.
- No new term increases R<sup>2</sup>
- Adding a term changes R<sup>2</sup> by less than 0.001.

The last condition triggered the training procedure to stop at `r nrow(fda_coefs)` model terms instead of the `r forest_fda_best$num_terms`  that were requested. Of these terms, `r num_main` used a single predictor and `r num_int` were interaction terms. The categorical predictor (county) was used `r num_ind` times in both main effects and interactions. To give the reader a sense of what the linear regression model looks like, @tbl-fda-coefs shows the set of coefficients, the model terms, and split points.

::::::: columns
::: {.column width="10%"}
:::

:::: {.column width="80%"}
::: {#tbl-fda-coefs}

```{r}
#| label: fda-coefs
#| echo: false

fda_coefs |>
  # dplyr::select(`Model Term`, Coefficient) |>
  dplyr::select(1:2) |>
  gt() |>
  fmt_number(n_sigfig = 3)
```

Model terms contained used the final FDA-MARS fit from the training set. There were `r nrow(fda_coefs)` total coefficients, `r num_main` of which used a single predictor, and `r num_int` were interaction terms. The categorical predictor (county) was used in `r num_ind` terms. 

:::
::::

::: {.column width="10%"}
:::
:::::::

To better understand what is driving the model, MARS has an _internal_ variable importance metric. As the model is created, new terms are added that improve the model's fit. MARS generally measures that using GCV. The model tabulates the change in GCV as each term is added and attribute the effect to the corresponding predictor(s). These aggregate GCV totals are normalized to be between zero and 100, with zero indicating that the predictor has no effect on the prediction function. For our model, @fig-forested-fda-imp shows the results. The annual precipitation and annual maximum temperature were used the most and tied for first place. Terrain roughness, the annual mean temperature, and longitude also had a significant impact on the classifier. There were `r length(excluded)` predictors that were not used at all: `r exluded_text`.

```{r}
#| label: fig-forested-fda-imp
#| echo: false
#| out-width: 80%
#| fig-width: 6.5
#| fig-height: 3
#| fig-cap: MARS feature importance for the final forested model.

fda_imp |> 
  ggplot(aes(gcv, reorder(text, gcv))) + 
  geom_point() +
  labs(y = NULL, x = "MARS Importance (GCV)")
```

The FDA via MARS is an interesting technique in that, on the spectrum ranging from fully interpretable models to black-box techniques, it falls somewhere in between. It might not yield the absolutely optimal performance, but it can often get close _and_ be well explained using visualizations, especially when the model is additive. 

For example, suppose that we chose an additive model based on the patterns shown in @fig-fda-tune. In this case, the shape of the relationship between the predictor and the outcome is independent of the other predictors. The values of the other predictors do affect the range of predicted probabilities for a predictor, but the profile shape is the same. With an additive model, we can plot profiles of the predictor values versus the probability of the event. @fig-fda-profiles shows an example from an additive fit. With this, we can describe how the model works in more details.

```{r}
#| label: fig-fda-profiles
#| echo: false
#| warning: false
#| message: false
#| out-width: 90%
#| fig-width: 8
#| fig-height: 5
#| fig-cap: Profile plots showing how each predictor related to the probability of forestation when an additive FDA-MDA is used. The y-axis has no numbers since the probabilities are relative. 
fda_additive_spec <- 
  discrim_flexible(num_terms = 30, prod_degree = 1, prune_method = "forward") |> 
  set_engine("earth", nk = 201, keepxy = TRUE)

fda_additive_fit <- 
  workflow(class ~ .,  fda_additive_spec) |> 
  fit(forested_train)

mars_additive_fit <- 
  fda_additive_fit |> 
  extract_fit_engine() |> 
  pluck("fit")

additive_terms <- rownames(earth::evimp(mars_additive_fit))
additive_num_terms <- additive_terms[!grepl("county", additive_terms)]
df_num_terms <- forested_train |> dplyr::select(all_of(additive_num_terms))
df_medians <- 
  forested_train |> 
  dplyr::select(-class, -county) |> 
  map_dbl(median) |> 
  as_tibble_row() |> 
  mutate(county = forested_train$county[1])

additive_preds <- NULL
for (i in seq_along(additive_num_terms)) {
  nm <- additive_num_terms[i]
  rng <- range(forested_train[[nm]])
  grid <- 
    tibble(x = seq(rng[1], rng[2], length.out = 500)) |> 
    setNames(nm) |> 
    vctrs::vec_cbind(df_medians |> dplyr::select(-all_of(nm)))
  
  prd <- 
    augment(fda_additive_fit, grid, type = "prob") |> 
    dplyr::select(all_of(nm), .pred_Yes) |> 
    setNames(c("value", "probability")) |> 
    mutate(variable = nm)
  
  additive_preds <- bind_rows(additive_preds, prd)
}

additive_preds <- 
  additive_preds |> 
  full_join(name_key, by = "variable") |> 
  mutate(Predictor = if_else(is.na(text), variable, text)) |> 
  filter(!is.na(probability))

additive_preds |> 
  ggplot(aes(value, probability)) + 
  geom_line() + 
  facet_wrap(~ Predictor, scale = "free") + 
  theme() +
  theme(axis.text.y = element_blank()) +
  labs(y = "Relative Probability", x = NULL)
```

For example, we can say that: 

- The probability of forestation appears to jump after 2020. 
- When the annual precipitation at a location is greater than 500, the probability significantly increases and remains high (all other factors being equal). 
- The downward spike in longitude corresponds to the unforested area east of Seattle.

and so on. 

`r r_comp("cls-nonlinear.html#sec-fda")`

## K-Nearest Neighbors  {#sec-cls-knn}

K-nearest neighbors (KNN) is predicated on the idea that data points that are close to one another in the predictor space should have outcomes that are also similar. If we are predicting a new sample, we find the _K_ training set points that are "closest" to the new data point and use their outcome values to estimate the prediction. There is essentially no training set here; everything happens when the sample is being predicted. KNN falls into a class of techniques called instance-based learning since it stores the training set instances and directly uses these data during prediction.  

What does "similar" and/or "close" mean? The operational definition of "proximity" or "closeness" requires specifying either a distance or similarity metric. While Euclidean distance is a common metric, the Minkowski distance provides a more flexible framework. For a new observation $\boldsymbol{u}$ with $p$ predictors, the Minkowski distance is defined as:

$$
d(\boldsymbol{u}, \boldsymbol{x}) ={\biggl (}\sum _{j=1}^{p}|u_j-x_{j}|^{degree}{\biggr )}^{\frac {1}{degree}},
$$ 

where $degree$ is the distance parameter. This formulation yields Euclidean distance when $degree = 2$, Manhattan distance (also called taxicab or city-block distance) when $degree = 1$, and intermediate distance metrics for other values of $degree$.

The choice of distance metric can substantially influence model performance and should be considered a tuning parameter in practice.

Beyond the Minkowski family, several specialized distance metrics may be appropriate depending on the structure and nature of the predictor space:

* Geodesic distance (discussed in @sec-isomap) is based on a general distance matrix to construct a graph-based representation of inter-point relationships.

* Haversine distance (introduced in @sec-centroids) accounts for the curvature of the Earth's surface and is the appropriate metric when predictors represent geographic coordinates (latitude and longitude).

* Mahalanobis distance [@JohnsonWichern; @DeMaesschalck20001] incorporates the covariance structure among predictors, effectively scaling predictor differences by their variances and correlations. This metric is particularly valuable when predictors exhibit substantial correlation and sufficient data are available to reliably estimate the covariance matrix. The Mahalanobis distance has a direct connection to the mathematics of discriminant analysis.

These distance-based metrics require numeric predictors. For the Minkowski distance, predictors must be expressed in the same units to prevent variables with larger scales from dominating the distance calculation. This is typically accomplished through centering and scaling (standardization) or other normalization techniques. Additionally, addressing skewness through appropriate transformations (e.g., Box-Cox, Yeo-Johnson, orderNorm, etc.) prior to distance calculation is recommended, as highly skewed distributions can distort relationships and degrade model performance.

For qualitative predictors, one approach is to encode them as indicator (dummy) variables, which should then be preprocessed in the same way as continuous predictors. However, when the predictor set contains a mixture of data types, Gower distance [@gower] provides a unified framework that accommodates different variable types without requiring uniform encoding.  Gower distance computes a dissimilarity measure separately for each predictor according to its data type, then aggregates these component distances:

$$
d(\boldsymbol{u}, \boldsymbol{x}) =\frac {1}{p}\sum _{j=1}^{p}d(u_j, x_{j}),
$$

where $d_j$ denotes the distance function for predictor $j$.  For continuous predictors, the component distance is range-normalized:

$$
d(u_j, x_{j})=1-{\frac {|u_{j}-x_{j}|}{R_{j}}}
$$

where $R_j$ represents the range of predictor $j$ in the training set. This normalization ensures all continuous predictors contribute equally regardless of their original scales.

For categorical predictors, the component distance is a simple mismatch indicator, $d(u_j, x_{j}) = I(u_j = x_j)$.  This binary metric assigns zero distance for matching categories and unit distance otherwise.

For a comprehensive treatment of distance metrics in the context of nearest neighbor methods, see @cunningham2021k.

For classification problems, KNN identifies the _K_ nearest neighbors and estimates class probabilities as the proportion of neighbors belonging to each class:

$$
\widehat{\pi}_{c} = \frac{1}{K}\sum_{k=1}^K y_{kc}
$$

where $y_{kc}$ is a binary indicator equal to 1 if neighbor $k$ belongs to class $c$, and 0 otherwise.  For small values of K, these empirical probability estimates can be unreliable, particularly when certain classes are underrepresented among the neighbors. The Laplace correction (previously discussed in @sec-naive-bayes) can be used to regularize the estimate:

$$
\widehat{\pi}_{c} = \frac{1}{K}\sum_{k=1}^K \frac{y_{kc} +\alpha}{K + \alpha C}
$$

where $C$ denotes the number of classes and $\alpha \in [0, 2]$ is a smoothing parameter. Setting $\alpha = 0$  yields the unregularized estimate, while $\alpha > 0$ shrinks extreme probabilities toward a uniform distribution.

It is common to fix _K_ and collect all neighbors, regardless of their distances to the query point. Alternatively, one can put a cap on the maximum distance or use a more informative approach of using weighted distances:

$$
\widehat{p}_{c} = \frac{1}{W}\sum_{k=1}^K g(d_k)\;y_{kc}
$$

where $g()$ is a kernel function that weights the distances and $W$ is the sum of the $g(d_k)$ [@dudani1976distance;@zuo2008kernel;@Samworth]. @fig-weight-kernels shows some commonly used functions [@hechenbichler2004weighted]. 


```{r}
#| label: fig-weight-kernels
#| echo: false
#| warning: false
#| message: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 2.75
#| fig-cap: Different kernel functions to downweight far-away neighbors. Values have been rescaled to have the same upper value.  
dists <- 
  crossing(
  distance = seq(0.1, 1, length.out = 100),
  kernel = c(
    "Rectangular",
    "Triangular",
    "Cosine",
    "Gaussian",
    "Inverse"
  )
) |> 
  mutate(
    weight = 
      case_when(
        kernel == "Rectangular" ~ 1 /2,
        kernel == "Inverse"~ 1 / distance,
        kernel == "Triangular"~ 1 - distance,
        kernel == "Epanechnikov" ~ 3 / 4 * (1 - distance^2),
        kernel == "Cosine" ~ pi / 4 * cos(pi * distance / 2),
        TRUE ~ dnorm(distance)
      )
  )

max_weight <- 
  dists |> 
  summarize(max_wt = max(weight), .by = c(kernel))

dists |> 
  right_join(max_weight, by = "kernel") |> 
  mutate(
    weight = weight / max_wt
  ) |> 
  ggplot(aes(distance, weight)) + 
  geom_line() + 
  facet_wrap(~ kernel, nrow = 1) + 
  lims(y = extendrange(0:1)) +
  labs(x = "Distance to Neighbor", y = "Weight")
```  

Outside of preprocessing methods, the primary tuning parameters are the number of neighbors, the weighting function, and the distance degree (if using a Minkowski distance). Recall from @fig-irrelevant-predictors that KNN models can be harmed when used with irrelevant predictors. As will be seen with these data, tuning an initial feature selection filter can improve model quality. 

```{r}
#| label: forested-knn
#| include: false
#| cache: true

knn_spec <- 
  nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) |> 
  set_mode("classification")

encode_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_predictor_best(all_numeric_predictors(), score = "imp_rf", prop_terms = tune()) |> 
  step_lencode_mixed(county, outcome = "class") |>
  step_orderNorm(all_numeric_predictors())

knn_wflow <- workflow(encode_rec, knn_spec)

kern_vals <- c("rectangular", "triangular", "cos", "gaussian", "inv")

knn_param <- 
  knn_wflow |> 
  extract_parameter_set_dials() |> 
  update(
    neighbors = neighbors(c(2, 50)),
    weight_func = weight_func(kern_vals)
  )

cls_mtr <- metric_set(brier_class, roc_auc, pr_auc, mn_log_loss)

set.seed(872)
forest_knn_res <-
  knn_wflow |>
  tune_grid(
    resamples = forested_rs,
    grid = 50,
    param_info = knn_param,
    metrics = cls_mtr,
    control = control_grid(
      save_pred = TRUE,
      save_workflow = TRUE,
      parallel_over = "everything"
    )
  )

knn_best_res <- show_best(forest_knn_res, metric = "brier_class")

forest_knn_mtr <-
  forest_knn_res |> 
  collect_metrics(summarize = FALSE) |> 
  mutate(wflow_id = "knn") |> 
  relocate(wflow_id)

forest_knn_best <- select_best(forest_knn_res, metric = "brier_class")
forest_knn_best_roc <- 
  forest_knn_res |> 
  collect_metrics() |> 
  inner_join(forest_knn_best |> dplyr::select(.config), by = ".config") |> 
  dplyr::filter(.metric == "roc_auc") |> 
  pluck("mean") |> 
  signif(digits = 3)

forest_knn_pred <-
  forest_knn_res |> 
  collect_predictions(parameters = forest_knn_best) |> 
  mutate(wflow_id = "knn") |> 
  relocate(wflow_id)
```

How well does this technique work for the forestation data? To start, we used preprocessing that included the same  random forest importance filter as @sec-naive-bayes (and tune the proportion fo predictors retained). After that, an effect encoding was used for the county column, and then applied an orderNorm transformation to all predictors to standardize them to the same units and potentially remove skewness. The model was tuned over the proportion of features retained (5% to 100%), the number of neighbors (two through fifty), the Minkowski degree parameter (between 0.1 and 2.0), and the kernel functions listed in @fig-weight-kernels. A total of fifty configurations were evaluated using resampling, and the Brier scores are shown in @fig-knn-cls-tune. 

```{r}
#| label: fig-knn-cls-tune
#| echo: false
#| warning: false
#| message: false
#| out-width: 80%
#| fig-width: 8
#| fig-height: 3
#| fig-cap: K-nearest neighbors tuning results for the forestation data.  

knn_best_res <- show_best(forest_knn_res, metric = "brier_class")
autoplot(forest_knn_res, metric = "brier_class") + 
   labs(y = "Brier Score")
```

Poor results occurred when there were fewer than twenty neighbors, and subsequently, the Brier scores remained consistently low. The distance parameter appeared to favor small values (around 0.5). There are no strong trends in terms of which kernel functions; Gaussian kernels appear to do well overall, but there are configurations that do well using any kernel. Numerically, the best results retained `r round(knn_best_res$prop_terms[1] * 100, 1)`% of the  predictors, used `r knn_best_res$neighbors[1]` neighbors, a Minkowski degree of `r signif(knn_best_res$dist_power[1], digits = 3)`, and a `r knn_best_res$weight_func[1] ` kernel function. In the figure above, we can see that there is a small increase in error when the entire predictor set is used. Otherwise, smaller feature sets are underfitting the data. The corresponding Brier score was `r signif(knn_best_res$mean[1], digits = 3)`, which is very competitive. The area under the ROC curve was also good, with a resampling estimate of `r forest_knn_best_roc`.

## Sidebar: Dot Products

Dot products will become important in several subsequent sections of this chapter. While distance metrics (such as those used in KNN) provide one framework for comparing vectors, dot products offer another perspective that captures different geometric properties. Understanding their relationship requires some basic linear algebra and geometry.

Vectors are characterized by two fundamental properties:

- Magnitude (or length) is denoted $||\boldsymbol{a}||$ for a vector $\boldsymbol{a}$ with $p$ elements. The standard measure is the Euclidean norm, computed as the square root of the sum of squared elements: $||\boldsymbol{a}|| = \sqrt{\sum_{i=1}^p a_i^2}$.

- Direction is defined relative to another reference (say another vector $\boldsymbol{y}$ with the same dimension ($p$)), typically quantified by the angle $\vartheta$ between two vectors of equal dimension.

Distance-based methods like KNN depend solely on the magnitude of the difference between vectors and do not consider the angle between them.

The dot product (or inner product) of two $p$-dimensional vectors is defined as:

$$
\boldsymbol{a}'\boldsymbol{b} = \sum_{i=1}^p a_ib_i
$$  {#eq-dot-prod-sum}

Note that the dot product does not satisfy the properties of a distance metric since the distance between itself could never be zero unless all of its elements are zero. 

Dot products measure both the magnitude of each vector as well as the angle between them ($\vartheta$). We can rewrite @eq-dot-prod-sum as

$$
\boldsymbol{a}'\boldsymbol{b} = ||\boldsymbol{a}||\,||\boldsymbol{b}|| \cos{\vartheta}
$$  {#eq-dot-prod-vec}

This formulation reveals that:  The dot product is maximized when vectors are parallel ($\vartheta = 0^{\circ}$, $\cos\vartheta = 1$), it equals zero when vectors are orthogonal ($\vartheta = 90^{\circ}$, $\cos\vartheta = 0$), and it is negative when vectors point in generally opposite directions ($\vartheta > 90^{\circ}$).  This dual sensitivity to both magnitude and direction distinguishes dot products from distance-based comparisons and makes them particularly useful in certain modeling contexts.

To illustrate, @fig-dot-products shows the dot products of a vector $\boldsymbol{x} = (2.0, 0.5)$ to a grid of vectors with elements ranging between $\pm 3$. We can see a diagonal region of light colored values that have dot products close to or at zero. This represents vectors that are orthogonal to $\boldsymbol{x}$. There is also a dotted line extending to the origin (0.0, 0.0) that coincides with this line.  

```{r}
#| label: dot-products
#| include: false

lin <- kernlab::vanilladot()

x_seq <- seq(-3, 3, by = 0.1)
x_n <- length(x_seq)
grid <- crossing(x = x_seq, y = x_seq)
grid_mat <- as.matrix(grid)

ref_x <- 2.0
ref_y <- 0.5
ref_df <- tibble(x = ref_x, y = ref_y)
ref_vec = c(ref_x, ref_y)
ref_prod <- t(ref_vec) %*% ref_vec
ref_prod <- lin(ref_vec, ref_vec)

ex_point <- c(1, 0)
ex_prod <- lin(ex_point, ref_vec)
ex_dist <- rbind(ex_point, ref_vec) |> dist() |> as.numeric()

lin_kern <- kernelMatrix(lin, grid_mat, matrix(ref_vec, nrow = 1))

lin_df <- tibble(
  `dot product` = as.vector(lin_kern),
  x = rep(x_seq, each = x_n),
  y = rep(x_seq, x_n)
)
```

```{r}
#| label: fig-dot-products
#| echo: false
#| out-width: 50%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: Dot product values of a grid of vectors relative to $\boldsymbol{x} = (2.0, 0.5)$ (the black point). The dashed black line shows the contour of constant dot products and the white dotted line is points to the origin.
#| warning: false

lin_df |>
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = `dot product`)) +
  geom_segment(
    aes(y = 0, x = 0, yend = ref_y, xend = ref_x),
    lty = 3, col = "white"
  ) +
  geom_point(data = ref_df, col = "black", cex = 2) +
  geom_contour(aes(z = `dot product`), breaks = ref_prod, col = "black", lty = 2) +
  coord_fixed() +
  paletteer::scale_fill_paletteer_c("grDevices::Geyser") +
  labs(x = expression(x[1]), y = expression(x[2])) +
  theme(legend.position = "top")
```

What is the dot product of $\boldsymbol{x}$ with itself? That value is $\boldsymbol{x}'\boldsymbol{x} =$ `r ref_prod`. The dashed black diagonal line shows which vectors have the same dot product as $\boldsymbol{x}$. These are points that point in the same direction (i.e., $\vartheta = 0^{\circ}$) and magnitude as our example vector. Note that this line is parallel to the region of zero values, reemphasizing that the zero region is orthogonal to vectors like $\boldsymbol{x}$. 

The vector associated with the largest dot product would move in the same direction as $\boldsymbol{x}$. Since  $||\boldsymbol{x}||$ is fixed and $\cos(\vartheta) = 0^{\circ}$, the magnitude of the vector would be as large as possible. Conversely, the vector with the smallest dot product will move in the opposite direction and as far as possible. For our $\boldsymbol{x}$, the dot product is maximized in the upper right corner of @fig-dot-products and minimized in the lower left corner. 

We'll see the dot product again in @sec-attention where it is intended to measure the similarity of two embedding vectors within a neural network, and again in @sec-cls-svm where it is the computational backbone of support vector machines. 

## Neural Networks via Multilayer Perceptrons {#sec-cls-nnet}

Neural networks are complex models comprised of multiple hierarchies of _latent variables_ designed to represent different motifs in the data. The model's architecture consists of a series of _layers_, each containing a set of units. Neural networks begin with the input layer that contains units representing our predictors. After this are one or more _hidden layers_, each containing a preset number of hidden units. These are artificial features that are influenced by the layer before.  For example, if there were four predictors and the first hidden layer had two hidden units, each hidden unit is defined by an equation that includes each of the predictors. In the broadest sense, it is similar to how, in PCA, each principal component is a linear combination of the predictors. However, the similarities between neural networks and PCA end there. 

These connections between units on different layers continue as layers are added. Eventually, the last layer is the _output layer_. For classification, this usually has as many units as there are class levels in the outcome. @fig-nnet-cls shows a general architectural diagram for a basic model with a single hidden layer, where the lines indicate that a model parameter exists to connect the nodes. 

```{r}
#| label: fig-nnet-cls
#| echo: false
#| out-width: 40%
#| fig-width: 4
#| fig-height: 5
#| fig-cap: Architecture of a simple neural network with a single layer of hidden units.
knitr::include_graphics("../premade/mlp.svg")
```

Traditionally, the connections that come into a hidden unit are combined using a linear predictor: each incoming unit has a corresponding slope, called a "weight" in the neural network literature^[For consistency, we'll use "slopes" and "intercepts" here.]. The linear combinations commonly include an intercept term (called a "bias"). These linear predictors are embedded within a nonlinear "activation" function ($\sigma$) to enable complex patterns. An example of an activation function would be a sigmoidal function, much like the logistic function used in @eq-logistic. 

::: {.note-box}
We will primarily be concerned with multilayer perceptrons (MLPs), a subset of the broader class of neural networks. MLPs are feed-forward models, meaning each layer only connects to the subsequent layer; there are no feedback loops where future units connect to units in previous layers.

As a counterexample, residual networks [@he2016deep] are connected to previous layers (a.k.a. skip layers); therefore, they are not multilayer perceptrons. Another popular example is the long-term short memory model for sequential data such as time series [@hochreiter1997long]. 
:::

<br> 

Let's walk through the network structure, starting at the input layer. We have predictors $x_1, \ldots x_p$. These will be mathematically connected to the next layer containing $m$ hidden units $H_\ell$ ($\ell=1, \ldots m$). First, we create a separate linear predictor for hidden unit $H_l$ denoted as $\beta_{\ell 0} + \beta_{\ell 1} x_{1} + \ldots + \beta_{\ell p} x_{p}$. We then embed this inside the activation function $\sigma()$ used for this layer so that there is a nonlinear relationship between the predictors and their new representation. Visually^[
This representation and the one below were adapted from Izaak Neutelings's excellent work, which can be found at [`https://tikz.net/neural_networks/`](https://tikz.net/neural_networks/) and is licensed under a CC BY-SA 4.0.]:

```{r}
#| label: nnet-start
#| echo: false
#| out-width: 80%
#| fig-width: 4
#| fig-height: 5
knitr::include_graphics("../premade/nnet_start.svg")
```

As we add more layers, each of the first set of hidden units is similarly connected to the hidden units in the next layer, and so on. The layers do not have to use the same activation function. Also, the layers can be specialized, such as the image processing layers shown in @fig-vgg16. For each hidden layer, we have to specify the number of units and the activation function. These, along with the number of layers, are tuning parameters. 

Note that the units in the subsequent hidden layer are nested inside linear combinations of the previous hidden layers (plus the input units). These models have sets of nested nonlinear functions and can be very complex, but are mathematically tractable. 

For the final layer, the previous set of hidden units is often connected to the _output units_ using a linear activation. For classification, there are often $C$ output units (i.e., one for each class):

```{r}
#| label: nnet-end
#| echo: false
#| out-width: 80%
#| fig-width: 4
#| fig-height: 5
knitr::include_graphics("../premade/nnet_end.svg")
```

Since linear activation is used for the final layer, the values of the output units ($Y$) are unlikely to be in the proper units for classification. To remedy this, the raw outputs are normalized to be probability-like via the softmax function: 

$$
\pi_{k} = \frac{\exp(Y_{k})}{\sum\limits_{k=1}^C \exp(Y_{k})}
$$

In this section, we'll describe the important elements of multilayer perceptrons, such as the different types of activation functions. We'll walk through a small but specific model. 

In later sections, we'll also spend a considerable amount of time discussing the various methods for training MLPs. There are also discussions on modern deep learning models that are specifically designed for tabular data. 

### Activation Functions {#sec-nnet-activation}

```{r}
#| label: act-grid
#| include: false
lp_grid <- seq(-3, 3, length.out = 100)

act_functions <- 
  bind_rows(
    tibble(input = lp_grid,
           output = binomial()$linkinv(lp_grid),
           activation = "logistic"),
    tibble(input = lp_grid,
           output = tanh(lp_grid),
           activation = "tanh"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_relu(lp_grid)),
           activation = "relu"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_elu(lp_grid)),
           activation = "elu"),
    tibble(input = lp_grid,
           output = lp_grid - tanh(lp_grid),
           activation = "tanhshrink"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_softshrink(lp_grid)),
           activation = "softshrink"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_gelu(lp_grid)),
           activation = "gelu"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_silu(lp_grid)),
           activation = "silu"),
    tibble(input = lp_grid,
           output = as.numeric(nn_softplus()(lp_grid)),
           activation = "softplus")
  )
```  

Historically, the most commonly used nonlinear activation functions were sigmoidal. This means that the linear combination of the elements in the previous layer was converted into a "squashed" format, typically bounded (e.g., logistic activation values range between 0 and 1). @fig-sigmoid-activations shows several such functions. The logistic (@eq-logistic) function and hyperbolic tangent are fairly similar in shape, and were typical defaults. 

```{r}
#| label: fig-sigmoid-activations
#| echo: false
#| out-width: 65%
#| fig-width: 6
#| fig-height: 2
#| fig-cap: "Different sigmoidal activation functions."

sig_lvl <- c("logistic", "tanh", "tanhshrink")

act_functions |>  
  filter(grepl("(tan)|(sig)|(logistic)", activation)) |> 
  mutate(activation = factor(activation, levels = sig_lvl)) |> 
  ggplot(aes(input, output)) + 
  geom_line() +
  facet_wrap(~ activation, scale = "free_y", nrow = 1) + 
  theme_bw()
```

One issue with bounded sigmoidal shapes, particularly the logistic function, is that they can become problematic when used in conjunction with many hidden layers. Since the logistic function is on [0, 1], repeated multiplication of such values can cause their values to converge to be very close to zero (as seen in @sec-naive-bayes). This is also true of their gradients, leading to the _vanishing gradient problem_. This is likely not an issue with shallow networks that have a handful of hidden layers.

The tanshrink function diminishes the impact of inputs that are in a neighborhood around zero, where it is sigmoidal. For larger input values, the output is linear in the output (with an offset of $\pm 1$). 

In modern neural networks, the rectified linear unit (ReLU) is the default activation function. As shown in @fig-relu-activations, the function resembles the hinge functions used by MARS; to the left of zero, the output is zero, and otherwise, it is linear. Much like MARS, this has the effect of isolating some region of the predictor space. However, there are a few key differences. MARS hinge functions affect a single predictor at a time, while neural networks' activation functions act on the linear combination of parameters. Also, neural networks with multiple layers recursively activate linear combinations inside the prior layers (and so on). 

```{r}
#| label: fig-relu-activations
#| echo: false
#| out-width: 90%
#| fig-width: 8
#| fig-height: 2
#| fig-cap: "Different ReLU-based activation functions."

relu_lvl <- c("relu", "softplus", "gelu", "silu", "elu")

act_functions |>  
  filter(grepl("(l[uU]$)|(softplus)", activation)) |> 
  mutate(activation = factor(activation, levels = relu_lvl)) |> 
  ggplot(aes(input, output)) + 
  geom_hline(yintercept = 0, lty = 3) + 
  geom_line() +
  facet_wrap(~ activation, nrow = 1) + 
  theme_bw()
```

There are various modified versions of ReLU that emulate the same form but have continuous derivatives. These are also shown in @fig-relu-activations, including softplus [@dugas2000incorporating;@glorot2011deep], gelu [@hendrycks2016gaussian], silu [@elfwing2018sigmoid],  elu [@clevert2015fast], and others. ReLU, and its offspring activation functions, appear to solve the issue of vanishing gradients. 

```{r}
#| label: nnet-ex-comps
#| include: false
#| cache: true
two_class_ex <- 
  parabolic |> 
  set_names(c("A", "B", "Class")) 

two_class_ex <-   
  recipe(Class ~ ., data = two_class_ex) |> 
  step_orderNorm(A, B) |> 
  prep() |> 
  bake(new_data = NULL)

x <- seq(-4, 4, length.out = 100)
grid <- crossing(A = x, B = x)

set.seed(123)
fit_1 <- brulee_mlp(
  Class ~ A + B,
  data = two_class_ex,
  hidden_units = 3,
  penalty = 0,
  activation = "relu",
  verbose = TRUE
)

coefs <- coef(fit_1)

grid_pred <- 
  fit_1 |> 
  predict(grid, type = "prob") |> 
  bind_cols(grid)

get_elements <- function(x, model) {
  xtbl <- x
  x <- as.matrix(x)
  lp_01 <- x %*% t(coefs$model.0.weight) + coefs$model.0.bias
  colnames(lp_01) <- paste0("lp_", 1:ncol(lp_01))
  layer_01 <- as.matrix(torch::nnf_relu(lp_01))
  colnames(layer_01) <- paste0("relu_", 1:ncol(layer_01))
  lp_12 <- layer_01 %*% t(coefs$model.2.weight) + coefs$model.2.bias
  colnames(lp_12) <- paste0("output_", 1:ncol(lp_12))
  prob <- exp(lp_12) / sum(exp(lp_12))
  bind_cols(xtbl, as_tibble(lp_01), as_tibble(layer_01), as_tibble(lp_12))
}

components <- NULL
for (i in 1:nrow(grid)) {
  components <- bind_rows(components, get_elements(grid[i,], fit_1))
}
```

Let's use an example data set with a simple MLP to demonstrate how ReLU activation works. @fig-nnet-ex displays the results of a model with a single layer comprised of three hidden units, utilizing ReLU activation. The model converged in `r fit_1$best_epoch` epochs and does a good job discriminating between the two classes. 

```{r}
#| label: fig-nnet-ex
#| echo: false
#| out-width: 50%
#| fig-width: 5
#| fig-height: 4.4
#| warning: false
#| fig-cap: "An example data set with two predictors and two classes. The black line is the class boundary from an MLP model using three hidden units with ReLU activation."

  grid_pred |> 
  ggplot(aes(A, B)) +
  geom_point(data = two_class_ex, aes(col = Class), alpha = 1/3) +
  geom_contour(aes(z = .pred_Class1), breaks = 0.5, col = "black") +
  coord_fixed() +
  scale_color_manual(values = c("#007FFFFF", "#FF7F00FF"))
```

With two predictors and three hidden units, the first set of coefficients for the model is three sets of linear predictors (i.e., an intercept and two slopes) for each of the three hidden units. The predictors were normalized to have the same mean and variance (zero and one, respectively). The three equations produced during training were: 

```{r}
#| label: nnet-ex-first
#| echo: false
#| results: "asis"
wt_1 <- format(coefs$model.0.weight, digits = 1)
wt_1 <- gsub("^ ", "+", wt_1)
wt_1[,1] <- paste0(wt_1[,1], "\\:A")
wt_1[,2] <- paste0(wt_1[,2], "\\:B")
lp_1 <- 
  cbind(
    format(coefs$model.0.bias, digits = 2),
    wt_1
  )
lp_1 <- apply(lp_1, 1, function(x) paste0(x, collapse = ""))
lp_1 <- paste0("H_{", 1:3, "}&=", lp_1, " \\notag")

cat("$$")
cat("\\begin{align}")
cat(paste0(lp_1, collapse = " \\\\ \n"))
cat("\\end{align}")
cat("$$")
```

The intercept estimate for the first hidden unit is significantly larger than those of the other two. That unit also has a large positive slope for predictor A. The second hidden unit emphasizes predictor B, while $H_{i3}$ has strong positive effects for both predictors. 

The top of @fig-nnet-ex-layer-1 displays a heatmap illustrating how these three artificial features vary across the two-dimensional predictor space (before activation). The black line segments show the contour corresponding to zero values. The first hidden unit has larger values on the right-hand side of the predictor space, with a slightly diagonal contour for zero. The pattern shown for the second hidden unit is nearly the opposite of the first, although the zero contour cuts more through the center of the space. Finally, the third unit is driven by both predictors with larger values in the upper right region. 

```{r}
#| label: fig-nnet-ex-layer-1
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 5
#| warning: false
#| fig-cap: "Heatmaps of the three linear predictors and their ReLU values across the predictor space. The black line emphasizes the location where the linear predictor is zero."

lp_layer <- 
  components |> 
  dplyr::select(-starts_with("relu_"), -starts_with("output")) |> 
  pivot_longer(cols = c(starts_with("lp_"))) |> 
  mutate(name = gsub("lp_", "hidden unit ", name)) |> 
  ggplot(aes(A, B)) + 
  geom_tile(aes(fill = value), show.legend = TRUE) + 
  geom_contour(aes(z = value), breaks = 0, col = "black") +
  facet_wrap(~ name, nrow = 1) + 
  scale_fill_gradient2(high = "#5D74A5FF") + 
  # coord_fixed() + 
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5),
    legend.title = element_blank()
  ) + 
  labs(title = "Linear Predictor")

relu_layer <- 
  components |> 
  dplyr::select(-starts_with("lp"), -starts_with("output")) |> 
  pivot_longer(cols = c(starts_with("relu_"))) |> 
  mutate(name = gsub("relu_", "hidden unit ", name)) |> 
  ggplot(aes(A, B)) + 
  geom_tile(aes(fill = value), show.legend = TRUE) + 
  geom_contour(aes(z = value), breaks = 0, col = "5AAE61FF") +
  facet_wrap(~ name, nrow = 1) + 
  scale_fill_gradient2(high = "#5D74A5FF") + 
  # coord_fixed() + 
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5),
    legend.title = element_blank()
  ) + 
  labs(title = "ReLU") 

lp_layer / relu_layer
```

The figure's bottom panels illustrate the effect of the ReLU function, where values less than zero are set to zero. At this point, $H_{i1}$ and $H_{i3}$ have the most substantial effect on the model, but in somewhat overlapping regions.

However, the model estimates additional parameters that will merge these regions using weights that are estimated to be most helpful for predicting the data. Those equations were estimated to be: 

```{r}
#| label: nnet-ex-last
#| echo: false
#| results: "asis"
wt_2 <- format(coefs$model.2.weight, digits = 1)
wt_2 <- gsub("^ ", "+", wt_2)
wt_2[,1] <- paste0(wt_2[,1], "H_{i1}")
wt_2[,2] <- paste0(wt_2[,2], "H_{i2}")
wt_2[,3] <- paste0(wt_2[,3], "H_{i3}")
lp_2 <- 
  cbind(
    format(coefs$model.2.bias, digits = 2),
    wt_2
  )
lp_2 <- apply(lp_2, 1, function(x) paste0(x, collapse = ""))
lp_2 <- paste0("Y_{i", 1:2, "}&=", lp_2, " \\notag")

cat("$$")
cat("\\begin{align}")
cat(paste0(lp_2, collapse = " \\\\ \n"))
cat("\\end{align}")
cat("$$")
```

Note that the coefficients' signs are opposite for each outcome unit. @fig-nnet-ex-layer-2 illustrates the transition from the hidden layer to the outcome layer. The weighted sum of the three variables at the top produces the regions of strong positive (or negative) activation for the two classes. Once again, the black curve indicates the location of the zero contour. 

```{r}
#| label: fig-nnet-ex-layer-2
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 5
#| warning: false
#| fig-cap: "Heatmaps of the three ReLU units and their output units across the predictor space."

output_layer <- 
  components |>
  dplyr::select(-starts_with("lp"), -starts_with("relu_")) |>
  pivot_longer(cols = c(starts_with("output"))) |>
  ggplot(aes(A, B)) +
  geom_tile(aes(fill = value)) +
  geom_contour(
    # data = grid_pred,
    aes(z = value),
    breaks = 0,
    col = "black"
  ) +
  facet_wrap(~ name, nrow = 1) + 
  scale_fill_gradient2(high = "#420F75FF", low = "#EAC541FF") + 
  coord_fixed() + 
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5),
    legend.title = element_blank()
  )  + 
  labs(title = "Output", y = NULL) 

relu_layer / output_layer
```

The two output panels at the bottom of the figure show nearly opposite patterns, which is not coincidental. The model converges to this structure, allowing the output units to favor one class or the other. The legend on the bottom right clearly shows inappropriate units; we'd like them to resemble probabilities. As previously mentioned, the softmax function coerces these values to be on [0, 1] and sum to one. This produces the class boundary seen in @fig-nnet-ex. 

As previously stated^[This is a missing reference; the comment is directed towards that (unmerged) content on [MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline) (=multivariate adaptive regression spline) "hinge" functions, which are similar to ReLU.] in @sec-fda, ReLU is not a smooth function, so this demonstration will somewhat differ from the other activation functions; they generally don't completely isolate regions of the predictor space. However, the overall flow of the network remains the same. 

Now that we've discussed the architecture of these models and how the layers relate to one another, let's examine how to estimate model parameters effectively.  

### Training and Optimization {#sec-nnet-training}

Training neural networks can be challenging. Their model structure is composed of multiple nested nonlinear functions that may not be smooth or continuous. The loss function for classification, commonly cross-entropy, is unlikely to be convex as model complexity increases with the addition of more layers. Consequently, we have no guarantees that a gradient-based optimizer will converge. There is also the possibility of local minima that might trap the training process, saddle points that could lead the search in the wrong direction, and regions of vanishing gradients [@hochreiter1998vanishing]. 

First is the challenge of local minima. In some cases, ML models can provide some guarantees regarding the existence of a global minimum loss value^[Generalized linear models are a good example of this]. This may not be the case for neural networks. Their loss surface may contain multiple valleys where the derivatives of some of the parameters are zero, but a better solution exists in another region of the parameter space. In other words, the model can appear to converge but has suboptimal performance. @baldi1989neural show that, under some conditions, single-layer feedforward networks can have a unique global minimum. It is expected that as more layers and units are added, the probability of finding a unique optimal solution decreases. When "caught" in a local valley, we need our optimization algorithm to be able to escape. 

A second related issue is saddle points. These are locations where all of the gradients are zero (i.e., a stationary point), but some directions have increasing loss, and others will produce a decrease. @dauphin2014identifying postulate that these will exist, and their propensity to occur increases with the number of model parameters. The concern here is that, depending on the type of multidimensional curvature encountered, the optimization algorithm may move in the direction of _increasing loss_ or become stuck. 

@fig-stationary has an example with two parameters and a highly nonlinear loss surface. There are four stationary points, the best of which minimizes the loss function at $\boldsymbol{\theta} = [`r round(stationary$x[stationary$type == "minimum"], 1)`, `r round(stationary$y[stationary$type == "minimum"], 1)`]$. The lower right region has a point associated with maximum loss. The other two locations are saddle points. The point at $\boldsymbol{\theta} = [`r round(stationary$x[stationary$type == "saddle point" & stationary$x > 6], 1)`, `r round(stationary$y[stationary$type == "saddle point" & stationary$x > 6], 1)`]$ is a good example; moving to the left or right of the point will increase the loss. Moving up or down will decrease the loss (with the upper value being the correct choice in this parameter range). Our optimizer should be able to move in the proper direction or, like simulated annealing, recover and move back to progressively better results. 

::: {#fig-stationary}

::: {.figure-content}

```{r}
#| label: stationary
#| echo: false
#| out-width: 60%
#| fig-width: 5.5
#| fig-height: 4.7

loss_grid |>
	rename(loss = res) |> 
	ggplot(aes(x, y)) +
	geom_tile(aes(fill = loss)) +
	geom_contour(aes(z = loss), col = "darkred", bins = 7, alpha = 3 / 4) +
	scale_fill_gradient(low = "white", high = "red") +
	scale_shape_manual(values = c(2, 6, 1)) +
	geom_point(
		data = stationary,
		aes(pch = type),
		cex = 2.5,
		col = "black"
	) + 
	coord_fixed(ratio = 5) + 
	labs(x = expression(theta[1]), y = expression(theta[2]))
```

:::

The loss surface for $\psi(\boldsymbol{\theta}) = \theta_1 cos(0.5\,\theta_1) - 2 \exp(-6(\theta_2 - 0.3)^2) + 0.2\, \theta_1 \theta_2$. Four stationary points are shown.

:::

Another possible factor is that many deep learning models are trained on vast amounts of data. This in itself presents constraints on how much data can be held in memory at once, as well as other logistical issues. This may not be an issue for the average application that uses tabular data; however, the problem still persists. 

Before proceeding, recall @sec-gradient-opt where gradient descent was introduced. There are two commonly used classes of gradient-based optimization: 

 - _First-order techniques_: $\quad\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \alpha\,g(\boldsymbol{\theta}_i)$,

- _Second-order methods_: $\quad\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \alpha H(\boldsymbol{\theta_i})^{-1}\,g(\boldsymbol{\theta}_i)$,

where $\boldsymbol{\theta}$ are the unknown parameters, $\alpha$ is the learning rate, $g(\boldsymbol{\theta}_i)$ is the gradient vector of first derivatives, and $H(\boldsymbol{\theta_i})$ is the Hessian matrix of second derivatives. The Hessian measures the curvature of the loss function at the current iteration. Pre-multiplying by its inverse rescales the gradient vector, generally making it more effective. We'll discuss them both in @sec-gradient-descent and @sec-newtons-method. Generally speaking, second-order methods are preferred since they are more accurate _when they are computationally feasible_. However, first-order search is more commonly used because it is faster to compute and can be significantly more memory-efficient.

To demonstrate, we can use an MLP model for the forestation data that contains one layer of 30 ReLU units and an additional layer of 5 ReLU units, yielding `r num_optimizer_param` model parameters. An L<sub>2</sub> penalty of 0.15 was set to regularize the cross-entropy loss function. A maximum of 100 epochs was specified, but training was instructed to stop after five consecutive epochs with a loss increase (measured using held-out data). An initial learning rate of $\alpha = 0.1$ was used, but a variable, step-wise rate schedule was implemented to modulate the rate across epochs, as described below. 

The model was first trained using basic gradient descent. For illustration, training was executed five times using different initial parameter values. The leftmost panel in @fig-optimizers shows the results. In each case, the loss initially decreased in the first set of epochs. However, in each case, the model used all 100 epochs. This approach was not very successful. There was little reduction in the loss function, and the terminal phase had a minuscule decrease with no real improvement. This _could_ be due to the search approaching the saddle point. In this case, the gradient becomes very close to zero, and first-order gradient searches have tiny, ineffective updates. We can measure the efficiency of optimizers by computing the time it takes to train a complete iteration. For gradient descent, the time was `r round(optimizer_times$per_epoch[optimizer_times$method == "GD"], 3)`s per iteration. 

```{r}
#| label: fig-optimizers
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 2.8
#| warning: false
#| fig-cap: "The loss function, using held out data, for the same MLP model for four optimization strategies. The lines on each panel represent runs initialized with different random numbers. The profiles are normalized to start from the same initial loss value."

optimizers |> 
  filter(method != "L-BFGS") |> 
  ggplot(aes(epoch, loss_norm, col = seed, group = seed)) +
  geom_line(show.legend = FALSE,
            linewidth = 3 / 4,
            alpha = 1 / 2) +
  facet_wrap(~ method, scale = "free_x", nrow = 1) +
  scale_x_continuous(breaks = pretty_breaks()) + 
  labs(x = "Epochs", y = "Cross-Entropy (Holdout)")
```

When training neural networks, _stochastic_ gradient descent (SGD) is the standard approach. SGD uses randomly allocated _batches_ training set samples. The gradient search is updated by processing each of these batches until the entire trained set has been used, which signifies the end of that epoch. In other words, a potentially large number of parameter updates are computed based on a small number of training point samples. The batch size used here was 64 data points. As a result, for this training set size, about 76 gradient updates are executed before the end of each epoch. 

For stochastic gradient descent, we track the process by _epochs_. An epoch is finished when the model has been updated using every data point in the training set. For this analysis, this means that there are about 76 parameter updates within an epoch. For regular gradient descent and second-order search methods, well count their progress as _iterations_ since there are no batches. 

As shown in @fig-optimizers, SGD does far better than plain GD and although none stop early, there is a relatively flat trend after about 20 epochs, where the search does not substantially reduce the loss. In terms of time per epoch, SGD takes much longer than GD at`r  round(optimizer_times$per_epoch[optimizer_times$method == "SGD"], 3)`s. 

As we'll see in the section below, many extensions to SGD have made it more effective. One improvement is to use _Nesterov momentum_ [@sutskever2013importance]. This initially computes a standard update to the parameters, then approximates the gradient at this position to compute a second prospective step. The actual update that is used is the lookahead position. This technique yields better results than SGD, with several iterations stopping early. However, the training time was longest, using `r round(optimizer_times$per_epoch[optimizer_times$method == "SGD with momentum"], 3)`s per epoch.  

::: {.note-box}
The main takeaways from this exercise are that training these models can be challenging and that the type of parameter optimization can significantly impact model quality. 
:::

Let's discuss a few operational aspects of gradient-based methods before discussing the first- and second-order search techniques. 

### Initialization {#sec-nnet-initialization}

Network parameter initialization can significantly impact the success of training. Given the size and complexity of these models, it is nearly impossible to analytically derive optimal methodologies for setting initial values. As such, parameters are initialized using random numbers. There are a few approaches to do this. @lecun2002efficient initialization uses a uniform distribution centered around zero with limits defined by the number of connections feeding into the current node (known as the "fan in" and is denoted as $m_{in}$). They used $U(\pm \sqrt{2/m_{in}})$ as the distribution. There have been a few variations of this: 

 - Glorot/Xavier initialization [@glorot2010understanding]: $U(\pm \sqrt{6/(m_{in}+m_{out})})$ 
 - He/Kaiming initialization [@he2015delving]: $N(0, \sqrt{2 / m_{in}})$.

One reason that initialization is important is to help mitigate numerical overflow errors. For example, the softmax transformation shown above exponentiates the raw output units (and sums them). If the output units have relatively large values, exponentiating and summing them can exceed allowable tolerances. For example, the output units shown in @fig-nnet-ex-layer-2 have a wide enough range that can be as large as `r max(round(abs(components$output_1)), 0)`. 

### Preprocessing {#sec-nnet-preprocessing}

Neural networks require complete, numeric features for computations. 

For non-numeric data, the tools described in @sec-categorical-predictors can be very effective. Additionally, there are pretrained neural network models that can translate the category string to a word embedding [@Boykis_What_are_embeddings_2023], such as  BERT [@devlin2019bert] or GPT-4 [@achiam2023gpt]. The potential upside to doing this is that the resulting numeric features may measure some relative context between categorical values. For example, suppose we have a predictor with levels `red`, `darkred`, `green`, `chartreuse`, and `white`. Basic encoders, such as binary indicators or hashing features, would not recognize that there are three clusters in these values and that sets such as {`red`, `darkred`} and {`green`, `chartreuse`} should have values that are "near" one another. LLMs can measure this semantic similarity and potentially generate more meaningful features. 

If the non-numeric predictors are not complex, a layer could be added early in the network, enabling the model to derive encodings based on the data when training. @guo2016entity describes a relatively simple architecture for doing this. While LLMs are better at creating embeddings overall, they are unaware of the _context_ in your data. Estimating the parameters associated with a simple embedding layer may be more effective, as it is optimized for your dataset. 

For numeric predictors, we suggest avoiding irrelevant predictors and highly collinear feature sets. @fig-irrelevant-predictors in @sec-feature-selection demonstrated the harm of including predictors unrelated to the outcome in a neural network. Multicollinearity can also affect these models, even if we do not invert the feature matrix or use the Hessian during optimization. Highly redundant predictors can result in the loss function having difficult characteristics. Using an L<sub>2</sub> penalty or a feature extraction method can be very helpful. 

Also, predictors with highly skewed distributions might benefit from a transformation to make their distribution more symmetric. If not, outlying values can exert significant influence in computations, potentially preventing the generation of high-quality parameter estimates. 

Finally, since the networks use random initialization, we should ensure that our predictors are standardized to have the same units. If there are only numeric predictors, centering/scaling or orderNorm operations can be very effective^[orderNorm is perhaps more attractive since it standardizes the predictors and coerces them to a symmetric distribution.]. Using this approach, we must also apply the same standardization to binary indicators created from categorical predictors.  Alternatively, we could leave 0/1 indicators as-is, but _range_ scale our predictors to also be between zero and one. This may have a disadvantage; the range will be common across predictors, but the mean and variance can be quite different. 

### Learning Rates {#sec-nnet-learning-rates}

The learning rate ($\alpha$) can profoundly affect model quality, perhaps more than the structural tuning parameters that define complexity (e.g., number of hidden units). A high learning range ($\approx$ 0.1) will help converge towards effective parameter estimates, but once there, it can cause the model to consistently overshoot the best location. One approach to modulating the rate is to use a scheduler [@lu2022gradient,chapt 4]. These are functions/algorithms that will change the rate (often to decrease it) over epochs. For example, to monotonically decrease the rate, two options are:

- inverse decay: $f(\alpha_0, \delta, i) = \alpha_0/(1 + \delta i)$ 
- exponential decay: $f(\alpha_0, \delta, i) = \alpha_0\exp(-\delta i)$

where $i$ is the epoch number and $\delta$ is a decay value. These two functions gradually decrease the rate so that, when near an optimum, it is slow enough not to exceed the optimal settings. Another approach is a step function that decreases the rate but maintains it at a constant level for a range of epochs. Those ranges can be predefined or set dynamically to decrease when the loss function is reduced. The function for the former is: 

- step decay: $f(\alpha_0, \zeta, r, i) = \alpha_0 \zeta^{\lfloor i/r\rfloor}$ 

where $\zeta$ is the reduction value, $r$ is the step length, and $\lfloor x\rfloor$ is the floor function. 

Finally, some schedulers modulate the rate up and down. A cyclic schedule [@smith2017cyclical] starts at a maximum value, decreases to a minimum value, and then increases cyclically. For some range $r$ and minimum rate $\alpha_0$, an epoch's location within the series is $cycle = \lfloor 1 + (i / (2r) )\rfloor$ and the rate is

- cyclic: $f(\alpha_0, \alpha, r, i) = \alpha_0 + ( \alpha - \alpha_0 ) \max(0, 1 - x)$

where $x = |(i/r) - (2\,cycle) + 1|$. This is a good option when we are unsure how fast or slow the convergence will be. If the rate is too high near an optimum value, it will soon decrease to an appropriate value. Additionally, if the search is stuck in a local optimum (or a saddle point), increasing the learning rate may help it escape. 

@fig-schedulers uses the loss function example from @fig-stationary to demonstrate the effect of rate schedulers. Two starting points are used. One (in orange) is very close to a saddle point, while the other starts from the upper right corner of the parameter space (in green). 

```{r}
#| label: fig-schedulers
#| echo: false
#| out-width: 85%
#| fig-width: 8
#| fig-height: 2.8
#| fig-cap: "The effect of different schedulers on the rate and success of convergence."
base_sgd_plot(rates, NULL) + facet_wrap(~schedule, nrow = 1)
```

Using a constant learning rate of $\alpha = 0.1$, the search marches to the optimum value, but oscillates wildly on approach. Ultimately, it falls short of the best estimates, as it is constrained to make large jumps on each iteration. The cyclic schedule allowed rates to move between $\alpha_0 = 0.001$ to $\alpha = 0.1$ with a cycle range of $r = 10$ epochs. This appears to work well, or at least, it had good timing. The decay panel represents inverse decay with $\delta = 0.025$. For the corner starting point, this is a very effective strategy. From the saddle point, we can observe that decreasing the learning rate results in fewer oscillations as the search progresses and eventually leads to the optimum. Finally, the panel for stepwise reduction used an initial learning rate of $\alpha_0 = 0.1$, $\eta = 0.5$, and a step length of 20 epochs. This approach worked well when the search began from the saddle location, but performed poorly when starting at the corner. The success depended on when the search was near the optimum; if not timed right, the oscillations were too wide to converge on the best point. A reduction in the step length might help. 

The schedule type and its additional parameter(s) are added to the list of possible values that could be optimized during tuning.

### Regularization {#sec-nnet-regularization}

As seen in a previous chapter, we can add a penalty to the loss function to discourage particularly large model coefficients. This is especially beneficial for neural network models. 

While @hanson1988comparing and @weigend1990generalization made some early proposals for penalizing the size of the estimated parameters, @krogh1991simple proposed to use L<sub>2</sub> regularization to improve generalization. This is now a standard approach for training neural networks, and "weight decay" is now synonymous with squared penalties. 

While either L<sub>1</sub>, L<sub>2</sub>, or a mixture of the two can be used, there is some reason to use just L<sub>2</sub> penalization. While either type of penalty might be able to combat multicollinearity, the quadratic penalty has the advantage of maintaining a smooth loss surface. With an L<sub>1</sub> penalty, we have at least one point on the loss surface that is not differentiable. 

As we will describe later in this chapter, @loshchilov2017decoupled note that there are cases when penalizing the loss function does not precisely match the definition of weight decay (in the neural network context of @hanson1988comparing). Their adjustment for these issues is called the "AdamW" method. 

Another regularization technique is called _dropout_. This method randomly removes some units (hidden or not) from the network. This drops incoming and outgoing connections to other units, which propagates through all of the layers. These temporary coercions to zero recur with each evaluation of the gradient. In the final training step, a full set of parameters is used. Dropout has an effect similar to averaging many smaller networks. The amount of dropout should be tuned.

### Early Stopping {#sec-early-stopping}

When should training stop? We can estimate the number of epochs that might be needed, but if we make an incorrect guess, we risk under- or overfitting the model. It makes sense to include the number of epochs as a tuning parameter, but this could be costly^[Although the submodel trick described in @sec-submodels could greatly improve the effectiveness of this approach.]. 

If we have some data to spare, an efficient and effective approach is to use early stopping. If we set aside a small proportion of the training set to measure the search's progress, we can set the _maximum_ number of epochs and stop training when the loss worsens (based on our holdout set). Often, we can tune the number of "stopping epochs.". If this value were five, we would stop training after five consecutive epochs where the loss function worsens (and use the parameter estimates from our last best epoch). Values greater than one protect against overreacting when the loss profile is noisy. 

### Batch Learning {#sec-batch-learning}

_Stochastic_ gradient descent is the most widely used optimizer for neural networks. Technically, that term is reserved for cases when the gradient and parameter updates are computed using a single training set point at a time. The term _mini-batch_ is used when the training set is randomly divided into a set of equally sized batches for multiple training set points, as we did earlier. 

Operationally, within each epoch, a random batch is selected to evaluate the gradient and corresponding update the parameters. This occurs for each batch until the entire set is evaluated. This is the end of the epoch. If we are monitoring a holdout set for early stopping, this is the point at which we evaluate the loss function at the current parameter estimate using the holdout set. After this, the process begins again by iterating through the batches. 

SGD is thought to work well because it injects a considerable amount of variation into the gradients and updates. This would generally be considered a bad thing, but, as with simulated annealing, it makes the optimization less greedy. The somewhat random directions the optimizer takes can help the optimization escape from local optimums and be more resilient when the search veers near a saddle point or other regions where the gradient may be flat or consistently zero. @hoffer2017train referred to SGD as a "high-dimensional random walk on a random potential" process. 

As the search proceeds, lowering the learning rate can mitigate the extra noise in the gradients so that, hopefully, the search does not move away from the best parameter estimates. 

We can tune the batch size if there is no intuition for what a good value might be. Smallish batch sizes could range from 16 to 512 training set points^[Typically on log<sub>2</sub> units], depending on the training set size. @keskar2016large remarks that smaller batches tend to explore the parameter space more thoroughly and that, as the batch size becomes larger, SGD will find a solution that is closer to the initial values. 

@loshchilov2017decoupled make a few interesting observations related to L<sub>2</sub> penalization with stochastic gradient descent. Their results show that as the number of batches increases, the optimal penalization becomes smaller. In other words, as the batch size decreases, the amount of penalization should also decrease. 

### Gradient Descent {#sec-gradient-descent}

Gradient descent methods for training neural networks have become a popular research topic over the last 15 years. We'll discuss a few major techniques here, but there are myriad other tools for training these models. In this section, we'll temporarily forgo discussing changes from epoch to epoch. Since we are focused on SGD, we'll use the term update to refer to the point at which the parameters move to a different location due to a batch step. 

#### Momentum {.unnumbered}

To start, let's consider a more common form of momentum in gradient search^[In @sec-nnet-training, we previously described Nesterov momentum. This is unrelated to the current tool bearing the same name.] Momentum is a useful tool that can help in situations such as the results in @fig-schedulers, where the updates make the search path make large, ineffective jumps. When the learning rate is consistently large, the search route bounces back and forth since the gradients are substantial in magnitude but moving in different consecutive directions. 

Momentum is one of several tools to adjust the gradient vector based on previous gradients. For iteration $i$, a vector is computed that is a combination of the current and previous gradients^[For the first update, the value is simply $v_1 = \alpha g(\boldsymbol{\theta}_i)$.]: 

$$
v_i = \phi v_{i-1} + \alpha_i g(\boldsymbol{\theta}_i)
$$

and this is used to make the update $\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - v_i$. This weighting system emphasizes recent gradients while also taking into account previous gradients. This can accelerate the search and diminish oscillations.   

Values of $\phi$ are typically between 0.8 and 0.99, representing a weighted average of previous gradient directions, with a greater emphasis on recent iterations as $\phi$ increases. If the directions have been fairly stable, there is little change; however, if they are oscillating, the direction takes a more consistent central route with fewer directional jumps. It also helps if the search is in a region where gradients are close to zero; historical gradients will have some non-zero values that are unlikely to become stuck with updates near zero. 

@fig-sgd shows the results when momentum is used for our 2D toy example^[Since the loss function for this example is based on a single, deterministic value, the momentum value was set very low ($\phi = 2/3$). However, the overall trend is consistent with results from a typical performance metric like cross-entropy.]. The cyclic rate settings were used for this example, and the results appear very similar to the previous path when initialized near the saddle point. The corner point starting value shows that momentum can lead to a location near the optimal value, but takes a more circuitous route. 

```{r}
#| label: fig-sgd
#| echo: false
#| out-width: 100%
#| fig-width: 8
#| fig-height: 2.8
#| warning: false
#| fig-cap: "Examples of four different optimizers for the loss surface previously shown in @fig-schedulers."

bind_rows(momentum_res, adam_res, rms_prop_res, adagrad_res) |>
  mutate(
    technique = factor(
      technique,
      levels = c("Momentum", "AdaGrad", "RMSprop + Nesterov", "ADAM")
    )
  ) |>
  base_sgd_plot(NULL) +
  facet_wrap( ~ technique, nrow = 1)
```

Momentum should be used in conjunction with a rate scheduler, or at the very least, without a consistently large learning rate. Making very large jumps in a central direction might lead the search to points far away from regions of improvement. Penalization also helps. 

#### Adaptive Learning Rates {.unnumbered}

There is also a class of gradient descent methods that try to avoid using a single learning rate for all of the elements of $\boldsymbol{\theta}$.  Instead, they use a constant learning rate and, at each update, adjust the rate using functions of the gradient vector so that the rates are different from each $\theta_j$. 

One approach is the adaptive gradient algorithm, a.k.a. AdaGrad [@duchi2011adaptive], that scales the constant learning rate $\alpha$ by the accumulated square of the gradients: 

$$
\boldsymbol{v}_i =\sum_{b=1}^t g(\boldsymbol{\theta}_b)^2 
$$
so that 

$$
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \frac{\alpha}{\sqrt{\boldsymbol{v}^2_i + \epsilon}}\, g(\boldsymbol{\theta}_i)
$$

where $\epsilon$ is a small constant to avoid division by zero. The square root in the denominator is required to keep the denominator units the same as those in the numerator. AdaGrad has the advantage of keeping the adjusted learning rate steady for elements of $g(\boldsymbol{\theta})$ that do not change substantially from update to update. At update $i$, the contribution to the learning rate is inversely proportional to the squared gradient value. 

The problem is the _unweighted_ accumulation of squared values. For a reasonable number of updates, elements of $\boldsymbol{v}$ can explode to very large values, and this prevents almost _any_ movement of $\boldsymbol{\theta}$ from update to update. For the example shown in @fig-sgd, some of these values were in the thousands, and progress clearly halted before reaching the optimal parameters. 

One method of fixing this problem is RMSprop [@hinton2012neural]. Here, the denominator uses an exponentially weighted moving average of the squared gradients: 

$$
\boldsymbol{v}_i = \rho\, \boldsymbol{v}_{i-1} + (1 - \rho)\, g(\boldsymbol{\theta}_i)^2 
$$

This formulation places more weight on the current and recent gradients than the statistic used by AdaGrad. The RMSprop update becomes: 

$$
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \frac{\alpha}{\sqrt{\boldsymbol{v}_i + \epsilon}}\, g(\boldsymbol{\theta}_i).
$$

The "RMS" in the name is due to taking the square root of the weighted sum of squared gradients (i.e., similar to a traditional root mean square). 

@goodfellow2016deep suggested using RMSprop with Nesterov momentum. This leads to a more complicated procedure. First, we compute our initial update: $\quad\tilde{\boldsymbol{\theta}}_{i} = \boldsymbol{\theta}_{i} - \phi\,\boldsymbol{\delta}_{i-1}$ where $\boldsymbol{\delta}_0$ is initialized with a vector of zeros. We compute the gradient at this temporary update and use this to compute $\boldsymbol{v}_i$. From here: 

$$
\boldsymbol{\delta}_{i} = \phi\,\boldsymbol{\delta}_{i-1} - \frac{\alpha}{\sqrt{\boldsymbol{v}_i + \epsilon}}\, g(\tilde{\boldsymbol{\theta}}_i)
$$

The results are in @fig-sgd. The curves are similar to those for AdaGrad except that they do reach the location of minimum loss. The path for the starting value near the saddle point exhibits several minor oscillations en route to the optimal result. 

The adaptive moment estimation (Adam) optimizer from @kinga2015method  extends the moving average approach to the gradient term as well:  

$$
\begin{align}
\boldsymbol{m}_i &= \phi\, \boldsymbol{m}_{i-1} + (1 - \phi)\, g(\boldsymbol{\theta}_i) \notag \\
\boldsymbol{v}_i &= \rho\, \boldsymbol{v}_{i-1} + (1 - \rho)\, g(\boldsymbol{\theta}_i)^2 \notag 
\end{align}
$$

Adam also tries to normalize these quantities so that the first few updates have larger values:   

$$
\begin{align}
\boldsymbol{m}^*_i &= \frac{\boldsymbol{m}_i}{1 - \phi^2}\notag \\
\boldsymbol{v}^*_i &= \frac{\boldsymbol{v}_i}{1 - \rho^2} \notag 
\end{align}
$$

This helps offset the initialization of $\boldsymbol{m}_0 = \boldsymbol{v}_0 = \boldsymbol{0}$. At update $i$, Adam's proposal is

$$
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \frac{\alpha}{\sqrt{\boldsymbol{v}^*_i + \epsilon}}\,\boldsymbol{m}^*_i.
$$

The results above show that Adam's path towards the optimal parameters is slightly wobbly compared to RMSprop but otherwise effective. 

Both $\phi$ and $\rho$ are tunable in these models and typically range within [0.8, 0.99]. The constant learning rate $\alpha$ is also tunable. In @fig-sgd, $\phi=\rho=0.9$. AdaGrad, Adam, and RMSprop used $\alpha = 0.1$. For RMSprop, decreasing the rate to $\alpha = 0.08$ removed the oscillation effects shown in @fig-sgd.

As previously alluded to, @loshchilov2017decoupled recognized that, for Adam, penalizing the loss function does not align with the definition of weight decay. Without a remedy, they found that Adam is not as effective as it could be. Their method, called AdamW, includes the penalty in the gradient definition. The gradient used to compute $\boldsymbol{m}_i$ and $\boldsymbol{v}_i$ is a combination of the previous "raw" gradient and the penalty ($\lambda$): 

$$
g(\boldsymbol{\theta}_i) = \nabla \psi(\boldsymbol{\theta}_{i}) - \lambda \boldsymbol{\theta}_{i}
$$

The updating equation also uses an extra term containing the penalty: 

$$
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \alpha_i\left[\frac{\boldsymbol{m}^*_i}{\sqrt{\boldsymbol{v}^*_i + \epsilon}} + \lambda \boldsymbol{\theta}_{i}\right].
$$

Note that the learning rate $\alpha$ now has a subscript. They also recommend trying a cyclic learning rate scheduler with AdamW.  

#### Other Methods {.unnumbered} 

As previously mentioned, there are numerous other variations of SGD that will not be discussed here, including AdaMax [@kinga2015method], Nadam [@dozat2016], MadGrad [@defazio2021], and others.  

### Newton's Method {#sec-newtons-method}

As previously shown, Newtons method (a.k.a., the NewtonRaphson method) uses both gradients and the Hessian matrix to update parameters. This approach can be very effective when the local loss surface can be approximated by a second-degree polynomial. When the local search includes a stationary point, the signs of the eigenvalues of the Hessian can characterize it: all non-negative imply a maximum, all non-positive imply a minimum, and mixed signs indicate a saddle point. 

One issue with the method is related to the Hessian. If there are $p$ parameters, the matrix needs to save $p(p+1)/2$ parameters. For neural networks, $p$ can be in the millions, so the basic application of Newtons method can be slow and might require infeasible amounts of memory (depending on the model size). We also need to invert the Hessian, making it even more costly.

We can approximate Newton's method using the Limited-memory BroydenFletcherGoldfarbShanno algorithm (L-BFGS) search [@lu2022gradient,chapt 6]. It uses past gradients to approximate the Hessian as the search proceeds. L-BFGS computes gradients at different values of $\boldsymbol{\theta}$ for more than just updating the parameters (i.e., Hessian approximation and the line search), so it is not a good solution when loss function or gradient evaluations are expensive. 

However, if we have a small dataset, a low-to-medium complexity architecture, or both, L-BFGS can be a very effective tool for estimating parameters. 

### Forestation Modeling  {#sec-nnet-forestation}

```{r}
#| label: load-forested-mlp
#| incude: false
# Code in R/forested_mlp.R
load("../RData/forested_mlp.RData")

brier_param_cor <- 
  cor(brier_and_params$mean, brier_and_params$num_param, 
      method = "spearman") |> 
  round(2)

num_metrics <- vctrs::vec_unique_count(mlp_ranks$.metric)
num_configs <- nrow(mlp_ranks) / num_metrics
num_grid <- vctrs::vec_unique_count(mlp_ranks$.config)

wide_mtr <- 
  mlp_ranks |>
  dplyr::select(wflow_id, .config, .metric, mean) |>
  pivot_wider(
    id_cols = c(wflow_id, .config),
    names_from = .metric,
    values_from = mean
  )
mtr_cor <- cor(wide_mtr$brier_class, wide_mtr$roc_auc) |> round(2)
```

For the forested data, there are a few options for preprocessing: 

 - Use binary indicators to represent the `r length(levels(forested_train$county))` counties or a supervised effect encoding strategy. 
 - Several predictors are highly skewed. We can leave them as-is before centering/scaling, or use an orderNorm transformation to coerce each predictor to a standard normal distribution. 
 - Deal with the high correlation for some predictors via PCA extraction or leave as-is in the data. 

Several combinations of these operations were used: 

 - An effect encoding with and without PCA feature extraction.
 - Standard binary indicators with and without PCA feature extraction and with and without a symmetry transformation (i.e., orderNorm versus centering/scaling). 

For the PCA extraction, all possible components were used. 

For the model, there are a fair number of tuning parameters to combine: 

 - One or two layers.
 - The number of hidden units. For each layer, these ranged from two to fifty units. 
 - The activation type: ReLU, elu, tanh, and tanhshrink.
 - L<sub>2</sub> regularization values between 10<sup>-10</sup> and 10<sup>-1</sup>.
 - Batch sizes between 2<sup>3</sup> and 2<sup>8</sup>.
 - Learning rates between 10<sup>-4</sup> and 10<sup>-1</sup>.
 - Learning rate schedules: constant, cyclic, and time decay.
 - Momentum values between [0.8, 0.99].

AdamW was used to train the models, with a maximum of 25 epochs allowed, but early stopping could occur after five consecutive epochs with poor performance. 

A space-filling design with `r num_grid` candidates was evaluated. The six preprocessing schemes listed above were each run with the same model grid, resulting in `r num_configs` candidates to be evaluated. 

@fig-forest-mlp-ranks shows the results for the `r num_configs` candidates. They are ranked by their resampling estimate of the Brier scores, and the figure presents these rankings from best to worst, breaking out the combinations of preprocessing scenarios. While each panel contains models with sufficiently low Brier scores, several trends are evident. The "Indicators" panel represents models using dummy variable indicators with no symmetry transformation, and this did not do well (with or without PCA feature extraction). 

```{r}
#| label: fig-forest-mlp-ranks
#| echo: false
#| out-width: 95%
#| fig-width: 9
#| fig-height: 5.2
#| fig-cap: Rankings of multilayer perceptron models for the forestation models using the Brier score. The panels represent different types of preprocessing, and the vertical lines are 90% confidence intervals.

mlp_ranks|> 
  mutate(
    encoding = map_chr(wflow_id, ~ strsplit(.x, split = "_")[[1]][1]),
    encoding = case_when(
      encoding == "orderNorm" ~ "Indicators + Symmetry",
      encoding == "plain" ~ "Indicators",
      .default = "Effect Encoding + Symmetry"
    ),
    encoding = factor(encoding, levels = c("Indicators", "Indicators + Symmetry", "Effect Encoding + Symmetry")),
    extraction = map_chr(wflow_id, ~ strsplit(.x, split = "_")[[1]][2]),
    extraction = ifelse(extraction == "pca", "PCA", "No PCA"),
    extraction = factor(extraction, levels = c("No PCA", "PCA")),
    layers = ifelse(grepl("2L", wflow_id), "2 layers", "1 layer"),
    layers = factor(layers, levels = c("1 layer", "2 layers")),
    lower = mean - qnorm(0.05) * std_err,
    upper = mean + qnorm(0.05) * std_err
  ) |> 
  filter(.metric == "brier_class") |> 
  ggplot(aes(x= rank, col = layers)) + 
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                alpha = 1 / 3) +
  geom_point(aes(y = mean), cex = 1/2) +
  facet_grid(extraction ~ encoding) + 
  scale_color_manual( values = c("#0076BBFF", "#E9A17CFF")) +
  labs(x = "Overall Rank", y = "Brier Score") +
  theme(legend.position = "top")
```

In fact, PCA has suboptimal results in each of the three bottom panels. This could be due to the components being unsupervised; they are not optimized for prediction. Alternatively, the last few components might be measuring noise and have less relevance to the model. Adding irrelevant predictors to neural networks can diminish their effectiveness. 

The two cases that did the best were the "Indicators + Symmetry" and "Effect Encoding + Symmetry" panels. While both used the orderNorm transformation, the former panel employed indicators, whereas the latter used a supervised effect encoding, where a single column was used to represent the county effect. This suggests that eliminating distributional skew in the predictors may yield a modest improvement in predictive performance. 

```{r}
#| label: tbl-mlp-top-five
#| tbl-cap: "The best models as ranked by the Brier score."
top_five <- 
  mlp_ranks |> 
  filter(.metric == "brier_class") |> 
  slice_min(mean, n = 5)

top_five |> 
  inner_join(
    brier_and_params |> distinct(wflow_id, .config, num_param),
    by = join_by(wflow_id, .config)
  ) |> 
  dplyr::select(wflow_id, .config, rank, num_param) |> 
  inner_join(mlp_best_mtr, by = join_by(wflow_id, .config)) |> 
  mutate(
    tmp1 = map2_chr(hidden_units, activation, ~ paste0(.x, " ", .y)),
    tmp2 = map2_chr(hidden_units_2, activation_2, ~ paste0(", ", .x, " ", .y)),
    tmp2 = ifelse(is.na(hidden_units_2), "", tmp2),
    Architecture = map2_chr(tmp1, tmp2, ~ paste0(.x, " ", .y)),
    Penalty = format(log10(penalty), digits = 3),
    Penalty = paste0("10<sup>", Penalty, "</sup>"),
    rate_schedule = ifelse(rate_schedule == "decay_time", "inverse decay", rate_schedule),
    learn_rate = format(log10(learn_rate), digits = 3),
    learn_rate = paste0("10<sup>", learn_rate, "</sup>"),
    learn_rate = map2_chr(learn_rate, rate_schedule, ~ paste0(.x, " (", .y, ")")),
    encoding = map_chr(wflow_id, ~ strsplit(.x, split = "_")[[1]][1]),
    encoding = case_when(
      encoding == "orderNorm" ~ "Indicators, Symmetry",
      encoding == "plain" ~ "Indicators",
      .default = "Effect Encoding, Symmetry"
    ),
    extraction = map_chr(wflow_id, ~ strsplit(.x, split = "_")[[1]][2]),
    extraction = ifelse(extraction == "pca", "PCA", "No PCA"),
    extraction = factor(extraction, levels = c("No PCA", "PCA")),
    Preprocessing = map2_chr(encoding, extraction, ~ paste0(.x, ", ", .y)),
    across(where(is.numeric), ~ format(.x, digits = 3))
  ) |> 
  dplyr::select(-wflow_id, -.config, -hidden_units, -hidden_units_2, -tmp1, -tmp2,
         -activation, -activation_2, -.estimator, -n, -std_err, -rate_schedule, 
         -encoding, -extraction, -penalty, -rank) |> 
  rename(`Learning Rate` = learn_rate, 
         `Batch Size` = batch_size, 
         Momentum = momentum, 
         `# Parameters` = num_param,
  ) |> 
  pivot_wider(
    id_cols = c(
      `Learning Rate`,
      `Batch Size`,
      Momentum,
      Architecture,
      Penalty,
      Preprocessing,
      `# Parameters`
    ),
    names_from = c(.metric),
    values_from = c(mean)
  ) |> 
  dplyr::select(
    Preprocessing,
    Architecture,
    `# Parameters`,
    Penalty,
    `Batch Size`,
    `Learning Rate`,
    Momentum,
    Brier = brier_class,
    ROC = roc_auc
  ) |> 
  gt() |> 
  fmt_markdown(columns = c(Penalty, `Learning Rate`)) |> 
  tab_spanner("Performance", columns = c(Brier, ROC)) |> 
  fmt_number(decimals = 3, columns = c(Brier, ROC)) |> 
  cols_width(
    Preprocessing ~ pct(20),
    Architecture ~ pct(11),
    `# Parameters` ~ pct(9),
    Penalty ~ pct(9),
    `Batch Size` ~ pct(9),
    `Learning Rate` ~ pct(15),
    Momentum ~ pct(10),
    Brier ~ pct(8),
    ROC ~ pct(8)
)
```

Digging in further, @tbl-mlp-top-five displays the top five candidates from a pool of 300. They are a mixture of one- and two-layer networks, but note that we have to go to the third decimal place to distinguish their Brier scores. Constant learning rates did well as long as their rates were fairly low and batch sizes were fairly small. The number of parameters varied as well; increasing the models complexity did not appear to help for this dataset. In fact, the rank correlation between the Brier score and the number of parameters was `r brier_param_cor`, implying that there is little evidence that adding more layers and/or units to the MLP improves the model.

The Brier scores are low and the areas under the ROC curves are all at least 0.94, indicating that the predictions effectively separate the classes and that the probability estimates are accurate. Across the candidates, the rank correlation between the Brier score and the ROC AUC was `r mtr_cor`; in this case, their optimization results are well-aligned. 

```{r}
#| label: mlp-mtr-diffs
#| inlcude: false
#| cache: true

mtr_differences <- function(split, ref_model) {
  mtr_vals <-
    split |>
    rsample::analysis() |>
    pivot_longer(cols = c(-class), names_to = "Model", values_to = "value") |>
    group_by(Model) |>
    cls_mtr(class, value) |>
    dplyr::select(-.estimator) |>
    pivot_wider(
      id_cols = c(.metric),
      names_from = Model,
      values_from = .estimate
    ) |>
    full_join(metrics_info(cls_mtr), by = ".metric") |>
    dplyr::select(-type)
  models <- colnames(mtr_vals)
  models <- setdiff(models, c(".metric", "direction", ref_model))
  for (i in models) {
    diffs <- mtr_vals[[ref_model]] - mtr_vals[[i]]
    new_nm <- paste0("loss_", i)
    mtr_vals[[new_nm]] <-
      if_else(mtr_vals$direction == "minimize", -diffs, diffs)
  }
  if (length(models) > 1) {
    mtr_vals <-
      mtr_vals |>
      dplyr::select(.metric, starts_with("loss_")) |>
      pivot_longer(
        cols = c(starts_with("loss_")),
        names_to = "term",
        values_to = "estimate"
      ) |>
      mutate(term = gsub("loss_", "", term))
  } else {
    mtr_vals <-
      mtr_vals |>
      dplyr::select(.metric, all_of(new_nm)) |>
      set_names(c(".metric", "estimate")) |>
      mutate(term = i)
  }
  mtr_vals
}

# ------------------------------------------------------------------------------

nonlin_pred <-
  mlp_best_res |>
  collect_predictions(parameters = mlp_best_config) |>
  dplyr::select(.row, mlp = .pred_Yes) |>
  full_join(
    forest_fda_pred |> dplyr::select(.row, class, fda = .pred_Yes),
    by = ".row"
  ) |>
  dplyr::select(-.row)

set.seed(789)
mlp_mtr_bt <-
  nonlin_pred |>
  bootstraps(times = 2000) |>
  mutate(stats = map(splits, mtr_differences, ref_model = "mlp"))

mlp_mtr_diffs <- 
  mlp_mtr_bt |>
  int_pctl(stats, alpha = 0.1)

mlp_mtr <- 
  mlp_best_res |> 
  collect_metrics() |> 
  inner_join(mlp_best_config |> dplyr::select(.config), by = ".config")

mlp_brier <- mlp_mtr$mean[mlp_mtr$.metric == "brier_class"]
fda_brier <- forest_fda_best_mtr$mean[forest_fda_best_mtr$.metric == "brier_class"]

mlp_roc <- mlp_mtr$mean[mlp_mtr$.metric == "roc_auc"]
fda_roc <- forest_fda_best_mtr$mean[forest_fda_best_mtr$.metric == "roc_auc"]
```

How do these results compare to the previous FDA model? The Brier scores are very close: `r signif(mlp_brier, 3)` for the neural network and `r signif(fda_brier, 3)` for FDA. The difference is very small, and a 90% confidence interval for the difference is (`r signif(mlp_mtr_diffs$.lower[mlp_mtr_diffs$.metric == "brier_class"], 3)`, `r signif(mlp_mtr_diffs$.upper[mlp_mtr_diffs$.metric == "brier_class"], 3)`), suggesting that performance for each model is within the experimental noise of the data. The difference in the area under the ROC curve is `r signif(mlp_roc - fda_roc, 3)` with interval (`r signif(mlp_mtr_diffs$.lower[mlp_mtr_diffs$.metric == "roc_auc"], 3)`, `r format(signif(mlp_mtr_diffs$.upper[mlp_mtr_diffs$.metric == "roc_auc"], 3), scientific = FALSE)`), leading us to a similar conclusion. Given that the FDA model uses fewer predictors and far fewer parameters, we might favor that model over our neural network. 

`r r_comp("cls-nonlinear.html#sec-cls-nnet")`

## Special Tabular Network Models  {#sec-cls-tab-net}

`r r_comp("cls-nonlinear.html#sec-cls-tab-net")`

## Support Vector Machines {#sec-cls-svm}

`r r_comp("cls-nonlinear.html#sec-cls-svm")`

## Chapter References {.unnumbered}

```{r}
#| label: cls-nonlinear-save
#| include: false

forest_knn_wflow_res <- as_workflow_set(knn = forest_knn_res)
save(
 forest_knn_res, forest_knn_best,
  file = "../RData/forested_knn.Rdata"
)
```