---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-nonlinear/"
---

# Complex Nonlinear Boundaries {#sec-cls-nonlinear}

```{r}
#| label: cls-nonlinear-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(torch)
library(mirai)
library(spatialsample)
library(brulee)
library(bestNormalize)
library(patchwork)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
set_options()
theme_set(theme_bw())
daemons(parallel::detectCores())

# ------------------------------------------------------------------------------
# load data 

load("../RData/forested_data.RData")
load("../RData/optimizers.RData")

# ------------------------------------------------------------------------------
# Gradient descent helpers

source("../R/setup_2D_GD.R")
```

## Nonlinear Discriminants  {#sec-nonlinear-da}

### Quadratic and Regularized Approaches {#sec-qda-rda}

### Mixture Discriminants {#sec-mda}

### Flexible Discriminants {#sec-fda}

### Naive Bayes {#sec-naive-bayes}

## Neural Networks via Multilayer Perceptrons {#sec-nnet}

Neural networks are complex models that are comprised of multiple hierarchies of _latent variables_ that are designed to represent different motifs in the data. The model's architecture consists of a series of _layers_, each containing a set of units. Neural networks begin with the input layer that contains units representing our predictors. After this are one or more _hidden layers_, each containing a preset number of hidden units. These are artificial features that are influenced by the layer before.  For example, if there were four predictors and the first hidden layer had two hidden units, each hidden unit is defined by an equation that includes each of the predictors. In the broadest sense, it is similar to how, in PCA, each principal component is a linear combination of the predictors. However, the similarities between neural networks and PCA end there. 

These connections between units on different layers continue as layers are added. Eventually, the last layer is the _output layer_. For classification, this usually has as many units as there are class levels in the outcome. @fig-nnet-cls shows a general architectural diagram for a basic model with a single hidden layer where the lines indicate that a model parameter exists to connect the nodes.. 

```{r}
#| label: fig-nnet-cls
#| echo: false
#| out-width: 40%
#| fig-width: 4
#| fig-height: 5
#| fig-cap: Architecture of a simple neural network with a single layer of hidden units.
knitr::include_graphics("../premade/mlp.svg")
```

Traditionally, the connections that come into a hidden unit are combined using a linear predictor: each incoming unit has a corresponding slope, called a "weight" in the neural network literature^[For consistency, we'll use "slopes" and "intercepts" here.]. The linear combinations commonly have an intercept term (called a "bias"). These linear predictors are embedded into a nonlinear "activation" function ($\sigma$) to enable complex patterns. An example of an activation function would be a sigmoidal function, much like the logistic function used in @eq-logistic. 

::: {.note-box}
We'll be primarily concerned with multilayer perceptrons (MLPs), a subset of the broader class of neural networks. MLPs are feed-forward models, meaning each layer only connects to the subsequent layer; there are no feedback loops where future units connect to units in previous layers.

As a counterexample, residual networks [@he2016deep] are connected to previous layers (a.k.a. skip layers); therefore, they are not multilayer perceptrons. Another popular example is the long-term short memory model for sequential data such as time series [@hochreiter1997long]. 
:::

<br> 

Let's walk through the network structure, starting at the input layer. We have predictors $x_1, \ldots x_p$. These will be mathematically connected to the next layer containing $m$ hidden units $H_\ell$ ($\ell=1, \ldots m$). First, we create a separate linear predictor for hidden unit $H_l$ denoted as $\beta_{\ell 0} + \beta_{\ell 1} x_{1} + \ldots + \beta_{\ell p} x_{p}$. We then embed this inside the activation function $\sigma()$ used for this layer so that there is a nonlinear relationship between the predictors and their new representation. Visually^[
This representation and the one below were adapted from Izaak Neutelings's excellent work, which can be found at [`https://tikz.net/neural_networks/`](https://tikz.net/neural_networks/) and is licensed under a CC BY-SA 4.0.]:

```{r}
#| label: nnet-start
#| echo: false
#| out-width: 80%
#| fig-width: 4
#| fig-height: 5
knitr::include_graphics("../premade/nnet_start.svg")
```

As we add more layers, each of the first set of hidden units is similarly connected to the hidden units in the next layer and so on. The layers do not have to use the same activation function. Also, the layers can be specialized, such as the image processing layers shown in @fig-vgg16. For each hidden layer, we have to specify the number of units and the activation function. These, along with the number of layers, are tuning parameters. 

Note that the units in the subsequent hidden layer are nested inside linear combinations of the previous hidden layers (plus the input units). These models have sets of nested nonlinear functions and can be very complex but are mathematically tractable. 

For the final layer, the previous set of hidden units is often connected to the _output units_ using a linear activation. For classification, there are often $C$ output units (i.e., one for each class):

```{r}
#| label: nnet-end
#| echo: false
#| out-width: 80%
#| fig-width: 4
#| fig-height: 5
knitr::include_graphics("../premade/nnet_end.svg")
```

Since linear activation is used for the final layer, the values of output units ($Y$) are unlikely to be in proper units to use for classification. To remedy this, the raw outputs are normalized to be probability-like via the softmax function: 

$$
\pi_{k} = \frac{\exp(Y_{k})}{\sum\limits_{k=1}^C \exp(Y_{k})}
$$

In this section, we'll describe the important elements of multilayer perceptrons, such as the different types of activation functions. We'll will walk through a small but specific model. 

In later sections, we'll also spend a good deal of time discussing the different ways to train MLPs. There are also discussions on modern deep learning models that are specifically designed for tabular data. 

### Activation Functions {#sec-nnet-activation}

```{r}
#| label: act-grid
#| include: false
lp_grid <- seq(-3, 3, length.out = 100)

act_functions <- 
  bind_rows(
    tibble(input = lp_grid,
           output = binomial()$linkinv(lp_grid),
           activation = "logistic"),
    tibble(input = lp_grid,
           output = tanh(lp_grid),
           activation = "tanh"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_relu(lp_grid)),
           activation = "relu"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_elu(lp_grid)),
           activation = "elu"),
    tibble(input = lp_grid,
           output = lp_grid - tanh(lp_grid),
           activation = "tanhshrink"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_softshrink(lp_grid)),
           activation = "softshrink"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_gelu(lp_grid)),
           activation = "gelu"),
    tibble(input = lp_grid,
           output = as.numeric(nnf_silu(lp_grid)),
           activation = "silu"),
    tibble(input = lp_grid,
           output = as.numeric(nn_softplus()(lp_grid)),
           activation = "softplus")
  )
```  

Historically, the most commonly used nonlinear activation functions were sigmoidal. This means that the linear combination of the elements of the previous layer was converted into a "squashed" format, usually bounded (e.g., logistic activation values range between zero and one). @fig-sigmoid-activations shows several such functions. The logistic (@eq-logistic) function and hyperbolic tangent are fairly similar in shape, and were typical defaults. 

```{r}
#| label: fig-sigmoid-activations
#| echo: false
#| out-width: 65%
#| fig-width: 6
#| fig-height: 2
#| fig-cap: "Different sigmoidal activation functions."

sig_lvl <- c("logistic", "tanh", "tanhshrink")

act_functions |>  
  filter(grepl("(tan)|(sig)|(logistic)", activation)) |> 
  mutate(activation = factor(activation, levels = sig_lvl)) |> 
  ggplot(aes(input, output)) + 
  geom_line() +
  facet_wrap(~ activation, scale = "free_y", nrow = 1) + 
  theme_bw()
```

One issue with bounded sigmoidal shapes, especially the logistic function, is that they can become problematic when used with many hidden layers. Since the logistic function is on [0, 1], repeated multiplication of such values can cause their values to converge to be very close to zero (as seen in @sec-naive-bayes). This is also true of their gradients, leading to the _vanishing gradient problem_. This is probably not an issue with shallow networks with a handful of hidden layers.  

The tanshrink function diminishes the impact of inputs that are in a neighborhood around zero, where it is sigmoidal. For larger input values, the output is linear in the output (with an offset of $\pm 1$). 

In modern neural networks, the rectified linear unit (ReLU) is the default activation function. As shown in @fig-relu-activations, the function resembles the hinge functions used by MARS; to the left-hand side of zero, the output is zero, and then it is linear otherwise. Much like MARS, this has the effect of isolating some region of the predictor space. However, there are a few key differences. MARS hinge functions affect a single predictor at a time, while neural networks' activation functions act on the linear combination of parameters. Also, neural networks with multiple layers recursively activate linear combinations inside the prior layers (and so on). 

```{r}
#| label: fig-relu-activations
#| echo: false
#| out-width: 90%
#| fig-width: 8
#| fig-height: 2
#| fig-cap: "Different ReLU-based activation functions."

relu_lvl <- c("relu", "softplus", "gelu", "silu", "elu")

act_functions |>  
  filter(grepl("(l[uU]$)|(softplus)", activation)) |> 
  mutate(activation = factor(activation, levels = relu_lvl)) |> 
  ggplot(aes(input, output)) + 
  geom_hline(yintercept = 0, lty = 3) + 
  geom_line() +
  facet_wrap(~ activation, nrow = 1) + 
  theme_bw()
```

There are various modified versions of ReLU that emulate the same form but have continuous derivatives. These are also shown in @fig-relu-activations, including softplus [@dugas2000incorporating;@glorot2011deep], gelu [@hendrycks2016gaussian], silu [@elfwing2018sigmoid],  elu [@clevert2015fast], and others. 

```{r}
#| label: nnet-ex-comps
#| include: false
#| cache: true
two_class_ex <- 
  parabolic |> 
  set_names(c("A", "B", "Class")) 

two_class_ex <-   
  recipe(Class ~ ., data = two_class_ex) |> 
  step_orderNorm(A, B) |> 
  prep() |> 
  bake(new_data = NULL)

x <- seq(-4, 4, length.out = 100)
grid <- crossing(A = x, B = x)

set.seed(123)
fit_1 <- brulee_mlp(
  Class ~ A + B,
  data = two_class_ex,
  hidden_units = 3,
  penalty = 0,
  activation = "relu",
  verbose = TRUE
)

coefs <- coef(fit_1)

grid_pred <- 
  fit_1 |> 
  predict(grid, type = "prob") |> 
  bind_cols(grid)

get_elements <- function(x, model) {
  xtbl <- x
  x <- as.matrix(x)
  lp_01 <- x %*% t(coefs$model.0.weight) + coefs$model.0.bias
  colnames(lp_01) <- paste0("lp_", 1:ncol(lp_01))
  layer_01 <- as.matrix(torch::nnf_relu(lp_01))
  colnames(layer_01) <- paste0("relu_", 1:ncol(layer_01))
  lp_12 <- layer_01 %*% t(coefs$model.2.weight) + coefs$model.2.bias
  colnames(lp_12) <- paste0("output_", 1:ncol(lp_12))
  prob <- exp(lp_12) / sum(exp(lp_12))
  bind_cols(xtbl, as_tibble(lp_01), as_tibble(layer_01), as_tibble(lp_12))
}

components <- NULL
for (i in 1:nrow(grid)) {
  components <- bind_rows(components, get_elements(grid[i,], fit_1))
}
```

Let's use an example data set with a simple MLP to demonstrate how ReLU activation works. @fig-nnet-ex shows the end results of a model with a single layer containing three hidden units using ReLU activation. The model converged in `r fit_1$best_epoch` epochs and does a good job discriminating between the two classes. 

```{r}
#| label: fig-nnet-ex
#| echo: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 4.4
#| warning: false
#| fig-cap: "An example data set with two predictors and two classes. The black line is the class boundary from an MLP model using three hidden units with ReLU activation."

  grid_pred |> 
  ggplot(aes(A, B)) +
  geom_point(data = two_class_ex, aes(col = Class), alpha = 1/3) +
  geom_contour(aes(z = .pred_Class1), breaks = 0.5, col = "black") +
  coord_fixed() +
  scale_color_manual(values = c("#007FFFFF", "#FF7F00FF"))
```

With two predictors and three hidden units, the first set of coefficients for the model are three sets of linear predictors (i.e., an intercept and two slopes) for each of the three hidden units. The predictors were normalized to have the same mean and variance (zero and one, respectively). The three equations produced during training were: 

```{r}
#| label: nnet-ex-first
#| echo: false
#| results: "asis"
wt_1 <- format(coefs$model.0.weight, digits = 1)
wt_1 <- gsub("^ ", "+", wt_1)
wt_1[,1] <- paste0(wt_1[,1], "\\:A")
wt_1[,2] <- paste0(wt_1[,2], "\\:B")
lp_1 <- 
  cbind(
    format(coefs$model.0.bias, digits = 2),
    wt_1
  )
lp_1 <- apply(lp_1, 1, function(x) paste0(x, collapse = ""))
lp_1 <- paste0("H_{", 1:3, "}&=", lp_1, " \\notag")

cat("$$")
cat("\\begin{align}")
cat(paste0(lp_1, collapse = " \\\\ \n"))
cat("\\end{align}")
cat("$$")
```

The intercept estimate for the first hidden unit is much larger than the other two. That unit also has a large positive slope for predictor A. The second hidden unit emphasizes predictor B, while $H_{i3}$ has strong positive effects for both predictors. 

The top of @fig-nnet-ex-layer-1 shows a heatmap of how these three artificial features change over the two-dimensional predictor space (before activation). The black line segments show the contour corresponding to zero values. The first hidden unit has larger values on the right-hand side of the predictor space, with a slightly diagonal contour for zero. The pattern shown for the second hidden unit is nearly the opposite of the first, although the zero contour cuts more through the center of the space. Finally, the third unit is driven by both predictors with larger values in the upper right region. 

```{r}
#| label: fig-nnet-ex-layer-1
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 5
#| warning: false
#| fig-cap: "Heatmaps of the three linear predictors and their ReLU values across the predictor space. The black line emphasizes the location where the linear predictor is zero."

lp_layer <- 
  components |> 
  select(-starts_with("relu_"), -starts_with("output")) |> 
  pivot_longer(cols = c(starts_with("lp_"))) |> 
  mutate(name = gsub("lp_", "hidden unit ", name)) |> 
  ggplot(aes(A, B)) + 
  geom_tile(aes(fill = value), show.legend = TRUE) + 
  geom_contour(aes(z = value), breaks = 0, col = "black") +
  facet_wrap(~ name, nrow = 1) + 
  scale_fill_gradient2(high = "#5D74A5FF") + 
  # coord_fixed() + 
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5),
    legend.title = element_blank()
  ) + 
  labs(title = "Linear Predictor")

relu_layer <- 
  components |> 
  select(-starts_with("lp"), -starts_with("output")) |> 
  pivot_longer(cols = c(starts_with("relu_"))) |> 
  mutate(name = gsub("relu_", "hidden unit ", name)) |> 
  ggplot(aes(A, B)) + 
  geom_tile(aes(fill = value), show.legend = TRUE) + 
  geom_contour(aes(z = value), breaks = 0, col = "5AAE61FF") +
  facet_wrap(~ name, nrow = 1) + 
  scale_fill_gradient2(high = "#5D74A5FF") + 
  # coord_fixed() + 
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5),
    legend.title = element_blank()
  ) + 
  labs(title = "ReLU", y = NULL) 

lp_layer / relu_layer
```

The figure's bottom panels show the ReLU function's effect; the values less than zero are set to zero. At this point, $H_{i1}$ and $H_{i3}$ have the most substantial effect on the model, but in somewhat overlapping regions.

However, the model estimates additional parameters that will merge these regions using weights that are estimated to be most helpful for predicting the data. Those equations were estimated to be: 

```{r}
#| label: nnet-ex-last
#| echo: false
#| results: "asis"
wt_2 <- format(coefs$model.2.weight, digits = 1)
wt_2 <- gsub("^ ", "+", wt_2)
wt_2[,1] <- paste0(wt_2[,1], "H_{i1}")
wt_2[,2] <- paste0(wt_2[,2], "H_{i2}")
wt_2[,3] <- paste0(wt_2[,3], "H_{i3}")
lp_2 <- 
  cbind(
    format(coefs$model.2.bias, digits = 2),
    wt_2
  )
lp_2 <- apply(lp_2, 1, function(x) paste0(x, collapse = ""))
lp_2 <- paste0("Y_{i", 1:2, "}&=", lp_2, " \\notag")

cat("\\begin{align}")
cat(paste0(lp_2, collapse = " \\\\ \n"))
cat("\\end{align}")
```

Note that the coefficients' signs are opposite for each outcome unit. @fig-nnet-ex-layer-2 illustrates the transition from the hidden layer to the outcome layer. The weighted sum of the three variables at the top produces the regions of strong positive (or negative) activation for the two classes. Once again, the black curve shows where the zero contour is located. 

```{r}
#| label: fig-nnet-ex-layer-2
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 5
#| warning: false
#| fig-cap: "Heatmaps of the three ReLU units and their output units across the predictor space."

output_layer <- 
  components |>
  select(-starts_with("lp"), -starts_with("relu_")) |>
  pivot_longer(cols = c(starts_with("output"))) |>
  ggplot(aes(A, B)) +
  geom_tile(aes(fill = value)) +
  geom_contour(
    # data = grid_pred,
    aes(z = value),
    breaks = 0,
    col = "black"
  ) +
  facet_wrap(~ name, nrow = 1) + 
  scale_fill_gradient2(high = "#420F75FF", low = "#EAC541FF") + 
  coord_fixed() + 
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5),
    legend.title = element_blank()
  )  + 
  labs(title = "Output", y = NULL) 

relu_layer / output_layer
```

The two output panels at the bottom of the figure show nearly opposite patterns, which is not coincidental. The model converges to this structure so that the output units favor one class or the other. The legend on the bottom right clearly shows inappropriate units; we'd like them to resemble probabilities. As previously mentioned, the softmax function coerces these values to be on [0, 1] and sum to one. This produces the class boundary seen in @fig-nnet-ex. 

As previously stated, ReLU is not a smooth function, so this demonstration will somewhat differ from the other activation functions; they generally don't completely isolate regions of the predictor space. However, the general flow of the network is still the same. 

Now that we've discussed the architecture of these models and how the layers relate to one another, let's examine how to estimate model parameters effectively.  

### Training and Optimization {#sec-nnet-training}

It can be challenging to train neural networks. Their model structure is composed of multiple nested nonlinear functions that may not be not smooth, continuous functions. The loss function for classification, commonly cross-entropy, is unlikely to be convex as model complexity increases with more and more layers. Consequently, we have no guarantees that a gradient-based optimizer will converge. There is also the possibility of local minima that might trap the training process, saddle points that could lead the search in the wrong direction, and regions of vanishing gradients. These issues will be discussed below. 

First is the challenge of local minima. In some cases, ML models can provide some guarantees regarding the existence of a global minimum loss value^[Generalized linear models are a good example of this]. This may not be the case for neural networks. Their loss surface may contain multiple valleys where the derivatives of some of the parameters are zero, but a better solution exists in another region of the parameter space. In other words, the model can appear to converge but has suboptimal performance. @baldi1989neural show that, under some conditions, single-layer feedforward networks can have a unique global minimum. It should be expected that as more layers and units are added, the probability of a unique best solution decreases. When "caught" in a local valley, we need out optimization algorithm to be able to escape. 

A second related issue is saddle points. These are locations where all of the gradients are zero (i.e., a stationary point), but some directions have increasing loss, and others will produce a decrease. @dauphin2014identifying postulate that these will exist, and their propensity to occur increases with the number of model parameters. The worry here is that, depending on the type of multidimensional curvature we face, the optimization algorithm might move in the direction of _increasing_ loss or may get stuck. 

@fig-stationary has an example with two parameters and a highly nonlinear loss surface. There are four stationary points, the best of which minimizes the loss function at $\boldsymbol{\theta} = [`r round(stationary$x[stationary$type == "minimum"], 1)`, `r round(stationary$y[stationary$type == "minimum"], 1)`]$. The lower right region has a point associated with maximum loss. The other two locations are saddle points. The point at $\boldsymbol{\theta} = [`r round(stationary$x[stationary$type == "saddle point" & stationary$x > 6], 1)`, `r round(stationary$y[stationary$type == "saddle point" & stationary$x > 6], 1)`]$ is a good example; moving to the left or right of the point will increase the loss. Moving up or down will decrease the loss (with the upper value being the correct choice in this parameter range). Our optimizer should be able to move in the proper direction or, like simulated annealing, recover and move back to progressively better results. 

::: {#fig-stationary}

::: {.figure-content}

```{r}
#| label: stationary
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 5

loss_grid |>
	rename(loss = res) |> 
	ggplot(aes(x, y)) +
	geom_tile(aes(fill = loss)) +
	geom_contour(aes(z = loss), col = "darkred", bins = 7, alpha = 3 / 4) +
	scale_fill_gradient(low = "white", high = "red") +
	scale_shape_manual(values = c(2, 6, 1)) +
	geom_point(
		data = stationary,
		aes(pch = type),
		cex = 2.5,
		col = "black"
	) + 
	coord_fixed(ratio = 5) + 
	labs(x = expression(theta[1]), y = expression(theta[2]))
```

:::

The loss surface for $\psi(\boldsymbol{\theta}) = \theta_1 cos(0.5\,\theta_1) - 2 \exp(-6(\theta_2 - 0.3)^2) + 0.2\, \theta_1 \theta_2$. Four stationary points are shown.

:::

Another possible factor is that many deep learning models are trained on vast amounts of data. This in itself presents constraints on how much data can be held in memory at once, as well as other logistical issues. This may not be an issue for the average application that uses tabular data, but the problem still exists. 

Before proceeding, recall @sec-gradient-opt where gradient descent was introduced. There are two commonly used classes of gradient-based optimization: 

 - _First-order techniques_: $\quad\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \alpha\,g(\boldsymbol{\theta}_i)$,

- _Second-order methods_: $\quad\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \alpha H(\boldsymbol{\theta_i})^{-1}\,g(\boldsymbol{\theta}_i)$,

where $\boldsymbol{\theta}$ are the unknown parameters, $\alpha$ is the learning rate, $g(\boldsymbol{\theta}_i)$ is the gradient vector of first derivatives, and $H(\boldsymbol{\theta_i})$ is the Hessian matrix of second derivatives. The Hessian measures the curvature of the loss function at the current iteration. Pre-multiplying by its inverse rescales the gradient vector, generally making it more effective. We'll discuss them both in @sec-gradient-descent and @sec-newtons-method. Generally speaking, second-order methods are preferred since they are more accurate _when they are computationally feasible_. However, first-order search is more commonly used because it is faster to compute and can be much more memory efficient.

To demonstrate, we can use an MLP model for the forestation data that contains one layer of 30 ReLU units and an additional layer of 5 ReLU units, yielding `r num_optimizer_param` model parameters. An L<sub>2</sub> penalty of 0.15 was set to regularize the cross-entropy loss function. A maximum of 100 epochs was specified, but training was instructed to stop after five bad epochs of loss (measured using held-out data). An initial learning rate of $\alpha = 0.1$ was used, but a variable, step-wise rate schedule was enacted to modulate the rate across epochs, as will be described below. 

The model was first trained using basic gradient descent. For illustration, training was executed five times using different initial parameter values. The leftmost panel in @fig-optimizers shows the results. In each case, the loss function had an initial reduction in the first set of epochs. However, in each case the model used all 100 epochs. This approach was not very successful. There was little reduction in the loss function, and the terminal phase had a minuscule decrease with no real improvement. This _could_ be due to the search approaching the saddle point. In this case, the gradient becomes very close to zero, and first-order gradient searches have tiny, ineffective updates.

```{r}
#| label: fig-optimizers
#| echo: false
#| out-width: 90%
#| fig-width: 8
#| fig-height: 2.8
#| warning: false
#| fig-cap: "The loss function, using held out data, for the same MLP model for four optimization strategies. The lines on each panel represent runs initialized with different random numbers. The profiles are normalized to start from the same initial loss value."

optimizers |> 
  ggplot(aes(epoch, loss_norm, col = seed, group = seed)) +
  geom_line(show.legend = FALSE,
            linewidth = 3 / 4,
            alpha = 1 / 2) +
  facet_wrap(~ method, scale = "free_x", nrow = 1) +
  scale_x_continuous(breaks = pretty_breaks()) + 
  labs(x = "Epochs", y = "Cross-Entropy (Holdout)")
```

As a different approach, an approximation to Newton's Method called the Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm (L-BFGS) method (REFERENCE). This uses a memory efficient approximation to the Hessian matrix. In this case, there was an immediate and enormous reduction in loss in the first epoch. In most cases, the next few epochs resulted in further reductions. Some of the training runs quickly converged, but one required 14 epochs. Compared to basic gradient descent, L-BFGS produced a much more effective model (based on the loss). The median time for optimization (per epoch) was `r round(optimizer_times$per_epoch[optimizer_times$method == "L-BFGS"], 3)`s, which was far slower than the median time for gradient descent (`r round(optimizer_times$per_epoch[optimizer_times$method == "GD"], 3)`s). However, the extra time is offset since L-BFGS will likely use far fewer epochs in this scenario. This approach works here because the number of model parameters is small for a nerual network and the data set is not huge.

For illustration, different variations of _stochastic_ gradient descent (SGD) are the norm. SGD uses randomly allocated _batches_ training set samples. The gradient search is updated by processing each of these batches until the entire trained set has been used, which signifies the end of that epoch. In other words, a potentially large number of parameter updates based on a small number of training point samples are used. The batch size used here was 64 data points. As a result, for this training set size, about 76 gradient updates are executed before the end of each epoch. 

As shown in @fig-optimizers, SGD does almost as well as L-BFGS, but most do not stop early; there is a relatively flat trend after about 20 epochs, where the search does not substantially reduce the loss. In terms of time per epoch, SGD was slightly better than L-BFGS at `r  round(optimizer_times$per_epoch[optimizer_times$method == "SGD"], 3)`s but will probably require more training. 

As we'll see in the section below, many extensions to SGD have made it more effective. One improvement is to use _Nesterov momentum_ [@sutskever2013importance]. This initially computes a standard update to the parameters, then approximates the gradient at this position to compute a second prospective step. The actual update that is used is the lookahead position. This technique shows results on par with L-BFGS, with several repeats stopping early. However, the training time was longest, using `r round(optimizer_times$per_epoch[optimizer_times$method == "SGD with momentum"], 3)`s per epoch.  

::: {.note-box}
The main takeaways of this exercise are that training these models can be difficult and that the type of parameter optimization can substantially impact model quality. 
:::

Let's discuss a few operational aspects of gradient-based methods before discussing the first- and second-order search techniques. 

### Initialization {#sec-nnet-initialization}

Network parameter initialization can significantly impact training success. Given the size and complexity of these models, optimal methodologies for setting initial values are nearly impossible to analytically derive. As such, parameters are initialized using random numbers. There are a few approaches to do this. @lecun2002efficient initialization uses a uniform distribution centered around zero with limits defined by the number of connections feeding into the current node (known as the "fan in" and is denoted as $m_{in}$). They used $U(\pm \sqrt{2/m_{in}})$ as the distribution. There have been a few variations of this: 

 - Glorot/Xavier initialization [@glorot2010understanding]: $U(\pm \sqrt{6/(m_{in}+m_{out})})$ 
 - He/Kaiming initialization [@he2015delving]: $N(0, \sqrt{2 / m_{in}})$.

One reason that initialization is important is to help mitigate numerical overflow errors. For example, the softmax transformation shown above exponentiates the raw output units (and sums them). If the output units have relatively large values, exponentiating and summing them can exceed allowable tolerances. For example, the output units shown in @fig-nnet-ex-layer-2 have a wide enough range that can be as large as `r max(round(abs(components$output_1)), 0)`. 

### Preprocessing {#sec-nnet-preprocessing}

Neural networks require complete, numeric features for computations. 

For non-numeric data, the tools described in @sec-categorical-predictors can be very effective. Additionally, there are pretrained neural network models that can translate the category string to a word embedding [@Boykis_What_are_embeddings_2023], such as  BERT [@devlin2019bert] or GPT-4 [@achiam2023gpt]. The potential upside to doing this is that the resulting numeric features may measure some relative context between categorical values. For example, suppose we have a predictor with levels `red`, `darkred`, `green`, `chartreuse`, and `white`. Basic encoders, such as binary indicators or hashing features, would not recognize that there are three clusters in these values and that sets such as {`red`, `darkred`} and {`green`, `chartreuse`} should have values that are "near" one another. LLMs can measure this semantic similarity and potentially generate more meaningful features. 

If the non-numeric predictors are not complex, a layer could be added early in the network, enabling the model to derive encodings based on the data when training. @guo2016entity describes a relatively simple architecture for doing this. While LLMs are better at creating embeddings overall, they are unaware of the _context_ in your data. Estimating the parameters associated with a simple embedding layer might be more effective since it is optimized for your data set. 

For numeric predictors, we suggest avoiding irrelevant predictors and highly collinear feature sets. @fig-irrelevant-predictors in @sec-feature-selection demonstrated the harm of including predictors unrelated to the outcome in a neural network. Multicollinearity can also affect these models, even if we do not invert the feature matrix or use the Hessian during optimization. Highly redundant predictors can result in the loss function having difficult characteristics. Using an L<sub>2</sub> penalty or a feature extraction method can be very helpful. 

Also, predictors with highly skewed distributions might benefit from a transformation to make their distribution more symmetric. If not, outlying values can exhibit significant leverage in computations, preventing high-quality parameter estimates. 

Finally, since the networks use random initialization, we should ensure that our predictors are standardized to have the same units. If there are only numeric predictors, centering/scaling or orderNorm operations can be very effective^[orderNorm is perhaps more attractive since it standardizes the predictors and coerces them to a symmetric distribution.]. Using this approach, we must also apply the same standardization to binary indicators created from categorical predictors.  Alternatively, we could leave 0/1 indicators as-is, but _range_ scale our predictors to also be between zero and one. This may have a disadvantage; the range will be common across predictors, but the mean and variance can be quite different. 

### Learning Rates {#sec-nnet-learning-rates}

The learning rate ($\alpha$) can profoundly affect model quality, perhaps more than the structural tuning parameters that define complexity (e.g., number of hidden units). A high learning range ($\approx$ 0.1) will help converge towards effective parameter estimates, but once there, it can cause the model to consistently overshoot the best location. One approach to modulating the rate is to use a scheduler [@lu2022gradient,chapt 4]. These are functions/algorithms that will change the rate (often to decrease it) over epochs. For example, to monotonically decrease the rate, two options are:

- inverse decay: $f(\alpha_0, \delta, i) = \alpha_0/(1 + \delta i)$ 
- exponential decay: $f(\alpha_0, \delta, i) = \alpha_0\exp(-\delta i)$

where $i$ is the epoch number and $\delta$ is a decay value. These two functions gradually decrease the rate so that, when near an optimum, it is slow enough not to exceed the optimal settings. Another approach is a step function that decreases the rate but holds it constant for a range of epochs. Those ranges can be predefined or set dynamically to decrease when the loss function is reduced. The function for the former is: 

- step decay: $f(\alpha_0, \zeta, r, i) = \alpha_0 \zeta^{\lfloor i/r\rfloor}$ 

where $\zeta$ is the reduction value, $r$ is the step length, and $\lfloor x\rfloor$ is the floor function. 

Finally, some schedulers modulate the rate up and down. A cyclic schedule [@smith2017cyclical] starts at a maximum value, decreases to a minimum value, and then increases cyclically. For some range $r$ and minimum rate $\alpha_0$, an epoch's location within the series is $cycle = \lfloor 1 + (i / (2r) )\rfloor$ and the rate is

- cyclic: $f(\alpha_0, \alpha, r, i) = \alpha_0 + ( \alpha - \alpha_0 ) \max(0, 1 - x)$

where $x = |(i/r) - (2\,cycle) + 1|$. This is a good option when we are unsure how fast or slow the convergence will be. If the rate is too high near an optimum value, it will soon decrease to an appropriate value. Also, if the search is stuck in a local optimum (or a saddle point), moving back to a larger learning rate might help it escape. 

@fig-schedulers uses the loss function example from @fig-stationary to demonstrate the effect of rate schedulers. Two starting points are used. One (in orange) is very close to a saddle point, while the other starts from the upper right corner of the parameter space (in green). 

```{r}
#| label: fig-schedulers
#| echo: false
#| out-width: 100%
#| fig-width: 8
#| fig-height: 2.8
#| fig-cap: "The effect of different schedulers on the rate and success of convergence."
base_sgd_plot(rates, NULL) + facet_wrap(~schedule, nrow = 1)
```

Using a constant learning rate of $\alpha = 0.1$, the search marches to the optimum value, but oscillates wildly on approach. Ultimately, it does not come close to the best estimates since it is constrained to make large jumps on each iteration. The decay panel represents inverse decay with $\delta = 0.025$. For the corner starting point, this is a very effective strategy. From the saddle point, we can see that the decrease in the learning rate results in fewer oscillations as the search proceeds and eventually finds its way to the optimum. The panel for stepwise reduction used an initial learning rate of $\alpha_0 = 0.1$, $\eta = 0.5$, and a step length of 20 epochs. This worked well when the search began from the saddle location, but it did poorly when starting at the corner. The success depended on when the search was near the optimum; if not timed right, the oscillations were too wide to converge on the best point. A reduction in the step length might help. Finally, the cyclic schedule allowed rates to move between $\alpha_0 = 0.001$ to $\alpha = 0.1$ with a cycle range of $r = 10$ epochs. This appears to work well or, at least, had good timing. 

The schedule type and its additional parameter(s) are added to the list of possible values that could be optimized during tuning.

### Regularization {#sec-nnet-regularization}

As seen in a previous chapter, we can add a penalty to the loss function to discourage particularly large model coefficients. This is especially beneficial for neural network models. 

While @hanson1988comparing and @weigend1990generalization made some early proposals for penalizing the size of the estimated parameters, @krogh1991simple proposed to use L<sub>2</sub> regularization to improve generalization. This is now a standard approach for training neural networks and "weight decay" is now synonymous with squared penalties. 

While either L<sub>1</sub>, L<sub>2</sub>, or a mixture of the two can be used, there is some reason to use just L<sub>2</sub> penalization. While either type of penalty might be able to combat multicollinearity, the quadratic penalty has the advantage of maintaining a smooth loss surface. With an L<sub>1</sub> penalty, we have at least one point on the loss surface that is not differentiable. 

As we will be described later in this chapter, @loshchilov2017decoupled note that there are cases when penalizing the loss function does not precisely match the definition of weight decay (in the neural network context of @hanson1988comparing). Their adjustment for this issues is called the "AdamW" method. 

Another regularization technique is called _dropout_. This method randomly removes some units (hidden or not) from the network. This drops incoming and outgoing connections to other units, which propagates through all of the layers. These temporary coercions to zero reoccur with each evaluation of the gradient. In the final training step, a full set of parameters is used. Dropout has an effect similar to averaging many smaller networks. The amount of dropout should be tuned.

### Early Stopping {#sec-early-stopping}

When should training stop? We can guess how many epochs might be needed, but if we guess poorly, we risk under- or overfitting the model. It makes sense to include the number of epochs as a tuning parameter, but this could be costly^[Although the submodel trick described in @sec-submodels could greatly improve the effectiveness of this approach.]. 

If we have some data to spare, an efficient and effective approach is to use early stopping. If we set aside a small proportion of the training set to measure the search's progress, we can set the _maximum_ number of epochs and stop training when the loss worsens (based on our holdout set). Often, we can tune the number of "stopping epochs.". If this value were five, we would stop training after five consecutive epochs where the loss function worsens (and use the parameter estimates from our last best epoch). Values greater than one protect against overreacting when the loss profile is noisy. 

### Batch Learning {#sec-batch-learning}

_Stochastic_ gradient descent is the most widely used optimizer for neural networks. Technically, that term is reserved for cases when the gradient and parameter updates are computed using a single training set point at a time. The term _mini-batch_ is used when the training set is randomly divided into a set of equally sized batches for multiple training set points, as we did earlier. 

Operationally, within each epoch, a random batch is selected to evaluate the gradient and corresponding update the parameters. This occurs for each batch until the entire set is evaluated. This is the end of the epoch. If we are monitoring a holdout set for early stopping, this is the point at which we measure the loss function at the current parameter estimate using the holdout. After this, the process begins again by iterating through the batches. 

SGD is thought to work well because it injects a considerable amount of variation into the gradients and updates. This would generally be thought to be a bad thing, but, as with simulated annealing, it makes the optimization less greedy. The somewhat random directions the optimizer takes can help the optimization escape from local optimums and be more resilient when the search veers near a saddle point or other regions where the gradient may be flat or consistently zero. @hoffer2017train referred to SGD as a "high-dimensional random walk on a random potential" process. 

As the search proceeds, lowering the learning rate can mitigate the extra noise in the gradients so that, hopefully, the search does not move away from the best parameter estimates. 

We can tune the batch size if there is no intuition for what a good value might be. Smallish batch sizes could range from 16 to 512 training set points^[Typically on log<sub>2</sub> units], depending on the training set size. @keskar2016large remarks that smaller batches tend to explore the parameter space more thoroughly and that, as the batch size becomes larger, SGD will find a solution that is closer to the initial values. 

@loshchilov2017decoupled make a few interesting observations related to L<sub>2</sub> penalization with stochastic gradient descent. Their results show that as the number of batches increases, the smaller the optimal penalization becomes. In other words, as the batch size goes down, the amount of penalization should also. 

### Gradient Descent {#sec-gradient-descent}

Gradient descent methods for training neural networks have become popular research topics in the last 15 years. We'll discuss a few major techniques here, but there are myriad other tools for training these models. In this section, we'll temporarily forgo discussing changes from epoch to epoch. Since we are focused on SGD, we'll use the term update to refer to the point at which the parameters move to a different location due to a batch step. 

#### Momentum {.unnumbered}

To start, let's consider a more common form of momentum in gradient search^[In @sec-nnet-training, we previously described Nesterov momentum. This is unrelated to the current tool bearing the same name.] Momentum is a useful tool that can help in situations such as the results in @fig-schedulers where the updates make the search path make large ineffective jumps. When the learning rate is consistently large, the search route bounces back and forth since the gradients are substantial in magnitude but moving in different consecutive directions. 

Momentum is one of several tools to adjust the gradient vector based on previous gradients. For iteration $i$, a vector is computed that is a combination of the current and previous gradients^[For the first update, the value is simply $v_1 = \alpha g(\boldsymbol{\theta}_i)$.]: 

$$
v_i = \phi v_{i-1} + \alpha_i g(\boldsymbol{\theta}_i)
$$

and this is used to make the update $\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - v_i$. This weighting system accentuates recent gradients but factors in previous gradients. This can accelerate the search and diminish occilations.   

Values of $\phi$ are usually between 0.8 and 0.99 and represent a weighted average of previous gradient directions, with larger emphasis on the recent iterations as $\phi$ increases. If the directions have been fairly stable, there is not much change, but if they are oscillating, the direction takes a more consistent central route with fewer directional jumps. It also helps if the search is in a region where gradients are close to zero; historical gradients will have some non-zero values that are not likely to get stuck with updates near zero. 

@fig-sgd shows the results when momentum is used for our 2D toy example^[Since the loss function for this example is based on a single, deterministic value, the momentum value was set very low ($\phi = 2/3$). However, the overall trend is consistent with results from a typical performance metric like cross-entropy.]. The cyclic rate settings were used for this example, and the results appear very similar to the previous path when initialized near the saddle point. The corner point starting value shows that momentum can lead to a location near the optimal value, but takes a more circuitous route. 

```{r}
#| label: fig-sgd
#| echo: false
#| out-width: 100%
#| fig-width: 8
#| fig-height: 2.8
#| warning: false
#| fig-cap: "Examples of four different optimizers for the loss surface previously shown in @fig-schedulers."

bind_rows(momentum_res, adam_res, rms_prop_res, adagrad_res) |>
  mutate(
    technique = factor(
      technique,
      levels = c("Momentum", "AdaGrad", "RMSprop + Nesterov", "ADAM")
    )
  ) |>
  base_sgd_plot(NULL) +
  facet_wrap( ~ technique, nrow = 1)
```

Momentum should be used with a rate scheduler, or at least without using a consistently large learning rate. Making very large jumps in a central direction might lead the search to points far away from regions of improvement. Penalization also helps. 

#### Adaptive Learning Rates {.unnumbered}

There is also a class of gradient descent methods that try to avoid using a single learning rate for all of the elements of $\boldsymbol{\theta}$.  Instead, they use a constant learning rate and, at each update, adjust the rate using functions of the gradient vector so that the rates are different from each $\theta_j$. 

One approach is the adaptive gradient algorithm, a.k.a. AdaGrad [@duchi2011adaptive], that scales the constant learning rate $\alpha$ by the accumulated square of the gradients: 

$$
\boldsymbol{v}_i =\sum_{b=1}^t g(\boldsymbol{\theta}_b)^2 
$$
so that 

$$
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \frac{\alpha}{\sqrt{\boldsymbol{v}^2_i + \epsilon}}\, g(\boldsymbol{\theta}_i)
$$

where $\epsilon$ is a small constant to avoid division by zero. 

The square root in the denominator is required to keep the denominator units the same as those in the numerator. Adam has the advantage of keeping the adjusted learning rate steady for elements of $g(\boldsymbol{\theta})$ that do not change substantially from update to update. At update $i$, the contribution to the learning rate is inversely proportional to the squared gradient value. 

The problem is the unweighted accumulation of squared values. For a reasonable number of updates, elements of $\boldsymbol{v}$ can explode to very large values, and this prevents almost _any_ movement of $\boldsymbol{\theta}$ from update to update. For the example shown in @fig-sgd, some of these values were in the thousands, and progress clearly halted before reaching the optimal parameters. 

One method of fixing this problem is RMSprop [@hinton2012neural]. Here, the denominator uses an exponentially weighted moving average of the squared gradients: 

$$
\boldsymbol{v}_i = \rho\, \boldsymbol{v}_{i-1} + (1 - \rho)\, g(\boldsymbol{\theta}_i)^2 
$$

This formulation places more weight on the current and recent gradients than the statistic used by AdaGrad. The RMSprop update becomes: 

$$
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \frac{\alpha}{\sqrt{\boldsymbol{v}_i + \epsilon}}\, g(\boldsymbol{\theta}_i).
$$

The "RMS" in the name is due to taking the square root of the weighted sum of squared gradients (i.e., similar to a traditional root mean square). 

@goodfellow2016deep suggested using RMSprop with Nesterov momentum. This leads to a more complicated procedure. First, we compute our initial update: $\quad\tilde{\boldsymbol{\theta}}_{i} = \boldsymbol{\theta}_{i} - \phi\,\boldsymbol{\delta}_{i-1}$ where $\boldsymbol{\delta}_0$ is initialized with a vector of zeros. We compute the gradient at this temporary update and use this to compute $\boldsymbol{v}_i$. From here: 

$$
\boldsymbol{\delta}_{i} = \phi\,\boldsymbol{\delta}_{i-1} - \frac{\alpha}{\sqrt{\boldsymbol{v}_i + \epsilon}}\, g(\tilde{\boldsymbol{\theta}}_i)
$$

The results are in @fig-sgd. The curves are similar to those for AdaGrad except that they do reach the location of minimum loss. The path for the starting value near the saddle point shows a number of minor oscillations on the way to the best result. 

The adaptive moment estimation (Adam) optimizer from @kinga2015method  extends the moving average approach to the gradient term as well:  

$$
\begin{align}
\boldsymbol{m}_i &= \phi\, \boldsymbol{m}_{i-1} + (1 - \phi)\, g(\boldsymbol{\theta}_i) \notag \\
\boldsymbol{v}_i &= \rho\, \boldsymbol{v}_{i-1} + (1 - \rho)\, g(\boldsymbol{\theta}_i)^2 \notag 
\end{align}
$$

Adam also tries to normalize these quantities so that the first few updates have larger values:   

$$
\begin{align}
\boldsymbol{m}^*_i &= \frac{\boldsymbol{m}_i}{1 - \phi^2}\notag \\
\boldsymbol{v}^*_i &= \frac{\boldsymbol{v}_i}{1 - \rho^2} \notag 
\end{align}
$$

This helps offset the initialization of $\boldsymbol{m}_0 = \boldsymbol{v}_0 = \boldsymbol{0}$. At update $i$, Adam's proposal is

$$
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \frac{\alpha}{\sqrt{\boldsymbol{v}^*_i + \epsilon}}\,\boldsymbol{m}^*_i.
$$

The results above show that Adam's path towards the optimal parameters is slightly wobbly compared to RMSprop but otherwise effective. 

Both $\phi$ and $\rho$ are tunable in these models and typically range within [0.8, 0.99]. The constant learning rate $\alpha$ is also tunable. In @fig-sgd, $\phi=\rho=0.9$. AdaGrad, Adam, and RMSprop used $\alpha = 0.1$. For RMSprop, decreasing the rate to $\alpha = 0.08$ removed the oscillation effects shown in @fig-sgd.

As was previously alluded to, @loshchilov2017decoupled recognized that, for Adam, penalizing the loss function does not match the definition of weight decay. Without a remedy, they found that Adam is not as effective as it could be. Their method, called AdamW, includes the penalty in the gradient definition. The gradient used to compute $\boldsymbol{m}_i$ and $\boldsymbol{v}_i$ is a combination of the previous "raw" gradient and the penalty ($\lambda$): 

$$
g(\boldsymbol{\theta}_i) = \nabla \psi(\boldsymbol{\theta}_{i}) - \lambda \boldsymbol{\theta}_{i}
$$

The updating equation also uses an extra term containing the penalty: 

$$
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i - \alpha_i\left[\frac{\boldsymbol{m}^*_i}{\sqrt{\boldsymbol{v}^*_i + \epsilon}} + \lambda \boldsymbol{\theta}_{i}\right].
$$

Note that the learning rate $\alpha$ now has a subscript. They also recommend trying a cyclic learning rate scheduler with AdamW.  

#### Other Methods {.unnumbered} 

As previously mentioned, there are numerous other variations of SGD that will not be discussed here, including AdaMax [@kinga2015method], Nadam [@dozat2016], MadGrad [@defazio2021], and others.  

### Newton's Method {#sec-newtons-method}

### Forestation Modeling  {#sec-nnet-forestation}

For the forested data, there are a few options for preprocessing: 

 - Use binary indicators to represent the XXX counties or a supervised effect encoding strategy. 
 - Several predictors are highly skewed. We can leave them as-is before centering/scaling, or use an orderNorm transformation to coerce each predictor to a standard normal distribution. 
 - Deal with the high correlation for some predictors via PCA extraction or leave as-is in the data. 

Several combinations of these operations were used: 

 - An effect encoding with and without PCA feature extraction.
 - Standard binary indicators with and without PCA feature extraction and with and without a symmetry transformation (i.e., orderNorm versus centering/scaling). 

For the PCA extraction, all possible components were used. 

For the model, there are a fair number of tuning parameters to combine: 

 - One or two layers.
 - The number of hidden units. For each layer, these ranged from two to fifty units. 
 - The activation type: ReLU, elu, tanh, and tanhshrink.
 - L<sub>2</sub> regulazation values between 10<sup>-10</sup> and 10<sup>-1</sup>.
 - Batch sizes between 2<sup>3</sup> and 2<sup>8</sup>.
 - Learning rates between 10<sup>-4</sup> and 10<sup>-1</sup>.
 - Learning rate schedules: constant, cyclic, and time decay.
 - Momentum values between [0.8, 0.99].
 - Early stopping triggers between three and twenty bad epochs. 

AdamW was used to train the models, and a maximum of 1,000 epochs (with early stopping) was allowed. 

A space-filling design with XXX candidates was evaluated. The XXX preprocessing schemes listed above were each run with the same model grid. 





## Special Tabular Network Models  {#sec-tab-net}

### Transformers {#sec-nnet-transformers}

### Attention {#sec-nnet-attention}

## K-Nearest Neighbors  {#sec-cls-knn}

### Discriminant Adaptive Nearest Neighbors  {#sec-dann}

## Support Vector Machines {#sec-cls-svm}

## Chapter References {.unnumbered}
