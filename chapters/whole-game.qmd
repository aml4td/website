---
knitr:
  opts_chunk:
    cache.path: "../_cache/whole-game/"
---

# The Whole Game {#sec-whole-game}

```{r}
#| label: whole-game-setup
#| include: false
source("../R/_common.R")

# ------------------------------------------------------------------------------

library(ragg)
library(doParallel)
library(patchwork)
library(tidymodels)
library(bestNormalize)
library(probably)
library(rules) 
library(gt)

# Also requires packages: Cubist, splines2, brulee

# ------------------------------------------------------------------------------
# Set options

tidymodels_prefer()
theme_set(theme_transparent())
cl <- makePSOCKcluster(parallel::detectCores(logical = TRUE))
registerDoParallel(cl)
set_options()

# ------------------------------------------------------------------------------
# Load data

source("../R/setup_deliveries.R")

```

Many resources for teaching machine learning gloss over some crucial details that might appear to be tangential to the model. As a result, they omit parts that are critical to the overall project. 

The notion of "the whole game" is to illustrate machine learning in a way that reflects the complexity of the entire process. @perkins2010 uses this phrase in the context of learning baseball. Instead of focusing players on one or two key aspects of the game, it is better to be exposed (at some level) to everything. In his words, a smaller focus "was kind of like batting practice without knowing the whole game." 

This chapter is a self-contained case study. 

Our goal is to provide a somewhat abbreviated, high-level overview to help readers understand the overall strategy before we get into the specific set of tactics that we might use. This chapter previews what is to come and sets the stage to discuss some important themes that may not be obvious at first glance. 

We'll avoid many details in this chapter by copiously referencing upcoming chapters.  

## Predicting Delivery Time  {#sec-delivery-times}

The illustrative example focuses on predicting the food delivery time (i.e., the time from the initial order to receiving the food). There is a collection of data containing `r format(nrow(deliveries), big.mark = ",")` orders from a specific restaurant. The predictors, previously shown in @tbl-deliveries, include: 

- The time, in decimal hours, of the order. 
- The day of the week for the order. 
- The approximate distance between the restaurant and the delivery location. 
- A set of `r length(grep("^item", names(deliveries)))` predictors that count the number of distinct menu items in the order.

The outcome is the time^[For event time data, it is possible to get incomplete data due to _censoring_. Suppose that, after 10 minutes you don't have your order yet. You don't know the exact delivery time but you do know that it is at least 10 minutes. There are specialized tools for analyzing and modeling censored data. Thankfully, all of our data are complete since we know the exact time of delivery.] (in minutes). 

```{r}
#| label: time-mode
#| echo: false

dens_t <- density(deliveries$time_to_delivery)
mode_ish <- round(dens_t$x[which.max(dens_t$y)], 0)
```

The next section describes the process of _splitting_ the overall data pool into different subsets and signifies the start of the modeling process. However, _how_ we make that split depends on some aspects of the data. Specifically, it is a good idea to examine the distribution of our outcome data (i.e., delivery time) to determine how to randomly split the data^[Overall, we advise looking at the other columns in the data _after_ the splitting.]. 


@fig-delivery-hist (panel (a)) uses a histogram to examine the distribution of the outcome data. It is right-skewed and there is a subset of long delivery times that stand out from the mainstream of the data. The most likely delivery time is about `r mode_ish` minutes, but there is also a hint of a second mode a few minutes earlier. 

```{r}
#| label: fig-delivery-hist
#| dev: "ragg_png"
#| fig-width: 9
#| fig-height: 4.25
#| fig-align: "center"
#| out-width: "80%"
#| fig-cap: "Histrograms of the training set outcome data with and without a transformation." 

delivery_hist <- 
  deliveries %>% 
  ggplot(aes(x = time_to_delivery)) +
  geom_histogram(bins = 30, col = "white") +
  geom_rug(alpha = 1 / 4) +
  labs(x = "Time Until Delivery (min)", title = "(a)")

delivery_log_hist <- 
  deliveries %>% 
  ggplot(aes(x = time_to_delivery)) +
  geom_histogram(bins = 30, col = "white") +
  geom_rug(alpha = 1 / 4) +
  labs(x = "Time Until Delivery (min)", title = "(b)") +
  scale_x_log10(breaks = log_2_breaks, labels = log_2_labs)

delivery_hist + delivery_log_hist
```


This skewness might affect how we split the data and is discussed in the next section. Another important question is: should we model a transformed version of the delivery times? If we have large outlying values, the model might be inappropriately pulled to these values, often at the expense of the overall quality of fit. One approach is to model the logarithm of the times (as shown in @eq-log-linear). @fig-delivery-hist (panel (b)) shows the distribution of the log (base 2) deliveries. It is more symmetric, and the potential second mode is more pronounced. This isn't a bad idea, and the primary consequence is how we interpret some of the metrics that are used to quantify how well the model fits the data^[These are described more in @sec-outcome-transformation.]. In the end, we decided to model the data in the original units but did try the analysis both ways (and did not find huge differences in the results). 


## Data Spending  {#sec-data-spending-whole-game}


```{r}
#| label: delivery-split
#| echo: false

source("../R/setup_deliveries.R")
```

An important aspect of machine learning is to use the right data at the right time. We don't start modeling with the entire pool of data.  As will be seen in @sec-resampling, we need to reserve some data to evaluate the model and make sure that those data do not overlap with those used to fit the model.

Often, our data spending strategy is to split the data into at least two subsets: 

- The **training set** is used to develop the model pipeline. It is used for investigating, fitting, and comparing them. 
- The **test set** is a smaller fraction of the data that is only used at the very end to independently validate how well we did. 

The training set is used for _a lot_ of our activities. We do need to evaluate multiple model pipelines along the way. We reserve the test set for the final assessment and should not use it for development. There are two main strategies for characterizing model efficacy during development. 

The first is to _resample_ the training set. This is an iterative computational tool to find good statistical estimates of model performance using the training set. 

The other option is to initially split the data into three subsets: the training set, the testing set, and the **validation set**. Validation sets are small subset used for evaluating the model repeatedly during development. 

Generally, we suggest using resampling unless the initial data pool is "sufficiently large." For these data, `r format(nrow(deliveries), big.mark = ",")` food orders is probably sufficient to choose a validation set over the resampling approach. 

To split the data, we'll allocate 60% for the training set and then use 20% for the validation set, and the remaining 20% for testing. The splitting will be conducted by stratification. To ensure that our delivery time distributions are approximately the same, it is temporarily "binned" into four quartiles. The three-way split is executed within each quartiles and the overall training/validation/testing sets are assembled by aggregating the corresponding data from the quartiles. As a result, the respective sample sizes are `r nrow(delivery_train)`/`r nrow(delivery_val)`/`r nrow(delivery_test)` deliveries. 

As previously stated, the majority of our time is spent with the training set samples. The first step in any model-building process is to understand the data, which can most easily be done through visualizations of the training set.

## Exploratory Data Analysis  {#sec-eda-whole-game}

Our goal now is to determine if there are aspects of predictors that would affect how we provide them to the model. We should look at individual variables as well as combinations multiple variables. 

To start, for the dense numeric predictors, how would we characterize their relationship to the outcome? Is it linear, nonlinear, or random? Let's start with the distance predictor. @fig-delivery-predictors(a) shows the values with the "scatter plot smoother" fit. The line is a flexible nonlinear approach that can adapt to any apparent pattern in the  plot^[The two strategies for creating flexible, smooth fits are splines and generalized additive models. These are described in detail in Sections [-@sec-splines] and [-@sec-cls-gam], respectively. If you have used the `ggplot2` function `geom_smooth()` function, you've used a smoother. ]. First, the plot shows that the distance distribution is also right-skewed (although a separate histogram would better illustrate this). The smoother shows a slightly nonlinear trend, although the nonlinearity is mainly in the right tail of the distances and might be an artifact of the small sample size in that region. Regardless, it appears to be an informative predictor. 

```{r}
#| label: fig-delivery-predictors
#| message: false
#| dev: "ragg_png"
#| fig-width: 8
#| fig-height: 8
#| fig-align: "center"
#| out-width: "80%"
#| fig-cap: "Visualizations of relationship between the outcome and several predictors using the training set." 

day_cols <-  c("#000000FF", "#24FF24FF", "#009292FF",  "#B66DFFFF", 
               "#6DB6FFFF",  "#920000FF",  "#FFB6DBFF")

delivery_dist <- 
  delivery_train %>% 
  ggplot(aes(x = distance, time_to_delivery)) +
  geom_point(alpha = 1 / 10, cex = 1) +
  labs(y = "Time Until Delivery (min)", x = "Distance (miles)", title = "(a)") +
  geom_smooth(se = FALSE, col = "red")

delivery_day <- 
  delivery_train %>% 
  ggplot(aes(x = day, time_to_delivery, col = day)) +
  geom_boxplot(show.legend = FALSE)  +
  labs(y = "Time Until Delivery (min)", x = NULL, title = "(c)") +
  scale_color_manual(values = day_cols)

delivery_time <- 
  delivery_train %>% 
  ggplot(aes(x = hour, time_to_delivery)) +
  labs(y = "Time Until Delivery (min)", x = "Order Time (decimal hours)", title = "(b)") +
  geom_point(alpha = 1 / 10, cex = 1) + 
  geom_smooth(se = FALSE, col = "red")

delivery_time_day <- 
  delivery_train %>% 
  ggplot(aes(x = hour, time_to_delivery, col = day)) +
  labs(y = "Time Until Delivery (min)", x = "Order Time (decimal hours)", title = "(d)") +
  geom_smooth(se = FALSE) + 
  scale_color_manual(values = day_cols)
 
( delivery_dist + delivery_time ) / (delivery_day + delivery_time_day) +
  plot_layout(guides = 'collect')  & 
  theme(legend.title = element_blank(), legend.position = "bottom")
```


```{r}
#| label: time-ratios
#| include: false
#| cache: true

time_ratios <- function(x) {
  x %>% 
    pivot_longer(cols = c(starts_with("item")), names_to = "predictor", values_to = "count") %>% 
    mutate(ordered = ifelse(count > 0, "yes", "no")) %>% 
    summarize(
      mean = mean(time_to_delivery),
      .by = c(predictor, ordered)
    ) %>% 
    pivot_wider(id_cols = predictor, names_from = ordered, values_from = mean) %>% 
    mutate(ratio = yes/no) %>% 
    select(term = predictor, estimate = ratio)
}

set.seed(624)
resampled_ratios <- 
  delivery_train %>% 
  select(time_to_delivery, starts_with("item")) %>% 
  bootstraps(times = 5000) %>% 
  mutate(stats = map(splits, ~ time_ratios(analysis(.x)))) %>% 
  int_pctl(stats, alpha = 0.1) %>% 
  mutate(
    term = gsub("_0", " ", term),
    term = factor(gsub("_", " ", term)),
    term = reorder(term, .estimate),
    increase = .estimate - 1,
    increase_lab = round(100 * increase, 1)
  ) %>% 
  arrange(desc(increase))
```

Panel (b) has a similar visualization for the order time. In this case, there is another nonlinear trend where the order increases with the hour, then peaks around 17:30, then begins to decrease. There is also an increase in delivery time variation as the order time approaches the peak. 

The day of the week is visualized via box plots in panel (c). There is an increasing trend in both mean and variation as we move from Monday to Friday/Saturday, then a decrease on Sunday. Again, this appears to be an important predictor. 

These three panels show relationships between individual predictors and delivery times. It is possible that multiple predictors can act _in concert_; their effect on the outcome occurs jointly. Panel (d) of @fig-delivery-predictors shows that there is an **interaction effect** between the day and hour of the order. Mondays and Tuesdays have shorter delivery times that don't change much over hours. Conversely, on Fridays and Saturdays, the delivery times are generally longer, with much higher peaks. This might explain the increase in delivery time variation that was seen in Panel (b). Now that we know of the existence of this interaction, we might add additional model terms to help the model understand and exploit this pattern in the training set. 

For the `r length(grep("^item", names(deliveries)))` itemized frequency counts, we might want to know if there was a change in delivery time when a specific item was in the order (relative to those where it is not purchased). To do this, mean delivery times were computed for orders with and without the item. The ratio of these times was computed and then converted to the percent increase in time. This is used to quantify the effect of item on delivery time. We'd also like to know what the variation is on this statistic and a technique called the bootstrap [@davison1997bootstrap] was used to create 90% confidence intervals for the ratio. @fig-delivery-increases shows the results. 

```{r}
#| label: fig-delivery-increases
#| message: false
#| fig-width: 5
#| fig-height: 5
#| fig-align: "center"
#| out-width: "50%"
#| fig-cap: "Ranking the items by the increase in delivery time when ordered at least once." 

resampled_ratios %>% 
  ggplot(aes(increase, term)) + 
  geom_vline(xintercept = 0, col = "red", alpha = .3) +
  geom_point() + 
  geom_errorbar(aes(xmin = .lower - 1, xmax = .upper - 1), width = .5) +
  scale_x_continuous(labels = scales::percent) +
  labs(y = NULL, x = "Increase in Delivery Time When Ordered") +
  theme(axis.text.y = element_text(hjust = 0))
```

There is a subset of items that increase the delivery time (relative to the experimental noise), many that seem irrelevant, and one that _decreases_ the time. 

More investigations into the data would be pursued to better understand the data and help us with feature engineering. For brevity, we'll stop here and introduce three ML models. 

## Model Development  {#sec-model-development-whole-game}

Which model to use? There is a multitude of tools that can be used to represent and predict these data. We’ll illustrate three in this section and the essential concepts to consider along the way. 

Before proceeding, we need to pick a performance metric. This is used to quantify how well the model fits the data. For any model fit, the _residual_ (usually denoted as  $\epsilon$, also called the error) is the difference between the model prediction and the observed outcome. We'd like our model to produce residuals near zero. One metric^[Discussed in @sec-reg-metrics] is the mean absolute error (MAE). For a data set with $n$ data points, the equation is 

$$
MAE = \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i| = \frac{1}{n}\sum_{i=1}^n |\epsilon_i| 
$$ {#eq-mae}

where $\hat{y}$ is the predicted numeric outcome value and $\epsilon$ is the model error. The MAE units are the same as the outcome which, in this case, is minutes. We desire to minimize this statistic.

To get started, we'll use a basic linear model for these data. 

```{r}
#| label: model-options
#| include: false

ctrl_rs <-
  control_resamples(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE
  )

get_iters <- function(x) {
  fit <- extract_fit_engine(x)
  tibble::tibble(iteration = seq(along = fit$loss), 
                 loss = fit$loss)
}

ctrl_grid <-
  control_grid(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE,
    extract = get_iters
  )
reg_metrics <- metric_set(mae, rmse, rsq)
```

### Linear Regression {.unnumbered}

Linear regression is probably the most well-known statistical model in history. It can predict a numeric outcome using one or more predictors using the equation:

$$
y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \ldots + \beta_px_{ip} + \epsilon_i
$$ {#eq-basic-linear-reg}

where $y_i$ is the outcome for the $i^{th}$ data point (out of $n_{tr}$), and $x_{ij}$ is value for the $j^{th}$ predictor (out of $p$). 

For @eq-basic-linear-reg, there are many ways to estimate the model parameters ($\beta$'s). _Ordinary least squares_ is used most often. It finds the values of the $\beta_j$ that that minimize sum of squared errors (SSE). @sec-reg-linear discusses several other ways to estimate the model parameters that use other criteria (variations of the SSE).  

The main assumptions about the data for ordinary least squares are that they are independent and identically distributed. Given what we know about the problem, both assumptions are at least mildly incorrect. Orders are being processed simultaneously (affecting independence), and our visualizations have indicated that there is an increase in variation with the mean (i.e., not identically distributed). However, we are most interested in prediction here, so we can often overlook slight to moderate violations of these assumptions^[If we want to make inferences on the parameters, such as p-values and confidence intervals, we we'll need to make more specific assumptions (such as normality of the errors).]

What are our $x_{ij}$ values in @eq-basic-linear-reg? We can't give the model equations values of `"Monday"` or `"Friday"`; OLS requires numbers in its equations. A simple workaround is to create binary indicator variables for six of the seven days of the week. For example, the indicator for Friday would have a value of one for orders placed on a Friday and zero otherwise^[Different ways to convert categorical predictors to numeric predictors are detailed in @sec-categorical-predictors.]. 

Also, for @eq-basic-linear-reg, what do we do about the numeric predictors that appear to have nonlinear trends? We can _augment_ the predictor by describing it with multiple columns. @eq-basic-linear-reg shows that linear regression is **linear in the parameters** (not the predictors). We can add terms such as $x_{ij} = distance_i^2$ to allow the fit to be nonlinear. Alternatively, an effective tool for enabling nonlinear trends is _spline functions_^[@sec-splines]. These are special polynomial terms. For example, we could represent that original predictor with 10 spline terms (i.e., columns in the data) for the order hour. As we add more terms, the model fit has increased  capacity to be nonlinear. We'll use 10 separate splines for both distance and the order hour (in lieu of the original data columns.)

```{r}
#| label: lm-fit
#| include: false
#| cache: true

spline_rec <- 
  recipe(time_to_delivery ~ ., data = delivery_train) %>% 
  step_dummy(all_factor_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_spline_natural(hour, distance, deg_free = 10) %>% 
  step_interact(~ starts_with("hour"):starts_with("day"))

lin_reg_wflow <- 
  workflow() %>% 
  add_model(linear_reg()) %>% 
  add_recipe(spline_rec)

set.seed(3838)
lin_reg_res <-
  fit_resamples(lin_reg_wflow,
                delivery_rs,
                control = ctrl_rs,
                metrics = reg_metrics)

lin_reg_fit <- fit_best(lin_reg_res)
lin_reg_coef <- tidy(lin_reg_fit)
lin_reg_metrics <- collect_metrics(lin_reg_res)
lin_reg_mae <- lin_reg_metrics$mean[lin_reg_metrics$.metric == "mae"]
```

How do we encode the interaction seen in @fig-delivery-predictors(d)? We have six indicator columns for the order day and ten spline columns for the order hour. We can facilitate having different nonlinear hour trends for each day by making `r length(grep("_x_", lin_reg_coef$term))` cross-product terms. For example, the interactions for Fridays would multiply each of the ten spline terms by the binary indicator associated with Fridays. 

These feature engineering operations result in a model with $p = `r nrow(lin_reg_coef)`$ $\beta$ parameters. We fit this model to the training set to estimate the model parameters. To evaluate it, we use the trained model to predict the validation set, and these predictions are the substrate to compute the MAE. For this model, the validation set MAE is `r round(lin_reg_mae, 3)` (i.e., `r dec_to_time(lin_reg_mae)`). If we think of MAE as an average error, this doesn't seem too bad. However, what does this _look like_? Let's plot the observed times against predicted values to get a better sense of how well this model did. @fig-delivery-lm-obs-pred has the results which also feature a scatter plot smoother. 

```{r}
#| label: fig-delivery-lm-obs-pred
#| message: false
#| dev: "ragg_png"
#| fig-width: 4
#| fig-height: 4
#| fig-align: "center"
#| out-width: "40%"
#| fig-cap: "A scatter plot of the observed and predicted delivery times, via a linear regression model, for the validation set. The red line is a smooth trend estimate." 

lin_reg_res %>% 
  collect_predictions() %>% 
  ggplot(aes(time_to_delivery, .pred)) +
  geom_abline(col = "green", lty = 2) +
  geom_point(alpha = 0.4, cex = 1) +
  geom_smooth(se = FALSE, col = "red") +
  coord_obs_pred() +
  labs(x = "Observed Delivery Time", y = "Predicted Delivery Time")
```

The best case would be that the points arrange themselves tightly around the diagonal line. The model slightly under-predicts for very rapid deliveries but substantially under-predicts for times greater than 40 minutes. However, overall, the model works well for the majority of the deliveries. 

From here, our next step would be to investigate poorly predicted samples and determine if there is some commonality to them. For example, are they short distances that are ordered late on a Friday, etc? If there appears to be a pattern, we would add additional model terms to compensate for the flaw and see if the validation set MAE decreases. We would iterate between exploratory analysis of the residuals, adding or subtracting features, then refitting the model. 

For illustration, we stop here^[We will re-examine the linear regression model for these data in  @sec-interactions-detection, where we discover (spoilers!) an important term left out of this analysis.] and take a look at very different model.  

### Rule-Based Ensemble {.unnumbered}

Cubist is a rule-based ensemble method. It creates a rule set by first generating a regression tree that is composed of a hierarchical set of if/then statements^[The details for regression trees and Cubist are in @sec-reg-trees]. @fig-reg-tree shows a simple tree for our data. A set of splits on different predictors results in six terminal nodes (at the bottom of the tree). A rule is a distinct path through the tree to a terminal node. For example, Node 11 in the figure has the corresponding rule: `hour >= 14.77 & day in {Fri, Sat} & distance >= 4.045`. For each rule, Cubist fits a linear regression to the data that the rule covers ($n = 261$ for Node 11). It builds new rule sets sequentially and uses many rules/regression equations to predict new samples. 


```{r}
#| label: fig-reg-tree
#| echo: false
#| out-width: 80%
#| fig-width: 12
#| fig-height: 6
#| fig-cap: An example of a regression tree used to create a set of six rules.

# See R/whole-game-tree.R
if (knitr::is_html_output()) {
  knitr::include_graphics("../premade/delivery-tree.svg")
} else {
  knitr::include_graphics("../premade/delivery-tree.pdf")
}
```
  
The method isolates different parts of the predictor space and, with these subsets, models the data using linear functions. The creation of multiple rules enables the model to emulate nonlinear functions easily. When a rule consists of multiple predictors, this constitutes an interaction effect. Additionally, the data preprocessing requirements are minimal: it can internally account for any missing predictor values, and there is no need to convert categorical predictors into binary indicator columns, and so on.


```{r}
#| label: cubist-fit
#| include: false
#| cache: true

cb_wflow <- 
  workflow() %>% 
  add_model(cubist_rules(committees = 100)) %>% 
  add_formula(time_to_delivery ~ .)

set.seed(3838)
cb_res <-
  fit_resamples(cb_wflow, delivery_rs, control = ctrl_grid, metrics = reg_metrics)

cb_metrics <- collect_metrics(cb_res)
cb_mae <- cb_metrics$mean[cb_metrics$.metric == "mae"]

cb_fit <- fit_best(cb_res)

get_rule_info <- function(number, x) {
  require(tidymodels)
  x %>% 
  extract_fit_engine() %>% 
  tidy(committees = number) %>% 
  mutate(
    rule_expr = map(rule, rlang::parse_expr),
    rule_mem = map_int(rule_expr, ~ length(all.vars(.x))),
    reg_mem = map_int(estimate, ~ nrow(.x) - 1),
    rule_set = number
  )
}

all_rules <- foreach(i = 1:100, .combine = bind_rows) %dopar% get_rule_info(i, x = cb_fit)
num_rules <- nrow(all_rules)
mean_rule_vars <- round(mean(all_rules$rule_mem), 1)
mean_reg_vars <- round(mean(all_rules$reg_mem), 1)

val_1_rules <- 
  foreach(i = 1:nrow(all_rules), .combine = c) %dopar% 
  rlang::eval_tidy(all_rules$rule_expr[[i]], data = delivery_val %>% slice(1))
val_1_rules <- sum(val_1_rules)
val_1_pct <- round(100 * val_1_rules / num_rules)

### 

cb_feat_wflow <- 
  workflow() %>% 
  add_model(cubist_rules(committees = 100)) %>% 
  add_recipe(spline_rec)

set.seed(3838)
cb_feat_res <-
  fit_resamples(cb_feat_wflow, delivery_rs, control = ctrl_grid, metrics = reg_metrics)

cb_feat_metrics <- collect_metrics(cb_feat_res)
cb_feat_mae <- cb_feat_metrics$mean[cb_feat_metrics$.metric == "mae"]
```

When fit to the training set, the Cubist model created an ensemble with `r format(num_rules, big.mark = ",")` rules. On average, the rules consisted of `r mean_rule_vars` variables and the the regression models contained `r mean_reg_vars` predictors. Two example rules, and their regression models, were:

```{r}
#| label: rule-example-1
#| echo: false
#| results: asis
#| cache: true

# Refit due to caching the previous chunk
cb_fit <- fit_best(cb_res)

cb_output <- capture.output(cb_fit %>% extract_fit_engine() %>% summary())
rule_starts <- grep("^  Rule ", cb_output)

# Remove the tabs and re-indent
cb_output <- gsub("\t", "", cb_output)

cb_output[grepl("if$",cb_output)] <- "if"
cb_output[grepl("then$",cb_output)] <- "then"

ifthens <- grepl("(if$)|(then$)", cb_output)

cb_output[!ifthens] <- paste("  ", cb_output[!ifthens])

rule_ex_1 <- cb_output[(rule_starts[1] + 2):(rule_starts[2] - 2)]
rule_ex_2 <- cb_output[(rule_starts[20] + 2):(rule_starts[21] - 2)]

cat("```\n")
cat(rule_ex_1, sep = "\n")
cat("```\n")
```

and 

```{r}
#| label: rule-example-2
#| echo: false
#| results: asis

cat("```\n")
cat(rule_ex_2, sep = "\n")
cat("```\n")
```

These two examples are reasonable and understandable. However, Cubist blends many rules together to the points where is becomes a black-box model. For example, the first data point in the validation set is affected by `r format(val_1_rules, big.mark = ",")` rules (about `r val_1_pct`% of the total ensemble). 

Does this complexity help? The validation set estimated MAE at `r dec_to_time(cb_mae)`, which is an improvement over the linear regression model. @fig-delivery-cubist-obs-pred visualizes the observed and predicted plot. The model under-predicts fewer points with long delivery times than the previous model but does contain a few very large residuals. 

```{r}
#| label: fig-delivery-cubist-obs-pred
#| message: false
#| dev: "ragg_png"
#| fig-width: 4
#| fig-height: 4
#| fig-align: "center"
#| out-width: "40%"
#| fig-cap: "A scatter plot of the observed and predicted delivery times from the Cubist model, for the validation set." 

cb_res %>% 
  collect_predictions() %>% 
  ggplot(aes(time_to_delivery, .pred)) +
  geom_abline(col = "green", lty = 2) +
  geom_point(alpha = 0.4, cex = 1) +
  geom_smooth(se = FALSE, col = "red") +
  coord_obs_pred() +
  labs(x = "Observed Delivery Time", y = "Predicted Delivery Time")
```

Would this model have benefited from using the feature set from the linear regression analysis? Not really. Refitting with those predictors had an MAE of  `r dec_to_time(cb_feat_mae)`. In this particular case, the original Cubist was able to estimate nonlinear trends and interactions without additional feature engineering. 

There is more that we can do with the Cubist model but, in this initial model screening phase, we should focus our efforts on evaluating a disparate set of models and features to understand what works, what doesn't work, and the difficulty level of the problem. Let's try one additional model. 

### Neural Network {.unnumbered}

A neural network is a highly nonlinear modeling technique. They relate the predictors to the outcome through intermediate substructures, called layers, that contain hidden units. The hidden units allow the predictors to affect the outcomes differently^[See Sections [-@sec-cls-nnet] and [-@sec-reg-nnet] for discussions and details.] allowing the model to isolate essential patterns in the data. As the number of hidden units (or layers of hidden units) increases, so does the complexity of the model. We’ll only consider a single layer network with multiple hidden units for these data.

Neural networks, like linear regression, require numbers as inputs. We'll convert the day of the week predictor to indicators. Additionally, the model requires that all of the predictors be in the same units. There are a few ways to accomplish this. First, we will compute the training set mean and standard deviation of each feature. To standardize them we subtract the mean and divide the result by the standard deviation. As a result, all features have a zero mean and a unit standard deviation. A common scaling of the predictor columns is a necessary precondition for fitting  neural networks

```{r}
#| label: nnet-fit-comps
#| include: false
#| cache: true

norm_rec <- 
  recipe(time_to_delivery ~ ., data = delivery_train) %>% 
  step_dummy(day) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

nnet_spec <- 
  mlp(
    hidden_units = tune(),
    penalty = 0.01,
    learn_rate = 0.1,
    epochs = 5000
  ) %>%
  set_mode("regression") %>%
  set_engine("brulee", stop_iter = 10, rate_schedule = "cyclic")

nnet_args <- 
  map(c(nnet_spec$args, nnet_spec$eng_args), rlang::eval_tidy) %>% 
  map(format, big.mark = ",")

nnet_wflow <- 
  workflow() %>% 
  add_model(nnet_spec) %>% 
  add_recipe(norm_rec)

nnet_grid <- tibble(hidden_units = 3:100)

set.seed(388)
nnet_res <-
  tune_grid(nnet_wflow,
            delivery_rs,
            control = ctrl_grid,
            grid = nnet_grid,
            metrics = reg_metrics)
```
```{r}
#| label: nnet-fit-descr
#| include: false

final_hidden_units <- 
  nnet_res %>% 
  select_by_pct_loss(hidden_units, metric = "mae", limit = 1) %>% 
  pluck("hidden_units")

best_nnet <- select_best(nnet_res, metric = "mae")

nnet_mae_vals <- 
  nnet_res %>% 
  collect_metrics() %>% 
  filter(.metric == "mae") %>% 
  arrange(desc(mean)) %>% 
  slice(-1) %>% 
  arrange(hidden_units)

nnet_mae <- 
  nnet_res %>% 
  collect_metrics() %>% 
  filter(.metric == "mae" & hidden_units == final_hidden_units) %>% 
  pluck("mean")

nnet_mae_median <- 
  nnet_res %>% 
  collect_metrics() %>% 
  filter(.metric == "mae" & 
           hidden_units >= final_hidden_units - 5 & 
           hidden_units <= final_hidden_units + 5) %>% 
  pluck("mean") %>% 
  median()

set.seed(466)
nnet_fit <- 
  nnet_wflow %>% 
  finalize_workflow(tibble(hidden_units = final_hidden_units)) %>% 
  fit(data = delivery_train)

nnet_param <- 
  nnet_fit %>% 
  extract_fit_engine() %>% 
  pluck("estimates") %>% 
  unlist() %>% 
  length()

nnet_cap <- 
  paste(
    "A scatter plot of the  validation set observed and predicted delivery times",
    "from a neural network with",  final_hidden_units, "hidden units."
  )
```


One question about this model is: how many hidden units should we use^[This model description is deliberately vague (so as not to confuse readers who are new to this model). Other details: each model could be trained for up to `r nnet_args$epochs` epochs. However, a small amount of the training set was randomly set aside to monitor the sums of squared errors at each epoch. Early stopping was triggered when this out-of-sample loss degraded for `r nnet_args$stop_iter` consecutive epochs. A rectified linear unit activation function was used between the predictor and hidden layers. A `r nnet_args$rate_schedule` learning rate scheduler was used with a maximum rate of `r nnet_args$learn_rate` and a weight decay value of `r nnet_args$penalty` was applied. In a more complete analysis, all of these values would also be tuned.]? This decision will determine the complexity of the model. As models become model complex, they have more capacity to find complicated relationships between the predictors and response. However, added complexity increases the risk of overfitting. This situation, discussed in @sec-complexity, occurs when a model fits exceptionally well to the training data but fails for new data when it finds patterns in the training data that are artifacts (i.e., do not generalize for other data sets). How should we choose the number of hidden units?

Unfortunately, no equation can directly compute an estimate the number of units from the data. Values such as these are referred to as tuning parameters or hyperparameters. However, one solution is to extend the validation process previously shown for the linear regression and Cubist models. We will consider a candidate set of tuning parameter values ranging from `r min(nnet_grid$hidden_units)` to `r max(nnet_grid$hidden_units)` hidden units. We'll fit a model to the training set for each of these `r nrow(nnet_grid)` values and use the validation set MAE values to select an appropriate value. The results of this process is shown in @fig-delivery-nnet-tune. 

```{r}
#| label: fig-delivery-nnet-tune
#| message: false
#| fig-width: 6.25
#| fig-height: 4
#| fig-align: "center"
#| out-width: "60%"
#| fig-cap: "The relationship between the number of hidden units and the mean absolute deviation from the validation set data." 

nnet_mae_vals %>% 
  ggplot(aes(hidden_units, mean)) +
  geom_point() + 
  geom_line() +
  labs(x = hidden_units()$label, y = "MAE")
```

A small number of hidden units performs poorly due to underfitting (i.e., insufficient complexity). Adding more improves the situation and, while the numerically best result corresponds to a value of `r  best_nnet$hidden_units`, there is appreciable noise in the MAE values^[The noisy profile suggests that our validation set might not be large enough to differentiate between subtle differences within or between models. If the problem required more precision in the performance statistics, a more thorough resampling method would be appropriate, such as cross-validation. These are described in Chapter @sec-resampling.] . Eventually, adding too much complexity will either result in the MAE values becoming larger due to overfitting or will drastically increase the time to fit each model. The MAE pattern in @fig-delivery-nnet-tune shows a plateau of values with a similar MAE once enought hidden units are added. Given the noise in the system, we'll select a value of `r final_hidden_units` hidden units, by determining the least complex model that is within 1% of the numerically best MAE. The MAE associated with the selected candidate value^[To some degree, this is an artificially optimistic estimate since we are using the same data to pick the best result and estimate the model's overall performance. This is called optimization bias and it is discussed more in @sec-nested-resampling. For the characteristics of these data, the optimism is likely to be small, especially compared to the noise in the MAE results.] was `r dec_to_time(nnet_mae)` (but is likely to be closer to `r dec_to_time(nnet_mae_median)`, given the noise in @fig-delivery-nnet-tune). 

@fig-delivery-nnet-obs-pred shows the familiar plot of the observed and predicted delivery times. There are fewer large residuals than the Cubist model but the underfitting for longer delivery times appears more pronounced. There is also a hint of under-predicted times for fast deliveries. 

```{r}
#| label: fig-delivery-nnet-obs-pred
#| message: false
#| dev: "ragg_png"
#| fig-width: 4
#| fig-height: 4
#| fig-align: "center"
#| out-width: "40%"
#| fig-cap: !expr nnet_cap

nnet_res %>% 
  collect_predictions(parameters = tibble(hidden_units = final_hidden_units)) %>% 
  ggplot(aes(time_to_delivery, .pred)) +
  geom_abline(col = "green", lty = 2) +
  geom_point(alpha = 0.4, cex = 1) +
  geom_smooth(se = FALSE, col = "red") +
  coord_obs_pred() +
  labs(x = "Observed Delivery Time", y = "Predicted Delivery Time")
```

These results suggest that the Cubist and neural network models have roughly equivocal performance^[We will revisit this statement in @sec-comparing-boot, where methods for estimating confidence intervals for validation and test results are presented.]. Like Cubist, the finalized model is very complex; it is a highly nonlinear regression with `r format(nnet_param, big.mark = ",")` model parameters (i.e., slopes and intercepts). 

## Choosing Between Model  {#sec-model-selection-whole-game}

There are a variety of criteria used to rank and compare model fits. Obviously, performance is important. Also, it is good practice to choose a model that is as simplistic as possible (subject to having acceptable performance). There are other practical factors too. If a model is to be deployed, the size of the files(s) required to automate predictions can be a constraint. There may also be data constraints; which predictors are easy or inexpensive to obtain? Does the number of predictors affect how quickly predictions can be made? 

While prediction is the focus for ML models, we often have to explain the model and/or predicted values to consumers. While we can develop _explainers_ for any model, some types are easier to explain than others. 

For this chapter, we have three models whose MAE values are within about _12 seconds_ of one another. Each has the same deficiency: they all significantly underpredict long delivery times. In fact, a few poor predictions may be a limiting factor for any regression metric; it might be impossible for one model's MAE to really stand out from the others. 

We'll select the linear regression model as our final model due to its relative simplicity and linear structure.  

## Calibration  {#sec-calibration-whole-game}

There is one option for remedying the systematic underprediction of long delivery times. @sec-reg-calibration describes _calibration_ methods that might be able to correct such issues. To do so, we use the held out predictions to estimate the average unwanted trend and remove it from new predictions. For the linear regression results, the estimated average trend is almost identical to the smooth line shown in @fig-delivery-lm-obs-pred. 

For our test set predictions, we'll remove this trend and, hopefully, this will recenter the long predictions to be closer to the diagonal line. 

```{r}
#| label: reg-calibration
#| echo: false
#| warning: false

lm_cal <- cal_estimate_linear(lin_reg_res)
```

## Test Set Results  {#sec-test-results-whole-game}

```{r}
#| label: final-delivery-model
#| echo: false
#| warning: false
#| cache: true

delivery_test_res <- 
  lin_reg_wflow %>% 
  last_fit(delivery_split, metrics = reg_metrics)

test_metrics_uncal <- collect_metrics(delivery_test_res)
test_mae_uncal  <- test_metrics_uncal$.estimate[test_metrics_uncal$.metric == "mae"]

calibrated_pred <- 
  delivery_test_res %>% 
  collect_predictions() %>% 
  cal_apply(lm_cal)

test_metrics_cal <- reg_metrics(calibrated_pred, time_to_delivery, .pred)
test_mae_cal  <- test_metrics_cal$.estimate[test_metrics_cal$.metric == "mae"]

is_same <- ifelse(dec_to_time(lin_reg_mae) == dec_to_time(test_mae_uncal), "_also_", "")
```

We can predict the test set using the linear regression fit from the training set. Recall that the validation set MAE was `r dec_to_time(lin_reg_mae)`. Coincidentally, without the additional calibration, the test set value was `r is_same` `r dec_to_time(test_mae_uncal)`. The similarity of these results gives us confidence that the performance statistics we estimated during development are consistent with the test set. The test set plot of the predictions is shown in @fig-delivery-test-obs-pred(a). 

The calibration results are shown in panel (b). There does appear to be an improvement in long delivery times; these are closer to the diagonal line. The MAE value is slightly improved: `r dec_to_time(test_mae_cal)`. The small difference in these results and the similar MAE values across models indicates that the few large residuals are probably the gating factor for these data. 

```{r}
#| label: fig-delivery-test-obs-pred
#| message: false
#| dev: "ragg_png"
#| fig-width: 8
#| fig-height: 4
#| fig-align: "center"
#| out-width: "80%"
#| fig-cap: Observed versus predicted test set delivery times for the final model. Results are shown for pre-calibrated and post-calibration. 

p_uncal <- 
  delivery_test_res %>% 
  collect_predictions() %>% 
  ggplot(aes(time_to_delivery, .pred)) +
  geom_abline(col = "green", lty = 2) +
  geom_point(alpha = 0.4, cex = 1) +
  geom_smooth(se = FALSE, col = "red") +
  coord_obs_pred() +
  labs(x = "Observed Delivery Time", y = "Predicted Delivery Time", title = "(a) uncalibrated")

p_cal <- 
  calibrated_pred %>% 
  ggplot(aes(time_to_delivery, .pred)) +
  geom_abline(col = "green", lty = 2) +
  geom_point(alpha = 0.4, cex = 1) +
  geom_smooth(se = FALSE, col = "red") +
  coord_obs_pred() +
  labs(x = "Observed Delivery Time", y = "Predicted Delivery Time", title = "(b) calibrated")

p_uncal + p_cal
```

At this point, we should spend a fair amount of time documenting the model and our development process. It might be helpful to employ inferential analyses to explain to interested parties which model components were most important (e.g., how much the interaction or nonlinear terms improve the model, etc.). It is very important to document where the model does not work well, its intended use, and the boundaries of the training set. Finally, if the model is deployed, we should also monitor its effectiveness over time and also understand if the sample population is changing over time too. 

The next section accentuates important aspects of this case study.  

## Themes {#sec-themes-whole-game}

At a high level, let's review the process that we just used to create a model;  @fig-model-building-process highlights several themes^[When resampling is used in place of a validation set, @sec-resampling has a similar diagram that reflects the subtle differences in data usage.]. The overall process involves initial data splitting, model development, then model selection, and validation. We’ll look at each of these in turn to discuss important points. 

```{r}
#| label: fig-model-building-process
#| echo: false
#| out-width: 60%
#| fig-cap: A general high level view of the process of creating a predictive model. The diagram reflects which steps use the test set, validation set, or the entire training set. @fig-within-model-process describes the operations labeled "model development". 

if (knitr::is_html_output()) {
  knitr::include_graphics("../premade/overall_process_validation.svg")
} else {
  knitr::include_graphics("../premade/overall_process_validation.pdf")
}
```


#### Data Spending {.unnumbered}

Although discussed in @sec-data-splitting, how we allocate data to specific tasks (e.g., model building, evaluating performance, etc.) is an essential aspect of modeling. In our analysis, a minority of the data were explicitly allocated to measure how well the model worked. The test set is meant to be unbiased since it was never used to develop the model. This philosophy of empirical validation is a crucial feature for judging how models function.

The majority of the data ends up in the training set. The core activities of creating, optimizing, and selecting are accomplished with these data. 

@fig-model-building-process highlights how we use the **right data at the right time**. A lack of a data usage methodology (or an inappropriate one) can lead to having a false sense of the quality of the model.

For example, when the linear regression model was calibrated, we chose to use the held-out samples as the substrate for estimating the unwanted trend. We could have used the re-predicted training set data but, as will be discussed in @sec-complexity, these values can be unrealistically optimistic. Also, we chose not to compute the MAE on the held-out predictions after they were used to estimate the calibration. This _dual use_ of the same data would result in an MAE value that is smaller than it should be. There are other ways of estimating the utility of the calibration prior to the test set; these are discussed in @sec-reg-calibration. 

#### Investigate Many Options {.unnumbered}

For the delivery data, three different models were evaluated. It is our experience that some modeling practitioners have a favorite model that is relied on indiscriminately (boosted trees immediately come to mind). The "No Free Lunch" Theorem [@Wolpert] argues that, without substantive information about the modeling problem and data, no single model will always do better than any other model. Because of this, a strong case can be made to try various techniques and then determine which to focus on. In our example, simple data visualizations elucidated the relationships between the outcome and the predictors. Given this knowledge, we might exclude some models from consideration, but there is still a wide variety of techniques to evaluate. 

For individual models, @fig-within-model-process summarizes the cyclic process of speculation, configuration, and validation of model parameters and features. For these data, we discovered most of the important trends prior to our first model fit. As previously mentioned, we would cycle through a few rounds where we try to make improvments for each of the models. 

```{r}
#| label: fig-within-model-process
#| echo: false
#| out-width: 90%
#| fig-cap: The cycle of developing a specific type of model. EDA standard for exploratory data analysis. 

if (knitr::is_html_output()) {
  knitr::include_graphics("../premade/diagram_within_model_cycle.svg")
} else {
  knitr::include_graphics("../premade/diagram_within_model_cycle.pdf")
}
```

For novel data sets, it is a challenge to know _a priori_ what good choices are for tuning parameters, feature engineering methods, and predictors. The modeler must iterate using choices informed by their experience and knowledge of the problem.  

#### Predictor Representations {.unnumbered}

This example revolves around a small set of predictors. In most situations, there will be numerous predictors of different types (e.g., quantitative, qualitative, etc.). It is unclear which predictors should be included in the model for new modeling projects and how they should be represented. 

When considering how to approach a modeling project, the stereotype is that the user focuses on which new, fancy model to apply to the data. The reality is that many of the most effective decisions that users must confront are which predictors to use and in what format.  Thoughtfully adding the right feature representations often obviates the need (and overhead and risks) for complex models. That is one of the lessons from this particular data set.

We'll put the feature engineering process on equal footing with model training For this reason, @fig-within-model-process shows that the choice of model features and parameters are both important.

While Cubist and the neural network were able to internally determine the more elaborate patterns in the data, our discovery of these same motifs has a higher value. We learned something about the nature of the data and can relate them to the model (via feature engineering) and the end-users^[In fact, the day-to-day users of the data are the best resources for feature engineering.]. Later, in @sec-interactions, we'll see how a black-box model can be used to suggests additional interactions to add to our linear regression model. This has the effect of eliminating some of the large residuals seen in @fig-delivery-lm-obs-pred. 

#### Model Selection {.unnumbered}

At some point in the process, a specific model pipeline must be chosen. This chapter demonstrated two types of selection. First, we chose some models over others: the linear regression model did as well as more complex models and was the final selection. In this case, we decided _between model pipelines_. There was also a second type of model selection shown. The number of hidden units for the neural network was determined by trying different values and assessing the results. This was also model selection, where we decided on the configuration of a neural network (as defined by the number of hidden units). In this case, we selected _within different neural networks_.

In either case, we relied on data separate from the training set to produce quantitative efficacy assessments to help choose. This book aims to help the user gain intuition regarding the strengths and weaknesses of different models to make informed decisions.

#### Measuring Performance {.unnumbered}

Before using the test set, two techniques were used to determine the model's effectiveness. First, quantitative assessments of statistics (i.e., MAE) from the validation set help the user understand how each technique would perform on new data. The other tool was to create simple visualizations of a model, such as plotting the observed and predicted values, to discover areas of the data where the model does particularly well or poorly. This qualitative information is lost when the model is gauged only using summary statistics and is critical for improving models. There are many more visualization methods for displaying the outcomes, their predictions, and other data. The figures shown here are pretty simplistic. 

Note that almost all of our approaches for model evaluation are data-driven in a way where we determine if the predicted values are "close" to the actual values. This focus on empirical validation is critical in proving that a model pipeline is _fit for purpose_. Our general mantra is  

::: {.important-box}
Always have a separate piece of data that can **contradict what you believe**.
:::

For models that require specific probabilistic assumptions, it is a good idea to check these too, primarily if a model will also be used for inference. However, before considering questions regarding theoretical assumptions, empirical validation should be used to ensure that the model can explain the data well enough to be worth our inferences. For example, if our validation set MAE was around 15 minutes, the model predictions would not show much fidelity to the actual times. Any inferences generated from such a model might be suspect.

## Summary {#sec-summary-whole-game}

At face value, model building appears straightforward:  pick a modeling technique, plug in the data, and generate a prediction.  While this approach will yield a predictive model, it unlikely to generate a reliable, trustworthy model.  To achieve this we must first understand the data _and_ the objective of the modeling. We finally proceed to building, evaluating, optimizing, and selecting models after these steps.

```{r}
#| label: teardown
#| include: false

save(lin_reg_res, file = "../RData/deliveries_lm.RData", compress = TRUE)

stopCluster(cl)
```

## Chapter References {.unnumbered}

