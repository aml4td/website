---
knitr:
  opts_chunk:
    cache.path: "../_cache/comparing-models/"
---

# Comparing Models {#sec-comparing-models}

```{r}
#| label: comparing-models-setup
#| include: false

source("../R/_common.R")
source("../R/_themes.R")
source("../R/_themes_ggplot.R")
source("../R/_themes_gt.R")

# ------------------------------------------------------------------------------
# Required packages

library(tidymodels)
library(embed)
library(future.mirai)
library(bestNormalize)
library(rules) 
library(discrim)
library(gt)
library(emmeans)
library(tidyposterior)
library(nlme)
library(broom.mixed)

# ------------------------------------------------------------------------------
# Set options
plan(mirai_multisession)
tidymodels_prefer()
theme_set(thm_lt)
set_options()

# ------------------------------------------------------------------------------
# Load previous data sets
load("../RData/forested_split_info.RData")
load("../RData/forested_data.RData")
load("../RData/forested_interactions.RData")
```

```{r}
#| label: model-options
#| include: false
ctrl_rs <-
    control_resamples(
        save_pred = TRUE,
        parallel_over = "everything",
        save_workflow = TRUE
    )

ctrl_grid <-
    control_grid(
        save_pred = TRUE,
        parallel_over = "everything",
        save_workflow = TRUE
    )

cls_mtr <- metric_set(accuracy)
```

```{r}
#| label: c50-fit
#| include: false
#| cache: true

c50_spec <-
    C5_rules(trees = tune(), min_n = tune()) |>
    set_engine("C5.0") |>
    set_mode("classification")

c50_wflow <-
    workflow() |>
    add_model(c50_spec) |>
    add_formula(class ~ .)

c50_grid <- expand.grid(trees = c(60, 70, 80, 90), min_n = c(8, 10, 12, 20))

set.seed(526)
c50_res <-
    tune_grid(
        c50_wflow,
        resamples = forested_rs,
        control = ctrl_grid,
        grid = c50_grid,
        metrics = cls_mtr
    )

c50_best <-
    select_best(c50_res, metric = "accuracy")

c50_metrics <-
    c50_res |>
    filter_parameters(parameters = c50_best) |>
    collect_metrics(summarize = FALSE)

c50_final_wflow <- finalize_workflow(c50_wflow, c50_best)

c50_validation_res <-
    c50_final_wflow |>
    last_fit(class ~ ., split = forested_split, metrics = cls_mtr)
```

```{r}
#| label: glmnet-fit
#| include: false
#| cache: true
logistic_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_lencode_mixed(county, outcome = "class") |>
  step_interact(!!forested_int_form) |>
  step_spline_natural(
    all_numeric_predictors(),
    -county,
    -eastness,
    -northness,
    -year,
    -contains("_x_"),
    deg_free = 10
  ) |>
  step_lincomb(all_predictors())

# ------------------------------------------------------------------------------

glmn_pen <- 10^seq(-6, -1, length.out = 50)
glmn_grid <- crossing(penalty = glmn_pen, mixture = c(0:4) / 4)

glmn_spec <- 
  logistic_reg(penalty = tune(), mixture = tune()) |> 
  set_engine("glmnet", path_values = !!glmn_pen)

# ------------------------------------------------------------------------------

logistic_wflow <- workflow(logistic_rec, glmn_spec)

logistic_res <- 
  logistic_wflow |> 
  tune_grid(
    resamples = forested_rs,
    grid = glmn_grid,
    control = ctrl_grid,
    metrics = cls_mtr
  )

logistic_best <-
    select_best(logistic_res, metric = "accuracy")

logistic_metrics <-
    logistic_res |>
    filter_parameters(parameters = logistic_best) |>
    collect_metrics(summarize = FALSE)

logistic_final_wflow <- finalize_workflow(logistic_wflow, logistic_best)

logistic_validation_res <-
    logistic_final_wflow |>
    last_fit(class ~ ., split = forested_split, metrics = cls_mtr)
```

```{r}
#| label: nb-fit
#| include: false
#| cache: true
nb_rec <-
  recipe(class ~ ., data = forested_train) |>
  step_orderNorm(all_numeric_predictors())

nb_wflow <- workflow(nb_rec, naive_Bayes())

nb_res <- 
  nb_wflow |> 
  fit_resamples(
    resamples = forested_rs,,
    control = ctrl_rs,
    metrics = cls_mtr
  )

# ------------------------------------------------------------------------------

nb_metrics <-
    nb_res |>
    collect_metrics(summarize = FALSE)
```

In chapters [-@sec-overfitting], [-@sec-resampling], and [-@sec-grid-search], we established several key fundamentals of building predictive models. To create an effective model, we must employ strategies to reduce overfitting, use resampling techniques to accurately measure predictive performance, and systematically search a tuning parameter space to identify values that optimize model accuracy. By following these approaches, we can have confidence that our model generalizes well to new, unseen data and provides a realistic estimate of its predictive performance.

With this foundation in place, the next step is to select the best model for a given data set.  When presented with new data, we typically evaluate multiple machine learning models, each with different sets of tuning parameters. Our goal is to select the model and parameter combination that yields the best predictive performance. But how do we determine which model and corresponding parameters are truly the best?

Making this decision requires considering several factors. As briefly discussed in @sec-whole-game, practical characteristics such as model complexity, interpretability, and deployability play an important role. These aspects are typically assessed qualitatively based on practical considerations.  Predictive performance, however, is an objective measure that can be evaluated quantitatively. However, comparing predictive performance across models raises an important question:  how can we be sure that one model's predictive performance is truly superior to another's?  Or, conversely, how can we conclude that multiple models have no significance difference in predictive performance?

To address these questions, we will examine both graphical and statistical techniques for comparing the predictive performance of different models. During the model tuning and training process, these comparisons may involve assessing various configurations of tuning parameters within a single model or comparing fundamentally different modeling approaches.

Cross-validation serves as the backbone of the tuning and training process. It systematically partitions the data into multiple subsets, enabling repeated estimation of model performance. The variability in predictions from the holdout sets provides critical information for selecting optimal tuning parameters or identifying the best-performing model.

In the first part of this chapter, we will focus on methods for evaluating models when multiple performance metrics are available. This includes techniques grounded in both Frequentist and Bayesian frameworks. After that, the case of a single data set is explored. This is relevant when a single holdout set is used during model development (i.e., a validation set) and also when the test set is evaluated.

## Resampled Data Sets {#sec-compare-resamples}

As a refresher, resampling produces multiple variations of the training set. These are used to produce model performance statistics that should mimic the results we would see on data sets similar to the training set. 

The tools in this section are very similar to those described for model racing in @sec-racing, and we'll use the same notation as that section. Similarly, we've discussed the two main philosophies of statistical analysis (Frequentist and Bayesian) in @sec-effect-encodings and @sec-freq-and-bayes. Many of those ideas play out again in this chapter. 

### Statistical Foundations {#sec-compare-stats}

Regardless of whether we take a Frequentist or Bayesian approach to comparing model performance, both methods aim to identify key factors that influence the outcome. The chosen approach is then applied to the data to quantify the impact of these factors. Specifically, we seek to determine whether their effect on the outcome is statistically significant—that is, unlikely to be due to random chance.  

In this chapter, we focus on tools for comparing predictive performance across models. These tools can be used to evaluate different tuning parameter settings within a single model or to compare performance across different model types. The insights gained from this analysis help inform decisions about which model(s) to use for predicting new samples.

To illustrate these statistical techniques, we will use the same data that will be used in later chapters on classification: the Washington State forestation data introduced in @sec-data-splitting. Our goal is to predict whether a location is classified as forested or non-forested. The data set includes predictors such as elevation, annual precipitation, longitude, latitude, and so on. 

Without going into much detail, we'll focus on comparing three modeling strategies that will be described in Part 4: 
 - A boosted tree ensemble using C5.0 trees as the base learner.  This model was tuned over the number of trees in the ensemble and the amount of data required to make additional splits (a.k.a. $n_{min}$). Grid search found that `r c50_best$trees` trees in the ensemble were optimal, in conjunction with $n_{min}$ = `r c50_best$min_n` training set points. 
 - A logistic regression model that was trained using penalization (@sec-logistic-penalized). Feature engineering included an effect encoding parameterization of the counties, a small set of interaction terms (described in  @sec-forestation-eda), and ten spline terms for several of its numeric predictors.The optimal tuning parameter values were a penalization value of 10<sup>`r round(log10(logistic_best$penalty), 2)`</sup> and `r round(logistic_best$mixture * 100)`% Lasso penalization. 
 - A naive Bayes model [@sec-naive-bayes] was used where the conditional distirbutions were modeled using a nonparametric density estimate. No feature preprocessing was used.
 
One difference between these analyses and those to come in Part 4 is that we've chosen to use classification accuracy as our metric^[This isn't normally our first choice. However, the outcome classes are fairly balanced. Also, accuracy is intuitively understandable, and the broader set of classification metrics has yet to be described in detail.]. Some of these results may differ from upcoming analyses. Previously, the spatially aware resampling scheme that was used generated `r nrow(forested_rs)` resamples of the training set. For this reason, there are `r nrow(forested_rs)` different accuracy "replicates" for each candidate value that was evaluated. 

Similar to previous sections, we'll produce models where these replicated peformance statistics $\widehat{Q}_{ij}$ as the outcomes of the model for resample $i$ and model $j$. This is the same general approach shown in @eq-perf-mod-racing and @eq-surrogate. We'll assume that there are $M$ models to compare (three in our case). 

### Frequentist Hypothesis Testing Methods {#sec-nhtm}

Let's begin by building towards an appropriate statistical model that will enable us to formally compare the predictive performance among models. To begin this illustration, let's suppose that each of these models were built separately using **different** 10-fold partitions of the training data.  In this scenario, the predictive performance on the first holdout fold would be unique, or independent, for each model.  If this was the case, then we could represent the desired performance metric with the following linear statistical model:

$$
Q_{ij} = \beta_0 + \sum_{j=1}^{M-1}\beta_jx_{ij} + \epsilon_{ij}
$$ {#eq-one-way-model}

In this equation $\beta_0$ is the overall average model performance across all hold out sets and models.  The term $\beta_{j}$ represents the incremental impact on performance due to the $j^{th}$ model ($j$ = 1, 2, ..., $M-1$ = total number of models), and $x_{ij}$ is an indicator function for the $j^{th}$ model.  In this example, the $\beta_j$ will be considered as a _fixed_ effect.  This indicates that these are the only models that we are considering.  If we were to have taken a random sample of models, then the $\beta_j$ would be treated a _random_ effect.  A random effect would be associated with a distribution for the parameter.  The distinction between fixed and random effects will come into play shortly.  Finally, $\epsilon_{ij}$ represents the part of the performance that is not explainable by the $i^{th}$ hold out set and $j^{th}$ model.  This is known as the residual or random error and we'll assumed to follow a Gaussian distribution with a mean of zero and an unknown variance.  The model along with the statistical assumption about the distribution of the residuals provides the statistical foundation for formally comparing models. For a specific data set, $\beta_0$ is estimated by the the mean accuracy of one of the models, and $\beta_{j}$ is estimated by the difference between this model and the mean of the $j^{th}$ model.  

@fig-variance-partition-one-way illustrates these separate components for the hold out set with the largest accuracy value for the boosting ensemble, where the overall intercept is determined by the logistic regression. The horizontal dashed line represents the overall mean of the accuracy estimates, and the blue bar shows the mean of the boosting estimates alone. It is slightly higher than the average. For that model, the orange line shows the  residual ($\epsilon_{ij}$ for the largest boosted tree accuracy replicate. The totality of these errors are used to measure the overall error in the model.

```{r}
#| label: fig-variance-partition-one-way
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: An illustration of how variation is partitioned based on the effect of the model (blue) and unexplainable causes (orange) when distinct cross-validation partitions are used to train each model. 
for_fig <-
    bind_rows(
        c50_metrics |>
            dplyr::select(id, .estimate) |>
            mutate(Model = "Boosting"),
        nb_metrics |>
            dplyr::select(id, .estimate) |>
            mutate(Model = "Naive Bayes"),
        logistic_metrics |>
            dplyr::select(id, .estimate) |>
            mutate(Model = "Logistic")
    ) |>
    rename(accuracy = .estimate, Fold = id) |>
    mutate(
        Model = factor(Model, levels = c("Boosting", "Logistic", "Naive Bayes")),
        Fold = factor(Fold)
    )

intercept_value <-
    for_fig |>
    group_by(Model) |>
    summarize(accuracy = mean(accuracy)) |>
    ungroup() |>
    dplyr::filter(Model == "Logistic") |>
    dplyr::select(accuracy) |>
    pull()

model_means <-
    for_fig |>
    group_by(Model) |>
    summarize(accuracy = mean(accuracy)) |>
    ungroup() |>
    mutate(Fold = "Fold03") |>
    dplyr::filter(Model == "Boosting")

fold_means <-
    for_fig |>
    group_by(Fold) |>
    summarize(accuracy = mean(accuracy)) |>
    ungroup()

max_fold_mean_segment <-
    tibble(
        Model = 0.9,
        y_begin = model_means |>
            dplyr::filter(Model == "Boosting") |>
            dplyr::select(accuracy) |>
            pull(),
        y_end = fold_means |>
            dplyr::filter(accuracy == max(accuracy)) |>
            pull(),
        accuracy = intercept_value,
        Fold = "Fold03"
    )

example_point <-
    for_fig |>
    dplyr::filter(Model == "Boosting") |>
    dplyr::filter(accuracy == max(accuracy))

intercept_segment <-
    tibble(
        Model = 0.9,
        y_begin = intercept_value,
        y_end = model_means |>
            dplyr::filter(Model == "Boosting") |>
            dplyr::select(accuracy) |>
            pull(),
        accuracy = intercept_value
    ) |>
    mutate(Fold = "Fold03")

error_segment <-
    tibble(
        Model = 0.9,
        y_begin = model_means |>
            dplyr::filter(Model == "Boosting") |>
            dplyr::select(accuracy) |>
            pull(),
        y_end = example_point$accuracy,
        accuracy = intercept_value
    )

error_segment2 <-
    tibble(
        Model = 0.9,
        y_begin = max_fold_mean_segment$y_end,
        y_end = example_point$accuracy,
        accuracy = intercept_value,
        Fold = "Fold10"
    )

intercept_text <-
    tibble(
        Model = 0.7,
        y = mean(c(intercept_segment$y_begin, intercept_segment$y_end)),
        Fold = "Fold03"
    )

error_text <-
    tibble(Model = 0.7, y = mean(c(error_segment$y_begin, error_segment$y_end)))

fold_text <-
    tibble(
        Model = 0.7,
        y = mean(c(max_fold_mean_segment$y_begin, max_fold_mean_segment$y_end)),
        Fold = "Fold03"
    )

error_text2 <-
    tibble(
        Model = 0.7,
        y = mean(c(error_segment2$y_begin, error_segment2$y_end)),
        Fold = "Fold03"
    )

ggplot(for_fig, aes(x = Model, y = accuracy)) +
    geom_point(color = "black", shape = 1) +
    geom_point(
        data = example_point,
        aes(x = Model, y = accuracy),
        color = "darkorange",
        shape = 1,
        size = 5
    ) +
    geom_hline(yintercept = intercept_value, linetype = 2, color = "black") +
    geom_point(
        data = model_means,
        aes(x = Model, y = accuracy),
        shape = 95,
        size = 10,
        color = "royalblue"
    ) +
    geom_errorbar(
        data = intercept_segment,
        aes(ymin = y_begin, ymax = y_end),
        color = "royalblue",
        width = 0.05
    ) +
    geom_errorbar(
        data = error_segment,
        aes(ymin = y_begin, ymax = y_end),
        color = "darkorange",
        width = 0.05
    ) +
    geom_text(
        data = intercept_text,
        aes(x = Model, y = y),
        label = "Model",
        color = "royalblue"
    ) +
    geom_text(
        data = error_text,
        aes(x = Model, y = y),
        label = "Error",
        color = "darkorange"
    ) +
    ylab("Accuracy")
```

```{r}
#| label: anova-results
#| include: false
#| cache: true

one_way_model <- glm(accuracy ~ Model, data = for_fig)
one_way_anova <- data.frame(anova(one_way_model))
one_way_anova_f <- one_way_anova$F[2]
one_way_anova_ndf <- one_way_anova$Df[2]
one_way_anova_ddf <- one_way_anova$Resid..Df[2]
one_way_anova_pvalue <- one_way_anova$Pr..F.[2]

one_way_model_random_intercept <- lme(
    accuracy ~ Model,
    random = ~ 1 | Fold,
    data = for_fig
)

anova_one_way_model_random_intercept <- data.frame(anova(one_way_model_random_intercept))
anova_one_way_model_random_intercept_f <- anova_one_way_model_random_intercept$F.value[2]
anova_one_way_model_random_intercept_ndf <- anova_one_way_model_random_intercept$numDF[2]
anova_one_way_model_random_intercept_ddf <- anova_one_way_model_random_intercept$denDF[2]
anova_one_way_model_random_intercept_pvalue <- anova_one_way_model_random_intercept$p.value[2]
```

This model enables us to distinctly partition the total variability in the data into two components:  

1. Explained variability:  The portion attributable to differences among models, represented by the blue bar. This is the **signal** in the data set. 
2. Unexplained variability: The portion due to random or unaccounted-for factors, represented by the orange bar, commonly called the **noise**.  

Our key question is whether the observed differences in the mean performance for each model reflect genuine differences among models or if they could simply be due to random variation. The Frequentist approach addresses this question using hypothesis testing. In this framework:  

- The null hypothesis ($H_0$) represents the assumption that all models have the same average performance. Formally, this can be written as:  
  $$
  H_0: \beta_k = \beta_m,
  $$
  for all $k = m$.
  
- The alternative hypothesis ($H_A$) asserts that at least two models have significantly different average predictive performance:  
  $$
  H_A: \beta_{k} \neq \beta_{m},
  $$  
  for any $k \neq m$.

To determine whether we have sufficient evidence to reject the null hypothesis, we compare:  

- Between-group variability (signal): Differences in mean performance across models.  
- Within-group variability (noise): Residual variation not explained by the models.  

This comparison is conducted using an _F_-test [@kutner2004applied], which assesses whether the signal is large enough relative to the noise to conclude that differences among models are statistically significant.  

In this case, the _F_-statistic is `r paste0("(", round(one_way_anova_f,2), ", ", paste0(one_way_anova_ndf), ", ", paste0(one_way_anova_ddf), ")")`^[This is common notation for reporting the _F_-statistic, where the first number represents the ratio of the mean square of the model to the mean square of the error.  The second and third numbers represent the degrees of freedom of the numerator and denominator, respectively.], with a corresponding p-value of `r pval(one_way_anova_pvalue)`. The p-value is small, however, is not small enough to conclude that there is  strong evidence to reject the null hypothesis.  Therefore, we would **not** conclude that there are differences in accuracy among the three models.  However, remember that we treated these data as if the resamples were completely different, which was not the case. As you'll see shortly, this is a seriously flawed analysis of our data. 

In practice when tuning and training models, we use the same cross-validation sets for evaluating model performance.  This is a more efficient approach since we only need to set up the cross-validation scheme once.  Moreover, the structure that is generated by using the same hold out sets will enable us to have a better ability to detect differences in predictive performance between models.

In this setting, each hold out set forms what is known as a block, which is a group of units that are similar to each other.  This grouping is an important piece of information which will aid in assessing if models have statistically different performance.  The predictive performance when tuning multiple models using the same cross-validation hold out sets can now be represented as:

$$
Q_{ij} = (\beta_0 + \beta_{0i}) + \sum_{j=1}^{M-1}\beta_jx_{ij} + \epsilon_{ij}
$$ {#eq-one-way-model-with-blocks}

Notice that @eq-one-way-model-with-blocks is nearly the same as @eq-one-way-model, with the additional term $\beta_{0i}$.  This term represents the incremental impact on performance due to the $i^{th}$ hold out set ($i$ = 1, 2,..., $B$ resamples).  In this model, the impact of the $B$ folds, represented by $\beta_{0i}$ are a random effect.  The reason the folds are considered as random effect is because the 10 folds were selected at random from an overall distribution of folds.  The impacts of each model are still considered as fixed effects.  When a model contains both fixed and random effects, it is called a _mixed_ effect model.  

@fig-variance-partition-one-way-with-blocks illustrates these separate components for the hold out set with the largest accuracy value for the boosted tree.  In this figure, the points are connected based on the fold. The green line shows that the residual for the best boosting accuracy can be explained by the fact that it comes from this specific fold (i.e., resample). For whatever reason, this specific assessment set was easier to predict since all models had the highest accuracies. The resample that is associated with each accuracy is a systematic and explainable effect in these data. It's also a "_nuisance_ parameter" because we don't care about this specific trend. 

Since we can quantify the effect of this nuisance parameter, we can subtract its effect from the overall error, yielding a much smaller value of $\epsilon_{ij}$ as evidenced by the updated vertical orange bar.  Notice that the amount of variability explained by the model is identical to @fig-variance-partition-one-way.  However, the unexplainable variability from @fig-variance-partition-one-way-with-blocks is now partially explained by the variation due to the fold, thus reducing the _unexplainable_ variability.  We have made our analysis much more precise by accounting for the resample effect^[This again echoes the discussion of racing from @sec-racing.]. 

```{r}
#| label: fig-variance-partition-one-way-with-blocks
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: An illustration of how variation is partitioned based on the effect of the model (blue), folds (green), and unexplainable causes (orange) when the same cross-validation partitions are used to train each model.  The lines connecting the points indicate the results for the same cross-validation fold.
 Fold03 <-
    for_fig |>
    dplyr::filter(Fold == "Fold03")

max_fold_mean <-
    fold_means |>
    dplyr::filter(accuracy == max(accuracy)) |>
    mutate(Model = "Boosting")

ggplot(for_fig, aes(x = Model, y = accuracy, group = Fold)) +
    geom_point(color = "black", shape = 1) +
    geom_point(
        data = example_point,
        aes(x = Model, y = accuracy),
        color = "darkorange",
        shape = 1,
        size = 5
    ) +
    geom_line(
      data = dplyr::filter(for_fig, Fold != "Fold03"), 
      alpha = 0.2, 
      show.legend = FALSE
    ) +
    geom_line(
        data = Fold03,
        aes(x = Model, y = accuracy, group = Fold),
        color = "#1BB6AFFF"
    ) +
    geom_hline(yintercept = intercept_value, linetype = 2, color = "black") +
    geom_point(
        data = model_means,
        aes(x = Model, y = accuracy),
        shape = 95,
        size = 10,
        color = "royalblue"
    ) +
    geom_errorbar(
        data = intercept_segment,
        aes(ymin = y_begin, ymax = y_end),
        color = "royalblue",
        width = 0.05
    ) +
    geom_point(
        data = max_fold_mean,
        aes(x = Model, y = accuracy),
        shape = 95,
        size = 10,
        color = "#1BB6AFFF"
    ) +
    geom_errorbar(
        data = max_fold_mean_segment,
        aes(ymin = y_begin, ymax = y_end),
        color = "#1BB6AFFF",
        width = 0.05
    ) +
    geom_errorbar(
        data = error_segment2,
        aes(ymin = y_begin, ymax = y_end),
        color = "darkorange",
        width = 0.05
    ) +
    geom_text(
        data = intercept_text,
        aes(x = Model, y = y),
        label = "Model",
        color = "royalblue"
    ) +
    geom_text(
        data = fold_text,
        aes(x = Model, y = y),
        label = "Fold",
        color = "#1BB6AFFF"
    ) +
    geom_text(
        data = error_text2,
        aes(x = Model, y = y),
        label = "Error",
        color = "darkorange"
    ) +
    ylab("Accuracy")

```

To understand if there are differences in performance among models, we again compare the variability explained by the different models with the unexplainable variability.  The signal relative to the noise is now much stronger, enabling us to more easily see differences in model performance.  The _F_-statistic from this model is `r paste0("(", round(anova_one_way_model_random_intercept_f,2), ", ", paste0(anova_one_way_model_random_intercept_ndf), ", ", paste0(anova_one_way_model_random_intercept_ddf), ")")` and has a corresponding p-value of `r pval(anova_one_way_model_random_intercept_pvalue)`.  By utilizing the same cross-validation scheme across models, there now is strong evidence of a difference in performance among the models.

We have now established that there is strong evidence of differences in average predictive performance among models.  But this result does not tell us which models are different.  To understand which models are different, we need to perform additional tests to compare each pair of models.  These tests are referred to as _post hoc_ tests because they are performed after the initial hypothesis test among all means. To learn more details about _post hoc_tests and the field of multiple comparisons, see @hsu1996multiple and/or @dudoit2008multiple .

#### Post Hoc Pairwise Comparisons and Protecting Against False Positive Findings {#sec-post-hoc}

After detecting a significant effect in a one-way analysis of variance (ANOVA), the next step is to determine which specific groups differ from each other. This is achieved using post hoc pairwise comparisons, which assess all possible pairwise differences while controlling for false positive findings.

When conducting multiple comparisons, the risk of making at least one Type I error (false positive) increases. If we compare many pairs of groups separately, the probability of incorrectly rejecting at least one null hypothesis accumulates, leading to spurious findings.  If this issue is not addressed, then we will eventually conclude that models are different when they actually are not.  To mitigate this, statistical procedures adjust for multiple comparisons to ensure that the overall false positive rate remains controlled.  Two methods for this purpose are Tukey's Honestly Significant Difference (HSD) [@tukey1949comparing] and the False Discovery Rate (FDR) procedures [@EfronHastie2016a,chap. 15].

Tukey’s HSD test focuses on controlling the probability of making at least one false discoveries; this is known as the familywise error rate (FWER). The HSD test is specifically designed for one-way ANOVA _post hoc_ comparisons and ensures that the FWER remains at a pre-determined significance level ($\alpha$ = 0.05). 

The test considers all possible pairwise differences while maintaining a constant error rate across comparisons. The test statistic is calculated as:

$$
T_{HSD} = \frac{\bar{Q}_i - \bar{Q}_j}{\sqrt{MS_{within} / n}}
$$

where, $\bar{x}_i$ and $\bar{x}_j$ are the sample means of groups $i$ and $j$, $MS_{within}$ is the mean square error from the ANOVA, and $n$ is the number of observations per group.

The critical value for $T_{HSD}$ is obtained from the studentized range distribution, which depends on the number of groups and the degrees of freedom from the ANOVA error term.

When we desire to compare the performance of each pair of models, then Tukey's HSD procedure will provide the appropriate adjustments to the p-values to protect against false positive findings.

In settings where _many hypotheses_ are tested simultaneously, we may be more interested in the false discovery rate (FDR): the expected proportion of false positives among all rejected hypotheses. With many comparisons, this can be more appropriate than controlling the familywise error rate. 

There are numerous FDR procedures in the literature. The method by @benjamini1995controlling is the simplest. It ranks the p-values from all pairwise comparisons from smallest to largest ($p_1$, $p_2$, ..., $p_m$) and compares them to an adjusted threshold:

1. Rank the p-values in ascending order, assigning each a rank $i$.

2. Compute the critical value for each comparison:
   $$
   p_{crit} = \frac{i}{m} \alpha
   $$
   where $m$ is the total number of comparisons and $\alpha$ is the desired FDR level.
   
3. Find the largest $i$ where $p_i \leq p_{crit}$, and reject all hypotheses with p-values at or below this threshold.

Unlike Tukey’s method, FDR control is more appropriate when performing many comparisons.  This procedure offers a balance between sensitivity (detecting true differences) and specificity (limiting false positives).

Now, let's compare how these methods perform when evaluating each pair of models in our example. @tbl-post-hoc-pvalues presents the estimated differences in accuracy, standard error, degrees of freedom, raw, unadjusted p-values, and the p-values adjusted using the Tukey and FDR methods for each pairwise comparison. Notice that the Tukey adjustment results in less significant p-values than the FDR adjustment. Regardless of the adjustment method, the results indicate that each model's performance differs statistically significantly from the others. Specifically, the boosting model has a higher accuracy than the other two models. 

```{r}
#| label: tbl-post-hoc-pvalues
#| tbl-cap: "Pairwise comparisons among the three models for predicting forestation classification.  The table includes the estimated difference in accuracy, standard error, degrees of freedom, the raw unadjusted p-value, and the corresponding Tukey and FDR adjusted p-values."

#Pairwise comparison adjustments
emmeans_results <- emmeans(one_way_model_random_intercept, ~Model)
pairwise_comparisons <- pairs(emmeans_results, adjust = "tukey")

one_way_model_random_intercept_em <- emmeans(
    one_way_model_random_intercept,
    "Model",
    data = for_fig
)
unadjusted_pairs <- data.frame(
    pairs(
        one_way_model_random_intercept_em,
        adjust = "none"
    )
)
tukey_pairs <- data.frame(
    pairs(
        one_way_model_random_intercept_em,
        adjust = "tukey"
    )
)
fdr_pairs <- data.frame(
    pairs(
        one_way_model_random_intercept_em,
        adjust = "fdr"
    )
)

pairwise_comparison_table <-
  bind_cols(
    unadjusted_pairs |>
      dplyr::select(contrast, estimate, SE, df, p.value) |>
      rename(Unadjusted = p.value),
    tukey_pairs |> dplyr::select(p.value) |> rename(Tukey = p.value),
    fdr_pairs |> dplyr::select(p.value) |> rename(`FDR` = p.value)
  ) |>
  rename(Comparison = contrast, Estimate = estimate, DF = df) |> 
  mutate(
    Unadjusted = scales::pvalue(Unadjusted, accuracy = 0.00001),
    Tukey = scales::pvalue(Tukey, accuracy = 0.00001),
    FDR = scales::pvalue(FDR, accuracy = 0.00001)
  )

pairwise_comparison_table |>
  gt() |>
  cols_align(
    align = "center"
  ) |>
  cols_align(
    columns = c("Unadjusted", "Tukey", "FDR"),
    align = "right"
  ) |>
  fmt_percent(columns = c("Estimate", "SE"), decimals = 2) |>
  tab_spanner(
    label = "p-value Adjustment",
    columns = c(Unadjusted, Tukey, FDR)
  ) |>
  cols_width(Comparison ~ px(200)) |>
  tab_options(table.width = pct(100))
```

In this table, the comparison between boosting and the logistic regression is not significant at the 0.05 level.  However, the comparison between the boosting and naive Bayes, and the comparison between logistic regression and naive Bayes are significantly different.  The largest difference (`r round(unadjusted_pairs$estimate[which(unadjusted_pairs$estimate == max(unadjusted_pairs$estimate))],3)` is between the boosted trees and naive Bayes.

#### Comparing Performance using Equivalence Tests {#sec-comparing-models-equivalence-tests}

In the previous section, we explored how to determine whether one model significantly outperforms another in predictive performance. However, there are cases where we may instead want to assess whether the models perform similarly in a practically meaningful way. To do this, we use an equivalence test, which differs from the previous hypothesis test that aimed to detect differences between models.

Equivalence testing is based on the concept that two models can be considered practically equivalent if their difference falls within a **predefined** margin of indifference, often denoted as $\theta$. This threshold represents the maximum difference that is considered practically insignificant.  To illustrate equivalence testing, let's begin by reviewing the hypothesis testing context for assessing differences between models.  Let $\beta_1$ and $\beta_2$ represent the mean predictive performance of two models. The null hypothesis for a standard two-sample test of difference is:

$$
H_O:  \beta_1 = \beta_2
$$

while the alternative hypothesis is:

$$
H_A:  \beta_1 \neq \beta_2
$$

However, an equivalence test reverses this logic. The null hypothesis is that the models are not equivalent, and the alternative hypothesis is that the models are practically insignificant:

$$
H_0:  |\beta_1 - \beta_2| \geq \theta
$$ {#eq-equivalence-null}

$$
H_A:  |\beta_1 - \beta_2| \lt \theta
$$ {#eq-equivalence-alternative}

Rejecting the null hypothesis using this framework will allow us to conclude that the models are equivalent within the predefined margin of indifference.

A commonly used approach for equivalence testing is the Two One-Sided Test (TOST) procedure [@schuirmann1987comparison]. This method involves conducting two separate one-sided hypothesis tests as defined in @eq-equivalence-null and @eq-equivalence-alternative.  Specifically, one test assesses the lower bound comparison while the other test assesses the upper bound comparison as follows:

Lower bound:

$$
H_0:  \beta_1 - \beta_2 \geq -\theta
$$

$$
H_A:  \beta_1 - \beta_2 > -\theta
$$
Upper bound:

$$
H_0:  \beta_1 - \beta_2 \leq \theta
$$
$$
H_A:  \beta_1 - \beta_2 > \theta
$$

Each test is conducted at a desired significance level $\alpha$, and if both null hypotheses are rejected, we conclude that the model performance is equivalent based on the predefined margin of indifference.

Schuirmann showed that the two one-sided hypotheses can be practically evaluated by constructing a $100(1-2\alpha)\%$ confidence interval for $\beta_1 - \beta_2$.  If this interval is contained within the boundaries of ($-\theta$, $\theta$), then we can reject the null hypothesis and conclude that the two models have equivalent predictive performance.

Selecting an appropriate margin of indifference is crucial and should be determined prior to conducting the statistical test.  What value should we choose?  The answer to this question is problem-specific and depends on what we believe makes a practical difference in performance.

From the previous section, we saw that the largest expected difference between any pair of models was `r round(unadjusted_pairs$estimate[which(unadjusted_pairs$estimate == max(unadjusted_pairs$estimate))],3)`.  For the sake of this illustration, let's suppose that the margin of practical indifference between models is at least \pm 3 percent in accuracy.^[When performing an equivalence test, the margin of practical indifference should be specified _before_ the test is performed.]

To perform the equivalence tests, we use the same ANOVA model as when testing for differences in @eq-one-way-model-with-blocks and estimate the 90% confidence intervals about each pairwise difference.  Because we are making multiple pairwise comparisons, the intervals need to be adjusted to minimize the chance of making a false positive finding.  This adjustment can be done using either the Tukey or FDR procedures.

@fig-equivalence-intervals presents the Tukey-adjusted 90% confidence intervals for each pairwise difference between models. For the forestation data, we would conclude that the boosting ensemble and the logistic regression are equivalent. However, we could not conclude that the naive Bayes was equivalent to either of the other models.  

```{r}
#| label: fig-equivalence-intervals
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| warning: false
#| fig-cap: Pairwise comparisons of models using equivalence testing.  The Tukey-adjusted 90% confidence interval of each pairwise difference is illustrated with vertical bands.  For these comparisons, the margin of practical equivalence was set to +/- 3% accuracy.

pairwise_comparisons <- pairs(
    one_way_model_random_intercept_em,
    adjust = 'tukey'
)
conf_int <-
    tibble(confint(pairwise_comparisons, adjust = "tukey", level = 0.90)) |>
    mutate(
        contrast = factor(
            contrast,
            levels = c(
                "Boosting - Logistic",
                "Boosting - Naive Bayes",
                "Logistic - Naive Bayes"
            )
        )
    )

ggplot(conf_int, aes(x = contrast, y = estimate)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 1 / 7) +
  geom_hline(yintercept = c(-0.03, 0.03), linetype = 2) +
  ylim(-0.07, 0.07) +
  labs(x = "Comparison", y = "Accuracy Difference") +
  scale_y_continuous(label = label_percent()) 
```

### Comparisons Using Bayesian Models {#sec-compare-resample-bayes}

How would we use a Bayesian approach to fit the model from @eq-one-way-model-with-blocks? To start, we can again assume a Gaussian distribution for the errors with standard deviation $\sigma$. One commonly used prior for this parameter is the exponential distribution; it is highly right-skewed and has a single parameter. Our strategy here is to use very uninformative priors so that the data has more say in the final posterior estimates than the prior. If our model were useless, the standard deviation would be very large and would probably be bounded by the standard deviation of the outcome. For our data, that value for the accuracy is `r round(sd(for_fig$accuracy), 3)`.  We'll specify an exponential prior to use this as the mean of its distribution. 

We'll also need to specify priors for the three $\beta$ parameters. A fairly wide Gaussian would suffice. We'll use $\beta_j \sim N(0, 5)$ for each. 

For the random effects $\beta_{0i}$, the sample size is fairly low (`r nrow(forested_rs)` samples per model). Let's specify a bell-shaped distribution with a wide tail: a $t$-distribution with a single degree of freedom. This would enable the Bayesian model to have some resample-to-resample effects that might be considered outliers if we assumed a Gaussian distribution. 

We'll discuss training Bayesian models in @sec-logistic-bayes, so we'll summarize here. Given our priors, there is no analytically defined distribution for the posterior. To train the model, we'll use a Markov-chain Monte Carlo (MCMC) method to slowly iterate the posterior to an optimal location, not unlike the simulated annealing process previously described. To do this, we create chains of parameters. These are a controlled random walk from a starting point. Once that walk appears to converge, we continue generating random data points that are consistent with the optimized posterior. These random points will numerically approximate the posterior, and from these, we can make inferences. We'll create 10 independent chains, and each will have a warm-up phase of 5,000 iterations and then another 5,000 samples to approximate the posterior. If all goes well, we will have a random sample of 50,000 posterior samples. 

```{r}
#| label: bayesian-calcs
#| include: false
#| cache: true
equiv_val <- 0.03
accuracy_values <-
    for_fig |>
    pivot_wider(
        id_cols = "Fold",
        names_from = "Model",
        values_from = "accuracy"
    ) |>
    rename(id = Fold)

bayes_model <-
  perf_mod(
    accuracy_values,
    seed = 310,
    iter = 10000,
    chains = 10,
    refresh = 0,
    prior_aux = rstanarm::exponential(floor(1/sd(for_fig$accuracy))), 
    prior_intercept = rstanarm::cauchy(0, 1),
    prior = rstanarm::normal(0, 5)
  )

# get resample correlation
variances <- 
  bayes_model$stan |> 
  tidy() |> 
  filter(!is.na(group)) |> 
  mutate(
    variances = estimate^2,
    total = sum(variances),
    pct = variances / total * 100
  ) |> 
  filter(term == "sd_(Intercept).id") |> 
  pluck("pct") |> 
  round(1)

#Get posterior samples
posterior_samples <-
    tidy(bayes_model) |>
    mutate(model = factor(model, levels = c("Boosting", "Logistic", "Naive Bayes")))

#Credible intervals for posterior distribution of model performance
credible_intervals <-
  posterior_samples |>
  group_by(model) |>
  summarize(
    Lower = quantile(posterior, probs = 0.05),
    Upper = quantile(posterior, probs = 0.95),
    Mid = quantile(posterior, probs = 0.5)
  ) |>
  ungroup() |> 
  mutate(ref = 6100) # A location just above the histogram

#Get pairwise contrasts between models
pairwise_diffs <-
    contrast_models(
        bayes_model,
        list_1 = c("Boosting", "Boosting", "Logistic"),
        list_2 = c("Logistic", "Naive Bayes", "Naive Bayes"),
        seed = 310
    ) |>
    mutate(
        color_group_equiv = ifelse(
            difference >= equiv_val | difference <= -equiv_val,
            "Not Equivalent",
            "Equivalent"
        )
    ) |>
    mutate(color_group_equiv = factor(color_group_equiv))

contrast_credible_intervals <-
    pairwise_diffs |>
    mutate(
        contrast = factor(
            contrast,
            levels = c(
                "Boosting vs. Logistic",
                "Boosting vs. Naive Bayes",
                "Logistic vs. Naive Bayes"
            )
        )
    ) |>
    group_by(contrast) |>
    summarize(
        Lower = quantile(difference, probs = 0.05),
        Upper = quantile(difference, probs = 0.95),
        Mid = quantile(difference, probs = 0.5)
    ) |>
    ungroup() |> 
  mutate(ref = 6100) # A location just above the histogram

pairwise_diffs_c50_logistic <-
    pairwise_diffs |>
    dplyr::filter(contrast == "Boosting vs. Logistic")

#Define the region of practical equivalence
#and get the probability of practical equivalence
difference_summary <-
    summary(pairwise_diffs, size = 0.03)
```

The posterior distributions for the mean accuracy for each of the three models are shown in @fig-bayesian-model-posterior along with 90% credible intervals. The posteriors are relatively bell-shaped, and although there are discernible shifts between the models, there are overlaps in these distributions.

We can also use the posteriors to estimate how much of the error in the data was associated with the resample-to-resample effect (i.e., the previously described nuisance parameter). The percentage of total error attributable to the resample-to-resample effects is `r variances`%. This illustrates how important it is to account for all of the systematic effects, wanted or unwanted, in the model. 

```{r}
#| label: fig-bayesian-model-posterior
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: Posterior distributions of accuracy for each of the models.  The horizontal bars represent the 90% credible intervals about the posterior accuracy values for each model.

ggplot(posterior_samples) +
  geom_histogram(
    aes(x = posterior),
    bins = 50,
    color = "white",
    fill = "blue",
    alpha = 0.4
  ) +
  facet_wrap(~model, ncol = 1) +
  geom_errorbar(
    data = credible_intervals,
    aes(y = ref, xmin = Lower, xmax = Upper),
    width = 800
  ) +
  geom_point(data = credible_intervals, aes(y = ref, x = Mid)) +
  labs(x = "Posterior Accuracy", y = "count") + 
  scale_x_continuous(label = label_percent()) 
```

Since we are interested in contrasting models, we can get a sample of the posterior and take the difference between the parameters that correspond to the model terms (i.e., $\beta$ parameters). We can create a large number of these samples to form posterior distributions of the pairwise contrasts and compute credible intervals. @fig-bayesian-contrast-posterior illustrates these results. 

```{r}
#| label: fig-bayesian-contrast-posterior
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: Posterior distributions of pairwise differences in accuracy for each pair of models.  The vertical dashed lines represent the 90% credible intervals for each contrast.

ggplot(pairwise_diffs) +
  geom_histogram(aes(x = difference), 
                 bins = 50, fill = "#EFB344FF", col = "white",  alpha = 0.8) +
  facet_wrap(~ contrast, ncol = 1)  +
  geom_vline(xintercept = 0, lty = 3) +
   geom_errorbar(
    data = contrast_credible_intervals,
    aes(y = ref, xmin = Lower, xmax = Upper),
    width = 800
   ) + 
  geom_point(data = contrast_credible_intervals, aes(y = ref, x = Mid)) +
  labs(x = "Posterior for the Differnece in Accuracy", y = "count") + 
  scale_x_continuous(label = label_percent()) 
```

Based on these intervals, the probability of a difference between each pair of models is substantial. Among them, boosting and logistic regression exhibit the smallest difference, with a `r round(length(which(pairwise_diffs_c50_logistic$difference > 0))/nrow(pairwise_diffs_c50_logistic) * 100, 1)`% probability that the boosting ensemble exhibits a higher accuracy than naive Bayes. 

In addition to utilizing the posterior distribution to compute probabilities of difference, we can also use this distribution to calculate probabilities of similarity or equivalence.  The region of practical equivalence (ROPE) defines an area of the parameter space that we consider practically insignificant [@kruschke2014doing].  When using the equivalence testing from @sec-comparing-models-equivalence-tests, we considered models equivalent if the accuracy was within 3%.  Using this region, the posterior distribution of the parameter of interest is used to generate a probability that the parameter is within this region.  If a large proportion of the posterior distribution falls within the ROPE, we can conclude that the difference is minimal.

The primary difference between the Frequentist and Bayesian equivalence testing lies in interpretation and flexibility. Frequentist equivalence testing is based on pre-specified significance levels and long-run error rates. Bayesian ROPE, on the other hand, provides a probability-based assessment of practical equivalence. This is particularly useful in predictive modeling, where slight differences in performance may not be operationally relevant.

@fig-bayesian-ROPE illustrates the ROPE for the forestation data, defined as +/-3% accuracy. The probability mass within this region, shown in blue, represents the likelihood that the models have equivalent performance based on this criterion. @tbl-bayesian-ROPE summarizes the probabilities for each model comparison. 

```{r}
#| label: fig-bayesian-ROPE
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: Posterior distributions of pairwise differences in accuracy for each pair of models.  The vertical dashed lines represent the region of practical equivalence.  The probability mass within this region represents the probability that the models are practically equivalent within +/- 3% accuracy.

#Histogram of posterior distributions between models
#Color bars of histogram differently on either side of +/- 3%
ggplot(pairwise_diffs, aes(x = difference, fill = color_group_equiv)) +
    geom_histogram(bins = 50, alpha = 0.5, color = "white") +
    facet_wrap(~contrast, ncol = 1) +
    scale_fill_manual(
        values = c("Equivalent" = "blue", "Not Equivalent" = "red")
    ) +
    geom_vline(xintercept = c(-0.03, 0.03), color = "black", linetype = 2) +
    xlab("Accuracy Difference") +
    theme(legend.position = "none") +
  scale_x_continuous(labels = label_percent())
```

For instance, the probability of practical equivalence between the boosting and logistic regression is `r round(difference_summary$pract_equiv[which(difference_summary$contrast=="Boosting vs Logistic")],3)`, suggesting a high likelihood that these models produce similar results within the defined threshold. In contrast, the probability of practical equivalence between the boosting model and the naive Bayes model is `r round(difference_summary$pract_equiv[which(difference_summary$contrast=="Boosting vs Naive Bayes")],3)`, indicating that boosting is likely to have superior predictive performance relative to the naive Bayes, based on the +/-3% accuracy criterion.

```{r}
#| label: tbl-bayesian-ROPE
#| tbl-cap: "Probability of practical equivalence between each pair of models' performance."

difference_summary |>
    dplyr::select(contrast, pract_equiv) |>
    rename(
        Comparison = contrast,
        `Probability of Practical Equivalence` = pract_equiv
    ) |>
    gt() |>
    cols_align(
        align = "center"
    ) |>
    fmt_number(
        columns = c("Probability of Practical Equivalence"),
        decimals = 3
    )
```

## Single Holdout Data Sets {#sec-compare-holdout}

Unlike resampled data sets, validation sets and test sets offer a single "look" at how well the model operates. In the case of the validation set, we would repeatedly use it to guide our model development process, much like how we would rely on resamples. For the test set, it should only be evaluated once, hopefully on a single model, but perhaps to make a final choice between a select few. It is not intended for repeat use. We'd like to conduct similar analyses to those described above. 

For the case of a single holdout, the focus will be on Frequentist methods by way of bootstrap sampling [@Efron1979boot;@efron2003second], which we have seen previously. 

You were first introduced to the bootstrap in @sec-bootstrap, where it was applied to estimate model performance during tuning. In essence, the bootstrap involves resampling with replacement from the available data to estimate the sampling variability of a statistic. This variability can then be used to construct confidence intervals of the performance metrics of interest, offering a range of plausible values for model performance on similar validation sets.

@fig-bootstrap-scheme illustrates the bootstrap process during the model building tuning phase. Recall that resampling with replacement from the training data generates multiple bootstrap samples of the same size as the original. Because sampling is done with replacement, some observations may appear multiple times within a sample, while others may be omitted. The omitted samples are used as holdout sets to evaluate model performance.

To adapt this idea to a single validation set, we treat the validation set as our data source and repeatedly draw bootstrap samples from the validation set itself. Each bootstrap sample is the same size as the validation set and is created via random sampling with replacement (as illustrated in @fig-bootstrap-single-holdout-graphic). The final trained model is used to predict outcomes for each bootstrap sample, and the selected performance metric is computed for each.

```{r}
#| label: fig-bootstrap-single-holdout-graphic
#| echo: false
#| out-width: '60%'
#| fig-cap: "A schematic of bootstraps resamples created from a validation set."
knitr::include_graphics("../premade/boostrap_stat.svg")
```

This process yields a distribution of performance metrics, from which bootstrap confidence intervals can be derived. These intervals offer an estimate of the uncertainty around the validation set performance and give us a sound way to express the expected range of performance across other validation sets of similar size.

```{r}
#| label: boot-int-calc
#| include: false
#| cache: true

both_intervals <- function(x, ...) {
  preds <- collect_predictions(x)
  trad <- 
    preds |>
    summarize(
      agree = sum(.pred_class == class),
      total = length(class)
    ) |> 
    mutate(hstat = list(tidy(binom.test(agree, total, conf.level = 0.9)))) |> 
    pluck("hstat") |> 
    pluck(1) |> 
    select(.lower = conf.low, .upper = conf.high, .estimate = estimate) |> 
    mutate(
      Method = "Analytical", 
      data = list(NA)
    )
  bt <- 
    bootstraps(preds, ...) |> 
    mutate(
      acc = 
        map_dbl(splits, 
                ~ analysis(.x) |> 
                  accuracy(class, .pred_class) |> 
                  pluck(".estimate"))
    ) |> 
    summarize(
      .lower = quantile(acc, probs = 0.05),
      .estimate = mean(acc),
      .upper = quantile(acc, probs = 0.95),
      data = list(acc)
    ) |> 
    mutate(Method = "Bootstrap")

  bind_rows(trad, bt)
}  

# ------------------------------------------------------------------------------

paired_class_pred <- 
  logistic_validation_res |> 
  collect_predictions() |> 
  select(.row, class, logistic = .pred_class) |> 
  full_join(
    c50_validation_res |> 
      collect_predictions() |> 
      select(.row, boosting = .pred_class),
    by = ".row"
  )

agreement <- mean(paired_class_pred$logistic == paired_class_pred$boosting) * 100

# ------------------------------------------------------------------------------

set.seed(382)
both_int_res <- 
  both_intervals(logistic_validation_res, times = 5000) |> 
  mutate(Model = "Logistic") |> 
  bind_rows(
    both_intervals(c50_validation_res, times = 5000) |> 
      mutate(Model = "Boosting") 
  ) |> 
  mutate(
    Model = factor(Model),
    Model = reorder(Model, -.estimate)
  )

boot_acc_reps <- 
  both_int_res |> 
  filter(Method == "Bootstrap") |> 
  select(Model, data) |> 
  unnest(data)

acc_ints <- 
  both_int_res |> 
  mutate(
    ref = if_else(Method == "Analytical", 750, 800) # y-axis location to place the intervals
  )

# ------------------------------------------------------------------------------

acc_diff_comps <- function(split) {
  acc <- 
    analysis(split) |> 
    select(-.row) |> 
    pivot_longer(cols = c(-class), values_to = ".pred_class", names_to = "Model") |> 
    group_by(Model) |> 
    accuracy(class, .pred_class) |> 
    arrange(Model)
  
  tibble(
    term = "Boosting vs. Logistic",
    estimate = acc$.estimate[1]- acc$.estimate[2]
  )
}

set.seed(524)
acc_boot_diif <- 
  collect_predictions(logistic_validation_res) |> 
  mutate(Model = "Logistic")  |> 
  bind_rows(
    collect_predictions(c50_validation_res) |> 
      mutate(Model = "Boosting") 
  ) |> 
  select(.row, Model, .pred_class, class) |> 
  pivot_wider(id_cols = c(.row, class), names_from = Model, values_from = .pred_class) |> 
  bootstraps(times = 5000) |> 
  mutate(stats = map(splits, acc_diff_comps)) |> 
  int_pctl(stats, alpha = .1)
```

There are several methods for computing bootstrap confidence intervals. See @davison1997bootstrap and Chapter 11 of @EfronHastie2016a. We'll describe one that is straightforward and often used: percentile intervals. The idea is simple: for an interval with $100 (1 - \alpha)$% confidence, we compute the $\alpha/2$ quantiles on both ends of the bootstrap distribution. For our 90% intervals, we determine the 5% and 95% quantiles. Since the method relies on the extreme tails, it is important to create many bootstrap samples (i.e., thousands) to guarantee dependable results. 

Let's suppose that we considered a wide variety of models and feature sets, and still found our boosted tree and logistic regression to be the best. We've previously seen that, while the ensemble appears to perform best, the results are fairly similar, and the logistic model is simpler and perhaps more explainable. We might take both of these pipelines to the test set to make one final comparison before making a choice.  

We fit the final models on the entire training set and then predict the test set of `r format(nrow(forested_test), big.mark = ",")` locations. The accuracies for the boosting and logistic models were very close; they were `r round(c50_validation_res$.metrics[[1]]$.estimate * 100, 1)`% and `r round(logistic_validation_res$.metrics[[1]]$.estimate * 100, 1)`%, respectively. Is this difference within the experimental noise or could there be a (statistical) difference? 

We arrange the data so that there are separate columns for the boosted tree and logistic regression predictions (i.e., they are merged by the row number of the test set). Using this data, 5,000 bootstrap samples were created. From each of these, we can compute intervals for the accuracy of each and the difference in accuracy between them. 

@fig-bootstrap-test shows the bootstrap distributions of the individual model results. They are fairly similar, with a small shift between them. Above each histogram are two 90% confidence intervals. One is the bootstrap percentile interval, and the other is a theoretically based asymmetric interval that assumes that the correct/incorrect data for each model follows a Bernoulli distribution. For each model, these intervals are almost identical. We computed each to demonstrate that the bootstrap has very good statistical performance (compared to its theoretical counterpart). 

```{r}
#| label: fig-bootstrap-test
#| echo: false
#| warning: false
#| fig-width: 6
#| fig-height: 6
#| fig-align: "center"
#| out-width: "65%"
#| fig-cap: For both models, the bootstrap distributions of the accuracy statistics are shown as histograms. The intervals at the top of each panel show the 90% confidence intervals computed using the bootstrap and via the traditional analytical method. 

boot_acc_reps |>
  ggplot() +
  geom_histogram(aes(x = data), col = "white", bins = 24) +
  facet_wrap( ~ Model, ncol = 1) +
  geom_point(
    data = acc_ints, 
    aes(x = .estimate, col = Method, pch = Method, y = ref),
    cex = 2
  ) +
  geom_errorbar(
    data = acc_ints, 
    aes(
      xmin = .lower,
      xmax = .upper,
      col = Method,
      y = ref
    )
  ) +
  labs(x = "Accuracy", y = "count") +
  scale_x_continuous(label = label_percent()) +
  scale_shape_manual(values = 15:16)
```

::: {.dangerous-box}
The confidence intervals between models substantially overlap. _Ordinarily_, we should take that as reliable information that there is no statistical difference between the two models. However, this is not true for this particular situation. 
:::

The issue is that the two sets of predictions are highly correlated. On the test set, they agree `r round(agreement, 1)`% of the time. 

When computing the difference between two things (A and B), the variance of their difference is: 

$$
Var[A-B] = Var[A] + Var[B] - 2Cov[A, B]
$$ {#eq-diff-var}

When A and B are related, their covariance will be very high and, if we ignore it, the power of any statistical test on that difference can be severely underpowered. This is the same issue as shown in @sec-nhtm.  When we didn't account for the resampling effect, we accidentally sabotaged the test's ability to find a difference (if there was one).  

For the test set, we compute the difference in accuracy by resampling the matched pairs of predictions. From this, the bootstrap estimate of the difference was `r round(acc_boot_diif$.estimate * 100, 1)`% with 90% bootstrap interval (`r round(acc_boot_diif$.lower * 100, 1)`%, `r round(acc_boot_diif$.upper* 100, 1)`%). This does indicate that the models have statistically indistinguishable accuracies, but the lower bound is _very_ close to zero. With slightly different models or a different random split of the data, they might very well be considered different^[One reason to avoid using metrics based on qualitative predictions (e.g., "yes" or "no") is that they have a diminished capability to detect differences. They tend to contain far less information than their corresponding class probabilities. A metric such as the Brier score would have additional precision and might be able to tell the two models apart.]. There is an overwhelmingly strong case to make that they are practically different, though. 

Also, as the data set size increases, the variation we expect to see in the summary statistic will decrease.  This concept is illustrated in @fig-interval-width.  In this figure, we have randomly selected subsets of varying size from the data set and then computed a 90% confidence interval using these smaller sets.  Notice that as the sample size decreases, the width of the confidence interval increases.  Therefore, the smaller the data set, the higher the variability will be in the estimate of model performance, and the more important it is to understand the range of potential predictive performance. This pattern is true of any interval procedure.

```{r}
#| label: fig-interval-width
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: 90% confidence interval widths of accuracy for data sets of varying size, assuming the same performance as the boosted ensemble.  
#| 
n_test <- nrow(forested_test)

totals <- floor(seq(250, n_test, length.out = 100))
events <- accuracy(paired_class_pred, class, boosting)$.estimate * totals
events <- floor(events)

map2_dfr(events, totals, ~ tidy(binom.test(.x, .y, conf.level = 0.90))) |> 
  mutate(
    size = totals,
    width = conf.high - conf.low) |> 
  ggplot(aes(x = totals)) + 
  geom_line(aes(y = width)) + 
  labs(x = "Data Set Size", y = "90% Confidence Interval Width") + 
  scale_y_continuous(label = label_percent()) 
```

## Conclusion

Selecting the best predictive model involves assessing performance metrics and accounting for practical considerations. Thus far, this chapter has outlined both Frequentist and Bayesian approaches to comparing model performance, emphasizing the importance of appropriate statistical techniques when evaluating differences. The Frequentist approach provides a formal hypothesis testing framework, while the Bayesian approach offers a more direct probability-based interpretation of model differences.  When there is just one data set for evaluation, like a validation set or test set, then the bootstrap technique can be used to create an interval about the performance metric (or differences between models).

Ultimately, while statistical significance can indicate meaningful differences between models, practical significance should also guide decision-making. Equivalence testing and Bayesian ROPE analysis help determine whether models perform similarly within a defined threshold of practical relevance. In our forestation classification example, these methods suggested that while the Cubist model demonstrated superior predictive performance overall, differences between models may not always be large enough to drive a clear choice.

In practice, model selection is rarely based on performance alone. Factors such as interpretability, computational cost, and deployment feasibility must also be considered. By combining quantitative performance assessment with practical domain knowledge, we can make informed decisions that balance accuracy with real-world constraints.

## Chapter References {.unnumbered}
