---
knitr:
  opts_chunk:
    cache.path: "../_cache/comparing-models/"
---

# Comparing Models {#sec-comparing-models}

```{r}
#| label: comparing-models-setup
#| include: false

source("../R/_common.R")
source("../R/_themes.R")
source("../R/_themes_ggplot.R")
source("../R/_themes_gt.R")

# ------------------------------------------------------------------------------
# Required packages
library(readr)
library(tidymodels)
library(patchwork)
library(kableExtra)
library(future.mirai)
library(leaflet)
library(ragg)
library(bestNormalize)
library(probably)
library(rules) 
library(gt)
library(emmeans)
library(tidyposterior)
library(nlme)

# ------------------------------------------------------------------------------
# Set options
plan(mirai_multisession)
tidymodels_prefer()
theme_set(thm_lt)
set_options()

# ------------------------------------------------------------------------------
# Load previous data sets
load("../RData/forested_split_info.RData")
load("../RData/forested_data.RData")
```

```{r}
#| label: model-options
#| include: false
ctrl_rs <-
    control_resamples(
        save_pred = TRUE,
        parallel_over = "everything",
        save_workflow = TRUE
    )

get_iters <- function(x) {
    fit <- extract_fit_engine(x)
    tibble::tibble(iteration = seq(along = fit$loss), loss = fit$loss)
}

ctrl_grid <-
    control_grid(
        save_pred = TRUE,
        parallel_over = "everything",
        save_workflow = TRUE,
        extract = get_iters
    )

class_metrics <- metric_set(accuracy)
```

```{r}
#| label: c50-fit
#| include: false
#| cache: true

c50_spec <-
    C5_rules(trees = tune(), min_n = tune()) %>%
    set_engine("C5.0") %>%
    set_mode("classification")

c50_wflow <-
    workflow() %>%
    add_model(c50_spec) %>%
    add_formula(class ~ .)

c50_grid <- expand.grid(trees = c(60, 70, 80, 90), min_n = c(8, 10, 12, 20))

set.seed(526)
c50_res <-
    tune_grid(
        c50_wflow,
        resamples = forested_rs,
        control = ctrl_grid,
        grid = c50_grid,
        metrics = class_metrics
    )

c50_best <-
    select_best(c50_res, metric = "accuracy")

c50_metrics <-
    c50_res %>%
    filter_parameters(parameters = c50_best) %>%
    collect_metrics(summarize = FALSE)

c50_final_wflow <- finalize_workflow(c50_wflow, c50_best)
```

```{r}
#| label: glmnet-fit
#| include: false
#| cache: true

norm_rec <-
    recipe(class ~ ., data = forested_train) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors()) %>%
    step_normalize(all_predictors())

glmnet_spec <-
    logistic_reg(penalty = tune(), mixture = tune()) %>%
    set_engine("glmnet") %>%
    set_mode("classification")

glmnet_wflow <-
    workflow() %>%
    add_model(glmnet_spec) %>%
    add_recipe(norm_rec)

glmnet_grid <-
    expand.grid(
        penalty = c(10^-4, 10^-3, 10^-2, 10^-1, 10^0),
        mixture = c(0.001, 0.01, 0.03)
    )

set.seed(526)
glmnet_res <-
    tune_grid(
        glmnet_wflow,
        resamples = forested_rs,
        control = ctrl_grid,
        grid = glmnet_grid,
        metrics = class_metrics
    )

glmnet_best <-
    select_best(glmnet_res, metric = "accuracy")

glmnet_metrics <-
    glmnet_res %>%
    filter_parameters(parameters = glmnet_best) %>%
    collect_metrics(summarize = FALSE)

glmnet_final_wflow <- finalize_workflow(glmnet_wflow, glmnet_best)
```

```{r}
#| label: glmnet-ns-fit
#| include: false
#| cache: true

norm_rec_ns <-
    norm_rec %>%
    step_ns(all_predictors())

glmnet_wflow_ns <-
    workflow() %>%
    add_model(glmnet_spec) %>%
    add_recipe(norm_rec_ns)

set.seed(526)
glmnet_res_ns <-
    tune_grid(
        glmnet_wflow_ns,
        resamples = forested_rs,
        control = ctrl_grid,
        grid = glmnet_grid,
        metrics = class_metrics
    )

glmnet_best_ns <-
    select_best(glmnet_res_ns, metric = "accuracy")

glmnet_metrics_ns <-
    glmnet_res_ns %>%
    filter_parameters(parameters = glmnet_best_ns) %>%
    collect_metrics(summarize = FALSE)

glmnet_final_wflow_ns <- finalize_workflow(glmnet_wflow_ns, glmnet_best_ns)
```

In chapters [-@sec-overfitting], [-@sec-resampling], and [-@sec-grid-search], we established several key fundamentals of building predictive models. To create an effective model, we must employ strategies to reduce overfitting, use resampling techniques to accurately measure predictive performance, and systematically search a tuning parameter space to identify values that optimize model accuracy. By following these approaches, we can have confidence that our model generalizes well to new, unseen data and provides a realistic estimate of its predictive performance.

With this foundation in place, the next step is to select the best model for a given data set.  When presented with new data, we typically evaluate multiple machine learning models, each with different sets of tuning parameters. Our goal is to select the model and parameter combination that yields the best predictive performance. But how do we determine which model and corresponding parameters are truly the best?

Making this decision requires considering several factors. As briefly discussed in @sec-whole-game, practical characteristics such as model complexity, interpretability, and deployability play an important role. These aspects are typically assessed qualitatively based on practical considerations.  Predictive performance, however, is an objective measure that can be evaluated quantitatively. However, comparing predictive performance across models raises an important question:  how can we be sure that one model's predictive performance is truly superior to another's?  Or, conversely, how can we conclude that multiple models have no significance difference in predictive performance?

To address these questions, we will examine both graphical and statistical techniques for comparing the predictive performance of different models. During the model tuning and training process, these comparisons may involve assessing various configurations of tuning parameters within a single model or comparing fundamentally different modeling approaches.

Cross-validation serves as the backbone of the tuning and training process. It systematically partitions the data into multiple subsets, enabling repeated estimation of model performance. The variability in predictions from the holdout sets provides critical information for selecting optimal tuning parameters or identifying the best-performing model.

In the first part of this chapter, we will focus on methods for evaluating models when multiple performance metrics are available. This includes techniques grounded in both frequentist and Bayesian frameworks.

Once a final model is selected, we often want to assess its predictive performance on an independent holdout or validation set. In this setting, we no longer benefit from the distributional insight offered by repeated resampling. Therefore, in the final part of the chapter, we will present methods for evaluating model performance when only a single evaluation data set is available.

## Resampled Data Sets {#sec-compare-resamples}

### Statistical Foundations {#sec-compare-stats}

Regardless of whether we take a frequentist or Bayesian approach to comparing model performance, both methods aim to identify key factors that influence the outcome. The chosen approach is then applied to the data to quantify the impact of these factors. Specifically, we seek to determine whether their effect on the outcome is statistically significantâ€”that is, unlikely to be due to random chance.  

In this chapter, we focus on tools for comparing predictive performance across models. These tools can be used to evaluate different tuning parameter settings within a single model or to compare performance across different model types. The insights gained from this analysis help inform decisions about which model(s) to use for predicting new samples.

To illustrate these statistical techniques, we will use the Washington State forestation data introduced in @sec-data-splitting. Our goal is to predict whether a location is classified as forested or non-forested. The key predictors in this dataset include elevation, annual precipitation, longitude, and latitude. Because the data points are geographically located, samples that are closer to each other are more related than those that are farther apart. When training a model, it is crucial to ensure that geographically close locations are not split between the training and test sets. @sec-data-splitting outlined a method to minimize the impact of geographic proximity, which we will apply throughout our modeling process. In this chapter, we will tune both a C5.0 model and a glmnet model, with and without the use of natural splines. We will then demonstrate how to compare the performance of these models.

### Frequentist Hypothesis Testing Methods {#sec-nhtm}

Let's begin by building towards a statistical model that will enable us to formally compare the predictive performance among models.  For this illustration, we will evaluate the performance of three models using 10-fold cross-validation.

To begin this illustration, let's suppose that each of these models were built separately using **different** 10-fold partitions of the training data.  In this scenario, the predictive performance on the first holdout fold would be unique, or independent, for each model.  If this was the case, then we could represent the desired performance metric with the following linear statistical model:

$$
y_{ij} = \beta_0 + \sum_{j=1}^{M-1}\beta_jx_{ij} + \epsilon_{ij}
$$ {#eq-one-way-model}

In this equation $y_{ij}$ represents the model performance from the $i^{th}$ hold-out set $j^{th}$ model, $\beta_0$ is the overall average model performance across all hold out sets and models.  The term $\beta_{j}$ represents the incremental impact on performance due to the $j^{th}$ model ($j$ = 1, 2, ..., $M-1$ = total number of models), and $x_{ij}$ is an indicator function for the $j^{th}$ model.  In this example, the $\beta_j$ will be considered as a _fixed_ effect.  This indicates that these are the only models that we are considering.  If we were to have taken a random sample of models, then the $\beta_j$ would be treated a _random_ effect.  A random effect would be associated with a distribution for the parameter.  The distinction between fixed and random effects will come into play shortly.  Finally, $\epsilon_{ij}$ represents the part of the performance that is not explainable by the $i^{th}$ hold out set and $j^{th}$ model.  This is known as the residual or random error and is assumed to follow a Gaussian distribution with a mean of zero and an unknown variance.  The model along with the statistical assumption about the distribution of the residuals provides the statistical foundation for formally comparing models.  For a specific data set, $\beta_0$ is estimated by the the mean accuracy of one of the models, and $\beta_{j}$ is estimated by the difference between this model and the mean of the $j^{th}$ model.  @fig-variance-partition-one-way illustrates these separate components for the hold out set with the largest accuracy value for the C5.0 model, where the overall intercept is determined by the glmnet+NS model.

```{r}
#| label: fig-variance-partition-one-way
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: An illustration of how variation is partitioned based on the effect of the model (blue) and unexplainable causes (orange) when distinct cross-validation partitions are used to train each model. 
for_fig <-
    bind_rows(
        c50_metrics %>%
            dplyr::filter(.metric == "accuracy") %>%
            dplyr::select(id, .estimate) %>%
            mutate(Model = "C5.0"),
        glmnet_metrics %>%
            dplyr::filter(.metric == "accuracy") %>%
            dplyr::select(id, .estimate) %>%
            mutate(Model = "glmnet"),
        glmnet_metrics_ns %>%
            dplyr::filter(.metric == "accuracy") %>%
            dplyr::select(id, .estimate) %>%
            mutate(Model = "glmnet+NS")
    ) %>%
    rename(accuracy = .estimate, Fold = id) %>%
    mutate(
        Model = factor(Model, levels = c("C5.0", "glmnet+NS", "glmnet")),
        Fold = factor(Fold)
    )

intercept_value <-
    for_fig %>%
    group_by(Model) %>%
    summarize(accuracy = mean(accuracy)) %>%
    ungroup() %>%
    dplyr::filter(Model == "glmnet+NS") %>%
    dplyr::select(accuracy) %>%
    pull()

model_means <-
    for_fig %>%
    group_by(Model) %>%
    summarize(accuracy = mean(accuracy)) %>%
    ungroup() %>%
    mutate(Fold = "Fold03") %>%
    dplyr::filter(Model == "C5.0")

fold_means <-
    for_fig %>%
    group_by(Fold) %>%
    summarize(accuracy = mean(accuracy)) %>%
    ungroup()

max_fold_mean_segment <-
    tibble(
        Model = 0.9,
        y_begin = model_means %>%
            dplyr::filter(Model == "C5.0") %>%
            dplyr::select(accuracy) %>%
            pull(),
        y_end = fold_means %>%
            dplyr::filter(accuracy == max(accuracy)) %>%
            pull(),
        accuracy = intercept_value,
        Fold = "Fold03"
    )

example_point <-
    for_fig %>%
    dplyr::filter(Model == "C5.0") %>%
    dplyr::filter(accuracy == max(accuracy))

intercept_segment <-
    tibble(
        Model = 0.9,
        y_begin = intercept_value,
        y_end = model_means %>%
            dplyr::filter(Model == "C5.0") %>%
            dplyr::select(accuracy) %>%
            pull(),
        accuracy = intercept_value
    ) %>%
    mutate(Fold = "Fold03")

error_segment <-
    tibble(
        Model = 0.9,
        y_begin = model_means %>%
            dplyr::filter(Model == "C5.0") %>%
            dplyr::select(accuracy) %>%
            pull(),
        y_end = example_point$accuracy,
        accuracy = intercept_value
    )

error_segment2 <-
    tibble(
        Model = 0.9,
        y_begin = max_fold_mean_segment$y_end,
        y_end = example_point$accuracy,
        accuracy = intercept_value,
        Fold = "Fold10"
    )

intercept_text <-
    tibble(
        Model = 0.7,
        y = mean(c(intercept_segment$y_begin, intercept_segment$y_end)),
        Fold = "Fold03"
    )

error_text <-
    tibble(Model = 0.7, y = mean(c(error_segment$y_begin, error_segment$y_end)))

fold_text <-
    tibble(
        Model = 0.7,
        y = mean(c(max_fold_mean_segment$y_begin, max_fold_mean_segment$y_end)),
        Fold = "Fold03"
    )

error_text2 <-
    tibble(
        Model = 0.7,
        y = mean(c(error_segment2$y_begin, error_segment2$y_end)),
        Fold = "Fold03"
    )

ggplot(for_fig, aes(x = Model, y = accuracy)) +
    geom_point(color = "black", shape = 1) +
    geom_point(
        data = example_point,
        aes(x = Model, y = accuracy),
        color = "darkorange",
        shape = 1,
        size = 5
    ) +
    geom_hline(yintercept = intercept_value, linetype = 2, color = "black") +
    geom_point(
        data = model_means,
        aes(x = Model, y = accuracy),
        shape = 95,
        size = 10,
        color = "royalblue"
    ) +
    geom_errorbar(
        data = intercept_segment,
        aes(ymin = y_begin, ymax = y_end),
        color = "royalblue",
        width = 0.05
    ) +
    geom_errorbar(
        data = error_segment,
        aes(ymin = y_begin, ymax = y_end),
        color = "darkorange",
        width = 0.05
    ) +
    geom_text(
        data = intercept_text,
        aes(x = Model, y = y),
        label = "Model",
        color = "royalblue"
    ) +
    geom_text(
        data = error_text,
        aes(x = Model, y = y),
        label = "Error",
        color = "darkorange"
    ) +
    ylab("Accuracy")
```

```{r}
#| label: anova-results
#| include: false
#| cache: true

one_way_model <- glm(accuracy ~ Model, data=for_fig)
one_way_anova <- data.frame(anova(one_way_model))
one_way_anova_f <- one_way_anova$F[2]
one_way_anova_ndf <- one_way_anova$Df[2]
one_way_anova_ddf <- one_way_anova$Resid..Df[2]
one_way_anova_pvalue <- one_way_anova$Pr..F.[2]

one_way_model_random_intercept <-lme(accuracy ~ Model, random = ~1|Fold, data=for_fig)
anova_one_way_model_random_intercept <- data.frame(anova(one_way_model_random_intercept))
anova_one_way_model_random_intercept_f <- anova_one_way_model_random_intercept$F.value[2]
anova_one_way_model_random_intercept_ndf <- anova_one_way_model_random_intercept$numDF[2]
anova_one_way_model_random_intercept_ddf <- anova_one_way_model_random_intercept$denDF[2]
anova_one_way_model_random_intercept_pvalue <- anova_one_way_model_random_intercept$p.value[2]
```

This model enables us to distinctly partition the total variability in the data into two components:  

1. Explained variability:  the portion attributable to differences among models, represented by the blue bar.  
2. Unexplained variability:  the portion due to random or unaccounted-for factors, represented by the orange bar.  

The variability explained by the model is often referred to as the **signal**, while the unexplained variability is known as **noise**.  

Our key question is whether the observed differences in model performance (@fig-variance-partition-one-way) reflect genuine differences among models or if they could simply be due to random variation. The frequentist approach addresses this question using hypothesis testing. In this framework:  

- The null hypothesis ($H_0$) represents the assumption that all models have the same average performance. Formally, this can be written as:  
  $$
  H_0: \beta_k = \beta_m,
  $$
  for all $k = m$.
  
- The alternative hypothesis ($H_A$) asserts that at least two models have significantly different average predictive performance:  
  $$
  H_A: \beta_{k} \neq \beta_{m},
  $$  
  for any $k \neq m$.

To determine whether we have sufficient evidence to reject the null hypothesis, we compare:  
- Between-group variability (signal): Differences in mean performance across models.  
- Within-group variability (noise): Residual variation not explained by the models.  

This comparison is conducted using an _F_-test [@kutner2004applied], which assesses whether the signal is large enough relative to the noise to conclude that differences among models are statistically significant.  

In this case, the _F_-statistic is `r paste0("(", round(one_way_anova_f,2), ", ", paste0(one_way_anova_ndf), ", ", paste0(one_way_anova_ddf), ")")`^[This is common notation for reporting the _F_-statistic, where the first number represents the ratio of the mean square of the model to the mean square of the error.  The second and third numbers represent the degrees of freedom of the numerator and denominator, respectively.], with a corresponding p-value of `r paste0(round(one_way_anova_pvalue,4))`. The p-value is small, however, is not small enough to conclude that there is  strong evidence to reject the null hypothesis.  Therefore, we would not conclude that there are differences in accuracy among the three models.  

In practice when tuning and training models, we use the same cross-validation sets for evaluating model performance.  This is a more efficient approach since we only need to set up the cross-validation scheme once.  Moreover, the structure that is generated by using the same hold out sets will enable us to have a better ability to detect differences in predictive performance between models.

In this setting, each hold out set forms what is known as a block, which is a group of units that are similar to each other.  This grouping is an important piece of information which will aid in assessing if models have statistically different performance.  The predictive performance when tuning multiple models using the same cross-validation hold out sets can now be represented as:

$$
y_{ij} = (\beta_0 + \beta_{0i}) + \sum_{j=1}^{M-1}\beta_jx_{ij} + \epsilon_{ij}
$$ {#eq-one-way-model-with-blocks}

Notice that @eq-one-way-model-with-blocks is nearly the same as @eq-one-way-model, with the additional term $\beta_{0i}$.  This term represents the incremental impact on performance due to the $i^{th}$ hold out set ($i$ = 1, 2,..., $v$ = total number of validation folds).  In this model, the impact of the folds, represented by $\beta_{0i}$ are a random effect.  The reason the folds are considered as random effect is because the 10 folds were selected at random from an overall distribution of folds.  The impacts of each model are still considered as fixed effects.  When a model contains both fixed and random effects, it is called a _mixed_ effect model.  @fig-variance-partition-one-way-with-blocks illustrates these separate components for the hold out set with the largest accuracy value for the C5.0 model.  In this figure, the points are connected based on the fold.  Notice that the amount of variability explained by the model is identical to @fig-variance-partition-one-way.  However, the unexplainable variability from @fig-variance-partition-one-way-with-blocks is now partially explained by the variation due to the fold, thus reducing the unexplainable variability.  To understand if there are differences in performance among models, we again compare the variability explained by the different models with the unexplainable variability.  The signal relative to the noise is now much stronger, enabling us to more easily see differences in model performance.  The _F_-statistic from this model is `r paste0("(", round(anova_one_way_model_random_intercept_f,2), ", ", paste0(anova_one_way_model_random_intercept_ndf), ", ", paste0(anova_one_way_model_random_intercept_ddf), ")")` and has a corresponding p-value of `r formatC(anova_one_way_model_random_intercept_pvalue, format="e", digits=2)`.  By utilizing the same cross-validation scheme across models, there now is strong evidence of a difference in performance among the models.

```{r}
#| label: fig-variance-partition-one-way-with-blocks
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: An illustration of how variation is partitioned based on the effect of the model (blue), folds (purple), and unexplainable causes (orange) when the same cross-validation partitions are used to train each model.  The lines connecting the points indicate the results for the same cross-validation fold.
 Fold03 <-
    for_fig %>%
    dplyr::filter(Fold == "Fold03")

max_fold_mean <-
    fold_means %>%
    dplyr::filter(accuracy == max(accuracy)) %>%
    mutate(Model = "C5.0")

ggplot(for_fig, aes(x = Model, y = accuracy, group = Fold)) +
    geom_point(color = "black", shape = 1) +
    geom_point(
        data = example_point,
        aes(x = Model, y = accuracy),
        color = "darkorange",
        shape = 1,
        size = 5
    ) +
    geom_line(alpha = 0.1) +
    geom_line(
        data = Fold03,
        aes(x = Model, y = accuracy, group = Fold),
        color = "purple"
    ) +
    geom_hline(yintercept = intercept_value, linetype = 2, color = "black") +
    geom_point(
        data = model_means,
        aes(x = Model, y = accuracy),
        shape = 95,
        size = 10,
        color = "royalblue"
    ) +
    geom_errorbar(
        data = intercept_segment,
        aes(ymin = y_begin, ymax = y_end),
        color = "royalblue",
        width = 0.05
    ) +
    geom_point(
        data = max_fold_mean,
        aes(x = Model, y = accuracy),
        shape = 95,
        size = 10,
        color = "purple"
    ) +
    geom_errorbar(
        data = max_fold_mean_segment,
        aes(ymin = y_begin, ymax = y_end),
        color = "purple",
        width = 0.05
    ) +
    geom_errorbar(
        data = error_segment2,
        aes(ymin = y_begin, ymax = y_end),
        color = "darkorange",
        width = 0.05
    ) +
    geom_text(
        data = intercept_text,
        aes(x = Model, y = y),
        label = "Model",
        color = "royalblue"
    ) +
    geom_text(
        data = fold_text,
        aes(x = Model, y = y),
        label = "Fold",
        color = "purple"
    ) +
    geom_text(
        data = error_text2,
        aes(x = Model, y = y),
        label = "Error",
        color = "darkorange"
    ) +
    ylab("Accuracy")

```

We have now established there is strong evidence of differences in average predictive performance among models.  But this result does not tell us which models are different.  To understand which models are different, we need to perform additional tests to compare each pair of models.  These tests are referred to as _post-hoc_ tests because they are performed after the initial hypothesis test among all means.

#### Post-hoc Pairwise Comparisons and Protecting against False Positive Findings {#sec-post-hoc}

After detecting a significant effect in a one-way analysis of variance (ANOVA), the next step is to determine which specific groups differ from each other. This is achieved using post-hoc pairwise comparisons, which assess all possible pairwise differences while controlling for false positive findings.

When conducting multiple comparisons, the risk of making at least one Type I error (false positive) increases. If we compare many pairs of groups separately, the probability of incorrectly rejecting at least one null hypothesis accumulates, leading to spurious findings.  If this issue is not addressed, then we will eventually conclude that models are different when they actually are not.  To mitigate this, statistical procedures adjust for multiple comparisons to ensure that the overall false positive rate remains controlled.  Two methods for this purpose are Tukey's Honestly Significant Difference (HSD) [@tukey1949comparing] and the False Discovery Rate (FDR) procedures [@benjamini1995controlling].

##### Tukeyâ€™s Honestly Significant Difference (HSD) {#sec-post-hoc-tukey}

Tukeyâ€™s HSD test is specifically designed for one-way ANOVA post-hoc comparisons and ensures that the familywise error rate (FWER) remains at a pre-determined significance level ($\alpha$ = 0.05). The test considers all possible pairwise differences while maintaining a constant error rate across comparisons. The test statistic is calculated as:

$$
 q = \frac{\bar{x}_i - \bar{x}_j}{\sqrt{MS_{within} / n}}
$$

where, $\bar{x}_i$ and $\bar{x}_j$ are the sample means of groups $i$ and $j$, $MS_{within}$ is the mean square error from the ANOVA, and $n$ is the number of observations per group.

The critical value for $q$ is obtained from the studentized range distribution, which depends on the number of groups and the degrees of freedom from the ANOVA error term.

When we desire to compare the performance of each pair of models, then Tukey's HSD procedure will provide the appropriate adjustments to the p-values to protect against false positive findings.

##### False Discovery Rate (FDR) {#sec-post-hoc-fdr}

In settings where many hypotheses are tested simultaneously, controlling the false discovery rate (FDR)â€”the expected proportion of false positives among all rejected hypothesesâ€”can be more appropriate than controlling the familywise error rate. The FDR procedure ranks the p-values from all pairwise comparisons from smallest to largest ($p_1$, $p_2$, ..., $p_m$) and compares them to an adjusted threshold:

1. Rank the p-values in ascending order, assigning each a rank $i$.
2. Compute the critical value for each comparison:
   $$
   p_{crit} = \frac{i}{m} \alpha
   $$
   where $m$ is the total number of comparisons and $\alpha$ is the desired FDR level.
3. Find the largest $i$ where $p_i \leq p_{crit}$, and reject all hypotheses with p-values at or below this threshold.

Unlike Tukeyâ€™s method, FDR control is more appropriate when performing many comparisons.  This procedure offers a balance between sensitivity (detecting true differences) and specificity (limiting false positives).

Now, let's compare how these methods perform when evaluating each pair of models in our example. @tbl-post-hoc-pvalues presents the estimated differences in accuracy, standard error, degrees of freedom, raw, unadjusted p-values and the p-values adjusted using the Tukey and FDR methods for each pairwise comparison. Notice that the Tukey adjustment results in less significant p-values than the FDR adjustment. Regardless of the adjustment method, the results indicate that each model's performance differs statistically significantly from the others. Specifically, the Cubist model has a higher accuracy than both the glmnet model with and without incorporating natural splines.

```{r}
#| label: tbl-post-hoc-pvalues
#| tbl-cap: "Pairwise comparisons among the three models for predicting forestation classification.  The table includes the estimated difference in accuracy, standard error, degrees of freedom, the raw unadjusted p-value, and the corrseponding Tukey and FDR adjusted p-values."

#Pairwise comparison adjustments
emmeans_results <- emmeans(one_way_model_random_intercept, ~Model)
pairwise_comparisons <- pairs(emmeans_results, adjust = "tukey")

one_way_model_random_intercept_em <- emmeans(
    one_way_model_random_intercept,
    "Model",
    data = for_fig
)
unadjusted_pairs <- data.frame(
    pairs(
        one_way_model_random_intercept_em,
        adjust = "none"
    )
)
tukey_pairs <- data.frame(
    pairs(
        one_way_model_random_intercept_em,
        adjust = "tukey"
    )
)
fdr_pairs <- data.frame(
    pairs(
        one_way_model_random_intercept_em,
        adjust = "fdr"
    )
)

pairwise_comparison_table <-
    bind_cols(
        unadjusted_pairs %>%
            dplyr::select(contrast, estimate, SE, df, p.value) %>%
            rename(Unadjusted = p.value),
        tukey_pairs %>% dplyr::select(p.value) %>% rename(Tukey = p.value),
        fdr_pairs %>% dplyr::select(p.value) %>% rename(`FDR` = p.value)
    ) %>%
    rename(Comparison = contrast, Estimate = estimate, DF = df)

pairwise_comparison_table %>%
    gt() %>%
    cols_align(
        align = "center"
    ) %>%
    fmt_scientific(
        columns = c("Unadjusted", "Tukey", "FDR"),
        exp_style = "e1",
        decimals = 2
    ) %>%
    fmt_number(columns = c("Estimate", "SE"), decimals = 3) %>%
    tab_spanner(
        label = "p-value Adjustment",
        columns = c(Unadjusted, Tukey, FDR)
    ) %>%
    cols_width(Comparison ~ px(200)) %>%
    tab_options(table.width = pct(100))
```

In this table, the comparison between C5.0 and the glmnet model using natural splines is not significant at the 0.05 level.  However, the comparison between the C5.0 and glmnet model not using natural splines, and the comparison between the glmnet model without and with natural splines are significantly different.  The largest difference (`r round(unadjusted_pairs$estimate[which(unadjusted_pairs$estimate == max(unadjusted_pairs$estimate))],3)` is between the C5.0 and glmnet model without including natural splines.

#### Comparing Performance using Equivalence Tests {#sec-comparing-models-equivalence-tests}

In the previous section, we explored how to determine whether one model significantly outperforms another in predictive performance. However, there are cases where we may instead want to assess whether the models perform similarly in a practically meaningful way. To do this, we use an equivalence test, which differs from the previous hypothesis test that aimed to detect differences between models.

Equivalence testing is based on the concept that two models can be considered practically equivalent if their difference falls within a **predefined** margin of indifference, often denoted as $\theta$. This threshold represents the maximum difference that is considered practically insignificant.  To illustrate equivalence testing, let's begin by reviewing the hypothesis testing context for assessing differences between models.  Let $\beta_1$ and $\beta_2$ represent the mean predictive performance of two models. The null hypothesis for a standard two-sample test of difference is:

$$
H_O:  \beta_1 = \beta_2
$$
while the alternative hypothesis is:

$$
H_A:  \beta_1 \neq \beta_2
$$
However, an equivalence test reverses this logic. The null hypothesis is that the models are not equivalent, and the alternative hypothesis is that the models are practically insignificant:

$$
H_0:  |\beta_1 - \beta_2| \geq \theta
$$ {#eq-equivalence-null}

$$
H_A:  |\beta_1 - \beta_2| \lt \theta
$$ {#eq-equivalence-alternative}

Rejecting the null hypothesis in under this framework will allow us to conclude that the models are equivalent within the predefined margin of indifference.

##### The Two One-Sided Test (TOST) Procedure

A commonly used approach for equivalence testing is the Two One-Sided Test (TOST) procedure [@schuirmann1987comparison]. This method involves conducting two separate one-sided hypothesis tests as defined in @eq-equivalence-null and @eq-equivalence-alternative.  Specifically, one test assesses the lower bound comparison while the other test assesses the upper bound comparison as follows:

Lower bound:
$$
H_0:  \beta_1 - \beta_2 \geq -\theta
$$

$$
H_A:  \beta_1 - \beta_2 > -\theta
$$
Upper bound:

$$
H_0:  \beta_1 - \beta_2 \leq \theta
$$
$$
H_A:  \beta_1 - \beta_2 > \theta
$$
Each test is conducted at a desired significance level $\alpha$, and if both null hypotheses are rejected, we conclude that the model performance is equivalent based on the predefined margin of indifference.

Schuirmann showed that the two one-sided hypotheses can be practically evaluated by constructing a $100(1-2\alpha)\%$ confidence interval for $\beta_1 - \beta_2$.  If this interval is contained within the boundaries of ($-\theta$, $\theta$), then we can reject the null hypothesis and conclude that the two models have equivalent predictive performance.

Selecting an appropriate margin of indifference is crucial and should be determined prior to conducting the statistical test.  What value should we choose?  The answer to this question is problem specific and depends on what we believe makes a practical difference in performance.

From the previous section we saw that the largest expected difference between any pair of models was `r round(unadjusted_pairs$estimate[which(unadjusted_pairs$estimate == max(unadjusted_pairs$estimate))],3)`.  For the sake of this illustration, let's suppose that the margin of practical indifference between models is at least \pm 3 percent in accuracy.^[When performing an equivalence test, the margin of practical indifference should be specified _before_ the test is performed.]

To perform the equivalence tests, we use the same ANOVA model for when testing for differences in @eq-one-way-model-with-blocks and estimate the 90% confidence intervals about each pairwise difference.  Because we are making multiple pairwise comparisons, the intervals need to be adjusted to minimize the chance of making a false positive finding.  This adjustment can be done using either the Tukey or FDR procedures.

@fig-equivalence-intervals presents the Tukey adjusted 90% confidence intervals for each pairwise difference between models.  For the forestation data, we would conclude that the C5.0 and glmnet model including natural splines are equivalent.  However, we could not conclude that the glmnet model without natural splines was equivalent to either the C5.0 model or the glmnet model with natural splines. 

```{r}
#| label: fig-equivalence-intervals
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: Pairwise comparisons of models using equivalence testing.  The Tukey adjusted 90% confidence interval of each pairwise difference is illustrated with vertical bands.  For these comparisons, the margin of practical equivalence was set to +/- 3% accuracy.

pairwise_comparisons <- pairs(
    one_way_model_random_intercept_em,
    adjust = 'tukey'
)
conf_int <-
    tibble(confint(pairwise_comparisons, adjust = "tukey", level = 0.90)) %>%
    mutate(
        contrast = factor(
            contrast,
            levels = c(
                "C5.0 - (glmnet+NS)",
                "C5.0 - glmnet",
                "(glmnet+NS) - glmnet"
            )
        )
    )

ggplot(conf_int, aes(x = contrast, y = estimate)) +
    geom_point(size = 2, color = "black") +
    geom_segment(aes(
        x = contrast,
        y = lower.CL,
        xend = contrast,
        yend = upper.CL
    )) +
    geom_hline(yintercept = c(-0.03, 0.03), linetype = 2, color = "black") +
    ylim(-0.07, 0.07) +
    xlab("Comparison") +
    ylab("Accuracy Difference")
```


### Comparisons Using Bayesian Models {#sec-compare-resample-bayes}

Thus far we have focused on the frequentist approach which utilizes the hypothesis testing framework, a statistical model, and distributional assumptions to estimate the probability of rejecting the null hypothesis in favor of the alternative hypothesis.  A Bayesian approach can also be used to compare models.  We first saw this modeling approach in @chapter-categorical-predictors where this technique was used to fit a linear mixed effect model.  As we will see, Bayesian modeling has some distinct advantages over a frequentist approach. This framework can be more intuitive and incorporates prior knowledge about the process being measured.

A Bayesian model applies Bayes' theorem to combine observed data with prior knowledge about parameter distributions. The key inputs to this process are:

* Likelihood: Information derived from the observed data.
* Prior distributions: Assumptions or knowledge about the parameters.

Using Bayes' theorem, these inputs are used to produce posterior distributions, which represent our updated beliefs about the parameters after considering the data. The posterior distributions are then used to make decisions.

If prior knowledge about the parameters is strong, we can specify precise prior distributions. However, when little or no prior information is available, we typically use vague or uninformative priors, which exert minimal influence on the results.  An excellent introduction to Bayesian modeling is @Rethinking.

To compare the predictive performance of different models using a Bayesian approach, we start with a model that represents the data, which is similar to the frequentist approach. The model from the previous section  (@eq-one-way-model-with-blocks) accounts for variation due to differences in predictive model performance, cross-validation folds, and random error.

We assume each sample follows a normal distribution:  

$$
y_{ij} \sim \mathcal{N}(\mu_j + \gamma_i, \sigma)
$$
The parameters from this model represent the average accuracy ($\mu_j$), the impact of cross-validation folds ($\gamma_i$), and the impact of random variation ($\sigma$).  Since we have no prior information about these parameters, we will use uninformative priors:

$$
\mu_j \sim \mathcal{N}[0.5, 0.1]
$$

$$
\gamma_i \sim \mathcal{U}(0,0.1)
$$

$$
\sigma \sim exponential(1)
$$

The prior distribution for the model mean is gaussian with a mean of 0.5 and standard deviation of 0.1, which allows more than 99% of values between 0.2 and 0.8--a fairly wide range.  The prior for fold effect is also gaussian centered at 0 with a standard deviation of 0.1 and reflects no preference towards positive or negative effects.  The prior distribution for the random variation follows an exponential distribution, which ensures that it remains positive.  The diffuse nature of the priors allow the data to have greater effect on the posterior distribution of the parameters.

Although the model has multiple parameters, our primary focus is $\mu_j$. By analyzing the posterior distributions of $\mu$, we can assess and compare the predictive impact of different models, guiding our decision-making process.

For our example modeling forestation classification, the posterior distributions of $\mu_j$ are illustrated in @fig-bayesian-model-posterior.  The results are similar to the frequentist approach where the C5.0 model posterior distribution exhibits higher accuracy values, while the glmnet models exhibits lower accuracy values.  The posterior distribution is the foundation from which decisions can be made.  From this distribution we can derive a *credible interval* which is a probabilistic statement about where the parameter lies given that the observed data and prior information are valid.

At face value, it seems like a confidence interval and credible interval are conveying the same information.  While these intervals may be numerically similar, their interpretations are different.  Confidence intervals are build on distributional assumptions such that a proportion of intervals will contain the true parameter in repeated sampling of the same population.  In contrast, a Bayesian credible interval allows us to make a direct probability statement about the parameter itself, which is more intuitive.

A credible interval can be derived for any parameter or function of parameters from the posterior distributions.  For example, we may be interested in the 95% credible interval for the accuracy of each model.  To derive these intervals, we compute quantiles of the posterior distributions.  These intervals are illustrated by vertical lines in @fig-bayesian-model-posterior.

```{r}
#| label: bayesian-calcs
#| include: false
#| cache: true

accuracy_values <-
    for_fig %>%
    pivot_wider(
        id_cols = "Fold",
        names_from = "Model",
        values_from = "accuracy"
    ) %>%
    rename(id = Fold)

bayes_model <-
    perf_mod(accuracy_values, seed = 310, iter = 5000, chains = 5, refresh = 0)

#Get posterior samples
posterior_samples <-
    tidy(bayes_model) %>%
    mutate(model = factor(model, levels = c("C5.0", "glmnet+NS", "glmnet")))

#Credible intervals for posterior distribution of model performance
credible_intervals <-
    posterior_samples %>%
    group_by(model) %>%
    summarize(
        Lower = quantile(posterior, probs = 0.025),
        Upper = quantile(posterior, probs = 0.975)
    ) %>%
    ungroup()

#Get pairwise contrasts between models
pairwise_diffs <-
    contrast_models(
        bayes_model,
        list_1 = c("C5.0", "C5.0", "glmnet+NS"),
        list_2 = c("glmnet+NS", "glmnet", "glmnet"),
        seed = 310
    ) %>%
    mutate(
        color_group_equiv = ifelse(
            difference >= 0.03 | difference <= -0.03,
            "Not Equivalent",
            "Equivalent"
        )
    ) %>%
    mutate(color_group_equiv = factor(color_group_equiv))

contrast_credible_intervals <-
    pairwise_diffs %>%
    mutate(
        contrast = factor(
            contrast,
            levels = c(
                "C5.0 vs. glmnet+NS",
                "C5.0 vs. glmnet",
                "glmnet+NS vs. glmnet"
            )
        )
    ) %>%
    group_by(contrast) %>%
    summarize(
        Lower = quantile(difference, probs = 0.025),
        Upper = quantile(difference, probs = 0.975)
    ) %>%
    ungroup()

pairwise_diffs_c50_glmnet_ns <-
    pairwise_diffs %>%
    dplyr::filter(contrast == "C5.0 vs. glmnet+NS")

#Define the region of practical equivalence
#and get the probability of practical equivalence
difference_summary <-
    summary(pairwise_diffs, size = 0.03)
```

```{r}
#| label: fig-bayesian-model-posterior
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: Posterior distributions of accuracy for each of the models.  The vertical dashed lines represent the 95% credible intervals about the posterior accuracy values for each model.

ggplot(posterior_samples, aes(x = posterior)) +
    geom_histogram(bins = 50, color = "white", fill = "blue", alpha = 0.4) +
    facet_wrap(~model, ncol = 1) +
    geom_vline(
        data = credible_intervals,
        aes(xintercept = Lower),
        color = "black",
        linetype = 2
    ) +
    geom_vline(
        data = credible_intervals,
        aes(xintercept = Upper),
        color = "black",
        linetype = 2
    ) +
    xlab("Posterior Accuracy")
```

We may also be interested in assessing the probability of a meaningful difference in performance between models. To quantify this, we generate posterior distributions for the pairwise differences in model performance. @fig-bayesian-contrast-posterior presents these distributions along with their corresponding 95% credible intervals. Based on these intervals, the probability of a difference between each pair of models is substantial. Among them, the C5.0 and glmnet model with natural splines exhibit the smallest difference, with a `r round(length(which(pairwise_diffs_c50_glmnet_ns$difference > 0))/nrow(pairwise_diffs_c50_glmnet_ns), 3)` probability that the C5.0 model exhibits a higher accuracy than the glmnet model with natural splines.

```{r}
#| label: fig-bayesian-contrast-posterior
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: Posterior distributions of pairwise differences in accuracy for each pair of models.  The vertical dashed lines represent the 95% credible intervals for each contrast.

ggplot(pairwise_diffs, aes(x = difference, fill = color_group_diff)) +
    geom_histogram(bins = 50, color = "white", fill = "blue", alpha = 0.4) +
    facet_wrap(~contrast, ncol = 1) +
    geom_vline(
        data = contrast_credible_intervals,
        aes(xintercept = Lower),
        color = "black",
        linetype = 2
    ) +
    geom_vline(
        data = contrast_credible_intervals,
        aes(xintercept = Upper),
        color = "black",
        linetype = 2
    ) +
    xlab("Accuracy Difference") +
    theme(legend.position = "none")
```


#### Region of Practical Equivalence (ROPE) {#sec-rope}

In addition to utilizing the posterior distribution to compute probabilities of difference, we can also utilize this distribution to compute probabilities of similarity or equivalence.  The region of practical equivalence (ROPE) defines a region of the parameter space that we would consider practically insignificant [@kruschke2014doing].  When using the equivalence testing from @sec-comparing-models-equivalence-tests, we considered models to be equivalent if the accuracy was within 3%.  Using this region, the posterior distribution of the parameter of interest is used to generate a probability that the parameter is within this region.  If a large proportion of the posterior distribution falls within the ROPE, then we can conclude that the difference is minimal.

The primary difference between the frequentist and Bayesian equivalence testing lies in interpretation and flexibility. Frequentist equivalence testing is based on pre-specified significance levels and long-run error rates. Bayesian ROPE, on the other hand, provides a probability-based assessment of practical equivalence. This is particularly useful in predictive modeling, where small differences in performance may not be operationally relevant.

@fig-bayesian-ROPE illustrates the ROPE for the forestation data, defined as +/-3% accuracy. The probability mass within this region, shown in blue, represents the likelihood that the models have equivalent performance based on this criterion. @tab-bayesian-ROPE summarizes the probabilities for each model comparison.

For instance, the probability of practical equivalence between the C5.0 and glmnet model including natural splines is `r round(difference_summary$pract_equiv[which(difference_summary$contrast=="C5.0 vs glmnet+NS")],3)`, suggesting a high likelihood that these models produce similar results within the defined threshold. In contrast, the probability of practical equivalence between the C5.0 model and the base glmnet model is `r round(difference_summary$pract_equiv[which(difference_summary$contrast=="C5.0 vs glmnet")],3)`, indicating that the C5.0 model is likely to have superior predictive performance relative to the base glmnet model, based on the +/-3% accuracy criterion.

```{r}
#| label: fig-bayesian-ROPE
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: Posterior distributions of pairwise differences in accuracy for each pair of models.  The vertical dashed lines represent the region of practical equivalence.  The probability mass within this region represents the probability that the models are practically equivalent within +/- 3% accuracy.

#Histogram of posterior distributions between models
#Color bars of histogram differently on either side of +/- 3%
ggplot(pairwise_diffs, aes(x = difference, fill = color_group_equiv)) +
    geom_histogram(bins = 100, alpha = 0.75, color = "white") +
    facet_wrap(~contrast, ncol = 1) +
    scale_fill_manual(
        values = c("Equivalent" = "blue", "Not Equivalent" = "red")
    ) +
    geom_vline(xintercept = c(-0.03, 0.03), color = "black", linetype = 2) +
    xlab("Accuracy Difference") +
    theme(legend.position = "none")
```

```{r}
#| label: tbl-bayesian-ROPE
#| tbl-cap: "Probability of practical equivalence between each pair of models' performance."

difference_summary %>%
    dplyr::select(contrast, pract_equiv) %>%
    rename(
        Comparison = contrast,
        `Probability of Practical Equivalence` = pract_equiv
    ) %>%
    gt() %>%
    cols_align(
        align = "center"
    ) %>%
    fmt_number(
        columns = c("Probability of Practical Equivalence"),
        decimals = 3
    )
```

## Single Holdout Data Sets {#sec-compare-holdout}

### Bootstrap Techniques {#sec-boot-ci}

After selecting the optimal tuning parameters, the training and testing data are combined to fit the final model. To evaluate this modelâ€™s predictive performance, we reserve a portion of the original dataâ€”often referred to as a validation set. If the validation set is representative of the training and testing data, we expect the modelâ€™s performance metric on the validation set to be consistent with the results observed during cross-validation.

However, unlike cross-validation, which yields a distribution of performance metrics across multiple resampled data sets, evaluation on a holdout set provides only a single performance estimate. Given the effort invested in model development, relying on just one value to assess predictive ability on unseen data may feel inadequate, especially since, by chance, the validation set result might deviate from the cross-validation results.

Can we quantify our uncertainty around the validation set performance?  Yes, we can use a statistical technique called the bootstrap to gain greater confidence in the performance estimate from a single data set [@Efron1979boot].

You were first introduced to the bootstrap in @sec-bootstrap, where it was applied to estimate model performance during tuning. In essence, the bootstrap involves resampling with replacement from the available data to estimate the sampling variability of a statistic. This variability can then be used to construct confidence intervals of the performance metrics of interest, offering a range of plausible values for model performance on similar validation sets.

@fig-bootstrap-scheme illustrates the bootstrap process during the model building tuning phase. Recall that resampling with replacement from the training data generates multiple bootstrap samples of the same size as the original. Because sampling is done with replacement, some observations may appear multiple times within a sample, while others may be omitted. The omitted samples are used as holdout sets to evaluate model performance.

To adapt this idea to a single validation set, we treat the validation set as our data source and repeatedly draw bootstrap samples from the validation set itself. Each bootstrap sample is the same size as the validation set and is created via random sampling with replacement (as illustrated in @fig-bootstrap-single-holdout-graphic). The final trained model is used to predict outcomes for each bootstrap sample, and the selected performance metric is computed for each.

```{r}
#| label: fig-bootstrap-single-holdout-graphic
#| echo: false
#| out-width: '60%'
#| fig-cap: "A schematic of bootstraps resamples created from a validation set."
knitr::include_graphics("../premade/boostrap_stat.svg")
```

This process yields a distribution of performance metrics, from which bootstrap confidence intervals can be derived. These intervals offer an estimate of the uncertainty around the validation set performance and give us a sound way to express the expected range of performance across other validation sets of similar size.

The bootstrap approach was applied to the three models for the forestation classification data.  @fig-bootstrap-validation-forested illustrates the 90% bootstrap confidence intervals of accuracy for each of the models.  Notice that these intervals are narrow.  This is because the number of validation set samples is large (n = `r nrow(forested_test)`).

```{r}
#| label: fig-bootstrap-validation-forested
#| echo: false
#| warning: false
#| cache: true
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: 90% bootstrap confidence intervals of the validation set accuracy for each model.  The circle represent the accuracy estimate of the validation set for each model.
# Get validation set performance and corresponding bootstrap CI
glmnet_validation_res <-
    glmnet_final_wflow %>%
    last_fit(class ~ ., split = forested_split, metrics = class_metrics)

glmnet_validation_boot <- int_pctl(glmnet_validation_res, alpha = 0.10)


c50_validation_res <-
    c50_final_wflow %>%
    last_fit(class ~ ., split = forested_split, metrics = class_metrics)

c50_validation_boot <- int_pctl(c50_validation_res, alpha = 0.10)

glmnet_validation_res_ns <-
    glmnet_final_wflow_ns %>%
    last_fit(class ~ ., split = forested_split, metrics = class_metrics)

glmnet_validation_boot_ns <- int_pctl(glmnet_validation_res_ns, alpha = 0.10)

validation_boot_accuracy <-
    bind_rows(
        c50_validation_boot %>% mutate(Model = "C5.0"),
        glmnet_validation_boot_ns %>% mutate(Model = "glmnet+NS"),
        glmnet_validation_boot %>% mutate(Model = "glmnet")
    ) %>%
    dplyr::filter(.metric == "accuracy") %>%
    mutate(Model = factor(Model, levels = c("C5.0", "glmnet+NS", "glmnet")))


ggplot(validation_boot_accuracy, aes(x = Model, y = .estimate)) +
    geom_point(color = "black", shape = 1) +
    geom_errorbar(
        data = validation_boot_accuracy,
        aes(ymin = .lower, ymax = .upper),
        color = "black",
        width = 0.05
    ) +
    ylab("Accuracy")
```

As the validation set size increases, the variation we expect to see in the summary statistic will decrease.  This concept is illustrated in @fig-bootstrap-single-holdout-varying-size.  In this figure, we have randomly selected subsets of varying size of the validation set and then applied the bootstrapping approach to these smaller sets.  Notice that as the sample size decreases, the width of the confidence interval increases.  Therefore, the smaller the validation set, the higher the variability will be in the estimate of model performance and the more important it is to understand the range of potential predictive performance.

```{r}
#| label: fig-bootstrap-single-holdout-varying-size
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: "center"
#| out-width: "70%"
#| fig-cap: 90% bootstrap confidence intervals of accuracy for validation sets of varying size for the C5.0 model.  The smaller the size of the validation set, the wider the interval.

n_validation <- nrow(c50_validation_res$.predictions[[1]])
set.seed(405)

c50_validation_res_200 <- c50_validation_res
n_200 <- sample(c(1:n_validation), size = 200)
c50_validation_res_200$.predictions[[1]] <-
    c50_validation_res$.predictions[[1]][n_200, ]
c50_validation_boot_200 <- int_pctl(c50_validation_res_200, alpha = 0.10)
rm(c50_validation_res_200)

c50_validation_res_1000 <- c50_validation_res
n_1000 <- sample(c(1:n_validation), size = 1000)
c50_validation_res_1000$.predictions[[1]] <-
    c50_validation_res$.predictions[[1]][n_1000, ]
c50_validation_boot_1000 <- int_pctl(c50_validation_res_1000, alpha = 0.10)
rm(c50_validation_res_1000)

c50_boot_accuracy <-
    bind_rows(
        c50_validation_boot_200 %>% mutate(Size = "200"),
        c50_validation_boot_1000 %>% mutate(Size = "1000"),
        c50_validation_boot %>% mutate(Size = "Original")
    ) %>%
    dplyr::filter(.metric == "accuracy") %>%
    mutate(Size = factor(Size, levels = c("200", "1000", "Original")))

ggplot(c50_boot_accuracy, aes(x = Size, y = .estimate)) +
    geom_point(color = "black", shape = 1) +
    geom_errorbar(
        data = c50_boot_accuracy,
        aes(ymin = .lower, ymax = .upper),
        color = "black",
        width = 0.05
    ) +
    ylab("Accuracy")
```


## Conclusion

Selecting the best predictive model involves assessing performance metrics and accounting for practical considerations. Thus far, this chapter outlined both frequentist and Bayesian approaches to comparing model performance, emphasizing the importance of appropriate statistical techniques when evaluating differences. The requentist approach provides a formal hypothesis testing framework, while the Bayesian approach offers a more intuitive probability-based interpretation of model differences.  When there is just one data set for evaluation, like a validation set, then the bootstrap technique can be used to create an interval about the performance metric.

Ultimately, while statistical significance can indicate meaningful differences between models, practical significance should also guide decision-making. Equivalence testing and Bayesian ROPE analysis help determine whether models perform similarly within a defined threshold of practical relevance. In our forestation classification example, these methods suggested that while the Cubist model demonstrated superior predictive performance overall, differences between models may not always be large enough to drive a clear choice.

In practice, model selection is rarely based on performance alone. Factors such as interpretability, computational cost, and deployment feasibility must also be considered. By combining quantitative performance assessment with practical domain knowledge, we can make informed decisions that balance accuracy with real-world constraints.


## Chapter References {.unnumbered}
