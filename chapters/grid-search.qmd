---
knitr:
  opts_chunk:
    cache.path: "../_cache/grid/"
---

# Grid Search {#sec-grid}

```{r}
#| label: grid-setup
#| include: false
source("../R/_common.R")

# ------------------------------------------------------------------------------

library(kableExtra)
library(lme4)
library(broom.mixed)
library(tidymodels)
library(finetune)
library(bonsai)
library(rlang)
library(doParallel)

# ------------------------------------------------------------------------------

source("../R/setup_two_class_example.R")

set.seed(943)
sim_tr <- sim_logistic(2000, sim_f)

set.seed(14)
sim_rs <- vfold_cv(sim_tr)

set.seed(14)
sim_nested_rs <- nested_cv(sim_tr, outside = vfold_cv(), inside = vfold_cv())

load("../RData/grid_benchmarks.RData")

# ------------------------------------------------------------------------------
# Set Options

cl <- makePSOCKcluster(parallel::detectCores(logical = TRUE))
registerDoParallel(cl)
tidymodels_prefer()
theme_set(theme_transparent())
set_options()

# ------------------------------------------------------------------------------
# Until the interface in the tune package is finalized, do some computations 
# offline
load("../RData/nested_res.RData")
```

Grid search is a method to optimize tuning parameters for a model pipeline. It creates a pre-defined set of candidate values and computes performance from each. From there, the numerically best candidate could be chosen, or the relationship between the tuning parameter(s) and model performance can be inspected to see if the model might benefit from additional optimization. 

Suppose there is a single tuning parameter, as in the $n_{min}$ example of @sec-grid-and-sequential; each candidate takes a scalar value (quantitative or qualitative). In other cases, there are multiple tuning parameters. For example, boosted tree models have numerous tuning parameters and we'll look at two of them in detail below. 

The previous chapter demonstrated that using external data to evaluate the model is crucial (be it resampling or a validation set). Grid search has no free lunch: we cannot simply fit the model and evaluate it by simply re-predicting the same data. 

@alg-grid-search formally describes the process. For a model with $m$ tuning parameters, we let $\Theta$ represent the collection of $s$ candidate values. For each specific combination of parameters ($\theta_j$), we resample the model to produce some measure of efficacy (e.g., $R^2$, accuracy, etc.)^[This process was illustrated in [Algorithm 9.1](tmp_resampling.html#alg-resampling).]. From there, the best value is chosen, or additional work is carried out to find a suitable candidate. 


::: {#alg-grid-search}

:::: {.columns}

::: {.column width="10%"}

:::

::: {.column width="80%"}


```pseudocode
#| html-line-number: true
#| html-line-number-punc: ":"

\begin{algorithm}
\begin{algorithmic}
\State $\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$
\State $B$: number of resamples
\State $M(\mathfrak{D}^{tr}, B)$: a mapping function to split $\mathfrak{D}^{tr}$ for each of  $B$ iterations.
\State $f()$: model pipeline 
\State $\Theta$: Parameter set ($s \times m$) with candidates $\theta_j$
\For{$j=1$ \To $s$}
  \State Generate $\hat{Q}_{j} =$ \Call{Resample}{$\mathfrak{D}^{tr}, f(\cdot;\theta_j), M(\mathfrak{D}^{tr}, B)$} corresponding to candidate $\theta_j$.
\EndFor
\State Determine $\hat{\theta}_{opt}$ that optimizes $\hat{Q}_{j}$.
\end{algorithmic}
\end{algorithm}
```

:::

::: {.column width="10%"}

:::

::::

Grid search for tuning parameter optimization that loops over the resampling algorithm shown in @alg-resample.

:::


```{r}
#| label: grids
#| include: false

bst_spec <- 
  boost_tree(learn_rate = tune(), trees = tune()) %>% 
  set_mode("classification")%>% 
  set_engine("lightgbm")

bst_wflow <-
  workflow() %>%
  add_model(bst_spec) %>%
  add_formula(class ~ A + B)

bst_param <- 
  bst_wflow %>% 
  extract_parameter_set_dials() %>% 
  update(learn_rate = learn_rate(c(-3, -1)))

bst_reg <-
  grid_regular(bst_param, levels = c(5, 3)) %>%
  mutate(grid = "Regular")

sfd_size <- nrow(bst_reg)

set.seed(nrow(bst_reg))
bst_random <-
  grid_random(bst_param, size = sfd_size) %>%
  mutate(grid = "Random")

bst_sfd <- grid_space_filling(bst_param, size = sfd_size) %>%
  mutate(grid = "Space-Filling")

seq_tree <- sort(unique(bst_reg$trees))
txt_tree <- knitr::combine_words(seq_tree)

seq_ord <- format(sort(unique(log10(bst_reg$learn_rate))))
seq_ord <- paste0("10<sup>", seq_ord, "</sup>")
lr_ord <- knitr::combine_words(seq_ord)

bst_grids <- 
  bind_rows(bst_reg, bst_random, bst_sfd) %>%
  mutate(
    grid = factor(grid, levels = c("Regular", "Random", "Space-Filling"))
  )
```

To demonstrate, this chapter will initially focus on grids for a boosted tree model with two tuning parameters^[These are described in more detail in @sec-cls-boosting]. The model creates a sequence of decision trees, each depending on the previous, to create a large ensemble of individual trees.  The model has tuning parameters released to the tree (e.g., tree depth and $n_{min}$) as well as ones related to the process of building the ensemble (such as the number of trees in the ensemble). One process parameter is the learning rate: the next tree uses information from the last tree to improve it. An important parameter is how much, or how _fast_ it learns. It could be that some data sets desire a large number of trees that evolve slowly (i.e., low learning rate), and other data sets work best with fewer trees that change rapidly. The learning rate parameter is greater than zero and boosted tree models typically work in the range of 10<sup>-5</sup> to  10<sup>-1</sup>. This parameter is best conceptualized in log units. 

We'll illustrate different strategies using this model setup with varying configurations of the learning rate and the number of trees in the ensemble. 

There are two main classes of grids: regular and irregular. We can view generating a collection of candidate models as a statistical design of experiments (DOE) problem [@BHH]. There is a long history of DOE, and we'll invoke relevant methods for each grid type. @santner2018design and @gramacy2020surrogates have excellent overviews of the DOE methods discussed in this chapter. 

The following two sections describe different methods for creating the candidate set $\Theta$. Subsequent sections describe strategies for making grid search efficient using tools such as parallel processing and model racing. 

## Regular Grids {#sec-regular-grid}

A regular grid starts with a sequence or set of candidate values for each tuning parameter and then creates all combinations. In statistics, this is referred to as a factorial design. The number of values per tuning parameter does not have to be the same. 

To illustrate a regular grid, we'll use `r xfun::numbers_to_words(length(seq_tree))` values of the number of trees (`r txt_tree`) and `r xfun::numbers_to_words(length(seq_ord))` values of the learning rate (`r lr_ord`). This grid of `r nrow(bst_reg)` candidates is shown in @fig-bst-grids. The grid covers the entire space with significant gaps in between.  

```{r}
#| label: fig-bst-grids
#| echo: false
#| out-width: 75%
#| fig-width: 7
#| fig-height: 3
#| fig-cap: Three types of grids (in columns) are illustrated with tuning parameters from a boosting model. Each grid contains 15 candidates. The space-filling design was created using the Audze-Eglais method described in @sec-irregular-grid. 

bst_grids %>%
  ggplot(aes(trees, learn_rate)) +
  geom_point(cex = 2, alpha = 2 / 3) +
  labs(x = trees()$label, y = learn_rate()$label) +
  facet_wrap(~ grid) +
  scale_y_log10() + 
  coord_fixed(ratio = 700)
```


```{r}
#| label: bst-grid-res
#| include: false
#| cache: true

format_best_bst <- function(x, ...) {
  bst <- select_best(x, ...)
  paste0(
      format(bst$trees, big.mark = ","), " trees and and a learning rate of 10<sup>",
      # sprintf("%1.1f", bst$learn_rate)
      signif(log10(bst$learn_rate), 3),
      "</sup>"
      )
}

# ------------------------------------------------------------------------------

set.seed(200)
bst_reg_res <- 
  bst_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    metrics = metric_set(brier_class),
    grid = bst_reg %>% select(-grid)
  )

bst_reg_best <- format_best_bst(bst_reg_res, metric = "brier_class")

bst_rnd_res <- 
  bst_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    metrics = metric_set(brier_class),
    grid = bst_random %>% select(-grid)
  )
bst_rnd_best <- format_best_bst(bst_rnd_res, metric = "brier_class")

bst_sfd_res <- 
  bst_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    metrics = metric_set(brier_class),
    grid = bst_sfd %>% select(-grid)
  )   
bst_sfd_best <- format_best_bst(bst_sfd_res, metric = "brier_class")
bst_sfd_brier <- signif(show_best(bst_sfd_res, metric = "brier_class", n = 1)$mean, 3)
bst_sfd_brier_se <- signif(show_best(bst_sfd_res, metric = "brier_class", n = 1)$std_err, 3)

bst_irr_grid_res <- 
  collect_metrics(bst_rnd_res) %>% 
  mutate(grid = "Random") %>% 
  bind_rows(
    collect_metrics(bst_sfd_res) %>% 
      mutate(grid = "Space-Filling")
  ) %>% 
  mutate(`learn_rate (log-10)` = log10(learn_rate)) %>% 
  select(-learn_rate) %>% 
  pivot_longer(
    c(trees, `learn_rate (log-10)`),
    names_to = "parameter",
    values_to = "value"
  ) 
```

We’ll use the *simulated training set* in @sec-complexity-overfitting with the same 10-fold cross-validation scheme described there. Once again, Brier scores were used to measure how well each of the `r nrow(bst_reg)`  configurations of boosted tree model predicted the data. @fig-bst-regular-grid-tuning shows the results: a very small number of trees  is a poor choice (due to underfitting), and there are several learning rate values that work well. Even though this is not a diverse set of candidate values, we can probably pick out a reasonable candidate with a small Brier score, such as `r bst_reg_best` (although there are a few other candidates that would be good choices). 

```{r}
#| label: fig-bst-regular-grid-tuning
#| echo: false
#| out-width: 50%
#| fig-width: 4
#| fig-height: 3.5
#| fig-cap: "The boosted tree tuning results for the regular grid shown in @fig-bst-grids."

autoplot(bst_reg_res)  +
  labs(y = "Brier Score") +
  theme(legend.position = "top")
```

The pattern shown in this visualization is interesting: the trajectory for the number of trees is different for different values of the learning rate. The large two learning rates are similar, showing that the optimal value is in the mid- to low-range of our grid. The results for the smallest learning rate indicates that better performance might be found using _more_ trees than we used in our grid. This indicates that there is an _interaction effect_ in our tuning parameters (just as we saw for predictors in @sec-interactions). This might not affect how we select the best candidate value for this grid, but it does help build some intuition for this particular model that might come in handy when you tune future models. 

Gaining intituion is much easier for regular grids than other designs since we have a full set of combinations. As we’ll see shortly, this interaction is very hard to see for a space-filling design. 

The primary downside to regular grids is that, as the number of tuning parameters increases, the number of points required to fill the space becomes extremely large (due to the curse of dimensionality). However, for some models and pre-processing methods, regular grids can be very efficient despite the number of tuning parameters (see the following section that describes the "submodel trick").  

## Irregular Grids {#sec-irregular-grid}

Irregular grids are not factorial in nature. A simple example is a _random grid_ where points are randomly placed using a uniform distribution on an appropriate range for each tuning parameter. A design of size `r sfd_size` is shown in @fig-bst-grids. There are some gaps and clustering of the points, but the space for each individual dimension is covered well. The tuning results are shown in the top panels of @fig-bst-irregular-grid-tuning. Because it is an irregular design, we can't use the same visualization as in @fig-bst-regular-grid-tuning. Instead, a "marginal" plot is shown, where each numeric tuning parameter is plotted against performance in a separate panel. 

The top left panel suggests that a very small learning rate is very bad, but otherwise, there is a strong trend. For the number of trees (in the top right panel), a very small should be avoided, and perhaps the maximum number tested would be a good choice. From this plot, it is very difficult to discover the interaction effect seen with the regular grid. However, this marginal plot is the main technique for visualizing the results when there are a moderate to large number of tuning parameters. The numerically best candidate was not too dissimilar from the regular grid: `r bst_rnd_best`

```{r}
#| label: fig-bst-irregular-grid-tuning
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 3.75
#| fig-cap: "The tuning results for the irregular grids shown in @fig-bst-grids. The lines are spline smooths."

bst_irr_grid_res %>% 
  ggplot(aes(value, mean)) + 
  geom_smooth(se = FALSE, method = lm, formula = y ~ splines::ns(x, df = 3),
              col = rgb(0, 0, 1, 1 / 5)) +
  geom_point(cex = 2, alpha = 2 / 3) +
  facet_grid(grid ~ parameter, scales = "free_x") +
  labs(y = "Brier Score", x = NULL)
```

Another type of irregular grid is a _space-filling design_ [@joseph2016space], where the goal is to make sure that the tuning parameter space is covered and that there is minimal redundancy in the candidate values. There are a variety of methods for achieving this goal. 

For example, @fig-bst-grids shows a `r sfd_size` point Audze-Eglais space-filling design. The space is covered more compactly than the regular design of the same size and is much more uniform than the `r sfd_size` point random design. The lower panels of @fig-bst-irregular-grid-tuning show the tuning results. The results are somewhat cleaner than the random grid results but show similar trends. Here, the numerically best candidate was: `r bst_sfd_best`. 

There are many types of space-filling designs, and it is worth taking the time to take a quick tour of some of them. 

The Latin hypercube design (LHD) is the most popular method for constructing space-filling designs [@Viana2016; @husslage2011space]. These designs have a simple definition. Suppose our hyperparameter space is rectangular and partitioned into smaller (hyper)cubes. From this, a LHD is a set of distinct points where no one dimension has multiple values in any bins^[Other applications may want the points to have properties such as orthogonality, symmetry, etc.]. We desire candidates that fill the space of each parameter and are not close to one another. 

The most basic approach to creating a Latin hypercube design is random sampling [@Mckay2000]. If there are $m$ parameters and we request $s$ candidate values, the parameter space is initially divided into $s^m$ hypercubes of equal size. For each tuning parameter, $s$ regions in its dimension are selected at random, and a value is placed in this box (also at random). This process repeats for each dimension. Suppose there are ten bins for a parameter that ranges between zero and one. If the first design point selects bin two, a random uniform value is created in the range `[0.1 0.2)`.  @fig-lhs-sampled shows three such designs, each generated with different random numbers. 

```{r}
#| label: fig-lhs-sampled
#| echo: false
#| out-width: 90%
#| fig-width: 9
#| fig-height: 3
#| fig-cap: Three replicate Latin hypercube sampling designs of size ten for two parameters. Each design uses different random numbers. The grid lines illustrate the 100 hypercubes used to generate the values.  

make_lhc <- function(ind, m, s) {
  des <- DiceDesign::lhsDesign(s, m, seed = ind + 287)$design
  colnames(des) <- paste("Parameter", letters[1:ncol(des)])
  des <- as_tibble(des)
  des$Replicate <- paste("Replicate", ind)

  des
}

set.seed(202)
map_dfr(1:3, ~ make_lhc(.x, 2, 10)) %>% 
  ggplot(aes(`Parameter a`, `Parameter b`)) + 
  geom_point(cex = 1.5) + 
  coord_equal() +
  geom_hline(yintercept = seq(0, 1, by = 1 / 10), alpha = 1 / 10) +
  geom_vline(xintercept = seq(0, 1, by = 1 / 10), alpha = 1 / 10) +
  lims(x = 0:1, y = 0:1) +
  theme(panel.grid.minor = element_blank()) + 
  facet_wrap(~ Replicate) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
  )
```

Technically, these designs cover the space of each predictor univariately. However, large multivariate regions can be empty. For example, one design only samples combinations along the diagonal. Additional constraints can make the design more consistent with our desires. 

```{r}
#| label: design-crit
#| include: false

rnd_maxmin <-
  encode_set(bst_random %>% select(-grid), pset = bst_param, as_matrix = TRUE) %>% 
  DiceDesign::mindist() %>% 
  round(2)

sfd_maxmin <-
  encode_set(bst_sfd %>% select(-grid), pset = bst_param, as_matrix = TRUE) %>% 
  DiceDesign::mindist() %>% 
  round(2)

reg_maxmin <-
    encode_set(bst_reg %>% select(-grid), pset = bst_param, as_matrix = TRUE) %>% 
    DiceDesign::mindist() %>% 
    round(2)
```

For example, we could choose points to maximize the minimum pairwise distances between the candidates. These designs are usually referred to as MaxiMin designs [@pronzato2017minimax]. Comparing the two irregular designs in @fig-bst-grids, the random grid has a maximum minimum distance value of `r rnd_maxmin`. In contrast, the corresponding space-filling design's value (`r sfd_maxmin`) is `r round(sfd_maxmin/rnd_maxmin, 1)`-fold larger^[Using Euclidean distance.]. The latter design was optimized its coverage, and we can see far less redundancy in the optimized design. 

A similar method, initially proposed by @audze1977new, maximizes a function of the inverse distances between $s$ candidate points: 

$$
criterion = \sum_{i=1}^s \sum_{j=1,\;i\ne j}^s\frac{1}{dist(\theta_i, \theta_j)^2}
$$

@bates2004formulation devised search methods to find optimal designs for this criterion.

Some other space-filling designs of note: 

- Maximum entropy sampling selects points based on assumptions related to the distributions of the tuning parameters and their covariance matrix [@shewry1987maximum;@joseph2015maximum]. 

- Uniform designs [@fang2000uniform;@wang2022design] are designs that optimally allocate points so that they are uniformly distributed in the space. 

While these methods can be constructed generally by sampling random points and using a search method to optimize a specific criterion, there has been scholarship that has pre-optimized designs for some combination of the number of tuning parameters and the requested grid size. 

The advantage of space-filling designs over random designs is that, for smaller designs, the candidates do a better job covering the space and have a low probability of producing redundant points. Also, it is possible to create a space-filling design so that the candidates for each numerical parameter are nearly equally spaced (as was done in @fig-bst-grids). 

Many designs assume that all the tuning parameter values are quantitative. That may not always be the case. For example, K-nearest neighbors can adjust its predictions by considering how far the cost are from a new point; more distant points should not have the same influence as close cost. A weighting function can be used for this purpose. For example, weights based on an inverse function may produce better results. Equal weighting is often called a “rectangular weighting” function. As a simple workaround, the unique parameter values are repeated as if they were $s$ distinct values when making the design. This allows them to be used with a Latin hypercube design and any space-filling design that uses this approach is not technically optimal. Practically speaking, this is an effective approach for tuning models.  However, there are _sliced_ LHD that can accomplish the same goal [@qian2012sliced;@ba2015optimal].

We recommend space-filling designs since they are more efficient than regular designs. Regular designs have a lot of benefits when using an unfamiliar modeling methodology since you will learn a lot more about the nuances of how the tuning parameters affect one another. 

## Efficient Computations for Conventional Grid Search {#sec-efficient-grid}

The computational cost of grid search can become large, depending on the resampling strategy and the number of candidates under consideration. In our example, a total of `r format(nrow(sim_rs) * sfd_size, big.mark = ",")` boosted tree models are evaluated before determining which candidates are most favorable to our data. 

We’ll look at three approaches to making grid search more efficient. One method (“submodels”) happens automatically, another approach (parallel processing) uses software engineering tools, and the last tool (racing) is a statistical solution.

For these section, a large space-filling design is used with a larger parameter range (`r large_param$object[[which(large_param$id == "trees")]]$range[[1]]` to `r large_param$object[[which(large_param$id == "trees")]]$range[[2]]` trees) and learning rates between 10<sup>`r large_param$object[[which(large_param$id == "learn_rate")]]$range[[1]]`</sup> and 10<sup>`r large_param$object[[which(large_param$id == "learn_rate")]]$range[[2]]`</sup>. Also, `r nrow(large_sfd)` grid points were used.

### Submodels {#sec-Submodels}

Some models can exploit the "submodel trick," where a single training model can predict many tuning parameter candidates. For example, the boosted tree model that we have been using creates a sequential ensemble of decision trees, each depending on the first. Suppose that a boosting model with 3,000 trees is created. Most implementations allow users to predict from this model for _any ensemble size_. Keeping all other parameters equal, if the grid has multiple values for the ensemble size, we can fit a 3,000 tree model and get very fast predictions for values less than 3,000. For a regular grid, this can effectively drop a dimension of the computations. Depending on the model and grid, the speed-up for using this approach can be well into double digits

Some different models and pre-processors can have this quality, including the glmnet model (@sec-penalized-logistic-regression), partial least squares (@sec-colinear-projections), principal component feature extraction (@sec-linear-feature-extraction), and others. 

Unfortunately, irregular designs cannot exploit the submodel trick since they are not factorial in nature. A hybrid design could be used where a dense sequence of $s_1$ parameter values is created for the tuning parameter associated with submodels and a separate space-filling design of size $s_2$ for the other parameters. These two grids can be crossed so that the space-filling design is replicated for each value of the submodel parameter. This produces $s_1\times s_2$ grid points but, effectively, only $s_2$ models are fit. 

for our larger grid, we created a similar regular grid of `r nrow(large_sfd)` points with the same expanded ranges of parameters. There were `r sqrt(nrow(large_sfd))` unique, evenly spaced values for both parameters. To evaluate this grid, we only need to train `r sqrt(nrow(large_sfd))` models. Compared to the analgous space-filling design, the regualr grid was `r round(large_grid_time[3]/large_grid_reg_time[3], 1)` times faster to evaluate. When coupled with 10-fold cross-validation, `r format(nrow(large_sfd) * nrow(sim_rs), big.mark = ",")` models are evaluated. 

### Parallel Processing {#sec-parallel}

Fortunately, none of the  `r format(nrow(large_sfd) * nrow(sim_rs), big.mark = ",")` models depend on one another and can be computed separately. Since almost all modern computers have multiple CPUs and GPUs, we can break the computations into different "chunks" of work and execute them simultaneously on other processors (or separate computers entirely). The parallel processing of models can significantly reduce the time it takes to tune using grid search.

```{r}
#| label: bst-speeds
#| include: false
bst_speedup <- round(sfd_seq_time[3]/sfd_time[3], 1)
```

For example, take the KNN space-filling design with `r nrow(bst_sfd)` candidates evaluated across `r nrow(sim_rs)` resamples. We spread these models  `r nrow(bst_sfd) * nrow(sim_rs)` model fits across `r num_workers` worker processes (on the same computer). Despite the meager computation costs of training each boosted tree, there was still a `r bst_speedup`-fold speedup (`r round(sfd_seq_time[3], 1)`s versus `r round(sfd_time[3], 1)`s) when run in parallel. 

However, there are some nuances related to parallel processing.

First, it is an excellent idea to parallelize the "longest loop" (literally or figuratively). Looking back at @alg-grid-search, there is a loop across the $s$ candidates in line 6. Line 7 contains the resampling loop across $B$ resamples. If the data set is large, it may be optimal to invert the loop where we parallel process across the $B$ resamples and execute the $s$ models within each. Keeping each resample on a single worker means less input/output traffic across the workers. See @tmwr [Section 13.5.2](https://www.tmwr.org/grid-search#parallel-processing). Alternatively, if the data are not large, it could be best to “flatten” the two loops into a single loop with $s\times B$ iterations (with as many workers as possible). 

Additionally, if expensive preprocessing is used, a naive approach where each pipeline is processed in parallel might be counterproductive because we unnecessarily repeat the same preprocessing. For example, suppose we are tuning a supervised model along with UMAP preprocessing. By sending each of the $s$ candidates to a worker, the same UMAP training occurs for each set of candidates that correspond to the supervised model. Instead, a _conditional process_ can be used where we loop across the UMAP tuning parameter combinations and, for each, run another loop across the parameters associated with the supervised model. 

One other consideration is memory. If the data used to train and evaluate is copied for each worker, the number of workers can be restricted to work within system memory. 

In any case, parallel processing can almost always improve the speed of model tuning. 

### Racing {#sec-racing}

Racing [@maron1997racing] is a technique that adaptively resamples the data during grid search. The goal is to cull tuning parameter combinations with no real hope of being optimal before they are completely resampled. For example, in our previous tuning of the boosting model, it is clear that a single tree performs very poorly. Unfortunately, we can’t know that until we’ve finished all the computations. 

Racing tries to circumvent this issue by doing an interim statistical analysis of the tuning results. From this, we can compute a probability (or score) that measures how likely each candidate will be the best (for some single metric). If a candidate is exceedingly poor, we can tell that after a few resamples^[Note that racing requires multiple assessment sets. A single validation set could not be used with racing.]. 

There are a variety of ways to do this. See @kuhn2014futility, @krueger2015fast, and @bergman2024don. The general process is shown in @alg-race. The resampling process starts normally for the first $B_{min}$ resamples. Lines 7-12 can be conducted in parallel for additional computation speed. After the initial resampling for all candidates, the interim analysis at iteration $b$ results in a potentially smaller set of $s_b$ candidates. Note that parallel processing can be incorporated into this process for each loop and the speed-up from using submodels also occurs automatically. 

At each resampling estimate beyond the first $B_{min}$ iterations, the current candidate set is evaluated for a single resample, and the interim analysis potentially prunes tuning parameter combinations. 

::: {#alg-race}

:::: {.columns}

::: {.column width="10%"}

:::

::: {.column width="80%"}
```pseudocode
#| label: alg-race
#| html-line-number: true
#| html-line-number-punc: ":"

\begin{algorithm}
\begin{algorithmic}
\State $\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$
\State $B$: number of resamples
\State Initial number of resamples $1 \lt B_{min} \lt B$ 
\State $M(\mathfrak{D}^{tr}, B)$: a mapping function to split $\mathfrak{D}^{tr}$ for each of  $B$ iterations.
\State $f()$: model pipeline 
\State $\Theta$: Parameter set ($s \times m$) with candidates $\theta_j$
\For{$j=1$ \To $s$}
  \For{$b=1$ \To $B_{min}$}
    \State Generate $\hat{Q}_{jb} =$ \Call{Resample}{$\mathfrak{D}^{tr}, f(\cdot;\theta_j), M_b(\mathfrak{D}^{tr}, B)$}
  \EndFor
\State Compute $\hat{Q}_{j} = 1/B_{min}\sum_b \hat{Q}_{jb}$. 
\EndFor
\State Eliminate candidates to produce $\Theta^b$ ($s_b \times m$)
  \For{$b = B_{min} + 1$ \To $B$} 
    \For{$j=1$ \To $s$}
      \State Generate $\hat{Q}_{jb} =$ \Call{Resample}{$\mathfrak{D}^{tr}, f(\cdot;\theta_j), M_b(\mathfrak{D}^{tr}, B)$}
      \State Update candidate subset $\Theta^b$
    \Endfor
  \Endfor
\State Determine $\hat{\theta}_{opt}$ that optimizes $\hat{Q}_j^k$.   
\end{algorithmic}
\end{algorithm}
```
:::

::: {.column width="10%"}

:::

::::

Using racing for grid search tuning parameter optimization. 

:::

The details are in the analysis used to discard tuning parameter combinations.  For simplicity, we’ll focus on a basic method that uses a form of analysis of variance (ANOVA), a standard statistical tool for determining whether there are differences between conditions. In our context, the conditions are the tuning parameter candidates; we ignore their values and treat them as qualitative samples from the distribution of all possible tuning parameter combinations. The ANOVA model uses the candidate conditions as the predictor and the metric of choice as the numeric outcome. 

There are two statistical considerations to think about. 

First, the ANOVA model is used for statistical inference; we’ll use it to compute confidence intervals on differences in performance. This means that the probabilistic assumptions about our data matter. The ANOVA method requires that the errors follow a Gaussian distribution. That distribution can be strongly influenced by the distribution of the outcome, and, in our case, this is a performance metric. Previously, we used the Brier score, which has non-negative values and might be prone to follow a right-skewed statistical distribution. However, each resampled Brier score is the average of differences, and the Central Limit Theorem suggests that as the number of data points used to compute the score increases, the sampling distribution will become more Gaussian. If we use this type of racing model, we should use it on a model and check the normality assumptions of the errors. 

The second statistical complication is related to the resamples. Basic ANOVA methods require the data to be independent of one another, which is definitely not the case for resampling results. A “within-resample” correlation occurs since some resamples are “easier” to predict than others. This means that the metrics associated with each resample are more similar to one another than to the metrics from other resamples. 

This extra correlation means that a simple ANOVA model cannot be used. Instead, our interim analysis can use a hierarchical random effects model. This is the same methodology used in @sec-effect-encodings for effect encodings. We’ll treat our set of resamples as a random sample of possible resampling indices. The ANOVA model itself is: 

$$
Q_{ij} =(\beta_0 + \beta_{0i}) + \beta_1x_{i1} + \ldots +  \beta_{s-1}x_{i(s-1)}+ \epsilon_{ij}
$$ {#eq-perf-mod-racing}

for $i=1,\ldots, B$ resamples and $j=1, \ldots, s$ candidates. This random intercept model assumes that the ranking of candidates is the same across resamples and only the magnitude of the pattern changes from resample to resample. 

This model’s form is the reference cell parameterization discussed in @sec-indicators For each interim analysis, the reference cell will be set to the current best candidate. This means that the $\hat{\beta}_j$ parameter estimates are loss of performance relative to the current best. A one-sided confidence interval is constructed to determine if a candidate should be removed. We can stop considering candidates whose interval does not contain zero. 

```{r}
#| label: racing-anova
#| include: false
#| cache: true

num_remaining <- large_race_res %>% collect_metrics() %>% nrow()
num_fit <- nrow(map_dfr(large_race_res$.metrics, I))

loss_text <- large_race_res %>% 
  select(.order, .metrics) %>% 
  mutate(remaining = map_int(.metrics, nrow)) %>% 
  arrange(.order) %>% 
  mutate(loss = dplyr::lag(remaining) - remaining) %>% 
  slice(-1) %>% 
  filter(loss > 0) %>% 
  mutate(
    chr_order = xfun::numbers_to_words(.order - 1),
    text = map_chr(loss, ~ cli::pluralize("({.x} configuration{?s})")),
    text = paste(chr_order, text)
  ) %>% 
  summarize(glue::glue_collapse(text, ", ", last = ", and "))

best_race <- show_best(large_race_res, metric = "brier_class", n = 1) 
best_grid <- show_best(large_grid_res, metric = "brier_class", n = 1) 
best_large <- format_best_bst(large_grid_res, metric = "brier_class")

if (isTRUE(all.equal(best_race[, c("trees", "learn_rate")], best_grid[, c("trees", "learn_rate")]))) {
  best_text <- paste0("The numerically best candidate set for racing and basic grid search were the same: ", 
                     best_large, ".")   
} else {
  best_text <- 
    glue::glue("The numerically best candidate set for racing and basic grid \\
               search were different. Racing chose a model with \\
               {best_race$trees} trees and a exponent value of \\
               {round(best_race$learn_rate, 3)}. Full grid search chose \\
               {best_grid$trees} trees and an exponent of \\
               {round(best_grid$learn_rate, 3)}. The difference in Brier \\
               scores was {signif(abs(best_race$mean - best_grid$mean), 2)}.")
}
```
```{r}
#| label: racing-filter
#| include: false

initial_resamples <- 
  large_race_res %>% 
  filter(.order <= control_race()$burn_in) %>% 
  unnest(.metrics) %>% 
  select(id, .config, .estimate, trees, learn_rate)

initial_means <- 
  initial_resamples %>% 
  summarize(mean = mean(.estimate), .by = c(.config))

initial_resamples <- 
  initial_resamples %>% 
  full_join(initial_means, by = ".config") %>% 
  arrange(mean) %>% 
  mutate(
    .config = factor(.config),
    .config = reorder(.config, mean),
    id = factor(id)
  )

brier_mod <- lmer(.estimate ~ .config + (1|id), data = initial_resamples)

brier_coefs <- tidy(brier_mod, conf.int = TRUE, conf.level = 0.95)
brier_param <- 
  brier_coefs %>% 
  arrange(estimate) %>% 
  filter(!is.na(estimate) & grepl("config", term)) %>% 
  mutate(
    candidate = row_number(),
    Decision = ifelse(conf.low <= 0, "keep", "remove"),
    Decision = factor(Decision, levels = c("remove", "keep"))
  )
num_removed <- sum(brier_param$Decision == "remove")
num_removed <- xfun::numbers_to_words(num_removed)
num_removed <- tools::toTitleCase(num_removed)

resample_var <- brier_coefs$estimate[brier_coefs$term == "sd__(Intercept)"]^2
residual_var <- brier_coefs$estimate[brier_coefs$term == "sd__Observation"]^2
within_corr <- resample_var / (resample_var + residual_var)
```

To demonstrate the racing procedure, we re-evaluated our larger a space-filling design. The order of the folds was randomized, and after `r control_race()$burn_in` resamples, the ANOVA method was used to analyze the results. @fig-racing-initial shows the one-sided 95% confidence intervals for the loss of Brier score relative to the current best configuration (`r initial_resamples$trees[1]` trees and a learnig rate of 10<sup>`r log10(initial_resamples$learn_rate[1])`</sup>). `r num_removed` were eliminated at this round. 

```{r}
#| label: fig-racing-initial
#| echo: false
#| out-width: 70%
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.25
#| fig-cap: The first interim analysis in racing using a larger space-filling design for the boosted tree model.

brier_param %>% 
  ggplot(aes(y = candidate)) + 
  geom_point(aes(x = estimate), cex = 1 / 2) +
  geom_vline(xintercept = 0, lty = 2) +
  geom_errorbar(aes(xmin = conf.low, xmax = estimate, col = Decision), 
                alpha = 1 / 3) +
  labs(x = "Drop in Brier Score", y = "Candidate Rank")
```

In the end, the racing process eliminated all but `r num_remaining` candidates. The process eliminated candidates at iterations `r loss_text[[1]]`. In all, `r num_fit` models were fit out of the possible `r format(nrow(large_sfd) * nrow(sim_rs), big.mark = ",")` (`r round(num_fit / (nrow(large_sfd) * nrow(sim_rs)) * 100, 1)`%). `r best_text`

The model in @eq-perf-mod-racing allows us to estimate the within-resample correlation coefficient. This estimates how similar the performance metric values are to one another relative to values between samples. The correlation estimate is `r signif(within_corr, 2)`, indicating a moderate similarity of metrics within a resample. 

In summary, racing can be an effective tool for screening a large number of models while using comprehensive grids. 


## Optimization Bias and Nested Resampling {#sec-nested-resampling}

In the last, we found that the numerically best candidate was `r best_large`. The corresponding resampling estimate of the Brier score was `r signif(best_grid$mean, 3)` with a standard error of `r signif(best_grid$std_err, 3)`. If someone were to ask us how well the optimal boosted tree performs, we would probably give them these performance estimates. 

However, there may be an issue in doing so. We are using grid search and resampling to find the best combination of parameters _and_ to estimate the performance. In some situations, this dual use of the model tuning process can introduce **optimization bias** where, to some degree, our performance statistics are optimistic. 

@tibshirani2009bias @boulesteix2009optimal and maybe @tibshirani2018excess

```{r}
#| label: nested-sfd
#| include: false
#| eval: true

large_race_selections <- 
  large_nest_grid_res %>% 
  select(id, .selected) %>% 
  unnest(.selected) %>% 
  count(.config, trees, learn_rate, name = "# Selections")

nested_stats <- 
  bind_rows(
    large_race_res %>% 
      show_best(metric = "brier_class", n = 1) %>% 
      mutate(Algorithm = "Racing", `Estimation Method` = "Single CV"),
    
    bst_race_nest_res %>% 
      collect_metrics() %>% 
      mutate(Algorithm = "Racing", `Estimation Method` = "Nested CV")
    
  ) %>% 
  select(`Estimation Method`, Brier = mean, std_err)  
nested_stats$bias <- c(
  (nested_stats$Brier[2] - nested_stats$Brier[1]) /
  nested_stats$Brier[1] * 100,
  NA)
nested_stats$bias <- format(nested_stats$bias, digits = 3)
nested_stats$bias[2] <- ""
nested_stats$SNR <- 
  c(
    (nested_stats$Brier[2] - nested_stats$Brier[1]) /
      max(nested_stats$std_err),
    NA)
nested_stats$SNR <- format(nested_stats$SNR, digits = 3)
nested_stats$SNR[2] <- ""
```

Let’s use two of the previous examples from this chapter to demonstrate nested resampling: the space-filling design with `r sfd_size` candidates and the larger grid used with racing. Each used the same 10-fold cross-validation indices. We’ll use this as the outer resampling method and, within each, we’ll create 10-fold inner cross-validations based on each of the 90% analysis sets of the outer resamples. 

@fig-nested-cv visualizes the selected candidates for each of the ten outer resamples. Since some of the grid points are selected more than once, the image uses the size of the points to illustrate this. Also, the figure’s axis ranges portrays the entire tuning space for the two parameters (based on the larger grid). There appears to be a ridge of best parameters that arcs across the middle of the parameter space. This indicates that there is a region of equivocal performance so that there are a surplus of candidates that could be used. 

```{r}
#| label: fig-nested-cv
#| echo: false
#| eval: true
#| out-width: 50%
#| fig-width: 4.5
#| fig-height: 4.8
#| fig-cap: The reults of space filling designs with the frequencies in which points were selected during nested resampling for a complete grid and with a large grid combined with racing. The range of each axis shows the entire hyperparameter space.

bst_rngs <- map(large_param$object, ~ as.numeric(.x$range))

large_race_selections %>%
  ggplot(aes(trees, learn_rate)) +
  geom_point(aes(size = `# Selections`), alpha = 3/4) +
  labs(x = trees()$label, y = learn_rate()$label) +
  lims(x = bst_rngs[[1]]) +
  # coord_fixed(ratio = diff(bst_rngs[[1]]) / diff(bst_rngs[[2]])) +
  scale_y_log10(limits = 10^bst_rngs[[2]]) +
  scale_size(breaks = 1:max(large_race_selections$`# Selections`), 
             range = c(2, 7)) +
  theme(legend.position = "top") 
```

@tbl-nested-cv shows how the performance statistics differs between a single 10-fold cross-validation and two-level nested cross-validation. For this scenario, the Brier score estimates are negligible worse for nested cross-validation. However, notice that the standard errors are very similar since the same external resampling indices are used. The percent bias from single resampling was `r nested_stats$bias[1]`%. This values is very small but lack any sense of uncertainty. The final column of the table shows the raw difference in Brier scores divided by the standard error to form a signal-to-noise ratio (SNR). This shows that the bias is small relative to the experimental noise^[To conceptualize this, many hypothesis testing statistics are signal-to-noise ratios. For example, to assess the simple difference between means, the basic t-test would probably use an SNF cutoff between 1.6 and 2 to be thought of as a “significant” signal.]. The computational costs of nested resampling of basic grid search and racing were `r sprintf("%2.1f", sfd_nest_time[3]/sfd_time[3])`-fold  and `r sprintf("%2.1f", large_nest_race_time[3]/large_race_time[3])`-fold slower, respectfully.

::: {#tbl-nested-cv}

:::: {.columns}

::: {.column width="20%"}

:::

::: {.column width="60%"}


```{r}
#| label: tbl-nested-cv
#| echo: false
#| eval: true
nested_stats %>%
  kable(digits = 3, 
        col.names = c("Resampling", "Mean", "Std. Err.", "Bias (%)", "SNR"),
        align = "ccccc") %>% 
  add_header_above(c(" ", "Brier Score" = 2, " " = 2)) %>% 
  kable_styling(full_width = FALSE)
```


:::

::: {.column width="20%"}

:::

::::

Brier scores for two tuning algorithms evaluated with regular 10-fold cross-validation and nested 10-fold cross-validations.

:::

The lack of significant bias in the analysis above does not discount the problem of optimization bias. It exists but can sometimes be within the experimental noise. We do know that some problems/scenareos are more susceptible than others. Model pipelines that include supervised feature selection have a substantial amount of variation. Another situation occurs in high dimensional biology where a relative small number of data points is coupled with an excessively large number of predictors. This $p >>> n$ issue tends to result in selection/optimization bias if not for bias corrections. 

Finally, although we have discussed it in the context of grid search, nested resampling can be utilized with interative search and other techniques.   

## Setting Parameter Ranges

Specifying the range of parameters use to define the grid may involve some guesswork. For some parameter types, there is a well understood and defined range. For others, such as learning rate, there is a lower bound (zero) and a loosely defined upper bound mostly based on convention and prior experience. For grids, we want to avoid configurations with few unique values and a wide range. This might not sample the section of the parameter space that includes nonlinearity and the region of optimal results. @fig-learn-rate-seq shows an example of this for the learning rate. The initial grid included three points that missed the regions of interest. @fig-learn-rate-grid did a better job with more grid points. In high dimensional parameter space, the likelihood of a poor grid increases, esepcially for relatively “small” grid sizes. 

When often envision of the relationship between a predictor and model performance as being a shark peak. If we cannot find the pinnacle, the model optimization would fail. Luckily, as seen in @fig-nested-cv, there are often substantial regions of parameter space with good performance. Model optimization may not be superficially easy but it is often not impossible. We only need to sample the optimal region once to be successful. 

If we worry that our tuning grid did not produce good results, another strategy is to _increase_ the parameter ranges and use an iterative approach that can naturally explore the parameter space and let the current set of results guide the exploration about the space. These methods are discussed in the next chapter. 


## Chapter References {.unnumbered}


```{r}
#| label: teardown-cluster
#| include: false

stopCluster(cl)
```

