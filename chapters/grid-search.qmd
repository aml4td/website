---
knitr:
  opts_chunk:
    cache.path: "../_cache/grid/"
---

# Grid Search {#sec-grid}

```{r}
#| label: grid-setup
#| include: false
source("../R/_common.R")

# ------------------------------------------------------------------------------

library(kableExtra)
library(lme4)
library(broom.mixed)
library(tidymodels)
library(finetune)
library(bonsai)
library(patchwork)
library(rlang)
library(future)

# ------------------------------------------------------------------------------

source("../R/setup_two_class_example.R")

set.seed(943)
sim_tr <- sim_logistic(2000, sim_f)

set.seed(14)
sim_rs <- vfold_cv(sim_tr)

set.seed(14)
sim_nested_rs <- nested_cv(sim_tr, outside = vfold_cv(), inside = vfold_cv())

load("../RData/grid_benchmarks.RData")

# ------------------------------------------------------------------------------
# Set Options

plan("multisession")
tidymodels_prefer()
theme_set(theme_transparent())
set_options()

num_workers <- parallel::detectCores()

# ------------------------------------------------------------------------------
# Until the interface in the tune package is finalized, do some computations 
# offline
load("../RData/nested_res.RData")
```

Grid search is a method to optimize a model pipeline's tuning parameters. It creates a pre-defined set of candidate values and computes performance for each. From there, the numerically best candidate could be chosen, or the relationship between the tuning parameter(s) and model performance can be inspected to see if the model might benefit from additional optimization. 

Suppose there is a single tuning parameter, as in the $n_{min}$ example of @sec-external-validation; each candidate takes a scalar value (quantitative or qualitative). For other models, there are multiple tuning parameters. For example, support vector machine models can have two or three tuning parameters, the Elastic Net model has two tuning parameters, and the boosted tree model has multiple tuning parameters.  We'll look at two of the boosted tree parameters in detail below. 

The previous chapter demonstrated that using external data to evaluate the model is crucial (be it resampling or a validation set). Grid search has no free lunch: we cannot simply fit the model to each candidate set and evaluate them by simply re-predicting the same data. 

@alg-grid-search formally describes the grid search process. For a model with $m$ tuning parameters, we let $\Theta$ represent the collection of $s$ candidate values. For each specific combination of parameters ($\theta_j$), we resample the model to produce some measure of efficacy (e.g., $R^2$, accuracy, etc.)^[This process was illustrated in [Algorithm 9.1](tmp_resampling.html#alg-resampling).]. From there, the best value is chosen, or additional work is carried out to find a suitable candidate. 


::: {#alg-grid-search}

:::: {.columns}

::: {.column width="10%"}

:::

::: {.column width="80%"}


```pseudocode
#| html-line-number: true
#| html-line-number-punc: ":"

\begin{algorithm}
\begin{algorithmic}
\State $\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$
\State $B$: number of resamples
\State $M(\mathfrak{D}^{tr}, B)$: a mapping function to split $\mathfrak{D}^{tr}$ for each of  $B$ iterations.
\State $f()$: model pipeline 
\State $\Theta$: Parameter set ($s \times m$) with candidates $\theta_j$
\For{$j=1$ \To $s$}
  \State Generate $\hat{Q}_{j} =$ \Call{Resample}{$\mathfrak{D}^{tr}, f(\cdot;\theta_j), M(\mathfrak{D}^{tr}, B)$} corresponding to candidate $\theta_j$.
\EndFor
\State Determine $\hat{\theta}_{opt}$ that optimizes $\hat{Q}_{j}$.
\end{algorithmic}
\end{algorithm}
```

:::

::: {.column width="10%"}

:::

::::

Grid search for tuning parameter optimization that loops over the resampling algorithm shown in @alg-resample.

:::


```{r}
#| label: grids
#| include: false

bst_spec <- 
  boost_tree(learn_rate = tune(), trees = tune()) %>% 
  set_mode("classification")%>% 
  set_engine("lightgbm")

bst_wflow <-
  workflow() %>%
  add_model(bst_spec) %>%
  add_formula(class ~ A + B)

bst_param <- 
  bst_wflow %>% 
  extract_parameter_set_dials() %>% 
  update(learn_rate = learn_rate(c(-3, -1)))

bst_reg <-
  grid_regular(bst_param, levels = c(5, 3)) %>%
  mutate(grid = "Regular")

sfd_size <- nrow(bst_reg)

set.seed(nrow(bst_reg))
bst_random <-
  grid_random(bst_param, size = sfd_size) %>%
  mutate(grid = "Random")

bst_sfd <- grid_space_filling(bst_param, size = sfd_size) %>%
  mutate(grid = "Space-Filling")

seq_tree <- sort(unique(bst_reg$trees))
txt_tree <- knitr::combine_words(seq_tree)

seq_ord <- format(sort(unique(log10(bst_reg$learn_rate))))
seq_ord <- paste0("10<sup>", seq_ord, "</sup>")
lr_ord <- knitr::combine_words(seq_ord)

bst_grids <- 
  bind_rows(bst_reg, bst_random, bst_sfd) %>%
  mutate(
    grid = factor(grid, levels = c("Regular", "Random", "Space-Filling"))
  )
```

To demonstrate, this chapter will initially focus on grids for a boosted tree model with two tuning parameters. Recall that a boosted tree is a collection of individual decision trees created sequentially. One parameter related to the process of creating the ensemble is the learning rate: the next tree uses information from the last tree to improve it. An important parameter is how much, or how _fast_ it learns. It could be that some data sets need a large number of trees that evolve slowly (i.e., low learning rate) to find an optimal value of the performance metric.  Alternatively, other data sets require fewer trees that change rapidly for optimal predictive performance. The learning rate parameter must be greater than zero, and is typically in the range of 10<sup>-5</sup> to  10<sup>-1</sup>.  Because the magnitude of this parameter is small, it is best conceptualized in log units. 

We'll illustrate different strategies of grid search using this model setup with varying configurations of the learning rate and the number of trees in the ensemble. 

There are two main classes of grids: regular and irregular. We can view generating a collection of candidate models as a statistical design of experiments (DOE) problem [@BHH]. There is a long history of DOE, and we'll invoke relevant methods for each grid type. @santner2018design and @gramacy2020surrogates have excellent overviews of the DOE methods discussed in this chapter. 

The following two sections describe different methods for creating the candidate set $\Theta$. Subsequent sections describe strategies for making grid search efficient using tools such as parallel processing and model racing. 

## Regular Grids {#sec-regular-grid}

A regular grid starts with a sequence or set of candidate values for each tuning parameter and then creates all combinations. In statistics, this is referred to as a factorial design. The number of values per tuning parameter does not have to be the same. 

To illustrate a regular grid, we'll use `r xfun::numbers_to_words(length(seq_tree))` values of the number of trees (`r txt_tree`) and `r xfun::numbers_to_words(length(seq_ord))` values of the learning rate (`r lr_ord`). This grid of `r nrow(bst_reg)` candidates is shown in @fig-bst-grids. The grid covers the entire space with significant lacuna in between.  

```{r}
#| label: fig-bst-grids
#| echo: false
#| out-width: 85%
#| fig-width: 7.5
#| fig-height: 2.5
#| fig-cap: Three types of grids (in columns) are illustrated with tuning parameters from a boosting model. Each grid contains 15 candidates. The space-filling design was created using the Audze-Eglais method described in @sec-irregular-grid. 

bst_grids %>%
  ggplot(aes(trees, learn_rate)) +
  geom_point(cex = 2, alpha = 2 / 3) +
  labs(x = trees()$label, y = learn_rate()$label) +
  facet_wrap(~ grid) +
  scale_y_log10() + 
  coord_fixed(ratio = 1000)
```


```{r}
#| label: bst-grid-res
#| include: false
#| cache: true

format_best_bst <- function(x, ...) {
  bst <- select_best(x, ...)
  paste0(
      format(bst$trees, big.mark = ","), " trees and and a learning rate of 10<sup>",
      # sprintf("%1.1f", bst$learn_rate)
      signif(log10(bst$learn_rate), 2),
      "</sup>"
      )
}

# ------------------------------------------------------------------------------

set.seed(200)
bst_reg_res <- 
  bst_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    metrics = metric_set(brier_class),
    grid = bst_reg %>% select(-grid)
  )

bst_reg_best <- format_best_bst(bst_reg_res, metric = "brier_class")

bst_rnd_res <- 
  bst_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    metrics = metric_set(brier_class),
    grid = bst_random %>% select(-grid)
  )
bst_rnd_best <- format_best_bst(bst_rnd_res, metric = "brier_class")

bst_sfd_res <- 
  bst_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    metrics = metric_set(brier_class),
    grid = bst_sfd %>% select(-grid)
  )   
bst_sfd_best <- format_best_bst(bst_sfd_res, metric = "brier_class")
bst_sfd_brier <- signif(show_best(bst_sfd_res, metric = "brier_class", n = 1)$mean, 3)
bst_sfd_brier_se <- signif(show_best(bst_sfd_res, metric = "brier_class", n = 1)$std_err, 3)

bst_irr_grid_res <- 
  collect_metrics(bst_rnd_res) %>% 
  mutate(grid = "Random") %>% 
  bind_rows(
    collect_metrics(bst_sfd_res) %>% 
      mutate(grid = "Space-Filling")
  ) %>% 
  mutate(`learn_rate (log-10)` = log10(learn_rate)) %>% 
  select(-learn_rate) %>% 
  pivot_longer(
    c(trees, `learn_rate (log-10)`),
    names_to = "parameter",
    values_to = "value"
  ) 
```

We’ll use the simulated training set in @sec-complexity with the same 10-fold cross-validation scheme described there. Once again, Brier scores were used to measure how well each of the `r nrow(bst_reg)`  configurations of boosted tree model predicted the data. @fig-bst-regular-grid-tuning shows the results: a single tree is a poor choice (due to underfitting), and there are several learning rate values that work well. Even though this is not a diverse set of candidate values, we can probably pick out a reasonable candidate with a small Brier score, such as `r bst_reg_best` (although there are a few other candidates that would be good choices). 

```{r}
#| label: fig-bst-regular-grid-tuning
#| echo: false
#| out-width: 50%
#| fig-width: 4
#| fig-height: 3.5
#| fig-cap: "The boosted tree tuning results for the regular grid shown in @fig-bst-grids."

autoplot(bst_reg_res)  +
  labs(y = "Brier Score") +
  theme(legend.position = "top")
```

The pattern shown in this visualization is interesting: the trajectory for the number of trees is different for different values of the learning rate. The two larger learning rates are similar, showing that the optimal number of trees is in the low- to mid-range of our grid. The results for the smallest learning rate indicate that better performance might be found using _more_ trees than were evaluated. This indicates that there is an _interaction effect_ in our tuning parameters (just as we saw for predictors in @sec-interactions). This might not affect how we select the best candidate value for the grid, but it does help build some intuition for this particular model that might come in handy when tuning future models. 

Gaining intuition is much easier for regular grids than other designs since we have a full set of combinations. As we’ll see shortly, this interaction is very hard to see for a space-filling design. 

The primary downside to regular grids is that, as the number of tuning parameters increases, the number of points required to fill the space becomes extremely large (due to the curse of dimensionality). However, for some models and pre-processing methods, regular grids can be very efficient despite the number of tuning parameters (see the following section that describes the "submodel trick").  

## Irregular Grids {#sec-irregular-grid}

Irregular grids are not factorial in nature. A simple example is a _random grid_ where points are randomly placed using a uniform distribution on an appropriate range for each tuning parameter. A design of size `r sfd_size` is shown in @fig-bst-grids. There are some gaps and clustering of the points, but the space for each individual dimension is covered well. The tuning results are shown in the top panels of @fig-bst-irregular-grid-tuning. Because it is an irregular design, we can't use the same visualization as in @fig-bst-regular-grid-tuning. Instead, a "marginal" plot is shown, where each numeric tuning parameter is plotted against performance in a separate panel. 

The top left panel suggests that a very small learning rate is bad, but otherwise, there is not a strong trend. For the number of trees (in the top right panel), a small number of trees should be avoided, and perhaps the maximum number tested would be a good choice. From this plot, it is impossible to discover the interaction effect seen with the regular grid. However, this marginal plot is the main technique for visualizing the results when there are a moderate to large number of tuning parameters. The numerically best candidate was not too dissimilar from the regular grid: `r bst_rnd_best`

```{r}
#| label: fig-bst-irregular-grid-tuning
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 3.75
#| fig-cap: "The tuning results for the irregular grids shown in @fig-bst-grids. The lines are spline smooths."

bst_irr_grid_res %>% 
  ggplot(aes(value, mean)) + 
  geom_smooth(se = FALSE, method = lm, formula = y ~ splines::ns(x, df = 3),
              col = rgb(0, 0, 1, 1 / 5)) +
  geom_point(cex = 2, alpha = 2 / 3) +
  facet_grid(grid ~ parameter, scales = "free_x") +
  labs(y = "Brier Score", x = NULL)
```

Another type of irregular grid is a _space-filling design_ [@joseph2016space], where the goal is to make sure that the tuning parameter space is covered and that there is minimal redundancy in the candidate values. There are a variety of methods for achieving this goal. 

For example, @fig-bst-grids shows a `r sfd_size` point Audze-Eglais space-filling design. The space is covered more compactly than the regular design of the same size and is much more uniform than the `r sfd_size` point random design. The lower panels of @fig-bst-irregular-grid-tuning show the tuning results. The results are somewhat cleaner than the random grid results but show similar trends. Here, the numerically best candidate was: `r bst_sfd_best`. 

There are many types of space-filling designs, and it is worth taking the time to take a quick tour of some of them. 

The Latin hypercube design (LHD) is the most popular method for constructing space-filling designs [@Viana2016; @husslage2011space]. These designs have a simple definition. Suppose our hyperparameter space is rectangular and partitioned into smaller (hyper)cubes. From this, a LHD is a set of distinct points where no one dimension has multiple values in any bins^[Other applications may want the points to have properties such as orthogonality, symmetry, etc.]. We desire candidates that fill the space of each parameter and are not close to one another. 

The most basic approach to creating a Latin hypercube design is random sampling [@Mckay2000]. If there are $m$ parameters and we request $s$ candidate values, the parameter space is initially divided into $s^m$ hypercubes of equal size. For each tuning parameter, $s$ regions in its dimension are selected at random, and a value is placed in this box (also at random). This process repeats for each dimension. Suppose there are ten bins for a parameter that ranges between zero and one. If the first design point selects bin two, a random uniform value is created in the range `[0.1 0.2)`.  @fig-lhs-sampled shows three such designs, each generated with different random numbers. 

```{r}
#| label: fig-lhs-sampled
#| echo: false
#| out-width: 90%
#| fig-width: 9
#| fig-height: 3
#| fig-cap: Three replicate Latin hypercube sampling designs of size ten for two parameters. Each design uses different random numbers. The grid lines illustrate the 100 hypercubes used to generate the values.  

make_lhc <- function(ind, m, s) {
  des <- DiceDesign::lhsDesign(s, m, seed = ind + 287)$design
  colnames(des) <- paste("Parameter", letters[1:ncol(des)])
  des <- as_tibble(des)
  des$Replicate <- paste("Replicate", ind)

  des
}

set.seed(202)
map_dfr(1:3, ~ make_lhc(.x, 2, 10)) %>% 
  ggplot(aes(`Parameter a`, `Parameter b`)) + 
  geom_point(cex = 1.5) + 
  coord_equal() +
  geom_hline(yintercept = seq(0, 1, by = 1 / 10), alpha = 1 / 10) +
  geom_vline(xintercept = seq(0, 1, by = 1 / 10), alpha = 1 / 10) +
  lims(x = 0:1, y = 0:1) +
  theme(panel.grid.minor = element_blank()) + 
  facet_wrap(~ Replicate) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
  )
```

Technically, these designs cover the space of each predictor uniformly. However, large multivariate regions can be empty. For example, one design only samples combinations along the diagonal. Additional constraints can make the design more consistent with our desires. 

```{r}
#| label: design-crit
#| include: false

rnd_maxmin <-
  encode_set(bst_random %>% select(-grid), pset = bst_param, as_matrix = TRUE) %>% 
  DiceDesign::mindist() %>% 
  round(2)

sfd_maxmin <-
  encode_set(bst_sfd %>% select(-grid), pset = bst_param, as_matrix = TRUE) %>% 
  DiceDesign::mindist() %>% 
  round(2)

reg_maxmin <-
    encode_set(bst_reg %>% select(-grid), pset = bst_param, as_matrix = TRUE) %>% 
    DiceDesign::mindist() %>% 
    round(2)
```

For example, we could choose points to maximize the minimum pairwise distances between the candidates. These designs are usually referred to as MaxiMin designs [@pronzato2017minimax]. Comparing the two irregular designs in @fig-bst-grids, the random grid has a maximum minimum distance between candidates  of `r rnd_maxmin`. In contrast, the corresponding space-filling design's value (`r sfd_maxmin`) is `r round(sfd_maxmin/rnd_maxmin, 1)`-fold larger^[Using Euclidean distance.]. The latter design was optimized for coverage, and we can see far less redundancy in this design. 

A similar method, initially proposed by @audze1977new, maximizes a function of the inverse distances between $s$ candidate points: 

$$
criterion = \sum_{i=1}^s \sum_{j=1,\;i\ne j}^s\frac{1}{dist(\theta_i, \theta_j)^2}
$$

@bates2004formulation devised search methods to find optimal designs for this criterion.

Some other space-filling designs of note: 

- Maximum entropy sampling selects points based on assumptions related to the distributions of the tuning parameters and their covariance matrix [@shewry1987maximum;@joseph2015maximum]. 

- Uniform designs [@fang2000uniform;@wang2022design] optimally allocate points so that they are uniformly distributed in the space. 

While these methods can be generally constructed by sampling random points and using a search method to optimize a specific criterion, there has been scholarship that has pre-optimized designs for some combination of the number of tuning parameters and the requested grid size. 

The advantage of space-filling designs over random designs is that, for smaller designs, the candidates do a better job covering the space and have a low probability of producing redundant points. Also, it is possible to create a space-filling design so that the candidates for each numerical parameter are nearly equally spaced (as was done in @fig-bst-grids). 

Many designs assume that all the tuning parameter values are quantitative. That may not always be the case. For example, K-nearest neighbors can adjust its predictions by considering how far the cost are from a new point; more distant points should not have the same influence as close cost. A weighting function can be used for this purpose. For example, weights based on the inverse of the distance between neighbors may produce better results. Equal weighting is often called a “rectangular weighting” function. The type of algebraic function used for weighting is a qualitative tuning parameter. 

A simple workaround for creating a space-filling design is to repeat the unique parameter values as if they were $s$ distinct values when making the design. This allows them to be used with a Latin hypercube design and any space-filling design that uses this approach is not technically optimal. Practically speaking, this is an effective approach for tuning models.  However, there are _sliced_ LHD that can accomplish the same goal [@qian2012sliced;@ba2015optimal].

We recommend space-filling designs since they are more efficient than regular designs. Regular designs have a lot of benefits when using an unfamiliar modeling methodology since you will learn a lot more about the nuances of how the tuning parameters affect one another. 


## Chapter References {.unnumbered}


