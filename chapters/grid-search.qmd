---
knitr:
  opts_chunk:
    cache.path: "../_cache/grid/"
---

# Grid Search {#sec-grid}

```{r}
#| label: grid-setup
#| include: false
source("../R/_common.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(finetune)
library(rlang)
library(sfd)
library(doParallel)

# ------------------------------------------------------------------------------

source("../R/setup_two_class_example.R")
load("../RData/grid_benchmarks.RData")

# ------------------------------------------------------------------------------
# Set Options

cl <- makePSOCKcluster(parallel::detectCores(logical = TRUE))
registerDoParallel(cl)
tidymodels_prefer()
theme_set(theme_transparent())
set_options()

```

Grid search is a method to optimize tuning parameters for a model pipeline^[Preprocessing and postprocessig operations can have tuning parameters.]. It creates a pre-defined set of candidate values and computes performance from each. From there, the numerically best candidate could be chosen, or the relationship between the tuning parameter(s) and model performance can be inspected to see if the model might benefit from additional optimization. 

Suppose there is a single tuning parameter, as in the $n_{min}$ example of @sec-grid-and-sequential; each candidate takes a scalar value (quantitative or qualitative). In other cases, there are multiple tuning parameters. For example, the K-nearest neighbors  (KNN) model has multiple tuning parameters and we'll look at two of them in detail below. 

The previous chapter demonstrated that using external data to evaluate the model is crucial (be it resampling or a validation set). Grid search has no free lunch: we cannot simply fit the model and evaluate it by simply re-predicting the same data. 

@alg-grid-search formally describes the process. For a model with $m$ tuning parameters, we let $\Theta$ represent the collection of $s$ candidate values. For each specific combination of parameters ($\theta_j$), we resample the model to produce some measure of efficacy (e.g., $R^2$, accuracy, etc.)^[This process was illustrated in [Algorithm 9.1](tmp_resampling.html#alg-resampling).]. From there, the best value is chosen, or additional work is carried out to find a suitable candidate. 


::: {#alg-grid-search}

:::: {.columns}

::: {.column width="10%"}

:::

::: {.column width="80%"}


```pseudocode
#| html-line-number: true
#| html-line-number-punc: ":"

\begin{algorithm}
\begin{algorithmic}
\State $\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$
\State $B$: number of resamples
\State $M(\mathfrak{D}^{tr}, B)$: a mapping function to split $\mathfrak{D}^{tr}$ for each of  $B$ iterations.
\State $f()$: model pipeline 
\State $\Theta$: Parameter set ($s \times m$) with candidates $\theta_j$
\For{$j=1$ \To $s$}
  \State Generate $\hat{Q}_{j} =$ \Call{Resample}{$\mathfrak{D}^{tr}, f(\cdot;\theta_j), M(\mathfrak{D}^{tr}, B)$} corresponding to candidate $\theta_j$.
\EndFor
\State Determine $\hat{\theta}_{opt}$ that optimizes $\hat{Q}_{j}$.
\end{algorithmic}
\end{algorithm}
```

:::

::: {.column width="10%"}

:::

::::

Grid search for tuning parameter optimization that loops over the resampling algorithm shown in @alg-resample.

:::


```{r}
#| label: knn-grids
#| include: false

knn_prm <- parameters(neighbors(c(2, 50)), dist_power())

knn_reg <-
  grid_regular(knn_prm, levels = c(5, 3)) %>%
  mutate(grid = "Regular")

seq_k <- sort(unique(knn_reg$neighbors))
txt_k <- knitr::combine_words(seq_k)

seq_ord <- format(sort(unique(knn_reg$dist_power)))
txt_ord <- knitr::combine_words(seq_ord)

sfd_size <- nrow(knn_reg)

vals <- map(knn_prm$object, ~ rep_len(value_seq(.x, sfd_size), length.out = sfd_size))

set.seed(nrow(knn_reg))
knn_random <-
  grid_random(knn_prm, size = sfd_size) %>%
  mutate(grid = "Random")

knn_sfd <- 
  get_design(2, sfd_size, type = "max_min_l1") %>% 
  update_values(vals) %>% 
  setNames(names(knn_random)) %>%
  mutate(grid = "Space-Filling")
 
knn_grids <- 
  bind_rows(knn_reg, knn_random, knn_sfd) %>%
  mutate(
    grid = factor(grid, levels = c("Regular", "Random", "Space-Filling"))
  )
```

To demonstrate, this chapter will initially focus on grids for a K-nearest neighbors model with two tuning parameters^[These are described in more detail in @sec-cls-knn]. Nearest neighbors were initially described in @sec-isomap for multidimensional scaling embeddings. Supervised KNN models predict a new point by finding the _K_ most similar data point in the training set. It then uses the _K_ outcome values to form a prediction for the new data. The number of neighbors is a tuning parameter. For our example, we’ll consider 2 to 50 neighbors. Another parameter focuses on how to compute the distance. There are a lot of different choices, and several commonly used distance metrics are special cases of Minkowski distance:

$$
d\left(\boldsymbol{x},\boldsymbol{x}'\right)={\biggl(}\sum_{j=1}^{p}|x_{j}-x_{j}'|^{\phi}{\biggr)}^{\frac {1}{\phi}}.
$$  {#eq-minkowski}

The value $\phi$ governs the order of the distance with $\phi = 2$ corresponding to Euclidean distance while $\phi = 1$ is Manhattan distance. We’ll look at distances with $\phi \in [1.0, 2.0]$.  We'll illustrate different strategies using this model setup with varying configurations of these two tuning parameters. 

There are two main classes of grids: regular and irregular. We can view generating a collection of candidate models as a statistical design of experiments (DOE) problem [@BHH]. There is a long history of DOE, and we'll invoke relevant methods for each grid type. @santner2018design and @gramacy2020surrogates have excellent overviews of the DOE methods discussed in this chapter. 

The following two sections describe different methods for creating the candidate set $\Theta$. Subsequent sections describe strategies for making grid search efficient using tools such as parallel processing and model racing. 

## Regular Grids {#sec-regular-grid}

A regular grid starts with a sequence or set of candidate values for each tuning parameter and then creates all combinations. In statistics, this is referred to as a factorial design. The number of values per tuning parameter does not have to be the same. 

To illustrate a regular grid, we'll use `r xfun::numbers_to_words(length(seq_k))` values of the number of neighbors (K = `r txt_k`) and `r xfun::numbers_to_words(length(seq_ord))` values of the distance order ($\phi$ = `r txt_ord`). This grid of `r nrow(knn_reg)` candidates is shown in @fig-knn-grids. The grid covers the entire space with significant gaps in between.  

```{r}
#| label: fig-knn-grids
#| echo: false
#| out-width: 75%
#| fig-width: 7
#| fig-height: 3
#| fig-cap: Three types of grids (in columns) are illustrated with tuning parameters from a K-nearest neighbor model. Each grid contains 15 candidates. The space-filling design was created using the MaxiMin method described in @sec-irregular-grid. 

knn_grids %>%
  ggplot(aes(neighbors, dist_power)) +
  geom_point(cex = 2, alpha = 2 / 3) +
  labs(x = neighbors()$label, y = dist_power()$label) +
  facet_wrap(~ grid) +
  coord_fixed(ratio = 50) 
```


```{r}
#| label: knn-grid-res
#| include: false
#| cache: false

knn_spec <- 
  nearest_neighbor(neighbors = tune(), dist_power = tune(), 
                   weight_func = "rectangular") %>% 
  set_mode("classification")

knn_wflow <-
  workflow() %>%
  add_model(knn_spec) %>%
  add_formula(class ~ A + B)

format_best_knn <- function(x, ...) {
  bst <- select_best(x, ...)
  paste0(
      bst$neighbors, " neighbors and and a Minkowski distance order of ",
      sprintf("%1.1f", bst$dist_power)
      )
}

# ------------------------------------------------------------------------------

knn_reg_res <- 
  knn_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    metrics = metric_set(brier_class),
    grid = knn_reg %>% select(-grid)
  )

knn_reg_best <- format_best_knn(knn_reg_res, metric = "brier_class")

knn_rnd_res <- 
  knn_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    metrics = metric_set(brier_class),
    grid = knn_random %>% select(-grid)
  )
knn_rnd_best <- format_best_knn(knn_rnd_res, metric = "brier_class")

knn_sfd_res <- 
  knn_wflow %>% 
  tune_grid(
    resamples = sim_rs,
    metrics = metric_set(brier_class),
    grid = knn_sfd %>% select(-grid)
  )   
knn_sfd_best <- format_best_knn(knn_sfd_res, metric = "brier_class")
knn_sfd_brier <- signif(show_best(knn_sfd_res, metric = "brier_class", n = 1)$mean, 3)
knn_sfd_brier_se <- signif(show_best(knn_sfd_res, metric = "brier_class", n = 1)$std_err, 3)

knn_irr_grid_res <- 
  collect_metrics(knn_rnd_res) %>% 
  mutate(grid = "Random") %>% 
  bind_rows(
    collect_metrics(knn_sfd_res) %>% 
      mutate(grid = "Space-Filling")
  ) %>% 
  rename(
    `# Nearest Neighbors` = neighbors,
    `Minkowski Distance Order` = dist_power) %>% 
  pivot_longer(
    c(`# Nearest Neighbors`, `Minkowski Distance Order`),
    names_to = "parameter",
    values_to = "value"
  ) 
```

We’ll use the simulated training set in @sec-complexity-overfitting with the same 10-fold cross-validation scheme described there. Once again, Brier scores were used to measure how well each of the `r nrow(knn_reg)`  configurations of KNN predicted the data. @fig-knn-regular-grid-tuning shows the results: a small number of nearest neighbors is a poor choice (due to overfitting), and the Minkowski distance parameter matters little for these data. Even though this is not a diverse set of candidate values, we can probably pick out a reasonable candidate with a small Brier score, such as `r knn_reg_best` (although any of the values of $\phi$ would be good choices). 

```{r}
#| label: fig-knn-regular-grid-tuning
#| echo: false
#| out-width: 50%
#| fig-width: 4
#| fig-height: 3.5
#| fig-cap: "The tuning results for the regular grid shown in @fig-knn-grids."

autoplot(knn_reg_res)  +
  labs(y = "Brier Score") +
  theme(legend.position = "top")
```

The nice thing about regular grids is that you can learn about how the tuning parameters interrelate as well as their relationship to model performance. It is possible for tuning parameters to interact with one another (in the same manner as predictors in @sec-interactions). This can usually be seen when visualizing the results of a regular grid. In our KNN example, the three profiles have roughly the same relationship with the performance metric. This indicates that the impact of these parameters is additive. Understanding how a model's tuning parameters affect performance can help improve model optimization. 

The primary downside to regular grids is that, as the number of tuning parameters increases, the number of points required to fill the space becomes extremely large (due to the curse of dimensionality). However, for some models and pre-processing methods, regular grids can be very efficient despite the number of tuning parameters (see the following section that describes the "submodel trick").  

## Irregular Grids {#sec-irregular-grid}

Irregular grids are not factorial in nature. A simple example is a _random grid_ where points are randomly placed using a uniform distribution on an appropriate range for each tuning parameter. A design of size `r sfd_size` is shown in @fig-knn-grids. There are some gaps and clustering of the points, but the space for each individual dimension is covered well. The tuning results are shown in the top panels of @fig-knn-irregular-grid-tuning. Because it is an irregular design, we can't use the same visualization as in @fig-knn-regular-grid-tuning. Instead, a "marginal" plot is shown, where each numeric tuning parameter is plotted against performance in a separate panel. 

The top left panel illustrates a clear signal that the number of neighbors matters for these data. However, recall that each point also corresponds to a random Minkowski parameter value. 

The top right panel is less clear regarding a relationship between the Minkowski parameter and the Brier score. From the regular grid results in @fig-knn-grids, we know definitively that there isn’t^[At least for this range of $\phi$.]. However, for the random design it may appear that something going on. The problem is that the two-neighbor model result is so poor that the location of its matching Minkowski parameter value may influence our impression of an effect. This wasn’t an issue in the regular design since you get to see all combinations of the two parameters.  The numerically best candidate was not too dissimilar from the regular grid: `r knn_rnd_best`.

```{r}
#| label: fig-knn-irregular-grid-tuning
#| echo: false
#| out-width: 80%
#| fig-width: 7
#| fig-height: 3.75
#| fig-cap: "The tuning results for the irregular grids shown in @fig-knn-grids. The lines are spline smooths."

knn_irr_grid_res %>% 
  ggplot(aes(value, mean)) + 
  geom_smooth(se = FALSE, method = lm, formula = y ~ splines::ns(x, df = 3),
              col = rgb(0, 0, 1, 1 / 5)) +
  geom_point(cex = 2, alpha = 2 / 3) +
  facet_grid(grid ~ parameter, scales = "free_x") +
  labs(y = "Brier Score", x = NULL)
```

For this example, the patterns for the number of neighbors do not change across values of the Minkowski distance parameter (i.e., there is no interaction). If this were not the case, the relationship between these parameters and performance would be more difficult to understand in marginal effect plots. Examples of this will be seen in later chapters, such as in TODO and TODO.

Another type of irregular grid is a _space-filling design_ [@joseph2016space], where the goal is to make sure that the tuning parameter space is covered and that there is minimal redundancy in the candidate values. There are a variety of methods for achieving this goal. 

For example, @fig-knn-grids shows a `r sfd_size` point “Audze-Eglais“ space-filling design. The space is covered more compactly than the regular design of the same size and is much more uniform than the `r sfd_size` point random design. The lower panels of @fig-knn-irregular-grid-tuning show the tuning results. The results are somewhat cleaner than the random grid results but would lead to the same conclusions. Here, the best candidate was similar to those found by the random grid: `r knn_sfd_best`. 

There are many types of space-filling designs, and it is worth taking the time to take a quick tour of some of them. 

The Latin hypercube design (LHD) is the most popular method for constructing space-filling designs [@Viana2016; @husslage2011space]. These designs have a simple definition. Suppose our hyperparameter space is rectangular and partitioned into smaller (hyper)cubes. From this, a LHD is a set of distinct points where no one dimension has multiple values in any bins^[Other applications may want the points to have properties such as orthogonality, symmetry, etc.]. We desire candidates that fill the space of each parameter and are not close to one another. 

The most basic approach to creating a Latin hypercube design is random sampling [@Mckay2000]. If there are $m$ parameters and we request $s$ candidate values, the parameter space is initially divided into $s^m$ hypercubes of equal size. For each tuning parameter, $s$ regions in its dimension are selected at random, and a value is placed in this box (also at random). This process repeats for each dimension. Suppose there are ten bins for a parameter that ranges between zero and one. If the first design point selects bin two, a random uniform value is created in the range `[0.1 0.2)`.  @fig-lhs-sampled shows three such designs, each generated with different random numbers. 

```{r}
#| label: fig-lhs-sampled
#| echo: false
#| out-width: 90%
#| fig-width: 9
#| fig-height: 3
#| fig-cap: Three replicate Latin hypercube sampling designs of size ten for two parameters. Each design uses different random numbers. The grid lines illustrate the 100 hypercubes used to generate the values.  

make_lhc <- function(ind, m, s) {
  des <- DiceDesign::lhsDesign(s, m, seed = ind + 287)$design
  colnames(des) <- paste("Parameter", letters[1:ncol(des)])
  des <- as_tibble(des)
  des$Replicate <- paste("Replicate", ind)

  des
}

set.seed(202)
map_dfr(1:3, ~ make_lhc(.x, 2, 10)) %>% 
  ggplot(aes(`Parameter a`, `Parameter b`)) + 
  geom_point(cex = 1.5) + 
  coord_equal() +
  geom_hline(yintercept = seq(0, 1, by = 1 / 10), alpha = 1 / 10) +
  geom_vline(xintercept = seq(0, 1, by = 1 / 10), alpha = 1 / 10) +
  lims(x = 0:1, y = 0:1) +
  theme(panel.grid.minor = element_blank()) + 
  facet_wrap(~ Replicate) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
  )
```

Technically, these designs cover the space of each predictor univariately. However, large multivariate regions can be empty. For example, one design only samples combinations along the diagonal. Additional constraints can make the design more consistent with our desires. 

```{r}
#| label: design-crit
#| include: false

rnd_maxmin <-
  encode_set(knn_random %>% select(-grid), pset = knn_prm, as_matrix = TRUE) %>% 
  DiceDesign::mindist() %>% 
  round(2)

mm_maxmin <-
  encode_set(knn_sfd %>% select(-grid), pset = knn_prm, as_matrix = TRUE) %>% 
  DiceDesign::mindist() %>% 
  round(2)

reg_maxmin <-
    encode_set(knn_reg %>% select(-grid), pset = knn_prm, as_matrix = TRUE) %>% 
    DiceDesign::mindist() %>% 
    round(2)
```

For example, we could choose points to maximize the minimum pairwise distances between the candidates. These designs are usually referred to as MaxiMin designs [@pronzato2017minimax]. Comparing the two irregular designs in @fig-knn-grids, the random grid has a maximum minimum distance value of `r rnd_maxmin`. In contrast, the corresponding space-filling design's value (`r mm_maxmin`) is `r round(mm_maxmin/rnd_maxmin, 1)`-fold larger^[Using Euclidean distance.]. The latter design was optimized its coverage, and we can see far less redundancy in the optimized design. 

A similar method, initially proposed by @audze1977new, maximizes a function of the inverse distances between $s$ candidate points: 

$$
criterion = \sum_{i=1}^s \sum_{j=1,\;i\ne j}^s\frac{1}{dist(\theta_i, \theta_j)^2}
$$

@bates2004formulation devised search methods to find optimal designs for this criterion.

Some other space-filling designs of note: 

- Maximum entropy sampling selects points based on assumptions related to the distributions of the tuning parameters and their covariance matrix [@shewry1987maximum;@joseph2015maximum]. 

- Uniform designs [@fang2000uniform;@wang2022design] are designs that optimally allocate points so that they are uniformly distributed in the space. 

While these methods can be constructed generally by sampling random points and using a search method to optimize a specific criterion, there has been scholarship that has pre-optimized designs for some combination of the number of tuning parameters and the requested grid size. 

The advantage of space-filling designs over random designs is that, for smaller designs, the candidates do a better job covering the space and have a low probability of producing redundant points. Also, it is possible to create a space-filling design so that the candidates for each numerical parameter are nearly equally spaced (as was done in @fig-knn-grids). 

Many designs assume that all the tuning parameter values are quantitative. That may not always be the case. For example, KNN can adjust its predictions by considering how far the neighbors are from a new point; more distant points should not have the same influence as close neighbors. A weighting function can be used for this purpose. For example, weights based on an inverse function may produce better results. Equal weighting is often called a “rectangular weighting” function. As a simple workaround, the unique parameter values are repeated as if they were $s$ distinct values when making the design. This allows them to be used with a Latin hypercube design and any space-filling design that uses this approach is not technically optimal. Practically speaking, this is an effective approach for tuning models.  However, there are _sliced_ LHD that can accomplish the same goal [@qian2012sliced;@ba2015optimal].

We recommend space-filling designs since they are more efficient than regular designs. Regular designs have a lot of benefits when using an unfamiliar modeling methodology since you will learn a lot more about the nuances of how the tuning parameters affect one another. 

## Optimization Bias and Nested Resampling {#sec-nested-resampling}

Looking back at the analysis of the space-filling design analysis shown in @fig-knn-irregular-grid-tuning, we found that the numerically best candidate value was `r knn_sfd_best`. The corresponding resampling estimate of the Brier score was `r knn_sfd_brier` with a standard error of `r knn_sfd_brier_se`. If someone were to ask us how well the KNN model performs, we would probably give them these estimates. 

However, there may be an issue in doing so. We are using grid search and resampling to find the best combination of parameters _and_ to estimate the performance. In some situations, this dual use of the model tuning process can introduce **optimization bias** where, to some degree, our performance statistics are optimistic. 

@tibshirani2009bias @boulesteix2009optimal and maybe @tibshirani2018excess




```{r}
#| label: nested-sfd
#| include: false
#| eval: false
knn_sfd_nest_res <-
  knn_wflow %>%
  tune_nested(
    resamples = sim_nested_rs,
    metrics = metric_set(brier_class),
    grid = knn_sfd %>% select(-grid)
  )

knn_sfd_nest_metrics <- collect_metrics(knn_sfd_nest_res)
knn_sfd_nest_brier <- signif(knn_sfd_nest_metrics$mean, 3)
knn_sfd_nest_brier_se <- signif(knn_sfd_nest_metrics$std_err, 3)


selected_configs <- 
  knn_sfd_nest_res %>%
  select(.selected) %>%
  unnest(.selected) %>%
  count(neighbors, dist_power, name = "# Selections") %>%
  mutate(method = "Grid Search")
```

```{r}
#| label: fig-nested-grid-selection
#| echo: false
#| eval: false
#| out-width: 40%
#| fig-width: 4
#| fig-height: 4
#| fig-cap: The KNN space filling design with the frequency that points selected during nested resampling. Empty circles represent points that were never selected.

knn_sfd %>%
  ggplot(aes(neighbors, dist_power)) + 
  geom_point(pch = 1, alpha = 1 / 2) +
  geom_point(
    data = selected_configs,
    aes(size = `# Selections`),
    col = "darkred"
  ) +
  labs(x = neighbors()$label, y = dist_power()$label) +
  coord_fixed(ratio = 50) +
  theme(legend.position = "top")
```

Nested resampling reports that the optimal Brier score for the tuning process is ` knn_sfd_nest_brier` with a standard error of ` knn_sfd_nest_brier_se`. 


## Efficient Computations for Conventional Grid Search {#sec-efficient-grid}

The computational cost of grid search can become large, depending on the resampling strategy and the number of candidates under consideration. In our example, a total of `r format(nrow(sim_rs) * sfd_size, big.mark = ",")` KNN models are evaluated before determining which candidates are most favorable to our data. 

We’ll look at three approaches to making grid search more efficient. One method (“submodels”) happens automatically, another approach (parallel processing) uses software engineering tools, and the last tool (racing) is a statistical solution.


### Submodels {#sec-Submodels}

Some models can exploit the "submodel trick," where a single training model can predict many tuning parameter candidates. For example, the boosted trees (@sec-boosting-cls) create a sequential ensemble of decision trees, each depending on the first. Suppose that a boosting model with 1,000 trees is created. Most implementations allow users to predict from this model for _any ensemble size_. Keeping all other parameters equal, if the grid has multiple values for the ensemble size, we can fit a 1,000 tree model and get very fast predictions for values less than 1,000. For a regular grid, this can effectively drop a dimension of the computations. Depending on the model and grid, the speed-up for using this approach can be well into double digits

Some different models and pre-processors can have this quality, including the glmnet model (@sec-penalized-logistic-regression), partial least squares (@sec-colinear-projections), principal component feature extraction (@sec-linear-feature-extraction), and others. 

Unfortunately, irregular designs cannot exploit the submodel trick since they are not factorial in nature. A hybrid design could be used where a dense sequence of $s_1$ parameter values is created for the tuning parameter associated with submodels and a separate space-filling design of size $s_2$ for the other parameters. These two grids can be crossed so that the space-filling design is replicated for each value of the submodel parameter. This produces $s_1\times s_2$ grid points but, effectively, only $s_2$ models are fit. 


### Parallel Processing {#sec-parallel}

Fortunately, none of these `r format(nrow(sim_rs) * sfd_size, big.mark = ",")` models depend on one another and can be computed separately. Since almost all modern computers have multiple CPUs and GPUs, we can break the computations into different "chunks" of work and execute them simultaneously on other processors (or separate computers entirely). The parallel processing of models can significantly reduce the time it takes to tune using grid search.

when does this become more effective? 

```{r}
#| label: knn-speeds
#| include: false
knn_speedup <- round(max(knn_times$time)/min(knn_times$time), 1)
```

Despite the meager computation costs of tuning the KNN model, there was still a `r knn_speedup`-fold speedup (`r round(max(knn_times$time), 1)`s versus `r round(min(knn_times$time), 1)`s) when using `r max(knn_times$cores)` parallel workers. 
There are some nuances related to parallel processing. It is an excellent idea to parallelize the "longest loop" (literally or figuratively). 

* Parallel individual model fits or resampling. 
* data preprocessing conditional computations
* keep data on one core or shotgun approach See @tmwr [Section 13.5.2](https://www.tmwr.org/grid-search#parallel-processing)

TODO move all timing computations to a separate file for batch processing

### Racing {#sec-racing}

Summed up in @alg-race.

::: {#alg-race}

:::: {.columns}

::: {.column width="10%"}

:::

::: {.column width="80%"}
```pseudocode
#| label: alg-race
#| html-line-number: true
#| html-line-number-punc: ":"

\begin{algorithm}
\begin{algorithmic}
\State $\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$
\State $B$: number of resamples
\State Initial number of resamples $1 \lt B_{min} \lt B$ 
\State $M(\mathfrak{D}^{tr}, B)$: a mapping function to split $\mathfrak{D}^{tr}$ for each of  $B$ iterations.
\State $f()$: model pipeline 
\State $\Theta$: Parameter set ($s \times m$) with candidates $\theta_j$
\For{$j=1$ \To $s$}
  \For{$b=1$ \To $B_{min}$}
    \State Generate $\hat{Q}_{jb} =$ \Call{Resample}{$\mathfrak{D}^{tr}, f(\cdot;\theta_j), M_b(\mathfrak{D}^{tr}, B)$}
  \EndFor
\State Compute $\hat{Q}_{j} = 1/B_{min}\sum_b \hat{Q}_{jb}$. 
\EndFor
\State Eliminate candidates to produce $\Theta^b$ ($s_b \times m$)
  \For{$b = B_{min} + 1$ \To $B$} 
    \For{$j=1$ \To $s$}
      \State Generate $\hat{Q}_{jb} =$ \Call{Resample}{$\mathfrak{D}^{tr}, f(\cdot;\theta_j), M_b(\mathfrak{D}^{tr}, B)$}
      \State Update candidate subset $\Theta^b$
    \Endfor
  \Endfor
\State Determine $\hat{\theta}_{opt}$ that optimizes $\hat{Q}_j^k$.   
\end{algorithmic}
\end{algorithm}
```
:::

::: {.column width="10%"}

:::

::::

Using racing for grid search tuning parameter optimization. 

:::

## Chapter References {.unnumbered}


```{r}
#| label: teardown-cluster
#| include: false

stopCluster(cl)
```

