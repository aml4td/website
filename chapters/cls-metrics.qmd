---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-metrics/"
---

# Characterizing Classification Models {#sec-cls-metrics}

```{r}
#| label: cls-metrics-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(discrim)
library(probably)
library(patchwork)
library(kableExtra)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_transparent())

set_options()
```

```{r}
#| label: data-setup
#| include: false

load("../RData/mushrooms.RData")
```

```{r}
#| label: mushrooms-nb
#| include: false
#| warnnig: false
#| cache: true

cls_mtr <- 
  metric_set(accuracy, bal_accuracy, detection_prevalence, f_meas, j_index, 
             kap, mcc, npv, ppv, precision, recall, sensitivity, specificity, 
             brier_class, mn_log_loss, pr_auc, roc_auc)

nb_res <- 
  naive_Bayes() %>% 
  fit_resamples(
    class ~ ., 
    resamples = shroom_rs,
    metrics = cls_mtr, 
    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
  )

nb_fit <- fit_best(nb_res)
nb_mtr <- collect_metrics(nb_res)
nb_val_pred <- collect_predictions(nb_res)
nb_val_roc <- roc_curve(nb_val_pred, class, .pred_edible)
nb_val_pr <- pr_curve(nb_val_pred, class, .pred_edible)
nb_val_gain <- gain_curve(nb_val_pred, class, .pred_edible)

nb_test_pred <- augment(nb_fit, shroom_test) %>% dplyr::select(class, starts_with(".pred"))
nb_cal_pred <- augment(nb_fit, shroom_cal)   %>% dplyr::select(class, starts_with(".pred"))
```


TODO

-   update data description; maybe talk a little about the example data
-   shinylive for roc/pr/gain
-   mention bootstrapping and permutations
-   desirability and other methods for a combined score?


Nature of classification predictions: hard and soft; information content; LLMs and no probs

introduce prevalence here?


## Example Data

For this chapter, a training set of `r format(nrow(shroom_cal), big.mark = ",")` data points, validation set of `r format(nrow(shroom_val), big.mark = ",")` data points, and a test set of `r format(nrow(shroom_val), big.mark = ",")` data points are used to illustrate various methods.

A naive Bayes model (see @sec-naive-bayes) was used to model these data. 

## Confusion Matrices

```{r}
#| label: cls-metrics
#| include: false

nb_conf_mat <-
  nb_val_pred %>% 
  conf_mat(class, estimate = .pred_class)

conf_mat_table <- 
  as_tibble(as.matrix(nb_conf_mat$table)) %>% 
  pivot_wider(id_cols = c(Prediction), names_from = c(Truth), values_from = n) %>% 
  mutate(Prediction = gsub("class_", "Class ", Prediction)) %>% 
  mutate(blank = "  ") %>% 
  relocate(blank, .after = "Prediction") 
```

::::::: columns
::: {.column width="37%"}
:::

:::: {.column width="26%"}
::: {#tbl-confusion-matrix}
```{r}
#| label: tbl-confusion-matrix
#| echo: false

conf_mat_table %>% 
  kbl(col.names = c("Prediction", "", "edible", "poisonous")) %>% 
  add_header_above(c(" " = 2, "Truth" = 2)) %>% 
  kable_styling(full_width = FALSE)
```

Confusion matrix for validation set predictions from the naive Bayes model.
:::
::::

::: {.column width="37%"}
:::
:::::::



## Assessing Hard Class Predictions

$$
accuracy = \frac{1}{n}\sum_{k=1}^Cn_{kk}
$$

where $n_{ij}$ are the number of counts in the *i* th row and the *j* th column of the confusion matrix, $n$ is the total number of data points, and $C$ is the number of classes. For the naive Bayes model, the accuracy computed on the validation set was `r round(nb_mtr$mean[nb_mtr$.metric == "accuracy"] * 100, 1)`{{< pct >}}.

$$
Kappa = \frac{accuracy - E}{1 - E}
$$

where $E$ is the expected accuracy based on the marginal totals of the confusion matrix. For two classes, this is:

$$
E =\frac{1}{n^{2}}\sum_{k=1}^2n_{k1}n_{k2}
$$

Matthews correlation coefficient [@MATTHEWS1975442; @GORODKIN2004367] (MCC) is another metric that can compare the observed and predicted classes. The equation for *C* classes is:

$$
{\displaystyle {\text{MCC}}={\frac {\displaystyle{\sum_{k=1}^C\sum_{l=1}^C\sum _{m=1}^C(n_{kk}n_{lm}-n_{kl}n_{mk})}}{{\sqrt {{\displaystyle \sum_{k=1}^C\left(\sum _{l=1}^Cn_{kl}\right)\left(\sum _{\substack{k'=1\\k'\neq k}}^C\sum _{l'=1}^Cn_{k'l'}\right)}}}{\sqrt {{\displaystyle\sum_{k=1}^C\left(\sum _{l=1}^Cn_{lk}\right)\left(\sum _{\substack{k'=1\\k'\neq k}}^C\sum _{l'=1}^Cn_{l'k'}\right)}}}}}}
$$

The statistic mirrors the correlation coefficient for numeric data in that it ranges from -1 to +1 and, like Kappa, has a similar interpretation. The MCC estimate, `r signif(nb_mtr$mean[nb_mtr$.metric == "mcc"], 3)`, is very close to the Kappa estimate.

## Metrics for Two Classes


::::::: columns
::: {.column width="30%"}
:::

:::: {.column width="40%"}
::: {#tbl-confusion-matrix-two-class}
```{r}
#| label: tbl-confusion-matrix-two-class
#| echo: false

tibble::tribble(
  ~Prediction, ~event, ~`non-event`,
      "event",   "TP",       "FP",
  "non-event",   "FN",       "TN"
  ) %>% 
  kbl() %>% 
  add_header_above(c(" " = 1, "Truth" = 2)) %>% 
  kable_styling(full_width = FALSE) 
```

A general confusion matrix for outcomes with two classes.
:::
::::

::: {.column width="30%"}
:::
:::::::


$$
Sensitivity = \frac{\text{correctly predicted events}}{\text{total events}} = \frac{TP}{TP + FN}
$$



$$
Specificity = \frac{\text{correctly predicted non-events}}{\text{total non-events}} = \frac{TN}{TN + FP}
$$


$$
J = Sensitivity + Specificity - 1
$$

$$
\begin{align}
PPV&= \frac{Sensitivity \times Prevalence}{(Sensitivity \times Prevalence) + ((1 - Specificity) \times (1 - Prevalence))}  \notag \\
\notag \\
NPV&= \frac{Specificity \times (1 - Prevalence)}{(Prevalence \times (1 - Sensitivity)) + (Specificity \times (1 - Prevalence))} \notag
\end{align}
$$


```{r}
#| label: fig-prevalence
#| echo: false
#| out-width: 65%
#| fig-width: 6
#| fig-height: 4
#| fig-cap: The effect of prevalence on the postive and negative predictive values using the sensitivity and specificity from the naive Bayes model. 

ppv_fn <- function(sens, spec, prev = 1 / 2) {
  ( sens * prev ) / ( (sens * prev) + ( (1 - spec) * (1 - prev) ) )
}
npv_fn <- function(sens, spec, prev = 1 / 2) {
  ( spec * (1 - prev) ) / ( ( (1 - sens) * prev) + ( (spec) * (1 - prev) ) )
}

x <- seq(0, 1, length.out = 100)
 
grid <- 
  crossing(
    sens = nb_mtr$mean[nb_mtr$.metric == "sensitivity"], 
    spec = nb_mtr$mean[nb_mtr$.metric == "specificity"], 
    prev = x
  ) %>% 
  mutate(
    PPV = pmap_dbl(list(sens, spec, prev), ppv_fn),
    NPV = pmap_dbl(list(sens, spec, prev), npv_fn)
  )

grid %>% 
  pivot_longer(cols = c(PPV, NPV), names_to = "metric: ") %>% 
  ggplot(aes(x = prev, y = value, col = `metric: `)) +
  geom_vline(xintercept = mean(nb_cal_pred$class == "edible"), lty = 2) +
  geom_line() + 
  labs(x = "Prevalence", y = NULL) +
  theme(legend.position = "top")
```


$$
PPV = \frac{Sensitivity}{Sensitivity \times (1 - Specificity)} = \frac{TP}{TP+FP}
$$

$$
Precision = \frac{\text{relevant and retrieved}}{\text{total retrieved}}=\frac{TP}{TP + FP}
$$ 

In the special case when the classes are balanced, this is equal to the positive predictive value (but is not otherwise).


$$
Recall = \frac{\text{relevant and retrieved}}{\text{total relevant}} = \frac{TP}{TP + FN}
$$ 

This is the same as sensitivity.

Additionally, the $F_1$ score is the harmonic mean of precision and recall:

$$
F_{1}=2\left(\frac{precision \times recall}{precision + recall}\right)
$$

More generally, the score can be written as

$$
F_{\beta}=(1+\beta_2)\left(\frac{precision \times recall}{(\beta^2 \times precision) + recall}\right)
$$




## Weighted performance metrics

Two class performance metrics are convenient summaries and are generally interpretable. As we saw earlier in this chapter, the metrics of accuracy and Kappa can directly be extended to more that two classes. Because of the desirable characteristics of other two-class performance metrics, conventions have been constructed to extend these metrics. These conventions can be applied to any two-class metric. We will use sensitivity, *S*, to illustrate the conventions.

The first approach, "macro averaging" computes $C$ versions of the statistic using a "one-versus-all" approach. For example, when $C = 3$, $S_1$ is the sensitivity when the first class is considered the event and the second and third classes are pooled into the non-event category. $S_2$ is the sensitivity when the second class is considered as the event, and so on. The macro estimate of sensitivity is computed as their average:

$$
S_{macro} = \frac{1}{C}\sum_{k=1}^C S_k
$$

where the $S_k$ are the one-versus-all metrics.

A naive average of sensitivities is appropriate when the classes are balanced. When the classes are unbalanced, an unweighted average will bias this metric towards the sensitivity of the largest class. To account for class imbalance, *Weighted macro averaging* uses the class frequencies $n_k$ as weights

$$
S_{macro, wts} = \sum_{k=1}^C \frac{S_k}{n_k}
$$

Finally, *micro averaging* is a more complex approach because it aggregates the ingredients that go into the specific calculations. Sensitivity, for example, requires the number of true-positive and false negatives. These are summed and used in the regular computation of sensitivity:

$$
S_{micro} = \frac{{\displaystyle \sum_{k=1}^C TP_k}}{\left( {\displaystyle \sum_{k=1}^C TP_k} \right) + \left( {\displaystyle \sum_{k=1}^C FN_k} \right) }
$$ 

Note that there are many performance metrics that naturally work with three or more classes. There is little point in using these averaging tools for accuracy, Kappa, MCC, ROC AUC, and others.


## Evaluating Probabilistic Predictions


```{r}
#| label: prob-metrics
#| include: false

# ------------------------------------------------------------------------------
# Compute the curves to get specific points

get_closest <- function(x, value) {
  x %>% 
    mutate(diff = abs(.threshold - value)) %>% 
    slice_min(diff, n = 1) %>% 
    dplyr::select(-diff) %>% 
    mutate(
      threshold = format(round(.threshold, 2)),
      threshold = sprintf("%2.2f", .threshold)
      )
}
roc_points <- 
  bind_rows(
    get_closest(nb_val_roc, .25), 
    get_closest(nb_val_roc, .5), 
    get_closest(nb_val_roc, .75),
  )

pr_points <- 
  bind_rows(
    get_closest(nb_val_pr, .25), 
    get_closest(nb_val_pr, .5), 
    get_closest(nb_val_pr, .75),
  )

gain_point <- 
  nb_val_gain %>% 
  mutate(diff = abs(.percent_found - 30)) %>% 
  slice_min(diff, n = 1)

# ------------------------------------------------------------------------------

set.seed(1)
rand_log_loss <- 
  purrr::map_dfr(1:25, ~ mutate(nb_val_pred, class = sample(class)), .id = "rep") %>% 
  group_by(rep) %>%  
  mn_log_loss(class, .pred_edible) %>% 
  summarize(mean = mean(.estimate)) %>% 
  pluck("mean")
```


### Cross-Entropy


$$
loss = -\frac{1}{N}\sum_{i=1}^N\sum_{k=1}^Cy_{ik}\log(\hat{p}_{ik})
$$

### Brier Scores

Another performance metric that uses the predicted class probabilities is the Brier score [@brier1950verification; @bella2013effect]. Suppose that the value $y_{ik}$ is a 0/1 indicator for whether the observed outcome $i$ corresponds to class $k$. Using this the score is:

$$
Brier = \frac{1}{N}\sum_{i=1}^N\sum_{k=1}^C (y_{ik} - \hat{p}_{ik})^2
$$

```{r}
#| label: decomp
#| eval: false
#| include: false
brier_decomp_2cls <- 
  nb_val_pred %>% 
  group_by(.pred_edible) %>% 
  summarize(n = n(), r = sum(class == "edible"), .groups = "drop") %>% 
  mutate(cal = n * (r - .pred_edible)^2, refine = n * r * (1- r)) %>% 
  summarize(cal = mean(cal), refine = mean(refine), .groups = "drop")
```

This is the mean squared error of the predicted class probabilities $\hat{p}_{ik}$. The Brier score can range between zero and two, but for two class problems, it is often rescaled to range between zero and one. A perfect model has a score of zero. If a performance metric achieves its smallest value when the probability estimate is perfect, it is called a *proper scoring rule*. There are theoretical benefits to using such a metric and, in the case of the Brier score, it also measures how well a model is calibrated (see @sec-calibration). In fact, the Brier score can be decomposed into different components, one of which specifically measures calibration. However, this can only be computed when the estimated probability values are not unique. For the naive Bayes model, the Brier score value was `r signif(nb_mtr$mean[nb_mtr$.metric == "brier_class"], 3)`.



### Reciever Operating Characteristic Curves

```{r}
#| label: fig-roc-curve
#| echo: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: Validation set ROC curves for the naive Bayes model The symbols represent different probability thresholds. 

autoplot(nb_val_roc) + 
  geom_point(
    data = roc_points,
    aes(
      y = sensitivity,
      x = 1 - specificity,
      col = threshold,
      pch = threshold
    ),
    cex = 4
  ) +
  scale_shape_manual(values = c(6, 1, 2))
```


### Precision-Recall Curves


```{r}
#| label: fig-pr-curve
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: A precision-recall curve, estimated with the validation set, for the naive Bayes model.

autoplot(nb_val_pr) + 
  geom_point(
    data = pr_points,
    aes(
      y = precision,
      x = recall,
      col = threshold,
      pch = threshold
    ),
    cex = 4
  ) +
  scale_shape_manual(values = c(6, 1, 2)) +
  coord_obs_pred()
```

### Gain Charts

```{r}
#| label: fig-gain-curve
#| echo: false
#| out-width: 50%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: "A gain curve based on the validation set. The dotted linear indicates the percentage of samples to query in order to capture 30{{< pct >}} of the samples from the first class."

nb_val_gain %>%
  autoplot() +
  geom_segment(
    data = gain_point,
    aes(
      x = .percent_tested, xend = .percent_tested,
      y = 0, yend = .percent_found
    ),
    col = "purple", linewidth = 1, lty = 3
  ) +
  geom_segment(
    data = gain_point,
    aes(
      x = 0, xend = .percent_tested,
      y = .percent_found, yend = .percent_found
    ),
    col = "purple", linewidth = 1, lty = 3
  ) +
  coord_obs_pred()
```




## Calibration {#sec-calibration}


```{r}
#| label: fig-cal-curves
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: Calibration curves for naive Bayes model. The colored regions are individual 90{{< pct >}} confidence intervals. 

cal_plot_1 <- 
  nb_val_pred %>% 
  cal_plot_windowed(class, .pred_edible, window_size = 0.2, step_size = 0.025) + 
  ggtitle("(a) moving 20% bins")

cal_plot_2 <- 
  nb_val_pred %>% 
  cal_plot_logistic(class, .pred_edible) + 
  ggtitle("(b) logistic regression")

cal_plot_1 + cal_plot_2
```

The downside to this binning approach is that, for small data sets, the results can have very large variability or there may not be enough data within each bin to make accurate judgements regarding calibration. One alternative is to use moving windows. For example, for fixed bin width of 0.20, compute the required statistics and then move the bin by a small amount (say 0.02). This would result in larger bins capturing more data that would slide across the range of probability values.

Another alternative is to fit a model to the data. Logistic regression is a relatively simple classification model for binary outcomes and is discussed in @sec-logistic-regression. We can fit this model using our observed outcomes and use the predicted class probabilities ($\hat{p}_i$) as the predictor. If the probabilities are well-calibrated, the model should show a linear relationship. One other feature that can be used in this diagnostic is to model the predictor with a smoothing spline. This enables the logistic model to reflect when the calibration is very poor. @fig-cal-curves (b) illustrates the same pattern with a much smaller degree of uncertainty since the model coefficients utilize all of the data.

```{r}
#| label: logistic_cal
#| include: false

logistic_cal <- nb_cal_pred %>% cal_estimate_logistic(truth = class, smooth = TRUE)
pred_cal <- nb_val_pred %>% cal_apply(logistic_cal)

num_logistic_vals <-  nb_cal_pred %>% distinct(.pred_edible) %>% nrow()
iso_cal <- nb_cal_pred %>% cal_estimate_isotonic(truth = class)
pred_cal_iso <- nb_val_pred %>% cal_apply(iso_cal)
num_isotonic_vals <- vctrs::vec_unique_count(pred_cal_iso$.pred_edible)

iso_boot_cal <- nb_cal_pred %>% cal_estimate_isotonic_boot(truth = class, times = 1000)
pred_cal_iso_boot <- nb_val_pred %>% cal_apply(iso_boot_cal)
num_isotonic_boot_vals <- vctrs::vec_unique_count(pred_cal_iso_boot$.pred_edible)
```

What can be done with poorly calibrated predictions? There are approaches for post-processing the predicted probabilities to have higher fidelity to the observed data. The customary method is to use a logistic regression fit, as seen in @fig-cal-curves (b), to re-calibrate the predictions. First a suitable logistic regression model is created in the same manner as the one used for diagnosis. Recall that the original predictions are used as predictors of the model. To re-calibrate a set of predictions, the original values are pushed through the model and the resulting logistic regression values are used as the new probability predictions.

@platt1999probabilistic  proposed using logistic regression as a method of converting any quantitative score into a probability-like value. *Platt scaling* also adjusts the class indicators from binary 0/1 values to values slightly different from those values.

This brings up questions about which data should be used to fit the calibration model and which to evaluate its efficacy. Weâ€™ve previously mentioned that using simple re-predictions of the training set is a very bad idea and that is especially true here. If the model was resampled, the out-of-sample predictions can be used to fit the calibration model. In our example, a single validation set was used. Unfortunately, if all of the held-out data were used for re-calibration, the only unused data left would be the test set (which should not be compromised).

TODO change below

Since there are `r format(nrow(pred_cal), big.mark = ",")` samples in the validation set, we will randomly select half to fit the calibration model and will use the other half to evaluate how well the calibration process worked. @fig-re-cal-curves shows the results where panel (a) uses a sliding window to assess the smaller data set using a window size of 20{{< pct >}} that moves in 5{{< pct >}} increments. From these figures, the class probabilities appear to have been improved. Note that these values now encompass the entire probability range.

```{r}
#| label: fig-re-cal-curves
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: Calibration curves for naive Bayes model _after_ re-calibration using logistic regression in conjunction with a calibration set.

cal_plot_3 <- 
  pred_cal %>% 
  cal_plot_windowed(class, .pred_edible, window_size = 0.2, step_size = 0.025) + 
  ggtitle("(a) moving 20% bins")

cal_plot_4 <- 
  pred_cal %>% 
  cal_plot_logistic(class, .pred_edible) + 
  ggtitle("(b) logistic regression")

cal_plot_3 + cal_plot_4
```

```{r}
#| label: re-cal
#| echo: false
re_class_stats <- pred_cal %>% cls_mtr(class, estimate = .pred_class, .pred_edible)
```

Using this approach to calibration, the new probability estimates have a Brier score of `r signif(re_class_stats$.estimate[re_class_stats$.metric == "brier_class"], 3)`. While this is smaller than the value (`r signif(nb_mtr$mean[nb_mtr$.metric == "brier_class"], 3)`) prior to calibration, the exact same data were not used. This makes it difficult to say that the improvement is real.

There other approaches for re-calibrating. First, is isotonic regression [@isotonic]. This tool fits a model that will generate monotonic, non-decreasing, predictions: $\hat{y}_i \le \hat{y}_{i+ 1}$ when $x_i \le x_{i+ 1}$. There are multiple methods to create such models but the most straightforward approach is the pooled-adjacent violators algorithm (PAVA) of @pava. This method adjusts a sequence of data points that are not monotonically increasing to be so. For example, @tbl-isotonic shows an example where the rows are ordered by their model's predicted probabilities. One column shows a binary indicator for class `A`. The first three rows have the same outcome (`B`) but the fourth row is actually class `A`. The PAVA algorithm then takes the averages of the binary indicators that bracket the discordant value and all of these probability estimates are given a value of 1/3. From the table, it is clear that there are three possible probability estimates after isotonic regression is used.

::::::: columns
::: {.column width="25%"}
:::

:::: {.column width="50%"}
::: {#tbl-isotonic}
```{r}
#| label: tbl-isotonic
#| echo: false

iso_example <- 
  tibble::tribble(
    ~Truth, ~Binary, ~Model, ~Isotonic,
    "B",       0,      0,         0,
    "B",       0,   0.01,         0,
    "B",       0,    0.1,         0,
    "A",       1,    0.2,      0.33,
    "B",       0,    0.4,      0.33,
    "B",       0,    0.5,      0.33,
    "A",       1,    0.6,         1,
    "A",       1,    0.7,         1,
    "A",       1,    0.8,         1,
    "A",       1,    0.9,         1
  )

iso_example %>% 
  kbl() %>% 
  add_header_above(c(" " = 2, "Estimated Probability of Class A" = 2)) %>% 
  kable_styling(full_width = FALSE)  
```


An example of calculations for calibration via isotonic regression.
:::
::::

::: {.column width="25%"}
:::
:::::::

The nice feature of this tool is that it is nonparametric and makes almost no distributional assumptions. However, one downside is that there are a small number of unique re-calibrated probability estimates. For this reason, it is advised to use isotonic regression for large data sets. For example, for the naive Bayes data used for calibration, there were originally `r num_logistic_vals` unique probability estimates while, after isotonic regression is applied, there are only `r num_isotonic_vals` possible values.

```{r}
#| label: fig-re-cal-iso-curves
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: Calibration curves for naive Bayes model _after_ re-calibration using isoreg regression.

cal_plot_5 <- 
  pred_cal_iso %>% 
  cal_plot_windowed(class, .pred_edible, window_size = 0.2, step_size = 0.025) + 
  ggtitle("(a) moving 20% bins")

cal_plot_6 <- 
  pred_cal_iso_boot %>% 
  cal_plot_windowed(class, .pred_edible, window_size = 0.2, step_size = 0.025) + 
  ggtitle("(b) moving 20% bins, bootstrapped")

cal_plot_5 + cal_plot_6
```

There are many other tools in the calibration toolbox, such as beta calibration [@betacal]. There are also methods for calibrating classification models with three or more classes [@NIPS2001abdbeb4d; @johansson21a; @NEURIPS2021bbc92a64].

Finally, it is important to re-compute performance using the re-calibrated predictions. Also note that, since the class probabilities have been altered, any hard predictions should be recomputed since some values may have moved across the chosen probability threshold. Using the set of `r nrow(pred_cal)` data points that have been re-calibrated, the re-estimated sensitivity and specificity were `r round(re_class_stats$.estimate[re_class_stats$.metric == "sensitivity"] * 100, 1)`{{< pct >}} and `r round(re_class_stats$.estimate[re_class_stats$.metric == "specificity"] * 100, 1)`{{< pct >}}, respectively and the area under the ROC curve was `r signif(re_class_stats$.estimate[re_class_stats$.metric == "roc_auc"], 3)`


## Ordered Categories

As previously mentioned, our outcome classes can have an inherent ordering.

examples - wordle

Since there is an ordering, it might be reasonable to think that misclassification severity relates to how "far" a predicted class is from its true class. For example, risk analysis systems, such as FMEA, involve enumerating different ways that a thing can fail and the corresponding consequences. Suppose that we have an ordinal scale for risk consisting of "low," "moderate," "severe," and "catastrophic". It is reasonable to think that predicting that the risk is moderate when it is actually catastrophic is very bad. Predicting a moderate risk to be low may not be as dire.

There are a few metrics that use hard classification predictions. We previously mentioned the Kappa statistic. To penalize misclassification errors based on their distance to the true classification, [@Agresti2002vi] developed the *weighted Kappa* statistic where the errors' weights depend on their position differences. Let's look at a four-class example from @apm where computing jobs run on a server executed very fast (VF), fast (F), moderately long (M), or a long time (L).

A linear weighting for the Kappa statistic is shown in @tbl-confusion-matrix-costs. There are bands of diagonal weights that are either 0, 1, 2, or 3. Weight Kappa statistics enable "more incorrect" errors more than others.

```{r}
#| label: ordered-stats
#| echo: false
#| results: hide
kap_linear <- function(data, truth, estimate, na_rm = TRUE, ...) {
  kap(
    data = data,
    truth = !!rlang::enquo(truth),
    estimate = !!rlang::enquo(estimate),
    weighting = "linear",
    na_rm = na_rm,
    ...
  ) %>% 
    mutate(.estimator = "linear")
}
kap_linear <- new_class_metric(kap_linear, "maximize")

kap_quadratic <- function(data, truth, estimate, na_rm = TRUE, ...) {
  kap(
    data = data,
    truth = !!rlang::enquo(truth),
    estimate = !!rlang::enquo(estimate),
    weighting = "quadratic",
    na_rm = na_rm,
    ...
  ) %>% 
    mutate(.estimator = "quadratic")
}
kap_quadratic <- new_class_metric(kap_quadratic, "maximize")

kap_set <- metric_set(kap, kap_linear, kap_quadratic)
kap_res <- 
  hpc_cv %>% 
  kap_set(truth = obs, estimate = pred)

# ------------------------------------------------------------------------------

mse_ordered <- function(dat) {
  ord_diffs <- as.integer(dat$obs) - as.integer(dat$pred)
  mean(ord_diffs^2)
}

set.seed(579)
hpc_perm <- permutations(hpc_cv, times = 50, permute = obs)

mse_perm <- 
  hpc_perm %>% 
  mutate(null = map_dbl(splits, ~ mse_ordered(analysis(.x))))

mean(mse_perm$null)

# ------------------------------------------------------------------------------

hpc_costs_vals <- tribble(
  ~estimate, ~truth, ~cost,
  "VF",      "VF",    0,
  "VF",      "F",     1,
  "VF",      "M",     5,
  "VF",      "L",    10,
  "F",       "VF",    1,
  "F",       "F",     0,
  "F",       "M",     5,
  "F",       "L",     5,
  "M",       "VF",    1,
  "M",       "F",     1,
  "M",       "M",     0,
  "M",       "L",     1,
  "L",       "VF",    1,
  "L",       "F",     1,
  "L",       "M",     1,
  "L",       "L",     0
)

hpc_costs_tbl <- 
  hpc_costs_vals %>% 
  mutate(truth = paste0("cost_", truth)) %>% 
  pivot_wider(id_cols = c(estimate), names_from = c(truth), values_from = cost) %>% 
  mutate(blank3 = "  ") %>% 
  dplyr::select(-estimate) %>% 
  relocate(blank3)
  

hpc_costs <- 
  hpc_cv %>%
  classification_cost(obs, VF:L, costs = hpc_costs_vals)

hpc_costs_perm <- 
  hpc_perm %>% 
  mutate(null = map_dbl(
    splits, 
    ~ classification_cost(analysis(.x), truth = obs, VF:L, costs = hpc_costs_vals)$.estimate)
  )
```

::::::: columns
::: {.column width="5%"}
:::

:::: {.column width="90%"}
::: {#tbl-confusion-matrix-costs}
```{r}
#| label: tbl-confusion-costs
#| echo: false

lvls <- levels(hpc_cv$obs)

hpc_conf_mat <-
  hpc_cv %>% 
  conf_mat(obs, estimate = pred) %>% 
  pluck("table") %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  pivot_wider(id_cols = c(Prediction), names_from = c(Truth), values_from = n) %>% 
  mutate(blank = "  ") %>% 
  relocate(blank, .after = "Prediction") 

hpc_errors <- toeplitz(1:4)
colnames(hpc_errors) <- rownames(hpc_errors) <- levels(hpc_cv$obs)
hpc_errors <- 
  hpc_errors %>%
  as.table() %>% 
  as.data.frame() %>% 
  set_names(c("Prediction", "Truth", "n")) %>% 
  mutate(Truth = paste0("wts_", Truth)) %>% 
  pivot_wider(id_cols = c(Prediction), names_from = c(Truth), values_from = n) %>% 
  mutate(blank2 = "  ") %>% 
  dplyr::select(-Prediction) %>% 
  relocate(blank2)
  
bind_cols(hpc_conf_mat, hpc_errors, hpc_costs_tbl <- 
  hpc_costs_vals %>% 
  mutate(truth = paste0("cost_", truth)) %>% 
  pivot_wider(id_cols = c(estimate), names_from = c(truth), values_from = cost) %>% 
  mutate(blank3 = "  ") %>% 
  dplyr::select(-estimate) %>% 
  relocate(blank3)) %>% 
  kbl(col.names = c("Prediction", "", lvls, "", lvls, "", lvls)) %>% 
  add_header_above(c(" " = 2, "Truth" = 4, " " = 1, "Linear Weights" = 4, " " = 1, "Custom Costs" = 4)) %>% 
  kable_styling(full_width = FALSE)
```

A four class ordinal outcome with a confusion matrix, a linear set of weights for each potiental error, and a custom cost scheme used by @apm.
:::
::::

::: {.column width="5%"}
:::
:::::::

We computed Kappa under three scenareos: equal weights (`r round(kap_res$.estimate[kap_res$.estimator == "multiclass"], 3)`), linear scores (`r round(kap_res$.estimate[kap_res$.estimator == "linear"], 3)`), and quadratic scores (`r round(kap_res$.estimate[kap_res$.estimator == "quadratic"], 3)`).

The mean squared error for these data was `r signif(sqrt(mse_ordered(hpc_cv)), 3)`. How bad is this? If we repeatedly permute the outcome classes and measure the RMSE we find that the worst case-scenario is about `r signif(sqrt(mean(mse_perm$null)), 3)`.

For the custom cost analysis, the average cost was `r signif(hpc_costs$.estimate, 3)` with a worst-case value of about `r signif(mean(hpc_costs_perm$null), 3)`.

## Multi-objective Assessments

Pareto optimal; desirability functions. J statistic

## Chapter References {.unnumbered}

```{r}
#| label: teardown-cluster
#| include: false

# stopCluster(cl)
```

