---
knitr:
  opts_chunk:
    cache.path: "../_cache/cls-metrics/"
---

# Characterizing Classification Models {#sec-cls-metrics}

```{r}
#| label: cls-metrics-setup
#| include: false

source("../R/_common.R")

# ------------------------------------------------------------------------------

library(tidymodels)
library(discrim)
library(probably)
library(patchwork)
library(kableExtra)

# ------------------------------------------------------------------------------
# set options

tidymodels_prefer()
theme_set(theme_transparent())

set_options()
```

```{r}
#| label: data-setup
#| include: false

load("../RData/mushrooms.RData")
```

```{r}
#| label: mushrooms-nb
#| include: false
#| warnnig: false
#| cache: true

shroom_n <- nrow(shroom_train) + nrow(shroom_val) + nrow(shroom_cal) + nrow(shroom_test)
shroom_xtab <- round(table(shroom_train$class)/nrow(shroom_train)* 100, 1)

cls_mtr <- 
  metric_set(accuracy, bal_accuracy, detection_prevalence, f_meas, j_index, 
             kap, mcc, npv, ppv, precision, recall, sensitivity, specificity, 
             brier_class, mn_log_loss, pr_auc, roc_auc)

nb_res <- 
  naive_Bayes() %>% 
  fit_resamples(
    class ~ ., 
    resamples = shroom_rs,
    metrics = cls_mtr, 
    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
  )

nb_fit <- fit_best(nb_res)
nb_mtr <- collect_metrics(nb_res)
nb_val_pred <- collect_predictions(nb_res)
nb_val_roc <- roc_curve(nb_val_pred, class, .pred_edible)
nb_val_pr <- pr_curve(nb_val_pred, class, .pred_edible)
nb_val_gain <- gain_curve(nb_val_pred, class, .pred_edible)

nb_test_pred <- augment(nb_fit, shroom_test) %>% dplyr::select(class, starts_with(".pred"))
nb_cal_pred <- augment(nb_fit, shroom_cal)   %>% dplyr::select(class, starts_with(".pred"))
```

Classification models are used to predict outcomes that are categorical in nature. They primarily produce two types of predictions: 

- *Hard class predictions* are the most likely category in the form of a single qualitative value. 
- *Soft class predictions* are a set of numeric scores, one per category, reflecting the likelihood of each. We’ll focus solely on probability estimates of each class for soft predictions. 

Hard class predictions are almost always created by finding the category corresponding to the largest value of the soft predictions^[TODO about generative models]. Below, we will single out the case where two possible categories exist and consider an alternate method of converting class probabilities into hard class predictions (via thresholding). 

Many classification models use probabilistic assumptions to estimate class probabilities. For example, logistic regression assumes that the outcome follows a Bernoulli distribution. From there, parameter estimates can be determined using maximum likelihood or Bayesian estimation, which then defines the probability estimates. Other algorithms, such as tree-based models, make no assumptions regarding the data. 

::: {.important-box}
It is very important to carefully choose which metric (or metrics) to optimize a model. The metric defines what your model should consider important, and it will almost always believe that your chosen metric is the only important criterion. This can often lead to unintended consequences. 
:::

This chapter describes myriad approaches for quantifying how well a classification model functions via performance metrics. 


## Choosing Appropriate Metrics

Let’s consider a few critical aspects of classification metrics.  To further this discussion, we’ll focus on specific metrics that are computed with hard or soft class predictions. For class probability predictions, we’ve seen the Brier score used repeatedly in the previous chapters. As a reminder, this statistic is similar to the sum of squared errors in regression: for each class, the estimated class probabilities are contrasted with their corresponding binary indicators. These differences are squared and averaged. Let’s juxtapose the Brier score with the most simple metric for hard predictions: the overall accuracy of the model (i.e., the proportion of samples that were correctly predicted). 

To start, should our metric be based on hard or soft predictions? This, of course, depends on how the model will be applied to data and how users will consume its predictions. However, a strong argument can be made that performance metrics that use soft predictions should be favored over those using hard predictions. Information theory uses the idea of _self-information_: if the value of a variable has some probability $\pi$ of occurring, we can measure the amount of information using $I = -log_2(\pi)$ where the units are called bits. 

Since accuracy is the average of binary indicators of “correctness.” Since there are two possible values, the information is $-log_2(1/2)$ = 1.0 bit. 

Suppose we are computing the Brier score, and for simplicity, the probability estimates can be measured to the second decimal place. In this case, there are 101 possible values, so the data used in the Brier score contain $-log_2(1/101)$ bits (about `r round(-log2(1/101), 1)`). 

This simple analysis implies that metrics that use probabilities will contain much more information (literally and figuratively) than class predictions. As a result, probability-based metrics will be more capable of discriminating between models. 

Let’s also consider two examples of how metrics can lead to unintended consequences. 

First, accuracy is very sensitive to class imbalances (where one or more classes do not occur very often). Suppose that we have two classes, and one only occurs 1% of the time (called the “minority class” in this context). When accuracy is used for these data, an inferior model can easily achieve 99% accuracy simply by always predicting the majority class. If accuracy is all that we care about, then we can satisfy the literal goal, but our model will probably fail to fulfill the intention of the model. 

The second example contrasts the concepts of _discrimination_ and _calibration_. 

- A model can discriminate the classes well when the probability estimates of the true class are larger than the probabilities for the others. In other words, being good at discrimination means that you can, to some degree, successfully discern the true classes 

- A classification model is well-calibrated when the estimated probability predictions are close to their real theoretical counterparts.   

Some metrics favor one of these characteristics over the other. For example, neural networks for classification use binary indicators for their outcome data and try to produce models so that their predictions are near these values. To ensure that the results are “probability like,” meaning that they add up to one and are on `[0, 1]`, the softmax transformation  [@Bridle1990] is applied to the raw predictions:

$$
\hat{p}_k^* = \frac{e^{\hat{y}_k}}{{\displaystyle  \sum_{l=1}^Ce^{\hat{y}_l}}}
$$

```{r}
#| label: discrim-vs-cal
#| include: false
sim_n <- 500
set.seed(107)
non_cal_example <- 
  tibble(
    .prob_event = c(rnorm(sim_n, mean = .55, sd = 0.025),
                    rnorm(sim_n, mean = .45, sd = 0.025)),
    class = factor(rep(c("event", "no-event"), each = sim_n))
  ) %>% 
  mutate(
    .pred_class = ifelse(.prob_event >= 1/2, "event", "no-event"),
    .pred_class = factor(.pred_class, levels = c("event", "no-event"))
  )
event_prob_rng <- range(non_cal_example$.prob_event) * 100
non_cal_acc <- accuracy_vec(non_cal_example$class, non_cal_example$.pred_class) * 100
non_cal_brier <- brier_class_vec(non_cal_example$class, non_cal_example$.prob_event)
```

While the softmax method ensures they have the correction range, it does not guarantee that a model with good discrimination is well-calibrated. @fig-discrim-vs-cal shows a hypothetical example that illustrates what can occur when the softmax method is used. The distributions of the two classes have a good degree of separation, indicating that the model can discriminate the true classes. The observed accuracy is large (`r round(non_cal_acc, 1)`%) but the predicted probabilities only range from `r round(event_prob_rng[1], 1)`% to `r round(event_prob_rng[2], 1)`%. It is improbable that the true probability values have such a narrow range, and we can assume that the calibration is poor here. The observed Brier score is large  (`r round(non_cal_brier, 3)`), supporting this assumption.

```{r}
#| label: fig-discrim-vs-cal
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 4.25
#| fig-cap: An example of a model that has good discrimination but poor calibration. 

p_ex_dens <- 
  non_cal_example %>% 
  ggplot(aes(.prob_event, col = class, fill = class)) + 
  stat_density(alpha = 1 / 4, trim = TRUE, adjust = 1.2) + 
  lims(x = 0:1) +
  labs(x = "Predicted Probability of Event") +
  theme(legend.position = "top") +
  scale_color_brewer(palette = "Accent") +
  scale_fill_brewer(palette = "Accent")
p_ex_dens
```

In some cases, the pattern shown in @fig-discrim-vs-cal may be acceptable. For example, if the goal of the model is to identify “events” (such as the most profitable consumers in a database), the model’s calibration may not be important; your main interest is in ranking the rows in a data set. However, we suggest that probability predictions are far more informative than class predictions, and thus, good calibration is a key feature of most classification models. 

## Example Data

To demonstrate classification metrics, we'll primarily use a data set with mushroom characteristics to predict whether specific mushrooms are edible or poisonous. We'll use these data in conjunction with a naive Bayes classification model (see @sec-naive-bayes)

The inital data pool contained `r format(shroom_n, big.mark = ",")` data points. The main partition of the data was into training ($n_{tr}$ = `r format(nrow(shroom_train), big.mark = ",")`), validation ($n_{val}$ = `r format(nrow(shroom_val), big.mark = ",")`), and test ($n_{te}$ = `r format(nrow(shroom_test), big.mark = ",")`). Additionally, a _calibration_ set of $n_{cal}$ = `r format(nrow(shroom_cal), big.mark = ",")` mushrooms were allocated for the processes described in @sec-calibration. 

The predictors of the outcome were:

:::: {.columns}

::: {.column width="50%"}
- does bruise or bleed (yes or no)
- cap color (categorical, 12 values)
- cap diameter (numeric)
- cap shape (categorical, 7 values)
- cap surface (categorical, 8 values)
- gill attachment (categorical, 8 values, contains unknowns)
- gill color (categorical, 12 values)
- gill spacing (close, distant, or none)
- habitat (categorical, 8 values)
- has ring (yes or no)
:::

::: {.column width="50%"}

- ring type (categorical, 9 values, contains unknowns)
- season (categorical, 4 values)
- spore print color (categorical, 7 values)
- stem color (categorical, 13 values)
- stem height (numeric)
- stem root (categorical, 4 values)
- stem surface (categorical, 8 values)
- stem width (numeric)
- veil color (categorical, 7 values)
- partial veil (partial or unknown)
:::

::::

The outcome data has relatively balanced classes with `r shroom_xtab["edible"]`% of the mushrooms being edible. 

No preprocessing was applied to the data for this model. Naive Bayes models do not require the categorical predictors to be decomposed into binary indicator columns or transformations of the numeric predictors. As shown above, the predictors with missing data were encoded as a level in one of the possible categories. 

We'll fit the model to the training set and, for the most part, use the validation set to demonstrate the various metrics. @fig-naive-bayes-probs shows the distirbution of the probability of the event (edible) for the validation set. 

```{r}
#| label: fig-naive-bayes-probs
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 4.25
#| fig-cap: The distribution of the validation set class probability estimates for a naive Bayes model, split by the true class of the mushroom. . 

nb_val_pred %>% 
  ggplot(aes(.pred_edible)) + 
  geom_vline(xintercept = 1 / 2, col = "blue", lty = 3) +
  geom_histogram(col = "white", binwidth = 0.02) + 
  facet_wrap(~ class, ncol = 1, labeller = "label_both") + 
  labs(x = "Probability of Being Edible")
```

## Assessing Hard Class Predictions {#sec-cls-hard-metrics}

One of the most fundamental ways to inspect and understand how well hard class prediction performs is the confusion matrix, simply a cross-tabulation of the observed and predicted predictions. From this, we can conceptualize where the model under-performs. An excellent model has the largest counts along the diagonal where the observed and predicted classes are the same. For the naive Bayes model, @tbl-confusion-matrix shows the confusion matrix for the mushroom data’s validation set. 

```{r}
#| label: cls-metrics
#| include: false

nb_conf_mat <-
  nb_val_pred %>% 
  conf_mat(class, estimate = .pred_class)

conf_mat_table <- 
  as_tibble(as.matrix(nb_conf_mat$table)) %>% 
  pivot_wider(id_cols = c(Prediction), names_from = c(Truth), values_from = n) %>% 
  mutate(Prediction = gsub("class_", "Class ", Prediction)) %>% 
  mutate(blank = "  ") %>% 
  relocate(blank, .after = "Prediction") 

acc_int <- binom.test(sum(diag(nb_conf_mat$table)), sum(nb_conf_mat$table), conf.level = 0.9)
```

::::::: columns
::: {.column width="37%"}
:::

:::: {.column width="26%"}
::: {#tbl-confusion-matrix}
```{r}
#| label: tbl-confusion-matrix
#| echo: false

conf_mat_table %>% 
  kbl(
    col.names = c("Prediction", "", "edible", "poisonous"),
    format.args = list(big.mark = ',')
  ) %>%
  add_header_above(c(" " = 2, "Truth" = 2)) %>% 
  kable_styling(full_width = FALSE)
```

Confusion matrix for validation set predictions from the naive Bayes model.
:::
::::

::: {.column width="37%"}
:::
:::::::

It may be helpful to also present the confusion matrix in terms of percentages. For example, we might think that the model is worse at predicting the poisonous than edible mushrooms since the upper right cell has a larger count (`r nb_conf_mat$table["edible", "poisonous"]`) than the lower left cell (`r nb_conf_mat$table["poisonous", "edible"]`). However, this difference is partially explained by their being more poisonous mushrooms in the right column than in the left column of edible samples. It can also be helpful to visualize the matrix as a heatmap where darker colors represent larger counts, especially when there are a large number of possible classes. 

We can also view the confusion matrix as a “sufficient statistic” since it has all the information required to compute the metrics based on hard class predictions. For example, the accuracy computed from our validation set was `r round(nb_mtr$mean[nb_mtr$.metric == "accuracy"] * 100, 1)`%, which is 

$$
accuracy = \frac{1}{n}\sum_{k=1}^Cn_{kk}
$$

where $n_{ij}$ are the number of counts in the *i* th row and the *j* th column of the confusion matrix, $n$ is the total number of data points, and $C$ is the number of classes. 


Many of the metrics in this section are in the form of a proportion. For each, confidence intervals can be computed using typical methods for binomial proportions. We suggest using asymmetric intervals, such as the “exact” technique in @clopper1934use. See @newcombe1998two and @meeker2017statistical for discussions of different methods for computing intervals on proportions. For the validation set results, the 90% confidence interval was (`r round(acc_int$conf.int[1]*100, 1)`%, `r round(acc_int$conf.int[2]*100, 1)`%).

As previously mentioned, accuracy is misleading when there is a class imbalance. There are a few ways to correct for imbalances, most notably the measure of agreement called Kappa [@Cohen1960]. This normalizes the accuracy by a measure of expected accuracy based on chance agreement ($E$): 

$$
Kappa = \frac{accuracy - E}{1 - E}
$$

where $E$ is the expected accuracy based on the marginal totals of the confusion matrix. For two classes, this is:

$$
E =\frac{1}{n^{2}}\sum_{k=1}^2n_{k1}n_{k2}
$$

Much like correlation statistics, the range of Kappa is $\pm$ 1, with values of one corresponding to perfect agreement and zero meaning negative agreement. While negative values are possible, they are uncommon. Interpreting Kappa can be difficult since no well-defined values indicate that the results are good enough. This is compounded by the fact that the values can change with different prevalences of the classes [@thompson1988reappraisal;@guggenmoos1996meaning]. For this reason, it isn’t easy to compare values between data sets or to have a single standard. There are also differences in range when the outcome data has different numbers of classes; a Kappa value of 0.5 has a different meaning when $C = 2$ than with some other number of possible classes. 

The Kappa statistic can also incorporate different weights for different types of errors [@cohen1968weighted]. This is discussed in more detail in @sec-ordered-categories below.

Using the data from @tbl-confusion-matrix, the Kappa value was `r round(nb_mtr$mean[nb_mtr$.metric == "kap"], 3)`. While subjective, this value probably indicates satisfactory agreement between the true classes and the predictions. 

Matthews correlation coefficient [@MATTHEWS1975442; @GORODKIN2004367] (MCC) is another metric that can compare the observed and predicted classes. The equation for *C* classes is:

$$
{\displaystyle {\text{MCC}}={\frac {\displaystyle{\sum_{k=1}^C\sum_{l=1}^C\sum _{m=1}^C(n_{kk}n_{lm}-n_{kl}n_{mk})}}{{\sqrt {{\displaystyle \sum_{k=1}^C\left(\sum _{l=1}^Cn_{kl}\right)\left(\sum _{\substack{k'=1\\k'\neq k}}^C\sum _{l'=1}^Cn_{k'l'}\right)}}}{\sqrt {{\displaystyle\sum_{k=1}^C\left(\sum _{l=1}^Cn_{lk}\right)\left(\sum _{\substack{k'=1\\k'\neq k}}^C\sum _{l'=1}^Cn_{l'k'}\right)}}}}}}
$$

The statistic mirrors the correlation coefficient for numeric data in that it ranges from -1 to +1 and, like Kappa, has a similar interpretation. The MCC estimate, `r signif(nb_mtr$mean[nb_mtr$.metric == "mcc"], 3)`, is very close to the Kappa estimate.


## Metrics for Two Classes {#sec-cls-two-classes}


::::::: columns
::: {.column width="30%"}
:::

:::: {.column width="40%"}
::: {#tbl-confusion-matrix-two-class}
```{r}
#| label: tbl-confusion-matrix-two-class
#| echo: false

tibble::tribble(
  ~Prediction, ~event, ~`non-event`,
  "event",   "TP",         "FP",
  "non-event",   "FN",         "TN"
) %>% 
  kbl() %>% 
  add_header_above(c(" " = 1, "Truth" = 2)) %>% 
  kable_styling(full_width = FALSE) 
```

A general confusion matrix for outcomes with two classes.
:::
::::

::: {.column width="30%"}
:::
:::::::


$$
Sens = \frac{\text{correctly predicted events}}{\text{total events}} = \frac{TP}{TP + FN}
$$



$$
Spec = \frac{\text{correctly predicted non-events}}{\text{total non-events}} = \frac{TN}{TN + FP}
$$



$$
\begin{align}
PPV&= \frac{Sens \times Prev}{(Sens \times Prev) + \left((1 - Spec) \times (1 - Prev)\right)}  \notag \\
\notag \\
NPV&= \frac{Spec \times (1 - Prev)}{\left(Prev \times (1 - Sens)\right) + \left(Spec \times (1 - Prev)\right)} \notag
\end{align}
$$


::: {#fig-prevalence}

::: {.figure-content}


```{shinylive-r}
#| label: fig-prevalence
#| viewerHeight: 550
#| standalone: true

library(shiny)
library(dplyr)
library(purrr)
library(ggplot2)
library(patchwork)
library(bslib)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")

# ------------------------------------------------------------------------------

ui <- fluidPage(
  theme = grid_theme,
  layout_columns(
    fill = FALSE,
    col_widths = breakpoints(sm = c(-3, 6, -3)),
    column(
      width = 10,
      sliderInput(
        inputId = "prev",
        label = "Prevelance",
        min = 0,
        max = 1,
        value = 1/2,
        width = "100%",
        step = 1/10
      )
    )
  ),
  as_fill_carrier(plotOutput('results'))
)

server <- function(input, output) {
  
  ppv_fn <- function(sens, spec, prev = 1 / 2) {
    ( sens * prev ) / ( (sens * prev) + ( (1 - spec) * (1 - prev) ) )
  }
  npv_fn <- function(sens, spec, prev = 1 / 2) {
    ( spec * (1 - prev) ) / ( ( (1 - sens) * prev) + ( (spec) * (1 - prev) ) )
  }
  
  
  output$results <-
    renderPlot({
      
      x <- seq(0, 1, length.out = 50)
      n <- length(x)
      cond_x <- (0:4) / 4
      rng <- extendrange(0:1, f = 0.02)
      
      ###
      
      pos_df <- expand.grid(spec = cond_x, sens = x)
      pos_df$prev <- input$prev
      pos_df$PPV <- ppv_fn(pos_df$sens, pos_df$spec, prev = pos_df$prev)
      pos_df$Specificity <- format(pos_df$spec, digits = 3)
      
      pos_p <- 
        pos_df %>% 
        ggplot(aes(x = sens, y = PPV, col = Specificity)) +
        geom_line(linewidth = 1) + 
        lims(x = 0:1, y = rng) + 
        theme_bw() +
        theme(
          legend.position = "top",
          legend.text = element_text(size = 9)
        ) +
        scale_color_brewer(palette = "Reds") +
        labs(x = "Sensitivity")
      
      ###
      
      neg_df <- expand.grid(sens = cond_x, spec = x)
      neg_df$prev <- input$prev
      neg_df$NPV <- npv_fn(neg_df$sens, neg_df$spec, prev = neg_df$prev)
      neg_df$Sensitivity <- format(neg_df$sens, digits = 3)
      
      neg_p <- 
        neg_df %>% 
        ggplot(aes(x = spec, y = NPV, col = Sensitivity)) +
        geom_line(linewidth = 1) + 
        lims(x = 0:1, y = rng) + 
        theme_bw() +
        theme(
          legend.position = "top",
          legend.text = element_text(size = 9)
        ) +
        scale_color_brewer(palette = "Blues") +
        labs(x = "Specificity")
      
      print(pos_p + plot_spacer() + neg_p + plot_layout(widths = c(6, 1 , 6)))
      
    }, res = 100)
}

app <- shinyApp(ui, server)

app
```
:::

A visualization of how the ingedients to the positive and negative predictive values affect the statistics. 

:::


$$
Precision = \frac{\text{relevant and retrieved}}{\text{total retrieved}}=\frac{TP}{TP + FP}
$$ 

In the special case when the classes are balanced, this is equal to the positive predictive value (but is not otherwise).


$$
Recall = \frac{\text{relevant and retrieved}}{\text{total relevant}} = \frac{TP}{TP + FN}
$$ 

This is the same as sensitivity.

Additionally, the $F_1$ score is the harmonic mean of precision and recall:

$$
F_{1}=2\left(\frac{precision \times recall}{precision + recall}\right)
$$

More generally, the score can be written as

$$
F_{\beta}=(1+\beta_2)\left(\frac{precision \times recall}{(\beta^2 \times precision) + recall}\right)
$$




## Weighted performance metrics {#sec-cls-metrics-wts}

Two class performance metrics are convenient summaries and are generally interpretable. As we saw earlier in this chapter, the metrics of accuracy and Kappa can directly be extended to more that two classes. Because of the desirable characteristics of other two-class performance metrics, conventions have been constructed to extend these metrics. These conventions can be applied to any two-class metric. We will use sensitivity, *S*, to illustrate the conventions.

The first approach, "macro averaging" computes $C$ versions of the statistic using a "one-versus-all" approach. For example, when $C = 3$, $S_1$ is the sensitivity when the first class is considered the event and the second and third classes are pooled into the non-event category. $S_2$ is the sensitivity when the second class is considered as the event, and so on. The macro estimate of sensitivity is computed as their average:

$$
S_{macro} = \frac{1}{C}\sum_{k=1}^C S_k
$$

where the $S_k$ are the one-versus-all metrics.

A naive average of sensitivities is appropriate when the classes are balanced. When the classes are unbalanced, an unweighted average will bias this metric towards the sensitivity of the largest class. To account for class imbalance, *Weighted macro averaging* uses the class frequencies $n_k$ as weights

$$
S_{macro, wts} = \sum_{k=1}^C \frac{S_k}{n_k}
$$

Finally, *micro averaging* is a more complex approach because it aggregates the ingredients that go into the specific calculations. Sensitivity, for example, requires the number of true-positive and false negatives. These are summed and used in the regular computation of sensitivity:

$$
S_{micro} = \frac{{\displaystyle \sum_{k=1}^C TP_k}}{\left( {\displaystyle \sum_{k=1}^C TP_k} \right) + \left( {\displaystyle \sum_{k=1}^C FN_k} \right) }
$$ 

Note that there are many performance metrics that naturally work with three or more classes. There is little point in using these averaging tools for accuracy, Kappa, MCC, ROC AUC, and others.


## Evaluating Probabilistic Predictions {#sec-cls-metrics-soft}


```{r}
#| label: prob-metrics
#| include: false

# ------------------------------------------------------------------------------
# Compute the curves to get specific points

get_closest <- function(x, value) {
  x %>% 
    mutate(diff = abs(.threshold - value)) %>% 
    slice_min(diff, n = 1) %>% 
    dplyr::select(-diff) %>% 
    mutate(
      threshold = format(round(.threshold, 2)),
      threshold = sprintf("%2.2f", .threshold)
    )
}
roc_points <- 
  bind_rows(
    get_closest(nb_val_roc, .25), 
    get_closest(nb_val_roc, .5), 
    get_closest(nb_val_roc, .75),
  )

pr_points <- 
  bind_rows(
    get_closest(nb_val_pr, .25), 
    get_closest(nb_val_pr, .5), 
    get_closest(nb_val_pr, .75),
  )

gain_point <- 
  nb_val_gain %>% 
  mutate(diff = abs(.percent_found - 30)) %>% 
  slice_min(diff, n = 1)

# ------------------------------------------------------------------------------

set.seed(1)
rand_log_loss <- 
  purrr::map_dfr(1:25, ~ mutate(nb_val_pred, class = sample(class)), .id = "rep") %>% 
  group_by(rep) %>%  
  mn_log_loss(class, .pred_edible) %>% 
  summarize(mean = mean(.estimate)) %>% 
  pluck("mean")
```


### Cross-Entropy {#sec-cross-entropy}


$$
loss = -\frac{1}{N}\sum_{i=1}^N\sum_{k=1}^Cy_{ik}\log(\hat{p}_{ik})
$$

### Brier Scores {#sec-brier}

Another performance metric that uses the predicted class probabilities is the Brier score [@brier1950verification; @bella2013effect]. Suppose that the value $y_{ik}$ is a 0/1 indicator for whether the observed outcome $i$ corresponds to class $k$. Using this the score is:

$$
Brier = \frac{1}{N}\sum_{i=1}^N\sum_{k=1}^C (y_{ik} - \hat{p}_{ik})^2
$$

```{r}
#| label: decomp
#| eval: false
#| include: false
brier_decomp_2cls <- 
  nb_val_pred %>% 
  group_by(.pred_edible) %>% 
  summarize(n = n(), r = sum(class == "edible"), .groups = "drop") %>% 
  mutate(cal = n * (r - .pred_edible)^2, refine = n * r * (1- r)) %>% 
  summarize(cal = mean(cal), refine = mean(refine), .groups = "drop")
```

This is the mean squared error of the predicted class probabilities $\hat{p}_{ik}$. The Brier score can range between zero and two, but for two class problems, it is often rescaled to range between zero and one. A perfect model has a score of zero. If a performance metric achieves its smallest value when the probability estimate is perfect, it is called a *proper scoring rule*. There are theoretical benefits to using such a metric and, in the case of the Brier score, it also measures how well a model is calibrated (see @sec-calibration). In fact, the Brier score can be decomposed into different components, one of which specifically measures calibration. However, this can only be computed when the estimated probability values are not unique. For the naive Bayes model, the Brier score value was `r signif(nb_mtr$mean[nb_mtr$.metric == "brier_class"], 3)`.



### Reciever Operating Characteristic Curves {#sec-roc}

```{r}
#| label: fig-roc-curve
#| echo: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: Validation set ROC curves for the naive Bayes model The symbols represent different probability thresholds. 

autoplot(nb_val_roc) + 
  geom_point(
    data = roc_points,
    aes(
      y = sensitivity,
      x = 1 - specificity,
      col = threshold,
      pch = threshold
    ),
    cex = 4
  ) +
  scale_shape_manual(values = c(6, 1, 2))
```


### Precision-Recall Curves {#sec-pr}


```{r}
#| label: fig-pr-curve
#| echo: false
#| warning: false
#| out-width: 60%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: A precision-recall curve, estimated with the validation set, for the naive Bayes model.

autoplot(nb_val_pr) + 
  geom_point(
    data = pr_points,
    aes(
      y = precision,
      x = recall,
      col = threshold,
      pch = threshold
    ),
    cex = 4
  ) +
  scale_shape_manual(values = c(6, 1, 2)) +
  coord_obs_pred()
```

### Gain Charts {#sec-gain}

```{r}
#| label: fig-gain-curve
#| echo: false
#| out-width: 50%
#| fig-width: 5
#| fig-height: 5
#| fig-cap: "A gain curve based on the validation set. The dotted linear indicates the percentage of samples to query in order to capture 30{{< pct >}} of the samples from the first class."

nb_val_gain %>%
  autoplot() +
  geom_segment(
    data = gain_point,
    aes(
      x = .percent_tested, xend = .percent_tested,
      y = 0, yend = .percent_found
    ),
    col = "purple", linewidth = 1, lty = 3
  ) +
  geom_segment(
    data = gain_point,
    aes(
      x = 0, xend = .percent_tested,
      y = .percent_found, yend = .percent_found
    ),
    col = "purple", linewidth = 1, lty = 3
  ) +
  coord_obs_pred()
```




## Calibration {#sec-calibration}


```{r}
#| label: fig-cal-curves
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: Calibration curves for naive Bayes model. The colored regions are individual 90{{< pct >}} confidence intervals. 

cal_plot_1 <- 
  nb_val_pred %>% 
  cal_plot_windowed(class, .pred_edible, window_size = 0.2, step_size = 0.025) + 
  ggtitle("(a) moving 20% bins")

cal_plot_2 <- 
  nb_val_pred %>% 
  cal_plot_logistic(class, .pred_edible) + 
  ggtitle("(b) logistic regression")

cal_plot_1 + cal_plot_2
```

The downside to this binning approach is that, for small data sets, the results can have very large variability or there may not be enough data within each bin to make accurate judgements regarding calibration. One alternative is to use moving windows. For example, for fixed bin width of 0.20, compute the required statistics and then move the bin by a small amount (say 0.02). This would result in larger bins capturing more data that would slide across the range of probability values.

Another alternative is to fit a model to the data. Logistic regression is a relatively simple classification model for binary outcomes and is discussed in @sec-logistic-regression. We can fit this model using our observed outcomes and use the predicted class probabilities ($\hat{p}_i$) as the predictor. If the probabilities are well-calibrated, the model should show a linear relationship. One other feature that can be used in this diagnostic is to model the predictor with a smoothing spline. This enables the logistic model to reflect when the calibration is very poor. @fig-cal-curves (b) illustrates the same pattern with a much smaller degree of uncertainty since the model coefficients utilize all of the data.

```{r}
#| label: logistic_cal
#| include: false

logistic_cal <- nb_cal_pred %>% cal_estimate_logistic(truth = class, smooth = TRUE)
pred_cal <- nb_val_pred %>% cal_apply(logistic_cal)

num_logistic_vals <-  nb_cal_pred %>% distinct(.pred_edible) %>% nrow()
iso_cal <- nb_cal_pred %>% cal_estimate_isotonic(truth = class)
pred_cal_iso <- nb_val_pred %>% cal_apply(iso_cal)
num_isotonic_vals <- vctrs::vec_unique_count(pred_cal_iso$.pred_edible)

iso_boot_cal <- nb_cal_pred %>% cal_estimate_isotonic_boot(truth = class, times = 1000)
pred_cal_iso_boot <- nb_val_pred %>% cal_apply(iso_boot_cal)
num_isotonic_boot_vals <- vctrs::vec_unique_count(pred_cal_iso_boot$.pred_edible)
```

What can be done with poorly calibrated predictions? There are approaches for post-processing the predicted probabilities to have higher fidelity to the observed data. The customary method is to use a logistic regression fit, as seen in @fig-cal-curves (b), to re-calibrate the predictions. First a suitable logistic regression model is created in the same manner as the one used for diagnosis. Recall that the original predictions are used as predictors of the model. To re-calibrate a set of predictions, the original values are pushed through the model and the resulting logistic regression values are used as the new probability predictions.

@platt1999probabilistic  proposed using logistic regression as a method of converting any quantitative score into a probability-like value. *Platt scaling* also adjusts the class indicators from binary 0/1 values to values slightly different from those values.

This brings up questions about which data should be used to fit the calibration model and which to evaluate its efficacy. We’ve previously mentioned that using simple re-predictions of the training set is a very bad idea and that is especially true here. If the model was resampled, the out-of-sample predictions can be used to fit the calibration model. In our example, a single validation set was used. Unfortunately, if all of the held-out data were used for re-calibration, the only unused data left would be the test set (which should not be compromised).

TODO change below

Since there are `r format(nrow(pred_cal), big.mark = ",")` samples in the validation set, we will randomly select half to fit the calibration model and will use the other half to evaluate how well the calibration process worked. @fig-re-cal-curves shows the results where panel (a) uses a sliding window to assess the smaller data set using a window size of 20{{< pct >}} that moves in 5{{< pct >}} increments. From these figures, the class probabilities appear to have been improved. Note that these values now encompass the entire probability range.

```{r}
#| label: fig-re-cal-curves
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: Calibration curves for naive Bayes model _after_ re-calibration using logistic regression in conjunction with a calibration set.

cal_plot_3 <- 
  pred_cal %>% 
  cal_plot_windowed(class, .pred_edible, window_size = 0.2, step_size = 0.025) + 
  ggtitle("(a) moving 20% bins")

cal_plot_4 <- 
  pred_cal %>% 
  cal_plot_logistic(class, .pred_edible) + 
  ggtitle("(b) logistic regression")

cal_plot_3 + cal_plot_4
```

```{r}
#| label: re-cal
#| echo: false
re_class_stats <- pred_cal %>% cls_mtr(class, estimate = .pred_class, .pred_edible)
```

Using this approach to calibration, the new probability estimates have a Brier score of `r signif(re_class_stats$.estimate[re_class_stats$.metric == "brier_class"], 3)`. While this is smaller than the value (`r signif(nb_mtr$mean[nb_mtr$.metric == "brier_class"], 3)`) prior to calibration, the exact same data were not used. This makes it difficult to say that the improvement is real.

There other approaches for re-calibrating. First, is isotonic regression [@isotonic]. This tool fits a model that will generate monotonic, non-decreasing, predictions: $\hat{y}_i \le \hat{y}_{i+ 1}$ when $x_i \le x_{i+ 1}$. There are multiple methods to create such models but the most straightforward approach is the pooled-adjacent violators algorithm (PAVA) of @pava. This method adjusts a sequence of data points that are not monotonically increasing to be so. For example, @tbl-isotonic shows an example where the rows are ordered by their model's predicted probabilities. One column shows a binary indicator for class `A`. The first three rows have the same outcome (`B`) but the fourth row is actually class `A`. The PAVA algorithm then takes the averages of the binary indicators that bracket the discordant value and all of these probability estimates are given a value of 1/3. From the table, it is clear that there are three possible probability estimates after isotonic regression is used.

::::::: columns
::: {.column width="25%"}
:::

:::: {.column width="50%"}
::: {#tbl-isotonic}
```{r}
#| label: tbl-isotonic
#| echo: false

iso_example <- 
  tibble::tribble(
    ~Truth, ~Binary, ~Model, ~Isotonic,
    "B",       0,      0,         0,
    "B",       0,   0.01,         0,
    "B",       0,    0.1,         0,
    "A",       1,    0.2,      0.33,
    "B",       0,    0.4,      0.33,
    "B",       0,    0.5,      0.33,
    "A",       1,    0.6,         1,
    "A",       1,    0.7,         1,
    "A",       1,    0.8,         1,
    "A",       1,    0.9,         1
  )

iso_example %>% 
  kbl() %>% 
  add_header_above(c(" " = 2, "Estimated Probability of Class A" = 2)) %>% 
  kable_styling(full_width = FALSE)  
```


An example of calculations for calibration via isotonic regression.
:::
::::

::: {.column width="25%"}
:::
:::::::

The nice feature of this tool is that it is nonparametric and makes almost no distributional assumptions. However, one downside is that there are a small number of unique re-calibrated probability estimates. For this reason, it is advised to use isotonic regression for large data sets. For example, for the naive Bayes data used for calibration, there were originally `r num_logistic_vals` unique probability estimates while, after isotonic regression is applied, there are only `r num_isotonic_vals` possible values.

```{r}
#| label: fig-re-cal-iso-curves
#| echo: false
#| out-width: 90%
#| fig-width: 7
#| fig-height: 4
#| fig-cap: Calibration curves for naive Bayes model _after_ re-calibration using isoreg regression.

cal_plot_5 <- 
  pred_cal_iso %>% 
  cal_plot_windowed(class, .pred_edible, window_size = 0.2, step_size = 0.025) + 
  ggtitle("(a) moving 20% bins")

cal_plot_6 <- 
  pred_cal_iso_boot %>% 
  cal_plot_windowed(class, .pred_edible, window_size = 0.2, step_size = 0.025) + 
  ggtitle("(b) moving 20% bins, bootstrapped")

cal_plot_5 + cal_plot_6
```

There are many other tools in the calibration toolbox, such as beta calibration [@betacal]. There are also methods for calibrating classification models with three or more classes [@NIPS2001abdbeb4d; @johansson21a; @NEURIPS2021bbc92a64].

Finally, it is important to re-compute performance using the re-calibrated predictions. Also note that, since the class probabilities have been altered, any hard predictions should be recomputed since some values may have moved across the chosen probability threshold. Using the set of `r nrow(pred_cal)` data points that have been re-calibrated, the re-estimated sensitivity and specificity were `r round(re_class_stats$.estimate[re_class_stats$.metric == "sensitivity"] * 100, 1)`{{< pct >}} and `r round(re_class_stats$.estimate[re_class_stats$.metric == "specificity"] * 100, 1)`{{< pct >}}, respectively and the area under the ROC curve was `r signif(re_class_stats$.estimate[re_class_stats$.metric == "roc_auc"], 3)`


## Ordered Categories {#sec-ordered-categories}

As previously mentioned, our outcome classes can have an inherent ordering.

examples - wordle

Since there is an ordering, it might be reasonable to think that misclassification severity relates to how "far" a predicted class is from its true class. For example, risk analysis systems, such as FMEA, involve enumerating different ways that a thing can fail and the corresponding consequences. Suppose that we have an ordinal scale for risk consisting of "low," "moderate," "severe," and "catastrophic". It is reasonable to think that predicting that the risk is moderate when it is actually catastrophic is very bad. Predicting a moderate risk to be low may not be as dire.

There are a few metrics that use hard classification predictions. We previously mentioned the Kappa statistic. To penalize misclassification errors based on their distance to the true classification, [@cohen1968weighted] developed the *weighted Kappa* statistic where the errors' weights depend on their position differences. Let's look at a four-class example from @apm where computing jobs run on a server executed very fast (VF), fast (F), moderately long (M), or a long time (L).

A linear weighting for the Kappa statistic is shown in @tbl-confusion-matrix-costs. There are bands of diagonal weights that are either 0, 1, 2, or 3. Weight Kappa statistics enable "more incorrect" errors more than others.

```{r}
#| label: ordered-stats
#| echo: false
#| results: hide
kap_linear <- function(data, truth, estimate, na_rm = TRUE, ...) {
  kap(
    data = data,
    truth = !!rlang::enquo(truth),
    estimate = !!rlang::enquo(estimate),
    weighting = "linear",
    na_rm = na_rm,
    ...
  ) %>% 
    mutate(.estimator = "linear")
}
kap_linear <- new_class_metric(kap_linear, "maximize")

kap_quadratic <- function(data, truth, estimate, na_rm = TRUE, ...) {
  kap(
    data = data,
    truth = !!rlang::enquo(truth),
    estimate = !!rlang::enquo(estimate),
    weighting = "quadratic",
    na_rm = na_rm,
    ...
  ) %>% 
    mutate(.estimator = "quadratic")
}
kap_quadratic <- new_class_metric(kap_quadratic, "maximize")

kap_set <- metric_set(kap, kap_linear, kap_quadratic)
kap_res <- 
  hpc_cv %>% 
  kap_set(truth = obs, estimate = pred)

# ------------------------------------------------------------------------------

mse_ordered <- function(dat) {
  ord_diffs <- as.integer(dat$obs) - as.integer(dat$pred)
  mean(ord_diffs^2)
}

set.seed(579)
hpc_perm <- permutations(hpc_cv, times = 50, permute = obs)

mse_perm <- 
  hpc_perm %>% 
  mutate(null = map_dbl(splits, ~ mse_ordered(analysis(.x))))

mean(mse_perm$null)

# ------------------------------------------------------------------------------

hpc_costs_vals <- tribble(
  ~estimate, ~truth, ~cost,
  "VF",      "VF",    0,
  "VF",      "F",     1,
  "VF",      "M",     5,
  "VF",      "L",    10,
  "F",       "VF",    1,
  "F",       "F",     0,
  "F",       "M",     5,
  "F",       "L",     5,
  "M",       "VF",    1,
  "M",       "F",     1,
  "M",       "M",     0,
  "M",       "L",     1,
  "L",       "VF",    1,
  "L",       "F",     1,
  "L",       "M",     1,
  "L",       "L",     0
)

hpc_costs_tbl <- 
  hpc_costs_vals %>% 
  mutate(truth = paste0("cost_", truth)) %>% 
  pivot_wider(id_cols = c(estimate), names_from = c(truth), values_from = cost) %>% 
  mutate(blank3 = "  ") %>% 
  dplyr::select(-estimate) %>% 
  relocate(blank3)


hpc_costs <- 
  hpc_cv %>%
  classification_cost(obs, VF:L, costs = hpc_costs_vals)

hpc_costs_perm <- 
  hpc_perm %>% 
  mutate(null = map_dbl(
    splits, 
    ~ classification_cost(analysis(.x), truth = obs, VF:L, costs = hpc_costs_vals)$.estimate)
  )
```

::::::: columns
::: {.column width="5%"}
:::

:::: {.column width="90%"}
::: {#tbl-confusion-matrix-costs}
```{r}
#| label: tbl-confusion-costs
#| echo: false

lvls <- levels(hpc_cv$obs)

hpc_conf_mat <-
  hpc_cv %>% 
  conf_mat(obs, estimate = pred) %>% 
  pluck("table") %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  pivot_wider(id_cols = c(Prediction), names_from = c(Truth), values_from = n) %>% 
  mutate(blank = "  ") %>% 
  relocate(blank, .after = "Prediction") 

hpc_errors <- toeplitz(0:3)
colnames(hpc_errors) <- rownames(hpc_errors) <- levels(hpc_cv$obs)
hpc_errors <- 
  hpc_errors %>%
  as.table() %>% 
  as.data.frame() %>% 
  set_names(c("Prediction", "Truth", "n")) %>% 
  mutate(Truth = paste0("wts_", Truth)) %>% 
  pivot_wider(id_cols = c(Prediction), names_from = c(Truth), values_from = n) %>% 
  mutate(blank2 = "  ") %>% 
  dplyr::select(-Prediction) %>% 
  relocate(blank2)

bind_cols(hpc_conf_mat, hpc_errors, hpc_costs_tbl <- 
            hpc_costs_vals %>% 
            mutate(truth = paste0("cost_", truth)) %>% 
            pivot_wider(id_cols = c(estimate), names_from = c(truth), values_from = cost) %>% 
            mutate(blank3 = "  ") %>% 
            dplyr::select(-estimate) %>% 
            relocate(blank3)) %>% 
  kbl(col.names = c("Prediction", "", lvls, "", lvls, "", lvls)) %>% 
  add_header_above(c(" " = 2, "Truth" = 4, " " = 1, "Linear Weights" = 4, " " = 1, "Custom Costs" = 4)) %>% 
  kable_styling(full_width = FALSE)
```

A four class ordinal outcome with a confusion matrix, a linear set of weights for each potiental error, and a custom cost scheme used by @apm.
:::
::::

::: {.column width="5%"}
:::
:::::::

We computed Kappa under three scenareos: equal weights (`r round(kap_res$.estimate[kap_res$.estimator == "multiclass"], 3)`), linear scores (`r round(kap_res$.estimate[kap_res$.estimator == "linear"], 3)`), and quadratic scores (`r round(kap_res$.estimate[kap_res$.estimator == "quadratic"], 3)`).

The mean squared error for these data was `r signif(sqrt(mse_ordered(hpc_cv)), 3)`. How bad is this? If we repeatedly permute the outcome classes and measure the RMSE we find that the worst case-scenario is about `r signif(sqrt(mean(mse_perm$null)), 3)`.

For the custom cost analysis, the average cost was `r signif(hpc_costs$.estimate, 3)` with a worst-case value of about `r signif(mean(hpc_costs_perm$null), 3)`.

## Multi-objective Assessments {#sec-multi-objectives}

Pareto optimal; desirability functions. J statistic


$$
J = Sensitivity + Specificity - 1
$$



TODO

-   shinylive for roc/pr/gain
-   mention bootstrapping and permutations
-   desirability and other methods for a combined score?


## Chapter References {.unnumbered}

```{r}
#| label: teardown-cluster
#| include: false

# stopCluster(cl)
```

