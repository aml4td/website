<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Missing Data – Applied Machine Learning for Tabular Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/numeric-predictors.html" rel="next">
<link href="../chapters/initial-data-splitting.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ac94f01b84e0cf733daf4ed4b084c36a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<meta name="shinylive:serviceworker_dir" content="..">
<script src="../site_libs/quarto-contrib/shinylive-0.9.1/shinylive/load-shinylive-sw.js" type="module"></script>
<script src="../site_libs/quarto-contrib/shinylive-0.9.1/shinylive/run-python-blocks.js" type="module"></script>
<link href="../site_libs/quarto-contrib/shinylive-0.9.1/shinylive/shinylive.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/shinylive-quarto-css/shinylive-quarto.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-7T996NL20Z"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-7T996NL20Z', { 'anonymize_ip': true});
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title"><span id="sec-missing-data" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Missing Data</span></span></h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../"></a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/aml4td/website/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/news.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">News</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/contributing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributing</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/whole-game.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Whole Game</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Preparation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/initial-data-splitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Initial Data Splitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/missing-data.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Missing Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/numeric-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transforming Numeric Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/categorical-predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Working with Categorical Predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/interactions-nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Interactions and Nonlinear Features</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/overfitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Overfitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/resampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Measuring Performance with Resampling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/grid-search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Grid Search</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/iterative-search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Iterative Search</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/feature-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/comparing-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Comparing Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Classification</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Regression</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Characterization</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Finalization</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-root-causes" id="toc-sec-root-causes" class="nav-link active" data-scroll-target="#sec-root-causes"><span class="header-section-number">4.1</span> Root Causes</a></li>
  <li><a href="#sec-resolve-missing-data" id="toc-sec-resolve-missing-data" class="nav-link" data-scroll-target="#sec-resolve-missing-data"><span class="header-section-number">4.2</span> Approaches for Resolving Missing Data</a>
  <ul class="collapse">
  <li><a href="#sec-missing-robustness" id="toc-sec-missing-robustness" class="nav-link" data-scroll-target="#sec-missing-robustness"><span class="header-section-number">4.2.1</span> Models that Tolerate Missing Data</a></li>
  <li><a href="#sec-removing-missing-data" id="toc-sec-removing-missing-data" class="nav-link" data-scroll-target="#sec-removing-missing-data"><span class="header-section-number">4.2.2</span> Removal</a></li>
  <li><a href="#sec-imputation" id="toc-sec-imputation" class="nav-link" data-scroll-target="#sec-imputation"><span class="header-section-number">4.2.3</span> Imputation</a></li>
  <li><a href="#sec-encoding-missing-data" id="toc-sec-encoding-missing-data" class="nav-link" data-scroll-target="#sec-encoding-missing-data"><span class="header-section-number">4.2.4</span> Encoding Missing Data</a></li>
  </ul></li>
  <li><a href="#sec-imputation-methods" id="toc-sec-imputation-methods" class="nav-link" data-scroll-target="#sec-imputation-methods"><span class="header-section-number">4.3</span> Specific Imputation Methods</a>
  <ul class="collapse">
  <li><a href="#sec-imputation-most-likely" id="toc-sec-imputation-most-likely" class="nav-link" data-scroll-target="#sec-imputation-most-likely"><span class="header-section-number">4.3.1</span> Most Likely Value</a></li>
  <li><a href="#sec-imputation-linear" id="toc-sec-imputation-linear" class="nav-link" data-scroll-target="#sec-imputation-linear"><span class="header-section-number">4.3.2</span> Linear Methods</a></li>
  <li><a href="#sec-imputation-knn" id="toc-sec-imputation-knn" class="nav-link" data-scroll-target="#sec-imputation-knn"><span class="header-section-number">4.3.3</span> Nearest Neighbors</a></li>
  <li><a href="#sec-imputation-trees" id="toc-sec-imputation-trees" class="nav-link" data-scroll-target="#sec-imputation-trees"><span class="header-section-number">4.3.4</span> Trees</a></li>
  </ul></li>
  <li><a href="#sec-when-to-address-missing-data" id="toc-sec-when-to-address-missing-data" class="nav-link" data-scroll-target="#sec-when-to-address-missing-data"><span class="header-section-number">4.4</span> When to Address Missing Data</a></li>
  <li><a href="#chapter-references" id="toc-chapter-references" class="nav-link" data-scroll-target="#chapter-references">Chapter References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span id="sec-missing-data" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Missing Data</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>It is not uncommon for some predictor values to be unknown. There can be a multitude of reasons. To illustrate, let’s consider a laboratory test for a respiratory disease. One or more results may be missing due to a failed control or an inappropriate database join. It is also possible that the test itself failed to produce a result. For example, the test might fail for diagnostics that use throat swabs because an interfering substance, such as food coloring from a lozenge, is on the swab.</p>
<p>One of our illustrative data, the Ames housing data, has missing values in 22 predictors. <a href="#fig-ames-missing" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ames-missing</span></a> illustrates the occurrence of missingness for the these data. This figure presents the full rectangular data matrix where each property (sorted alphabetically by neighborhood) is on the x-axis, and each predictor is on the y-axis. Red indicates that the value was missing in the data.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ames-missing" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ames-missing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="missing-data_files/figure-html/fig-ames-missing-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ames-missing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: A visualization of the missing data for the Ames Housing data set.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The figure highlights several characteristics. The most noteworthy: missing data affects <em>every</em> property (i.e., row) in the data. Pool quality is missing for nearly every property, while the columns for types of alleys, basements, and fences as well as the quality of fireplaces are missing for the vast majority of properties. Second, there are specific patterns of missing information that are visually apparent. For example, when missing data occurs in one garage variable, it likely occurs across other garage variables (condition, finish, quality, type, and year built). The same is true for basement variables (condition, exposure, quality, type 1, and type 2). Other variables, like fence, fireplace quality, and lot frontage do not appear to have any visual structure to their missingness.</p>
<p>An “upset plot” is a method for visualizing high-dimensional Venn diagrams <span class="citation" data-cites="lex2014upset">(<a href="#ref-lex2014upset" role="doc-biblioref">Lex et al. 2014</a>)</span>. We can use this to explore potential patterns of missingness across predictors. For example, <a href="#fig-ames-patterns" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ames-patterns</span></a> shows that the majority of the properties in Ames are missing values for ally, fence, or pool.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ames-patterns" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ames-patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="missing-data_files/figure-html/fig-ames-patterns-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ames-patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: An upset plot showing a few patterns of missingness in the Ames data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The problem with missing data is that many models are not naturally equipped to deal with this lack of information. As a simple example, consider the multiple linear regression model. Deriving the regression coefficients depends on operations on the predictor values (e.g., computing the covariance matrix). These calculations cannot be performed if the predictors contain any missing values; this is also true for the majority of models. Consequently, it is crucial to address the presence of missing data to build any of these predictive methods<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. In addition to addressing missing data, the pattern and nature of missingness can sometimes serve as a significant predictor of the response. In this chapter, we will explore the root causes of missing data, examine approaches for resolving the problem, and understand when it should be addressed in the modeling process.</p>
<p><span class="citation" data-cites="allison2002missing">Allison (<a href="#ref-allison2002missing" role="doc-biblioref">2002</a>)</span> has an excellent and succinct summary of relevant statistical concepts and methods. <span class="citation" data-cites="emmanuel2021survey">Emmanuel et al. (<a href="#ref-emmanuel2021survey" role="doc-biblioref">2021</a>)</span> and <span class="citation" data-cites="hasan2021missing">Hasan et al. (<a href="#ref-hasan2021missing" role="doc-biblioref">2021</a>)</span> provide literature surveys on methods for missing data, while <span class="citation" data-cites="nijman2022missing">Nijman et al. (<a href="#ref-nijman2022missing" role="doc-biblioref">2022</a>)</span> describes a survey on just how <em>badly</em> these topics are described/documented in specific studies in the literature.</p>
<section id="sec-root-causes" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-root-causes"><span class="header-section-number">4.1</span> Root Causes</h2>
<p>When encountering missing data, the primary question is, “Why are these values missing?” Knowing is a good idea, and the answer can substantially affect how we compensate for the problem. In some instances, the answer might be apparent or can be inferred from the data. In the example of the laboratory test, suppose a random defect in the test kit was to blame for missing measurements. In this instance, the value was missing completely at random (MCAR). MCAR means that the mechanisms that affect missingness in the predictor are known and unrelated to the columns in the data set. When this is the case, our complete case sample (achieved by using rows with no missing values) corresponds to sampling from the same distribution as the entire data set (i.e., includes the missing rows).</p>
<p>With MCAR, we have significant latitude in terms of how to handle the situation. A complete case analysis will not induce any systematic bias in our results but, due to the smaller sample size, the model will have increased variability.</p>
<p>As a counter-example, suppose that patients with more severe respiratory illnesses were more likely to deal with their symptoms by using a lozenge, possibly leading to missing values. If the likelihood of a missing predictor value is a function of some other variable(s) <em>contained in the data</em>, this is called missing at random (MAR). You can think of this as conditional MCAR: each each level of a column describing lozenge usage (yes/no), the data are MCAR. <span class="citation" data-cites="bhaskaran2014difference">Bhaskaran and Smeeth (<a href="#ref-bhaskaran2014difference" role="doc-biblioref">2014</a>)</span> contains practical examples that help distinguish MAR and MCAR.</p>
<p>Another example data set will be introduced in <a href="#sec-hotel-rates" class="quarto-xref"><span class="quarto-unresolved-ref">sec-hotel-rates</span></a>, where we try to predict the cost of a hotel room (known as the “average daily rate” or ADR). The data set includes data on the method of booking the room, such as the name of the travel agent, the travel company, and the customer’s country of origin. The agent variable in the hotel rate data was missing for 22.7% of the data. We don’t always know if the agent is missing because no agent was used or because the value was not recorded. Treating the missingness of the agent as a binary outcome, we can use a simple recursive partitioning model (<a href="#sec-cart-cls" class="quarto-xref"><span class="quarto-unresolved-ref">sec-cart-cls</span></a>) to understand potential relationships with other predictors. This might suggest the mechanism(s) influencing the occurrence of missing data. <a href="#fig-hotels-agent-missing" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-hotels-agent-missing</span></a> illustrates the predictors that partition the missing agent data into increasing homogeneous groups. For example, non-missing company values are highly associated with missing values. This might mean that the company making the reservation used automated systems instead of a specific person in the company being associated with the reservation. Lead time also appears to be related to missingness, with shorter lead times occurring more frequently with missing agent values. We don’t know whether lead time influences missingness or vice-versus. However, this analysis would provide a direction to begin an investigation.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-hotels-agent-missing" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hotels-agent-missing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="missing-data_files/figure-html/fig-hotels-agent-missing-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hotels-agent-missing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: A classification tree to predict the missing category of agent in the hotel rate dataset. “TA” stands for travel agent and “TO” means tour operators.
</figcaption>
</figure>
</div>
</div>
</div>
<p>With some simple exploration, as demonstrated with the Ames and hotel rate data sets, we can understand the causes or potential reasons for missing data. However, determining the cause of missing data may be more challenging for many other data sets. We need a framework to understand and manage missing data in such cases.</p>
<p>One helpful framework involves examining the mechanisms behind missing data, including structural deficiencies, random occurrences, and specific causes.</p>
<p>First, there can be structural deficiencies. This type of missing data arises when necessary information about a predictor is omitted. Moreover, it is often the easiest to address once the missing information has been identified. The agent variable in the hotel rate data is one example of a structural deficiency. As a second example, we saw earlier in the Ames housing data that several of the garage predictors were simultaneously missing across 5% of homes. The missing information was because these homes did not have a garage<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Some or all of the variables of finish, quality, type, and year built for garages will likely be important for determining a home’s value. Therefore, we will need an approach to address these structural deficiencies.</p>
<p>There can also be random occurrences. Missing values often occur at random with no defined root cause. For example, a patient in a clinical trial may miss a scheduled visit due to a scheduling mishap. As another example, sporadically occurring severe weather can create missing data for collection devices that depend on continuous power. Generally, missing data due to random occurrences can happen a small percentage of the time and can be remedied. However, if randomly missing data occurs a large percentage of the time, the measurement system should be evaluated, and the collected variables should be scrutinized before being included in the modeling process. For a detailed understanding of this type of problem, see <span class="citation" data-cites="little2019statistical">Little and Rubin (<a href="#ref-little2019statistical" role="doc-biblioref">2019</a>)</span>.</p>
<p>We might be able to identify specific causes for missingness. As we will see later in this chapter (<a href="#sec-encoding-missing-data" class="quarto-xref"><span class="quarto-unresolved-ref">sec-encoding-missing-data</span></a>), several basement variables in the original Ames data have missing values. This information was not missing because there was some failure in recording the data. The explanation was much simpler: these homes had no basements. This type of missing data is the most challenging to manage, and the appropriateness of techniques can vary. Hence, understanding the nature of the missing data is crucial before applying any methods.</p>
<p>The worst situation is missing not at random (MNAR)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, where we do not know the factors that influence the probability that values are missing. The best approach to MNAR data is not to have MNAR data; it is essential to determine why the data are missing and, hopefully, relate that to columns in the data. Otherwise, we can use imputations and other methods described below to solve the issues. However, there will likely be significant ambiguity about the analysis and how well it works. With models built in the MNAR assumption, it would behoove us to conduct extensive sensitivity analyses to assess the robustness of our approach to the missing data problem.</p>
<p>As an example, Chapters 12 - 15 of <span class="citation" data-cites="apm">Kuhn and Johnson (<a href="#ref-apm" role="doc-biblioref">2013</a>)</span> showed multiple classification models to predict the probability that a grant would be accepted. Two categorical predictors contained encoded values of “unknown” and had very high empirical associated with the outcome classes:</p>
<blockquote class="blockquote">
<p>Informative missingness seemed to occur in these data; unknown values of the contract value band and sponsor code were heavily used in many models. This itself is a likely surrogate or signal for some other piece of information.</p>
</blockquote>
<p>This is not a good place to be: we cannot explain the existence of two of our main predictors. It would be that the populations of grants that had missing data for these variables correspond to very successful results or that the process that assembled the data contains a systematic flaw. We do know that we can’t induce success in a grant application by just labeling the value as unknown. Clearly, we would want to investigate this situation to determine what is happening.</p>
<div class="important-box">
<p>Understanding these mechanisms will guide us in choosing appropriate techniques for handling missing data appropriately.</p>
</div>
<p><a href="https://tidymodels.aml4td.org/chapters/missing-data.html#sec-missing-eda"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-resolve-missing-data" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-resolve-missing-data"><span class="header-section-number">4.2</span> Approaches for Resolving Missing Data</h2>
<p>There are three general ways to resolve missing values: removal, imputation, or encoding. Each approach has advantages and disadvantages, and which we choose should depend on the specific problem context and data set. We will review these approaches in the subsections below and provide guidance on when each would be appropriate.</p>
<section id="sec-missing-robustness" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-missing-robustness"><span class="header-section-number">4.2.1</span> Models that Tolerate Missing Data</h3>
<p>Instead of directly addressing the missing values, we could sidestep it by using models that tolerate missingness. Let’s examine a few.</p>
<p>The CART decision tree model <span class="citation" data-cites="breiman1984classification">(<a href="#ref-breiman1984classification" role="doc-biblioref">Breiman et al. 1984</a>)</span> recursively identifies variables and split points that optimally partition the data into subsets whose outcome frequency distributions are more homogeneous. For each selected variable and split point, additional variables and split points are identified with the next best ability to partition the data into more pure subsets. These additional variables are called <em>surrogate splits</em>. When a sample has a missing value for a predictor in the tree, the surrogate predictors are then used to direct the sample toward the appropriate terminal node.</p>
<p>While C5.0 <span class="citation" data-cites="quinlan1993c4 apm">(<a href="#ref-quinlan1993c4" role="doc-biblioref">Quinlan 1993</a>; <a href="#ref-apm" role="doc-biblioref">Kuhn and Johnson 2013</a>)</span> is also a decision tree, it adopts a unique approach to addressing missing values. This method utilizes fractional counts in subsequent splits based on the frequency distribution of missing data for a predictor. This approach enables the model to estimate where the missing values might fall within the partitioning.</p>
<p>Boosted trees, such as xgboost <span class="citation" data-cites="chen2016xgboost">(<a href="#ref-chen2016xgboost" role="doc-biblioref">Chen and Guestrin 2016</a>)</span>, are also based on a recursive partitioning framework. However, xgboost’s approach to addressing missing data is more complex and is called <em>sparsity-aware split finding</em>. In the model-building process, the algorithm determines which direction would be more optimal for each node in the tree if a sample had a missing value for that predictor.</p>
<p>Random forest <span class="citation" data-cites="breiman2001random">(<a href="#ref-breiman2001random" role="doc-biblioref">Breiman 2001</a>)</span> has several approaches to handling missing values. The naive approach internally imputes using the median of non-missing values for continuous predictors or the most frequent value for categorical predictors. A more advanced approach identifies the nearest non-missing samples to the sample with the missing value and imputes the value based on a weighted distance score.</p>
<p>The primary benefit of these models is that we can use the original data as-is. They are eliminate the propagation of errors that can occur when imputation tools are used. The drawback is that the number of models that can be applied to the data is very limited. As we know from the No Free Lunch theorem, no one model will be optimal for all problems. Therefore, we must address the missing data problem head-on to adequately explore the predictive ability of a wide range of models.</p>
<div class="dangerous-box">
<p>These models only solve the logistical aspect of missing data. If the nature of missingness in your data causes a systematic bias, the models above <em>will not</em> correct this bias.</p>
</div>
<p>For example, naive Bayes <span class="citation" data-cites="webb2010naive">(<a href="#ref-webb2010naive" role="doc-biblioref">Webb, Keogh, and Miikkulainen 2010</a>)</span> builds models by analyzing each predictor independently of the others. When a predictor contains a missing value, this missing sample’s information is omitted from probability calculations for only the affected predictors. If a distribution is systematically affected by missingness, the probabilities computed during prediction will be biased and may result in poor results in new samples. If missing data informs our understanding of the response, omitting the missing samples will be detrimental to the model.</p>
<p><a href="#fig-missing-naive-bayes" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-missing-naive-bayes</span></a> shows a two-class classification example where one predictor is used. The top panel shows the probability of a missing predictor value is related to its own value (i.e., MCAR). The predictor value is less likely to be complete was its value increases.</p>
<p>Naive Bayes would use the predictor data to compute <em>conditional densities</em> of the predictor for each outcome class. The middle plot shows what these densities should resemble if the dataset were complete. There is a small overlap between the predictor densities; decent model performance is achievable.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-missing-naive-bayes" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-missing-naive-bayes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="missing-data_files/figure-html/fig-missing-naive-bayes-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-missing-naive-bayes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: An example of how missingness can cause bias in the training data and impair a model fit.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The bottom set of densities reflects the observed data. The density for class level <code>B</code> is more affected and would appear to have a tighter distribution. This induces more overlap in the densities, making the classification problem more difficult.</p>
<p>This demonstrates that a model being agnostic to missing values does not mean that the missing value problem does not go away.</p>
</section>
<section id="sec-removing-missing-data" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="sec-removing-missing-data"><span class="header-section-number">4.2.2</span> Removal</h3>
<p>The simplest method for managing missing data is to eliminate the predictors <em>or</em> samples that contain them. However, the deletion of data requires careful consideration of several factors within the dataset. When deleting data, the order in which we assess the proportion of missing values (in columns or rows) is important to consider. In some scenarios, there are many more predictors than samples; samples are often difficult or expensive to collect. In this case, it would be wise to first identify predictors that should be removed due to excessive missing values, then proceed to identify and possibly remove samples that have excessive missing values. If, on the other hand, the data contain many more samples than predictors, then the removal procedure could be reversed. In any case, however, we need to remember that the samples are our currency for tuning models and assessing model performance. Therefore, we will often place a higher priority on preserving samples over predictors.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ames-missing-distribution" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ames-missing-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="missing-data_files/figure-html/fig-ames-missing-distribution-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ames-missing-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.5: The distribution of percent missing values across predictors for the Ames housing data set.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Let’s return to the Ames data set. In this example, there are many more samples (<span class="math inline">\(n\)</span>=2930) than predictors (<span class="math inline">\(p\)</span>=73). While this is true, <a href="#fig-ames-missing" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ames-missing</span></a> clearly illustrates that a few of the predictors were missing for the vast majority of the samples. The proportion of missing values across the predictors is shown in <a href="#fig-ames-missing-distribution" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ames-missing-distribution</span></a>. This figure reveals that 4 predictors have more than 20% missing sample values. Because the percentage of missing values is large, these predictors would be candidates for removal. In the hotel rate data, the agent and company predictors had missing values of 22.7% and 91%, respectively. In our experience, we have tended to remove predictors that have more than 20%-30% missing values. This percentage is simply a point of guidance and not a hard rule to be applied to every data set. After removing these predictors, the Ames data set now has 0.7% missing values and the hotel rate data set had 0.05% missing values.</p>
<p>After removing <em>predictors</em> with excessive missing data, we can then consider removing <em>samples</em> with excessive missing data. No sample in the Ames data had more than 12.1% of missing predictors. The samples with the greatest percentage of missing predictors were those that had missing basement information. No sample in the hotel rate data had more than 3.6% of missing predictors. Neither of these percentages is large enough to merit the removal of these samples. Therefore, we will keep these samples and utilize an imputation or encoding procedure to fill in these gaps.</p>
<p>There are a couple of caveats to removing predictors or samples based on missing information. First, the missing information in a predictor may be informative for predicting the response. For the hotel rate data, the missing status of the agent variable is associated with the response. Therefore, it may be better to encode the missing status for this variable rather than eliminate the variable altogether. This approach will be discussed below. Second, as seen in <a href="#fig-missing-naive-bayes" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-missing-naive-bayes</span></a>, removing samples may create a bias in the remaining data which would impact the predictive ability of the model on future samples.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/missing-data.html#sec-removing-missing-rows"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a> <a href="https://tidymodels.aml4td.org/chapters/missing-data.html#sec-removing-missing-cols"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-imputation" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="sec-imputation"><span class="header-section-number">4.2.3</span> Imputation</h3>
<p>Imputation is the process of using other existing information (i.e., the predictors) to estimate what each missing value might have been. In other words, we will build an imputation model to fill in the missing column(s) so that we can run the primary machine-learning model. A separate imputation model is required for each column that contains (or could contain) missing data.</p>
<p>Imputation is a well-researched statistical methodology. This technique has traditionally been used for inferential models, focusing on maintaining the validity of test statistics to support hypothesis testing. As mentioned in earlier chapters, there is an important distinction between the objectives of statistical inference and prediction accuracy. See <span class="citation" data-cites="sperrin2020missing">Sperrin et al. (<a href="#ref-sperrin2020missing" role="doc-biblioref">2020</a>)</span> for a discussion related to imputation.</p>
<p>In the statistical literature <span class="citation" data-cites="d2024behind">(<a href="#ref-d2024behind" role="doc-biblioref">D’Agostino McGowan, Lotspeich, and Hepler 2024</a>)</span>, two terms can help us understand the distinction:</p>
<ul>
<li><p><strong>Deterministic imputation</strong> creates a single model to estimate the missing predictor’s value using one or more predictors in the training set<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. After imputation, the new values are treated as known (i.e., not random variables).</p></li>
<li><p><strong>Stochastic imputation</strong> involves creating many imputation models with the goal of creating a distribution of possible values for the missing data. It is used primarily to conduct proper inferential analyses and includes traditional multiple imputation techniques.</p></li>
</ul>
<p>One important concept heavily featured in subsequent chapters is resampling, where we’ll train our preprocessors and supervised models using slightly different data sets. The goal of resampling is to estimate model performance accurately. Although resampling will involve repeated re-imputation of missing data, it is not a stochastic imputation method.</p>
<p>Let’s highlight a couple of key distinctions when considering imputation for the purpose of inference versus prediction. One is that inferential models generally make assumptions about the statistical distributions of the predictors. Conversely, many machine learning models, like support vector machines, tree-based models, and neural networks, do not make such assumptions. Therefore, stochastic imputation is less relevant for predictive models. A second difference is that multiple imputation methods focus on understanding relationships within the existing data, while predictive models aim for generalizable relationships to unseen samples. This difference has implications on when imputation should be applied to the data, which will be discussed later in <a href="#sec-when-to-address-missing-data" class="quarto-xref"><span class="quarto-unresolved-ref">sec-when-to-address-missing-data</span></a>. Finally, in some cases, including the <em>outcome</em> data in the imputation model might make sense. This is inappropriate for our goals here (i.e., deterministic imputation). While <span class="citation" data-cites="d2024behind">D’Agostino McGowan, Lotspeich, and Hepler (<a href="#ref-d2024behind" role="doc-biblioref">2024</a>)</span> explains the theoretical reasons why this is the case, our objections can be viewed as purely functional: if we require the outcome to make decisions, we cannot predict new samples where the outcome is unknown.</p>
<p>What are the important characteristics of an imputation technique that will be used in a prediction model? There are a few that we will focus on:</p>
<ul>
<li><p>Tolerate other missing data: as we saw in the Ames data, multiple variables within a sample may be missing. Therefore, an imputation technique should be feasible in the presence of other missing data.</p></li>
<li><p>Handle different predictor types: many data sets have different variables, such as numeric, categorical, and ordinal. The method should be able to accommodate numeric and qualitative predictors seamlessly and without changing the nature of the data (e.g., categorical predictors should be be converted to indicator columns).</p></li>
<li><p>Produce efficient prediction equations: each predictor with missing data will require an equation. Imputation for large data sets, when used with resampling approaches during model training, will increase computation time as well as the size of the imputation equation. Therefore, the more efficient the imputation approach, the less computation time will be needed to obtain the final model.</p></li>
<li><p>Ensure robustness: the method should be stable and not overly affected by outliers.</p></li>
<li><p>Be accurate: the results should be close to what the actual value would have been<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p></li>
</ul>
<p>By considering each of these characteristics, imputation can effectively alleviate missing data problems, enhancing models’ quality and predictive performance. There are several imputation methods that meet most or all of the above characteristics. These imputation techniques fall into two general categories: most likely value and model-based (i.e., “regression imputation”).</p>
<p>To illustrate how these methods work, we will use two simple, simulated two-predictor classification data sets. <a href="#fig-imputation-simulation-data" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-simulation-data</span></a> displays the simulated data. For these data sets, the optimal class separation is defined by the black lines. We will initially focus on the nonlinear data set. From these data set, 10% of the samples will be randomly selected. Of these samples, half will have the value of first predictor deleted, while the other half will have the second predictor deleted.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-imputation-simulation-data" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-imputation-simulation-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="missing-data_files/figure-html/fig-imputation-simulation-data-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-imputation-simulation-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.6: Two complete two-class simulated datasets with 200 data points, one with a linear relationship between the predictors and the other nonlinear. The optimal partitions of the classes are represented by the black lines.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="https://tidymodels.aml4td.org/chapters/missing-data.html#imputation"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
<section id="sec-encoding-missing-data" class="level3" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="sec-encoding-missing-data"><span class="header-section-number">4.2.4</span> Encoding Missing Data</h3>
<p>When a predictor takes categorical values, an alternative approach to imputation is encoding. Consider the basement exposure variable in the original Ames data set. Possible exposure values are: “good,” “average,” “minimum,” and “no exposure” and contains 83 missing values. The most likely value approach would impute the missing values with the “no exposure” category since this is the most frequent category. A model-based approach would utilize information across the rest of the variables to predict which of the 4 available categories would be best. Would imputing with one of the 4 available categories be a good approach for the missing samples? In this case, the answer is “no!”. With a little more investigation, we can see that the reason the exposure variable contains missing values for these houses is because these houses <em>do not have basements</em>.</p>
<p>When a value is missing for a specific reason like we see with the basement exposure variable, encoding the missing information will be more informative to the models. In an encoding procedure, we simply acknowledge that the value is missing by creating a new category such as “unknown,” “unspecified,” or “not applicable.” In the case of basement exposure, a more appropriate categorization would be “no basement.”</p>
<p>The encoding procedure is mostly used for categorical variables. However, there are times when encoding can be applied to continuous variables. In many analytical procedures, an instrument cannot reliably provide measurements below a specified limit. Measurements of samples above the limit are returned as numeric values, but measurements below the limit are returned as either “BLQ” (below lower limit of quantitation) or “&lt; 3.2”, where 3.2 is the lower limit of quantitation. These values cannot be included with the continuous values. What can we do in this situation? One approach would be to impute the BLQ values with a reasonable numeric value less than lower limit of quantitation<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. This information could also be encoded by creating a new variable that contains two values, “ALQ” and “BLQ”, which would identify samples that could and could not be measured. If this variable is related to the outcome, then the ability to measure the quantity may be predictively informative.</p>
</section>
</section>
<section id="sec-imputation-methods" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-imputation-methods"><span class="header-section-number">4.3</span> Specific Imputation Methods</h2>
<p>Technically, almost any model that can make a prediction is a potential candidate for imputing data. However, some are better than others and we summarize a few of the most used below.</p>
<section id="sec-imputation-most-likely" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="sec-imputation-most-likely"><span class="header-section-number">4.3.1</span> Most Likely Value</h3>
<p>The simplest approach to imputation is to replace a missing value with its most likely value based on that predictor’s non-missing values. For numeric data, the most likely value can be summarized by the mean or median. For categorical data, the most likely value is the mode (or most frequent value). Most likely value imputation meets many of the desirable characteristics for imputation. They can tolerate missing values from other predictors (because they operate one-predictor-at-a-time) and can handle different predictor types while also producing efficient prediction equations. Furthermore, achieving a robust imputation can be done using either the median or the trimmed mean <span class="citation" data-cites="barnett1994outliers">(<a href="#ref-barnett1994outliers" role="doc-biblioref">Barnett and Lewis 1994</a>)</span>. The trimmed mean is the mean of the middle-most samples. For example, we could compute the mean of the samples with 5% of the the most extreme (smallest and largest) samples removed. Doing this would minimize the impact of any extreme sample values on the computation of the mean. However, imputing with the most likely value for a single predictor may not produce an imputed value that is close to the true value.</p>
<p>To demonstrate, we have imputed the missing values in the simulated data using the mean and median imputation techniques for both data sets seen in <a href="#fig-imputation-simulation-data" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-simulation-data</span></a>. <a href="#fig-imputation-exploration" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-exploration</span></a> shows the values of the original samples that were selected to have missing values along with the imputed values based on mean and median imputation.</p>
<p>For both data sets, the imputed values are towards the center over the overall plot and away from the parabolic/linear regions where we know the actual data are. The median imputed values are less affected by extreme values and are closer to the region of actual data. Both techniques, however, are unable to place most of the missing data near their true values.</p>
<p>For many data sets, and especially for sets with a minimal number of missing values, most likely value imputation may be sufficiently good for what we need. The procedure is fast but is terribly inaccurate. But there may be occasions when when we need the imputations to be closer to the true (yet unknown) values.</p>
<div id="fig-imputation-exploration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-imputation-exploration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="figure-content">
<pre class="shinylive-r" data-engine="r"><code>#| label: shiny-imputation-exploration
#| out-width: "80%"
#| viewerHeight: 425
#| standalone: true

library(shiny)
library(ggplot2)
library(dplyr)
library(recipes)
library(bslib)

source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R")
source("https://raw.githubusercontent.com/aml4td/website/main/R/shiny-imputation-exploration.R")

app</code></pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-imputation-exploration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.7: A visualization of the four new features for different linear embedding methods. The data shown are the validation set results from the two simulations shown in <a href="#fig-imputation-simulation-data" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-simulation-data</span></a>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-imputation-linear" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="sec-imputation-linear"><span class="header-section-number">4.3.2</span> Linear Methods</h3>
<p>When a data set has many predictors and the predictors have some correlation with each other, then multiple linear regression can be a more effective imputation technique than the most likely value methods. That is, multiple linear regression will utilize the information contained in other predictors to estimate an imputed value that will be closer to what the actual value would have been. Multiple linear regression also produces a straightforward, compact prediction equation for each predictor with missing values which is computationally efficient when applied to new samples. However, this technique cannot naturally handle missing values in the predictors used to develop the imputation model. In practice, therefore, the imputation approach utilizes a complete case analysis.</p>
<p>Returning to the simulation illustration, the linear imputation method creates two simple linear regression models, where one model uses predictor 1 as the response and predictor 2 as the predictor, and the other model reverses the roles of the predictors.</p>
<p>Selecting the nonlinear data set in <a href="#fig-imputation-exploration" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-exploration</span></a> illustrates the impact of this technique. Assuming that we have conducted some exploratory data analysis, we would be aware of the nonlinear relationship between the predictors. As such, the imputation model for the second predictor included an additional squared term. This greatly improves the imputation quality. Unfortunately, the same approach is not possible for imputing the first predictor (which is poorly predicted).</p>
<p>However, if these predictors were linearly related, then this technique would generate values closer to the actual values. This can be seen by choosing the linear data set in <a href="#fig-imputation-exploration" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-exploration</span></a>.</p>
<p>Since this imputation method is basically linear regression, we can use it for slightly more sophisticated results. For example, if we want to impute a predictor’s values based on some other predictors categories, linear regression can achieve this. There is also the possibility of interactions, splines, and so on (as long as the “predictors” in the imputation model are not missing themselves).</p>
<p>When the predictor that requires imputation is categorical, then logistic or multinomial regression can be used to generate an imputed category.</p>
<p><a href="#fig-imputation-distance-comparison" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-distance-comparison</span></a> provides another visual comparison of the imputation techniques. In this figure, each point represents the Euclidean distance from the imputed sample to its known location. The mean, median, and linear methods have similar distributions of distances.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-imputation-distance-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-imputation-distance-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="missing-data_files/figure-html/fig-imputation-distance-comparison-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-imputation-distance-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.8: A comparison of the distribution of distances between the actual and imputed samples across imputation techniques for the nonlinear data shown in <a href="#fig-imputation-simulation-data" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-simulation-data</span></a>. The mean, median, and linear techniques perform similarly, while K-NN and bagging generate imputed values that are modestly closer to the actual values.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-imputation-knn" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="sec-imputation-knn"><span class="header-section-number">4.3.3</span> Nearest Neighbors</h3>
<p><span class="math inline">\(K\)</span>-nearest neighbor calculations will be discussed in <a href="#sec-mds" class="quarto-xref"><span class="quarto-unresolved-ref">sec-mds</span></a>, where it will be used in some multidimensional scaling methods, and in <a href="#sec-knn-cls" class="quarto-xref"><span class="quarto-unresolved-ref">sec-knn-cls</span></a> in the context of a supervised model. In short, when imputing a missing predictor value for a new sample, the <span class="math inline">\(K\)</span> most similar samples from the training set are determined (using all complete predictor values). The <span class="math inline">\(K\)</span> predictor values for the predictor of interest are summarized via the mean (for numeric outcomes) or the mode. This summary statistic is used to fill in the missing value. This is a localized version of the most likely value approach.</p>
<p><a href="#fig-imputation-exploration" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-exploration</span></a> shows that a nearest-neighbor approach generally increases the accuracy of the imputation, especially for the first predictor. For the linear case, both predictors have high-quality imputations.</p>
</section>
<section id="sec-imputation-trees" class="level3" data-number="4.3.4">
<h3 data-number="4.3.4" class="anchored" data-anchor-id="sec-imputation-trees"><span class="header-section-number">4.3.4</span> Trees</h3>
<p>As mentioned earlier, tree-based models are a reasonable choice for imputation techniques because many types of trees do not require complete data themselves. They generally provide good accuracy and do not extrapolate values beyond the bounds of the training data. While a single tree can be used for imputation, it will likely have low bias but high variance. Ideally, we would like imputed values to have low bias as well as low variance.</p>
<p>Tree ensembles, such as bagging and random forests (Chapters <a href="#sec-trees-cls" class="quarto-xref"><span class="quarto-unresolved-ref ref-noprefix">sec-trees-cls</span></a> and <a href="#sec-trees-reg" class="quarto-xref"><span class="quarto-unresolved-ref ref-noprefix">sec-trees-reg</span></a>), help solve this issue since they blend the predictors from many individual tree models. However, these methods can be computationally taxing for moderate- to large-sized data sets. Specifically, random forests require many trees (hundreds to thousands) to achieve a stable and reliable imputation model. This comes at the cost of a large computational footprint, which may become a challenge as the number of predictors with missing data increases; a separate model must be trained and retained for each predictor. As discussed later, bagged tree models tend to be smaller and faster than their random forest counterparts. Typically, there is a marginal loss in accuracy from using bagging instead of random forests.</p>
<p>Like nearest-neighbor imputation, the bagged tree places most of the imputed values close to the original data distribution, as seen in <a href="#fig-imputation-exploration" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-exploration</span></a>. The distribution of distances (<a href="#fig-imputation-distance-comparison" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-distance-comparison</span></a>) for bagged imputation is similar to that of nearest neighbors. <a href="#fig-imputation-exploration" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-imputation-exploration</span></a> provides a comparison of the imputation techniques discussed here with the nonlinear simulated data and a data set in which the two groups are optimally separated by a linear boundary. In this comparison, the percentage of missing data can be adjusted to demonstrate how each imputation method performs.</p>
<p>Which imputation technique we choose depends on the problem. If a few predictors have missing values, then using <span class="math inline">\(K\)</span>-NN or bagged imputation may not add much computational burden to the modeling process. However, if many predictors have missing values and the data set is large, then the model-based imputation techniques may become cumbersome. In this case, beginning with a most likely value imputation approach may be prudent. If the predictive performance of the optimally tuned machine learning technique is high, then the imputation approach was sufficient. However, if predictive performance lags, we could implement a different imputation technique while training models.</p>
<p><a href="https://tidymodels.aml4td.org/chapters/missing-data.html#imputation"><i class="fa-brands fa-r-project fa-Large" aria-label="r-project"></i></a></p>
</section>
</section>
<section id="sec-when-to-address-missing-data" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="sec-when-to-address-missing-data"><span class="header-section-number">4.4</span> When to Address Missing Data</h2>
<p>When missing values are present, it may be tempting to immediately address this problem and create a complete data set prior to beginning the modeling or data splitting processes. We recommend that imputation be done as part of the model development process as discussed in <a href="#sec-whole-game" class="quarto-xref"><span class="quarto-unresolved-ref">sec-whole-game</span></a>. In the diagram presented in <a href="#fig-model-building-process" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-model-building-process</span></a>, imputation would occur as part of the training process. Recall, the model based imputation techniques discussed earlier have parameters that can be tuned. Understanding how these parameters affect model performance would be done during the training process, and an optimal value can be selected.</p>
<p>It is also important to understand that imputation is fundamentally a preprocessing step–we must do this before commencing model building. Therefore, we must think about where imputation should be located in the order of preprocessing steps. Specifically, imputation should occur as the first step. It is advisable to impute qualitative predictors before creating indicator variables to maintain the binary nature of the resulting data. Moreover, imputation should precede parameter estimation steps. For example, if centering and scaling are done before imputation, the resulting means and standard deviations will reflect biases and issues from the missing data. This approach ensures the integrity and accuracy of subsequent data processing and analysis stages.</p>
</section>
<section id="chapter-references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="chapter-references">Chapter References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-allison2002missing" class="csl-entry" role="listitem">
Allison, P. 2002. <em>Missing Data</em>. SAGE Publications, Inc.
</div>
<div id="ref-barnett1994outliers" class="csl-entry" role="listitem">
Barnett, V, and T Lewis. 1994. <em>Outliers in Statistical Data</em>. Vol. 3. 1. Wiley New York.
</div>
<div id="ref-bhaskaran2014difference" class="csl-entry" role="listitem">
Bhaskaran, K, and L Smeeth. 2014. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=What+is+the+difference+between+missing+completely+at+random+and+missing+at+random+&amp;as_ylo=2014&amp;as_yhi=2014&amp;btnG=">What Is the Difference Between Missing Completely at Random and Missing at Random?</a>”</span> <em>International Journal of Epidemiology</em> 43 (4): 1336–39.
</div>
<div id="ref-breiman2001random" class="csl-entry" role="listitem">
Breiman, L. 2001. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Random+forests&amp;as_ylo=2001&amp;as_yhi=2001&amp;btnG=">Random Forests</a>.”</span> <em>Machine Learning</em> 45: 5–32.
</div>
<div id="ref-breiman1984classification" class="csl-entry" role="listitem">
Breiman, L, J Friedman, C Stone, and RA Olshen. 1984. <em>Classification and Regression Trees</em>. CRC Press.
</div>
<div id="ref-chen2016xgboost" class="csl-entry" role="listitem">
Chen, T, and C Guestrin. 2016. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Xgboost+A+scalable+tree+boosting+system&amp;as_ylo=2016&amp;as_yhi=2016&amp;btnG=">Xgboost: A Scalable Tree Boosting System</a>.”</span> In <em>Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 785–94.
</div>
<div id="ref-d2024behind" class="csl-entry" role="listitem">
D’Agostino McGowan, L, S Lotspeich, and S Hepler. 2024. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=The+Why+behind+including+Y+in+your+imputation+model&amp;as_ylo=2024&amp;as_yhi=2024&amp;btnG=">The "Why" Behind Including <span>"Y"</span> in Your Imputation Model</a>.”</span> <em>Statistical Methods in Medical Research</em> 33 (6): 996–1020.
</div>
<div id="ref-emmanuel2021survey" class="csl-entry" role="listitem">
Emmanuel, T, T Maupong, D Mpoeleng, T Semong, B Mphago, and O Tabona. 2021. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=A+survey+on+missing+data+in+machine+learning&amp;as_ylo=2021&amp;as_yhi=2021&amp;btnG=">A Survey on Missing Data in Machine Learning</a>.”</span> <em>Journal of Big Data</em> 8: 1–37.
</div>
<div id="ref-hasan2021missing" class="csl-entry" role="listitem">
Hasan, K, A Alam, S Roy, A Dutta, T Jawad, and S Das. 2021. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Missing+value+imputation+affects+the+performance+of+machine+learning+A+review+and+analysis+of+the+literature+2010+2021+&amp;as_ylo=2021&amp;as_yhi=2021&amp;btnG=">Missing Value Imputation Affects the Performance of Machine Learning: A Review and Analysis of the Literature (2010-2021)</a>.”</span> <em>Informatics in Medicine Unlocked</em> 27: 100799.
</div>
<div id="ref-apm" class="csl-entry" role="listitem">
Kuhn, M, and K Johnson. 2013. <em><span>Applied Predictive Modeling</span></em>. Springer.
</div>
<div id="ref-lex2014upset" class="csl-entry" role="listitem">
Lex, A, N Gehlenborg, H Strobelt, R Vuillemot, and H Pfister. 2014. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=+UpSet+visualization+of+intersecting+sets&amp;as_ylo=2014&amp;as_yhi=2014&amp;btnG="><span>UpSet</span>: Visualization of Intersecting Sets</a>.”</span> <em><span>IEEE</span> Transactions on Visualization and Computer Graphics</em> 20 (12): 1983–92.
</div>
<div id="ref-little2019statistical" class="csl-entry" role="listitem">
Little, R, and D Rubin. 2019. <em>Statistical Analysis with Missing Data</em>. Vol. 793. John Wiley &amp; Sons.
</div>
<div id="ref-nijman2022missing" class="csl-entry" role="listitem">
Nijman, SWJ, AM Leeuwenberg, I Beekers, I Verkouter, JJL Jacobs, ML Bots, FW Asselbergs, KGM Moons, and TPA Debray. 2022. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Missing+data+is+poorly+handled+and+reported+in+prediction+model+studies+using+machine+learning+a+literature+review&amp;as_ylo=2022&amp;as_yhi=2022&amp;btnG=">Missing Data Is Poorly Handled and Reported in Prediction Model Studies Using Machine Learning: A Literature Review</a>.”</span> <em>Journal of Clinical Epidemiology</em> 142: 218–29.
</div>
<div id="ref-quinlan1993c4" class="csl-entry" role="listitem">
Quinlan, R. 1993. <em><span>C4.5</span>: Programs for Machine Learning</em>. Morgan Kaufmann Publishers.
</div>
<div id="ref-sisk2023imputation" class="csl-entry" role="listitem">
Sisk, R, M Sperrin, N Peek, M van Smeden, and G Martin. 2023. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Imputation+and+missing+indicators+for+handling+missing+data+in+the+development+and+deployment+of+clinical+prediction+models+a+simulation+study&amp;as_ylo=2023&amp;as_yhi=2023&amp;btnG=">Imputation and Missing Indicators for Handling Missing Data in the Development and Deployment of Clinical Prediction Models: A Simulation Study</a>.”</span> <em>Statistical Methods in Medical Research</em> 32 (8): 1461–77.
</div>
<div id="ref-sperrin2020missing" class="csl-entry" role="listitem">
Sperrin, M, G Martin, R Sisk, and N Peek. 2020. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Missing+data+should+be+handled+differently+for+prediction+than+for+description+or+causal+explanation&amp;as_ylo=2020&amp;as_yhi=2020&amp;btnG=">Missing Data Should Be Handled Differently for Prediction Than for Description or Causal Explanation</a>.”</span> <em>Journal of Clinical Epidemiology</em> 125: 183–87.
</div>
<div id="ref-webb2010naive" class="csl-entry" role="listitem">
Webb, G, E Keogh, and R Miikkulainen. 2010. <span>“<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C7&amp;q=Na+i+ve+Bayes+&amp;as_ylo=2010&amp;as_yhi=2010&amp;btnG=">Na<span>ı̈</span>ve Bayes.</a>”</span> <em>Encyclopedia of Machine Learning</em> 15 (1): 713–14.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Exceptions are listed below.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>However, note that these properties have complete values for their size column (i.e., a zero ft<sup>2</sup> garage). <a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Also called not missing at random (NMAR), <em>informative missingness</em>, or <em>nonignorable</em> missing data.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="sisk2023imputation">Sisk et al. (<a href="#ref-sisk2023imputation" role="doc-biblioref">2023</a>)</span> calls this approach, when using multiple predictors for imputation, as “regression imputation.” Their definition would exclude simple “most likely value” imputations.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Unfortunately, we may not be able to judge the accuracy of the imputation for most data sets since the missing data will never be known.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>One approach is to generate a random uniform predictor value between zero and the limit of quantitation).<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/aml4td\.org");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/initial-data-splitting.html" class="pagination-link" aria-label="Initial Data Splitting">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Initial Data Splitting</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/numeric-predictors.html" class="pagination-link" aria-label="Transforming Numeric Predictors">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transforming Numeric Predictors</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js" type="text/javascript"></script>
<script type="text/javascript">
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>




</body></html>