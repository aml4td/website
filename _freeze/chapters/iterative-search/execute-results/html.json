{
  "hash": "77a264b4f3fea094aa91966192b5d959",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/iterative/\"\n---\n\n\n\n\n# Iterative Search {#sec-iterative-search}\n\n\n\n\n\n\n\n\n\nGrid search is a static procedure; we predetermine which candidates will be evaluated before beginning. How can we adaptively optimize tuning parameters in a sequential manner? Perhaps more importantly, _when_ is this a good approach? \n\nPreviously, we’ve seen that a plateau of good performance in the parameter space is possible. This is often the case but will not always be true. If the region of optimal performance is small, we must create a large space-filling design to find it. When there are many tuning parameters, the computational expense can be unreasonable. An even worse situation is one where the previously described speed-up techniques (such as the submodel trick from @sec-submodels) are not applicable. Racing can be very efficient, but if the training set is very large, it may be that using a validation set is more appropriate than multiple resamples; in this case, racing cannot be used. Finally, there are practical considerations when the data set is very large. Most parallel processing techniques require the data to be in memory for each parallel worker, restricting their utility. \n\nGenerally, we are not often constrained by these issues _for models used for tabular data_. However, there are a few models that might be better optimized sequentially. The first two that come to mind are large neural networks^[Sections [-@sec-cls-svm] and [-@sec-reg-svm]]. and support vector machines^[Sections [-@sec-cls-nnet] and [-@sec-reg-nnet]] (SVMs) [@scholkopf2001;@cristianini2004]. For the former, as the number of layers increases, so does the number of tuning parameters. Neural networks also have many important tuning parameters associated with the model’s training, such as the learning rate, regularization parameters, etc. These models also require more preprocessing than others; they are suspectable to the effects of uninformative predictors, missing data, and collinearity. Depending on the data set, there can be many pre-model tuning parameters. \n\nSupport vector machines are models with fewer tuning parameters than neural networks but with similar preprocessing requirements. Unfortunately, the tuning parameter space can have large areas of poor performance with \"islands\" where the model works well. The location of these can change from data set to data set. We’ll see an example of this shortly where two model parameters are tuned.\n\nThese two models, in particular, are more likely to benefit from an optimization method that chooses candidates as the process evolves. \n\nIn theory, any iterative optimization procedures can be used. In general, gradient methods, such as steepest descent or Newton’s method, are the most commonly used technique for nonlinear optimization. These tools are suboptimal when it comes to parameter tuning but play important parts in other areas of machine learning and, for this reason, they will be discussed later. \n\nDerivative-free techniques can be helpful for model tuning. Traditional examples are genetic algorithms, simulated annealing, particle swarm optimization, etc. We’ll consider the first two of these in Sections [-@sec-sim-anneal] and [-@sec-genetic-algo]. Currently, the most well-known iterative tool for tuning models is Bayesian optimization. This will be examined in @sec-bayes-opt. However, before this, @sec-bayes will flesh out what it means for a model to be Bayesian. \n\nTo get started, let's revisit a data set. \n\n## Example: Predicting Barley Amounts using Support Vector Machines {#sec-barley-svm}\n\n\n\n\n\n\n\n\n\nWe’ll return to the data previously seen in @sec-barley where we want to predict the percentage of barley oil in a mixture. Recall that the predictors are extremely correlated with one another. In @sec-embeddings, we considered embedding methods like PCA to preprocess the data and these models were able to achieve RMSE values of about 6% (shown in @fig-barley-linear-bakeoff). \n\nIn this chapter, we’ll model these data in two different scenarios. First, we’ll use them as a \"toy problem\" where only two tuning parameters are optimized for a support vector machine model. This is a little unrealistic, but it allows us to visualize how iterative search methods work in a 2D space. Second, in @sec-bayes-opt-nnet, we’ll get serious and optimize a larger group of parameters for neural networks simultaneously with additional parameters for a specialized preprocessing technique. \n\nLet's start with the toy example that uses an SVM regression model. This highly flexible nonlinear model can represent the predictor data in higher dimensions using a _kernel transformation_ [@hofmann2008]. This type of function combines numeric predictor vectors from two data points using a dot product. There are many different types of kernel functions, but for a _polynomial_ kernel, it is: \n\n$$\nk(\\boldsymbol{x}_1, \\boldsymbol{x}_2) = (a\\boldsymbol{x}_1'\\boldsymbol{x}_2 + b)^q\n$$ {#eq-kernel-poly}\n\nwhere $a$ is called the scaling factor, $b$ is a constant offset value, and $q$ is the polynomial degree. The dot product of predictor vectors ($\\boldsymbol{x}_i$) measures both angle and distance between points. Note that for the kernel function to work in an appropriate way, the two vectors must have elements with consistent units (i.e., they have been standardized). \n\nThe kernel function operates much like a polynomial basis expansion; it projects a data point into a much larger, nonlinear space. The idea is that a more complex representation of the predictors might enable the model to make better predictions. \n\nFor our toy example, we'll take the 550 predictors, project them to a smaller space of 10 principal components, then standardize those features to have the same mean and standard deviation. A quartic polynomial is used with zero offset. The scale parameter $a$ will be tuned. It helps define how much influence the dot product has in the polynomial expansion. \n\nThe most commonly tuned SVM parameter is independent of the different kernel functions: the _cost value_. This dictates how much a poorly predicted model should penalize the performance metric (when evaluated on the training set). Higher cost values will encourage the SVM to become increasingly more complex. We've previously seen the effect of modulating the SVM cost value in @fig-two-class-overfit. For small costs, the SVM classification model did not try too hard to classify samples correctly. When increased to an extreme, it significantly overfit the training set points.\n\nJust as in @sec-embeddings, the RMSE will be computed from a validation set and these statistics will be used to guide our efforts. \n\nLet's define a wide  space for our two tuning parameters: cost will vary from 2<sup>-10</sup> to 2<sup>10</sup> and the scale factor^[Why do these two parameters use different bases? It’s mostly a convention. Both parameters have valid values that can range across many orders of magnitude. The change in cost tends to occur more slowly than the scaling factor, so a smaller base of 2 is often used.] is allowed to range from 10<sup>-10</sup> to 10<sup>-0.1</sup>. \n\n@fig-svm-grid visualizes the RMSE across these ranges^[These are not simulated data, so this surface is an approximation of the true RMSE via the validation set using a very large regular grid. The RMSE results are estimates and would change if we used different random numbers for data splitting.] where darker colors indicate smaller RMSE values. The lower left diagonal area is a virtual \"dead zone\" with very large RMSE results that don't appear to change much. There is also a diagonal wedge of good performance (symbolized by the darker colors). The figure shows the location of the smallest RMSE value and a diagonal ridge of parameter combinations with nearly equal results. Any points on this line would produce good models (at least for this toy example). Note that there is a small locale of excessively large RMSE values in the upper right. Increasing both tuning parameters to their upper limits is a bad idea.  \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A visualization of model performance when only the SVM cost and scaling factor parameters are optimized. It shows the combination with the smallest RMSE and a ridge of candidate values with nearly equivalent performance.](../figures/fig-svm-grid-1.svg){#fig-svm-grid fig-align='center' width=50%}\n:::\n:::\n\n\n\n\nThe next section will discuss gradient descent optimization and why it isn’t a good choice for model tuning. The two subsequent sections will discuss how two traditional global search methods, simulated annealing and genetic algorithms, can be used to search the parameter space. After this, the focus is on Bayesian optimization. Finally, we end with an extended analysis of the barley prediction problem. \n\n## Sidebar: Gradient-Based Optimization {#sec-gradient-opt}\n\nTo formalize the idea of optimization, we must have an **objective function** that defines what we are trying to optimize. This can also be called a loss function (specifically to be minimized). In either case, this value is signified by $\\psi()$. The parameters that modify $\\psi()$ are denoted as $\\boldsymbol{\\theta}$, which is $p \\times 1$ in dimension and is assumed to be real numbers. We'll also assume that $\\psi()$ is smooth and generally differentiable and, without losing generality, assume that _smaller values are better_. \n\nWe’ll denote the first derivative ($\\psi'(\\boldsymbol{\\theta})$), for simplicity, as $g(\\boldsymbol{\\theta})$ ($p\\times 1$). The matrix of second deriviatives, called the Hessian matrix, is symbolized as $H(\\boldsymbol{\\theta})$ ($p\\times  p$). \n\nWe start with an initial guess, $\\boldsymbol{\\theta}_0$, and compute the gradient at this point, yielding a $p$ dimensional directional vector. To get to our next parameter value, simple gradient descent uses the update:\n\n$$\n\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\alpha\\:g(\\boldsymbol{\\theta}_i)\n$$ {#eq-gd-step}\n\nThe value $\\alpha$ defines how far we should travel in our new direction. This can be a constant value^[We’ve seen $\\alpha$ before when it was called the learning rate (and will revisit it later in this chapter).] or a secondary procedure, called a line search, can be used to increase the value of $\\alpha$ until the objective function worsens.\n\nWe proceed to iterate this process until some measure of convergence is achieved. For example, the optimization could be halted if the objective function does not improve more than a very small value. @lu2022gradient and @zhang2019gradient are excellent introductions to gradient-based optimization that is focused on training models.\n\nAs a simple demonstration, @fig-grad-descent shows $\\psi(\\theta) = \\theta\\: cos(\\theta/2)$ for values of $\\theta$ between $\\pm 10.0$. We want to minimize this function. There is a _global_ minimum at about $\\theta \\approx 6.85$ while there are _local_ minima at $\\theta = -10.0$ and $\\theta \\approx -1.72$. These are false solutions where some search procedures might become trapped. \n\n:::: {.columns}\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"90%\"}\n\n::: {#fig-grad-descent}\n\n::: {.figure-content}\n\n```{shinylive-r}\n#| label: fig-grad-descent\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# source(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nlight_bg <- \"#fcfefe\" # from aml4td.scss\ngrid_theme <- bs_theme(\n  bg = light_bg, fg = \"#595959\"\n)\n\ntheme_light_bl<- function(...) {\n  \n  ret <- ggplot2::theme_bw(...)\n  \n  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n  ret$panel.background  <- col_rect\n  ret$plot.background   <- col_rect\n  ret$legend.background <- col_rect\n  ret$legend.key        <- col_rect\n  \n  ret$legend.position <- \"top\"\n  \n  ret\n}\n\nui <- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(xs = c(3, 3, 3, 3), sm = 4),\n    column(\n      width = 12,\n      sliderInput(\n        \"start\",\n        label = \"Starting Value\",\n        min = -6,\n        max = 10,\n        value = 3,\n        step = 1\n      )\n    ),\n    column(\n      width = 12,\n      numericInput(\n        \"rate\",\n        label = \"Leaning Rate\",\n        min = 0,\n        max = 3,\n        value = 2,\n        step = 0.1\n      ),\n      checkboxInput(\n        \"decay\",\n        label = \"Decay?\",\n        value = FALSE\n      )\n    ),\n    column(\n      width = 12,\n      sliderInput(\n        \"iter\",\n        label = \"Iteration\",\n        min = 0L,\n        max = 50L,\n        step = 1L,\n        value = 0L,\n        width = \"100%\"\n      )\n    )\n  ),\n  as_fill_carrier(plotOutput(\"iterations\"))\n)\n\nserver <- function(input, output) {\n  library(ggplot2)\n  library(dplyr)\n  \n  cyclic <- function(epoch, initial = 0.001, largest = 0.1, step_size = 5) {\n    if (largest < initial) {\n      tmp <- initial\n      largest <- initial\n      initial <- tmp\n    } else if (largest == initial) {\n      initial <- initial / 10\n    }\n    cycle <- floor( 1 + (epoch / 2 / step_size) )\n    x <- abs( ( epoch / step_size ) - ( 2 * cycle) + 1 )\n    initial + ( largest - initial ) * max( 0, 1 - x)\n  }\n  decay_time <- function(epoch, initial = 0.1, decay = 1) {\n    initial / (1 + decay * epoch)\n  }  \n  \n  fn <- function(x) x * cos(.5 * x) \n  # D(quote(x * cos(.5 * x)), \"x\")\n  deriv_1 <- function(x) cos(0.5 * x) - x * (sin(0.5 * x) * 0.5)\n  \n  # ------------------------------------------------------------------------------\n\n  max_iter <- 50\n  mx <- max_iter + 1\n  \n  x_seq <- seq(-10, 10, length.out = 1000)\n  y_seq <- fn(x_seq)\n\n  # ------------------------------------------------------------------------------\n  \n  p_base <- \n    dplyr::tibble(x = x_seq, y = y_seq) %>% \n    ggplot(aes(x, y)) +\n    geom_line(linewidth = 1, alpha = 1 / 4) +\n    labs(x = \"Parameter\", y = \"Objective Function\") +\n    theme_bw()\n  \n  output$iterations <-\n    renderPlot({\n      x_cur <- input$start\n      res <- dplyr::tibble(x = rep(NA, mx), y = rep(NA, mx), iter = 0:max_iter)\n      res$x[1] <- x_cur\n      res$y[1] <- fn(x_cur)\n      \n      if (input$iter >= 1) {\n        for(i in 1:input$iter) {\n          \n          if (input$decay) {\n            # rate <- cyclic(i, initial = input$rate / 2, largest = input$rate)\n            rate <- decay_time(i - 1, initial = input$rate, decay = 1)\n          } else {\n            rate <- input$rate\n          }\n          \n          x_new <- x_cur - rate * deriv_1(x_cur)\n          y_new <- fn(x_new)\n          res$x[i+1] <- x_new\n          res$y[i+1] <- y_new\n          x_cur <- x_new\n        }\n        deriv_val <- deriv_1(x_cur) # for printing\n      }\n      # start 3, lr 2 is good example\n      p <- \n        p_base +\n        geom_point(data = res %>% filter(x >= -10 & x <= 10),\n                   aes(col = y), \n                   show.legend = FALSE, cex = 2, \n                   alpha = 3 / 4) +\n        geom_vline(xintercept = x_cur, col = \"black\", lty = 3) +\n        geom_rug(data = res %>% filter(x >= -10 & x <= 10), \n                 aes(x = x, y = NULL, col = y),\n                 alpha = 1 / 2,\n                 show.legend = FALSE) + \n        scale_color_viridis_c(\n          option = \"plasma\",\n          breaks = 0:max_iter,\n          limits = extendrange(y_seq)\n        )\n      \n      if (input$iter > 0) {\n        lbl <- paste(\"gradient:\", signif(deriv_val, digits = 3))\n        if (input$decay) {\n          lbl <- paste0(lbl, \", learning rate:\", signif(rate, digits = 3))\n        }\n      } else {\n        lbl <- \"initial guess\"\n      }\n      p <- p + labs(title = lbl) \n      \n      print(p)\n      \n    }, res = 100)\n}\n\napp <- shinyApp(ui, server)\napp\n```\n\n:::\n\nAn example of simple gradient descent for the function $\\psi(\\theta) = \\theta\\: cos(\\theta/2)$.\n\n:::\n\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n:::: \n\nThe figure enables the choice of $\\theta_0$, the value of $\\alpha$, and how many iterations to used. Consider a few configurations:\n\n- Starting at $\\theta = 3$, a learning rate of $\\alpha = 2.0$ is inappropriate. The search initially moves towards the global minimum but then reverses course and jumps past the best value, then becomes trapped around one of the local optima. \n\n- If we keep $\\theta = 3$ and decrease learning rate to $\\alpha = 1.0$, we quickly find the best result. \n\n- However, if we tried decreasing the learning rate too low, say $\\alpha = 0.01$, the optimization moves too slowly. \n\nClearly, the learning rate is important.  The starting value also matters; values less than 2.0 never appear to reach the optimum. \n\nThis gradient descent algorithm outlined above is extremely basic. There are far more complex versions, the most well-known of which is the Newton-Raphson method (a.k.a. Newton’s Method) that incorporates the second derivative matrix $H(\\boldsymbol{\\theta})$ in the updating formula^[However, computing the Hessian matrix increases the computational cost by about 3-fold.]. \n\nWe often know _when_ gradient-based optimization will work well. If we know the equation for the objective function, we can determine its properties, such as whether it is a convex function, and theory can tell us if a global optimum can be found via the use of gradients. For example, squared error loss $\\psi(\\boldsymbol{\\theta}) = (y-\\hat{y})^2$, is convex when  the relationship for the predicted value $\\hat{y}$ is a well-behaved function of $\\boldsymbol{\\theta}$ (such as in linear regression). \n\nHowever, there are additional considerations when it comes to optimizations for predictive models. First, since data are not deterministic, the loss function is not only a random variable but can be excessively noisy. This noise can have a detrimental effect on how well the optimization proceeds. \n\nSecond, our objective function is usually a performance metric (such as RMSE) and not all metrics are mathematically well-behaved (or smooth or convex). In the case of deep neural networks, the model equations can lead to a non-convex function even when the objective function itself is simple (e.g., squared error loss).  In this case, we must worry that our optimized parameters correspond to a local optimum, not a global one. This can occur because traditional gradient methods are _greedy_; they proceed in a direction that is always optimal for the current estimates of $\\boldsymbol{\\theta}$. This makes a lot of sense and is mostly effective. However, with complex or non-standard objective functions, the optimization can get stuck in a local optimum.\n\nThere are additional complications related to model where the $\\boldsymbol{\\theta}$ values are the tuning parameters. These might not be real numbers. For example, the number of spline terms is a whole number, and the updating equation might yield a fractional value for the number of terms. Still other parameters are qualitative in nature, such as the type of activation function in a neural network.\n\nDuring model tuning, we know to avoid repredicting the training set (due to overfitting). Procedures such as resampling make the evaluation of $\\psi()$ computationally expensive since multiple models should be trained. Unless we use symbolic gradient equations, the numerical process of approximating the gradient vector $g(\\boldsymbol{\\theta})$ can require a large number of function evaluations. \n\nLater, we’ll discuss *stochastic gradient descent* (SGD)[@udl2023,Chapter 6]. This variation is often used to train very complex networks when there is a large amount of training data. SGD is a way to approximate the gradient vector using a small subset of the data at a time. While this does cause variation in the direction of descent, this variation often has the beneficial side effect of helping to escape from local minima. Also, when there are large amounts of training data, computer memory may be insufficient to hold the entirety of the data. SGD may be the only solution because it does not require all of the data at once (and can also be much faster).\n\nUntil then, the next three chapters will describe different stochastic optimization, gradient-free  methods that are well-suited for parameter tuning since they can sidestep some of the issues described above. \n\n## Simulated Annealing  {#sec-sim-anneal}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nNon-greedy search methods are not constrained to always proceed in the absolute best direction (as defined by the gradient). One such method is _simulated annealing_ (SA)[@spall2005introduction;@kirkpatrick1983optimization]. It is a controlled random search that moves in random directions but with some amount of control over the path. It can also incorporate restarts if the algorithm moves into clearly poor regions.\n\nGiven a starting value, SA creates a random perturbation of the current candidate solution, often residing within some local neighborhood of the current value.  The objective function is evaluated and compared to the previous value. If the new candidate produces an improvement, the process continues by using this candidate to make the next step. If it is not, there are two options: \n\n - We can accept the current value as \"suboptimal\" and base our next perturbation on it, or\n - discard the current solution as if it did not occur. The next candidate point is a perturbation of the last \"acceptable\" point. \n\nFor our SVM example, suppose we start with a candidate where $x_0 = \\left[log_2 (cost), log_{10} (scale)\\right[$ was $x_0 = [-10, -0.1]$ and had an associated RMSE value of 6.10%. For the next candidate, a perturbation of this values is created, say $x_1 = [-7.11, -0.225]$. Suppose that the corresponding RMSE was measured at 5.97%. Since $x_1$ has a better performance metric it is automatically accepted and is used to create the next parameter. \n\nHowever, suppose that the RMSE value for $x_1$ was 7.00%, meaning that the new candidate did worse than the initial one.  \n\nSimulated annealing generates a probability threshold for accepting the worse solution. Typically, the probability depends on two quantities: \n\n* The difference in prior and current estimates of the objective function. If the new candidate is nearly as good as the current one, the probability of acceptance should be larger than that of a severely unsatisfactory candidate. \n* The probability of acceptance should decrease over search iterations. An exponentially decaying function, called a \"cooling schedule,\" often used.\n\nAn often used equation for the probability, assuming that smaller values are better, is \n\n$$\nPr[accept] = \\exp\\Bigl[-i\\bigl(\\hat{Q}(\\boldsymbol{\\theta}_{i}) - \\hat{Q}(\\boldsymbol{\\theta}_{i-1})\\bigr)\\Bigr]\n$$\n\nwhere $i$ is the iteration number. To compare 6.1% versus 7.0%, the acceptance probability is 40.7%. To make the determination, a random uniform number is generated and is used to compare to a random number to decide on the next step. If this difference were to occur at a later iteration, say $i = 5$, the probability would drop to 1.1%.\n\nOne small matter is related to the scale of the objective function. The difference in the exponent is very sensitive to scale. If, for example, instead of percentages we were to use the proportions 0.61 and 0.70, the probability of acceptance would change from 40.7% to 91.4%. One way to mitigate this issue is to use a normalized difference by dividing the raw difference by the previous objective function (i.e., (0.70-0.61) / 0.70). This is the approach used in the SA analyses here. \n\nThis process is continued until either a pre-defined number of iterations is reached or if there is no improvement by a certain number of iterations. The best result found at any time in the optimization is used as the final value; there is no real notion of \"convergance\" for this method. Also, as previously mentioned, a restart rule can stop simulated annealing from delving too far into suboptimal regions if no best results have occurred within some period of time. When the processes is restarted, can be continue from either the last best candidate point from previous iterations or at a random point in the parameter space. \n\nHow should the candidates be perturbed from iteration to iteration? When the tuning parameters are all numeric, we can create a random distance and angle from the current values. A similar process can be used for integers by \"flooring\" them to the nearest whole number. For qualitative tuning parameters, a random subset of parameters is chosen to change to a different value chosen at random. The amount of change should be large enough to search the parameters space and potentially get out of a local optimum.\n\nIt is also important to conduct computations on the parameters in their potentially transformed space. This ensures that the entire range of the parameter values is treated equally. For example, we’ve been working with the values on their log (base 2) scale for the SVM cost parameter. When perturbing the values, we make sure to change them in the log space. This is true of all of the other search methods described in this chapter. \n\nTo illustrate, a very small initial space-filling design with three candidates was generated and evaluated. The results are in @tbl-svm-initial. To start the SA search^[These values will be also used as the initial substrate for Bayesian optimization.], we will start with the candidate with the smallest RMSE and proceed for 50 iterations without a rule for early stopping. A restart to the last known best results was enforced after eight suboptimal iterations. \n\n:::: {.columns}\n\n::: {.column width=\"25%\"}\n:::\n\n::: {.column width=\"50%\"}\n\n::: {#tbl-svm-initial}\n\n\n\n\n::: {#tbl-svm-initial .cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-responsive\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> RMSE (%) </th>\n   <th style=\"text-align:right;\"> Cost (log-2) </th>\n   <th style=\"text-align:right;\"> Scale (log-10) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 6.10 </td>\n   <td style=\"text-align:right;\"> -10 </td>\n   <td style=\"text-align:right;\"> -0.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 7.33 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> -5.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 17.04 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -10.0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nThe initial set of candidates used for iterative search with the toy example from @fig-svm-grid. One candidate does poorly while the other two have relatively similar results. \n\n:::\n\n:::\n\n::: {.column width=\"25%\"}\n:::\n\n:::: \n \n@fig-sa-example contains an animation of the results of the SA search. In the figure, the initial points are represented by open circles, and a grey diagonal line shows the ridge of values exists that corresponds to the best RMSE results. \n\n:::: {.columns}\n\n::: {.column width=\"15%\"}\n:::\n\n::: {.column width=\"70%\"}\n\n::: {#fig-sa-example}\n\n::: {.figure-content}\n\n```{shinylive-r}\n#| label: fig-sa-example\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\n\n# TODO add these apps to their own R files on GH\nui <- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  sliderInput(\n    \"iter\",\n    label = \"Iteration\",\n    min = 1L,\n    max = 50L,\n    step = 1L,\n    value = 1L,\n    width = \"100%\"\n  ),\n  as_fill_carrier(plotOutput(\"path\")),\n  renderText(\"notes\")\n)\n\nserver <- function(input, output) {\n  \n  load(url(\"https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_sa.RData\"))\n  load(url(\"https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_large.RData\"))\n  \n  num_cuts <- 50\n  rd_or <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, \"OrRd\")))(num_cuts)\n  \n  # ------------------------------------------------------------------------------\n  \n  x_rng <- 10^extendrange(c(-10, -1/10))\n  y_rng <- 2^extendrange(c(-10, 10))\n  \n  log10_labs <- trans_format(\"log10\", math_format(10^.x, function(x) format(x, digits = 3)))\n  log2_labs <- trans_format(\"log2\", math_format(2^.x, function(x) format(x, digits = 3)))\n\n  # ------------------------------------------------------------------------------\n  \n  sa_history <- \n    sa_history %>% \n    mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts)))\n  \n  # TODO pre-compute these\n  sa_init <- \n    sa_history %>% \n    filter(.iter == 0)\n  best_init <- \n    sa_init %>% \n    slice_min(mean) %>% \n    select(.iter, cost, scale_factor, RMSE)\n  poor_init <- \n    anti_join(sa_init, best_init, by = c(\".iter\", \"cost\", \"scale_factor\")) %>% \n    select(.iter, cost, scale_factor)\n  \n  initial_plot <- \n    regular_mtr %>%\n    mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts))) %>% \n    ggplot(aes(scale_factor, cost, col = RMSE)) +\n    geom_point(data = sa_init, cex = 3, pch = 1, col = \"black\") +\n    geom_line(\n      data = regular_mtr %>% slice_min(mean, n = 18),\n      stat = \"smooth\",\n      col = \"black\",\n      method = lm,\n      se = FALSE,\n      formula = y ~ x,\n      alpha = 1 / 8,\n      linewidth = 2\n    ) + \n    scale_x_log10(limits = x_rng,\n                  labels = log10_labs,\n                  expand = expansion(add = c(-1 / 5, -1 / 5))) +\n    scale_y_continuous(limits = y_rng, trans = \"log2\", labels = log2_labs,\n                       expand = expansion(add = c(-1/2, -1/2))) +\n    scale_color_manual(values = rd_or, drop = FALSE) +\n    coord_fixed(ratio = 1/2) +\n    theme_bw() +\n    theme(legend.position = \"none\")\n  \n  # ------------------------------------------------------------------------------\n  \n  output$path <-\n    renderPlot({\n      \n      current_path <- \n        paths[[input$iter]] %>% \n        mutate(mean = RMSE,\n               RMSE = cut(RMSE, breaks = seq(5, 31, length = num_cuts)))\n      last_best <- \n        iter_best[[input$iter]] %>% \n        mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts)))\n      \n      last_best_iter <-\n        last_best %>% \n        pluck(\".iter\")\n\n      lab <- iter_label[[input$iter]]\n      \n      current_plot <- \n        initial_plot +\n        geom_path(data = current_path, col = \"black\", alpha = 1 / 5) +\n        geom_point(\n          data = current_path %>% filter(.iter > 0),\n          aes(scale_factor, cost, col = RMSE),\n          cex = 2\n        ) +\n        geom_point(data = last_best, cex = 4, aes(col = RMSE), pch = 8,\n                   show.legend = FALSE) +\n        labs(x = \"Scaling Factor\", y = \"Cost\", title = lab) \n        \n      print(current_plot)\n      \n    }, res = 100)\n  \n  output$notes <-\n    renderText({\n      current_path <- \n        resolve_sa_path(sa_history, input$iter) %>% \n        anti_join(poor_init, by = c(\".iter\", \"cost\", \"scale_factor\"))\n      describe_result(current_path)\n    })\n}\n\napp <- shinyApp(ui, server)\n```\n\n:::\n\nAn example of how simulated annealing can investigate the tuning parameter space. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n:::\n\n:::\n\n::: {.column width=\"15%\"}\n:::\n\n:::: \n\nDuring the search, there were 4 iterations where a new global best result was discovered (iterations 1, 14, 15, and 40). There were also 5 restarts  at iterations 9, 23, 31, 39, and 48. In the end, the best results occurred with a cost value of 2<sup>-2.5</sup> and a scale factor of 10<sup>-0.98</sup>. The corresponding validation set RMSE was 5.78%. With this random seed, the search gets near the ridge of best performance shown in @fig-svm-grid but only lingers there for short times. It does spend a fair amount of time meandering in regions of poor performance. \n\nSimulated annealing has some attractive qualities. First, the process of creating a new candidate works for any type of parameter (e.g., real, integer, or qualitative). This isn’t the case for the two other iterative methods we’ll discuss. Also, the perturbation process is very fast; there is very little computational overhead to acquire the next objective function value. Also, since we are effectively simulating new candidate sets, we can potentially apply constraints to parameters or groups of parameters. For example, if a tuning parameter could only be odd integers, this would not be a significant problem for simulated annealing. \n\nThere are a few downsides to this method. Compared to the other search methods, SA makes small incremental changes. If we start far away from the optimum, many iterations might be required to reach it.  One way to mitigate this issue is to do a small space-filling design and start from the best point (as we did). In fact, applying SA search after a grid search (perhaps using racing) can be a good way to verify that the grid search was effective. \n\nAnother disadvantage is that a single candidate is processed at a time. If the training set size is not excessive, we could parallel process the multiple candidates simultaneously. We could make a batch of perturbations and pick the best value to keep or apply the probabilistic process of accepting a poor value. \n\nThis optimization took 31.5s per candidate to execute for these data. \n\n## Genetic Algorithms  {#sec-genetic-algo}\n\nGenetic algorithms (GAs)[@Mitchell1996AnIT;@eiben2015introduction] are an optimization method that mimics the process of evolution via natural selection. Our application (parameter tuning) isn't well suited to GAs; a standard GA search will use hundreds to millions of objective function evaluations. We will use a small search due to practical considerations. Despite this, genetic algorithms can often find a solution near the optimal value fairly quickly and, as previously stated, there is often a region of acceptable candidates. Finally, GAs are very unlikely to become stuck in a locally optimal space. \n\nInstead of search iterations, genetic algorithms are counted in _generations_. A generation is a group of candidate values that are evaluated at the same time (as a batch)^[Think of the candidates within a generation as a population of people.]. Once their corresponding performance metrics are computed, a small set of the best candidates is selected and is used to create the next generation via _reproduction_. Reproduction would entail combining a pair of \"parent\" candidates by swapping information^[Analogous to chromosomes.], and then random mutations can be applied. Once the next generation is created, the process continues for some pre-defined time limit or maximum number of generations. \n\nHow, exactly, does this work? The first step is to pick a numerical representation for each candidate. Let’s consider methods for our main types of data. \n\n#### Real-Valued Parameters {.unnumbered}\n\nFor real numbers, there are two encoding methods: one option is to keep them as-is (i.e., floating point values) and another converts the values to a binary encoding.\n\nWhen keeping the values as real numbers, we can make two children from a pair of well-performing candidates via a linear combination. For candidate vectors $\\boldsymbol{\\theta}_j$, we can use:\n\n$$\n\\begin{align}\n\\boldsymbol{\\theta}^{\\:kid}_1 &= \\alpha \\boldsymbol{\\theta}^{\\:par}_1 + (1 - \\alpha) \\boldsymbol{\\theta}^{\\:par}_2 \\notag \\\\\n\\boldsymbol{\\theta}^{\\:kid}_2 &= (1-\\alpha) \\boldsymbol{\\theta}^{\\:par}_1 + \\alpha \\boldsymbol{\\theta}^{\\:par}_2 \\notag \\\\\n\\end{align}\n$$\n\nwhere $\\alpha$ is a random standard uniform number. Notice that the two children's candidate values will always be in-between the values of their parents. \n\nFor mutation, a candidate’s values are either locally perturbed or simulated with rate $\\pi_m$. For example, we might mutate the log cost value by simulating a random number across the range that defines its search space. \n\nBinary encodings for real numbers were suggested in the early days of genetic algorithms. In this case, each candidate value is encoded as a set of binary integers. For example, consider a log<sub>2</sub> cost value of -2.34. To convert this to binary, we multiply it by 100 (assuming a limit of two decimal places) to convert it to an integer. If we use 8 binary digits (a.k.a. \"bits\") to represent 234, we get `11101010`. If the candidates could have both positive and negative numbers, we can add an extra bit at the start that is 1 when the value is positive, yielding:  `011101010`. \n\nThe method of _cross-over_ was often used for the reproduction of binary representations. For a single cross-over, a random location between digits was created for each candidate value, and the binary digits were swapped. For example, a representation with five bits might have parents `ABCDE` and `VWXYZ.`  If they were crossed over between the second and third elements, the children would be `ABXYZ` and `VWCDE`. There are reproduction methods that use multiple cross-over points to create a more granular sharing of information. \n\nThere are systematic bias that can occur when crossing binary representations as described by @rana1999distributional and @Soule2009. For example, there are positional biases. For example, if the first bits (`A` and `V`) capture the sign of the value, the sign is more likely to follow the initial bits than the later bits to an offspring. \n\nFor mutating a binary representation, each child’s bit would be flipped at a rate of $\\pi_m$. \n\nAfter these reproduction and mutation, the values are decoded into real numbers. \n\n#### Integer Parameters {.unnumbered}\n\nFor integers, the same approach can be used as real numbers, but after the usual operations, the decimal values are coerced to integers via rounding. If a binary encoding is used, the same process can be used for integers as real numbers; they are all just bits to the encoding process. \n\n#### Qualitative Parameters {.unnumbered}\n\nFor qualitative tuning parameters, one (inelegant) approach is to encode them into values on the real line or as integers. For example,  for a parameter with values \"red,\", \"blue,\" and \"green\", we could map them to bins of `[0, 1/3)`, `[1/3, 2/3)`, and `[2/3, 1]`. From here, we reproduce and mutate them as described above, then convert them back to their non-numeric categories. This is more palatable when there is a natural ordering of the values but is otherwise a workable but unfortunate approach. Mutation is simple though; at rate $\\pi_m$, a value is flipped to a random selection of the possible values.\n\n### Assembling Generations {#sec-ga-generations}\n\nNow that we know how to make new candidates, we can form new generations. We start by choosing the size. Normally, the minimum population size within a generation is at least 25-50 candidates. This may be infeasible depending on the computational cost of the model and the characteristics of the training set, such as size. We can use fewer points in the generation, but the risk is that we will never sample any acceptable results. In that case, combining the best results only results in more mediocre candidates. Increasing the mutation rate may help avoid this problem. For the first iteration, random sampling of the space is often used. Unsurprisingly, we strongly recommend that a space-filling design for the initial candidate set. \n\nFinally, there is the choice of which candidates to reproduce at each generation. There are myriad techniques for selecting which candidates are used to make the next generation of candidates. The simplest is to pick two parents by randomly selecting them with probabilities that are proportional to their performance (i.e., the best candidates are chosen most often). There is also the idea of _elitism_ in selection. Depending on the size of the generation, we could retain a few of the best-performing candidates from the previous generation. The performance values of these candidates would not have to be recomputed (saving time), and their information is likely to persist in a few good candidates for the next generation.\n\n### Two Parameter Example {#sec-2D-example}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nTo illustrate genetic algorithms in two dimensions, a population size of 8 was used for 7 generations. These are _not_ good default values but were instead chosen to be at parity with the previous SA search and the optimization method used in the next section. The values of the two tuning parameters were kept as floating point values. The parental selection was conducted using sampling weights proportional to the RMSE values (smaller being better), a mutation rate of 10%, and a single candidate was retained at each generation via elitism.  The computations within a generation where computed in parallel. In parallel, this optimization took 17.2s per candidate (on average).  @fig-ga-example illustrates the results of the search.   \n\n:::: {.columns}\n\n::: {.column width=\"15%\"}\n:::\n\n::: {.column width=\"70%\"}\n\n::: {#fig-ga-example}\n\n::: {.figure-content}\n\n```{shinylive-r}\n#| label: fig-ga-example\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\n\nui <- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  sliderInput(\n    \"gen\",\n    label = \"Generation\",\n    min = 1L,\n    max = 7L,\n    step = 1L,\n    value = 1L,\n    width = \"100%\"\n  ),\n  as_fill_carrier(plotOutput(\"generations\"))\n)\n\nserver <- function(input, output) {\n  \n  num_cuts <- 50\n  rd_or <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, \"OrRd\")))(num_cuts)\n  \n  load(url(\"https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_ga.RData\"))\n  load(url(\"https://raw.githubusercontent.com/aml4td/website/main/RData/two_param_iter_large.RData\"))\n\n  ga_history$mean <- ga_history$fitness\n  ga_history <- \n    ga_history %>% \n    mutate(RMSE = cut(mean, breaks = seq(5, 31, length = num_cuts)))\n\n  # ------------------------------------------------------------------------------\n  \n  x_rng <- 10^extendrange(c(-10, -1/10))\n  y_rng <- 2^extendrange(c(-10, 10))\n  \n  log10_labs <- trans_format(\"log10\", math_format(10^.x, function(x) format(x, digits = 3)))\n  log2_labs <- trans_format(\"log2\", math_format(2^.x, function(x) format(x, digits = 3)))\n  \n  # ------------------------------------------------------------------------------\n  \n  output$generations <-\n    renderPlot({\n      \n      last_best <- \n        ga_history %>%\n        filter(generation <= input$gen) %>% \n        slice_min(mean)\n      \n      current_gen <- ga_history %>% filter(generation == input$gen) \n      \n      p <- \n        ga_history %>%\n        ggplot(aes(scale_factor, cost)) +\n        geom_line(\n          data = regular_mtr %>% slice_min(mean, n = 18),\n          stat = \"smooth\",\n          col = \"black\",\n          method = lm,\n          se = FALSE,\n          formula = y ~ x,\n          alpha = 1 / 8,\n          linewidth = 2\n        ) +\n        geom_point(data = current_gen, aes(col = RMSE), alpha = 2/4, cex = 2, \n                   show.legend = FALSE) +\n        geom_point(data = last_best, cex = 4, aes(col = RMSE), pch = 8,\n                   show.legend = FALSE) +\n        labs(x = \"Scaling Factor\", y = \"Cost\") +\n        scale_x_log10(limits = x_rng,\n                      labels = log10_labs,\n                      expand = expansion(add = c(-1 / 5, -1 / 5))) +\n        scale_y_continuous(limits = y_rng, trans = \"log2\", labels = log2_labs,\n                           expand = expansion(add = c(-1/2, -1/2))) +\n        scale_color_manual(values = rd_or, drop = FALSE) +\n        coord_fixed(ratio = 1/2) +\n        theme_bw() +\n        theme(legend.location = \"none\")\n      \n      print(p)\n      \n    }, res = 100)\n}\n\napp <- shinyApp(ui, server)\n```\n\n:::\n\nSeveral generations of a genetic algorithm that search tuning parameter space. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n:::\n\n:::\n\n::: {.column width=\"15%\"}\n:::\n\n:::: \n\nA space-filling design was used to ensure that the initial population was diverse. Fortuitously, one design point was very close to the ridge, with an RMSE of 5.83%.  After this, there were 4 generations with better candidates (with almost identical performance): 5.82%, 5.772%, 5.765%, and 5.764%. We can see that, after three generations, the search is concentrated around the ridge of optimal performance. In later generations, some candidates have outliers in one dimension; this is the effect of mutation during reproduction. \n\n### Summary {#sec-ga-summary}\n\nThis small toy example is somewhat problematic for demonstrating genetic algorithms. The example’s low parameter dimensionality and the relatively low computational expense per candidate might deceive one into believing that genetic algorithms could be the answer to most of our problems.  Genetic algorithms are versatile and powerful tools for global optimization. Like simulated annealing, there is very little overhead between generations. However, in most real applications, there are more than two parameter dimensions. This means that larger populations are required and, most likely, many more generations. If the data size is not massive, we've seen that parallel processing can help^[If you have access to high-performance computing infrastructure,  the parallel processing can run the individuals in the population in parallel and, within these, any resampling iterations can also be parallelized.]. \n\nNow we'll take a look at what makes a Bayesian model Bayesian.\n\n## Sidebar: Bayesian Models {#sec-bayes}\n\nWe've superficially described an application of Bayesian analysis in @sec-effect-encodings. Before discussing Bayesian optimization, we should give a general description of Bayesian analysis, especially since it will appear again several times after this chapter. \n\nMany models make probabilistic assumptions about their data or parameters. For example, a linear regression model has the form\n\n$$\ny_i = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p + \\epsilon_i\n$$\n\nUsing ordinary least squares estimation, we can make assumptions regarding the model errors ($\\epsilon_i$). We can assume that the residuals are independent of one another and follow a Gaussian distribution with zero mean and a constant standard deviation. From there, it follows that the regression parameters ($\\beta$ coefficients) also follow Gaussian distributions. \n\nBased on these assumptions, our objective function is the Gaussian likelihood function, generally denoted as $\\ell(z|\\theta)$, although we often maximize the log of this value. We fix the outcome and predictor data and try to find values of $\\sigma$ and the $\\beta$  parameters that maximize the objective function (log $\\ell(z|\\theta)$). This process is maximum likelihood estimation. \n\n::: {.important-box}\nOne important point is that each model parameter is treated as a single value. Our maximum likelihood estimate (MLE) is a point estimate and, based on our assumptions about the residuals, we know the distribution of the MLEs. \n:::\n\nThe consequence of this is that we cannot make inferences about the true, unknown model parameters since they are considered to be single points. Instead, our inference focuses on the MLEs. This leads to the circuitous explanation of hypothesis tests and confidence intervals. For example, the explaination of a 90% confidence interval is:\n\n> \"We believe that if we were to repeat this experiment a large number of times, the true parameter value would fall between $L$ and $U$ 90% of the time.\" \n\nA Bayesian approach uses a different philosophy regarding our probability assumptions. It assumes that the unknown parameters are drawn from a prior distribution that reflects what we know about them. We'll denote our prior distribution as $\\pi(\\theta)$. The \"prior\" adjective is important; the modeler should specify it before seeing the observed data. \n\nFor example, recall our model for the time to deliver food in @eq-log-linear. A reasonable prior for the regression parameter associated with distance would have a distribution that has zero probability for values less than zero. We would never think that (log) delivery times decrease with distance. If we worked at the restaurant, we might be able to be more specific based on what we've seen. There might be a basic belief that, all other things are equal, and we might think that every mile further away from the restaurant increases time two-fold. From there, we could find a probability distribution that would fit those assumptions. \n\nOnce we have our prior distributions for our parameters and assumptions regarding our data distributions, we can write down the equations required to estimate our results. Bayes' Rule is a basic probability statement that combines the prior distribution with our likelihood function: \n\n$$\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) \\ell(x|\\theta)} {\\pi(x)}\n$$ {#eq-bayes-rule-distr}\n\n$\\pi(\\theta|x)$ is the posterior distribution: the probability distribution of our parameters, given the observed data. This is the endpoint for any Bayesian analysis. \n\nAnother important point is that Bayesian estimation has a much more difficult goal than maximum likelihood estimation.  The latter needs to find point estimates of its parameters while Bayesian estimation has to estimate the entire posterior distribution $\\pi(\\theta|x)$. In a moment, we’ll look at a simple problem with a simple solution. However, in most other cases, the computational requirements for Bayesian estimation are considerably higher. That's the bad news.\n\nThe good news is that, once we find the posterior distribution, it is incredibly useful. First, we can make direct statements about parameter values. Unlike confidence intervals, Bayesian methods allow us to say things like \n\n> \"We believe that there is a 90% probability that the true value of the parameter is between $L$ and $U$.\" \n\nIt also lets us easily make similar statements regarding more complex combinations of parameters (such as ratios, etc).\n\nFinally, we should mention the effect of the prior on the computations. It does pull the likelihood $\\ell(x|\\theta)$ towards it. For example, suppose that for @eq-log-linear we used a highly restrictive prior for $\\beta_1$ that was a uniform distribution between [0.7, 0.9]. In that case, the posterior would be confined to this range^[We’ll see how the prior can affect some calculations later in @sec-naive-bayes. ]. That said, the effect of the prior on the posterior _decreases_ as our training set size _increases_. We saw this in @sec-effect-encodings where two travel agents were contrasted; one with many reservations in the data and another with very few. \n\nTo illustrate, let’s look at a very simple example. \n\n### A Single Proportion  {#sec-single-proportion}\n\n\n\n\n\n\n\n\n\nFor the forestry data last seen in @sec-spatial-resampling, the outcome is categorical with values \"yes\" and \"no\" for the question \"Is this location forested?\" We might use the training data set to estimate the rate ($\\pi$) that the event occurs. The simplest statistic is the sample proportion: the number of events divided by the number of total data points: ($\\hat{\\pi}$ = 56.185%). \n\nTo estimate this rate using maximum likelihood estimation, we might assume that the data follow a binomial distribution with theoretical probability $\\pi$. From there we can solve equations that find a value of $\\pi$ that correspond to the largest likelihood. It turns out that the sample proportion is also the maximum likelihood estimate. \n\nInstead of treating the unknown parameter as a single value, Bayesian methods propose that the parameter comes from a _distribution_ of possible values. This distribution reflects our prior understanding or belief about the parameter based on what we know of the situation. For this reason, it is called a _prior_ distribution. If we think that locations in Washington state are very likely to be forested, we would choose a distribution that has more area for higher probabilities. \n\nFor a binomial model, an example of a prior is the Beta distribution. The Beta distribution is very flexable and has values range between zero and one. It is indexed by two parameters: $\\alpha$ and $\\beta$. @fig-beta-dist shows different versions of the Beta for different values.  \n\nTo settle on a specific prior, we would posit different questions such as:\n\n- \"Are there rate values that we would _never_ believe possible?\"\n- \"What is the most likely value that we would expect and how certain are we of that?\" \n- \"Do we think that the distribution of possible values is symmetric?\"\n\nand so on.\n\nFrom these answers, we would experiment with different values of $\\alpha$ and $\\beta$ until we find a combination that encapsulates what we believe. If we think that larger probabilities of forestation are more likely, we might choose a Beta prior that has values of $\\alpha$ that are larger than $\\beta$, which places more mass on larger values (as seen below). This is an example of an _informative prior_, albeit a weak one. As our prior distribution is more peaked it reflects that, before seeing any data, we have strong beliefs. If we honestly have no idea, we could choose a uniform distribution between zero and one using $\\alpha = \\beta = 1$.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Examples of the Beta distribution for different values of $\\alpha$ and $\\beta$.](../figures/fig-beta-dist-1.svg){#fig-beta-dist fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nWe'll use values of $\\alpha = 5$ and $\\beta = 3$. If the training set has $n_{tr}$ points and $n_+$ of them are known to be forested, the ordinary sample proportion estimate is $\\hat{p} = n_+ / n_{tr}$. In a Bayesian analysis, the final estimate is a function of both the prior distribution and our observed data. For a Beta prior, the Bayesian estimate is \n\n$$\n\\hat{p}_{BB} = \\frac{n_+ +\\alpha}{n_{tr} + \\alpha + \\beta}\n$$ {#eq-beta-bin-mean}\n\nIf, before seeing the data, we had chosen $\\alpha = 5$ and $\\beta = 1$, we estimate that $\\hat{p}_{BB} = 56.219$%. This is pretty close to our simple estimate because there is so much data in the training set ($n_{tr}$ = 4,834) that the influence of the prior is severely diminished. As a counter-example, suppose we took a very small sample that resulted in $n_y$ = 1 and $n_{tr}$ = 3, the prior would pull the estimate from the MLE of 33.3% to 54.5%. \n\nNow suppose that for regulatory purposes, we were required to produce an interval estimate for our parameter. From these data and our prior, we could say that the true probability of forestation has a 90% chance of being between 55% and 57.4%. \n\nWe did mention that it can be very difficult to compute the posterior distribution. We deliberately chose the Beta distribution because, if we assume a binomial distribution for our data and a $Beta(\\alpha, \\beta)$ prior, the posterior is  $Beta(\\alpha + n_y, n_{tr} - n_y + \\beta)$. More often than not, we will not have a simple analytical solution for the posterior. That said, we’ll see this happen again in @sec-gp.\n\nFor a Bayesian model, the predictions also have a posterior _distribution_, reflecting the probability of a wide range of values. As with non-Bayesian models, we often summarize the posterior using the most likely value (perhaps using the mean or mode of the distribution). We can also measure the uncertainty in the prediction using the estimated standard deviation. \n\n::: {.note-box}\nTo summarize: \n\n - Bayesian models require a prior distribution for all of our model parameters. \n - The prior and observed data are combined into a posterior distribution.\n - Different statistics can be estimated from the posterior, including individual predictions.  \n:::\n\nWe’ll encounter even more Bayesian methods in subsequent chapters. In @sec-cls-metrics, we'll describe the process of developing a prior for muticlass problem based on the results of Wordle scores.\n\nNow that we know more about Bayesian statistics, let's see how they can be used to tune models.\n\n## Chapter References {.unnumbered}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}