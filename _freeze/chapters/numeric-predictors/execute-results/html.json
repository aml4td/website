{
  "hash": "a616f1151bd079adb99e4ef776f83b64",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/transformations/\"\n---\n\n\n# Transforming Numeric Predictors {#sec-numeric-predictors}\n\n\n\n\n\n\n\n## Embeddings\n\nWhen there are a multitude of predictors, it might make sense to condense them into a smaller number of artificial features. This smaller set should represent what is essential in the original data to be useful. This process is often called _feature extraction_ or _manifold learning_. We’ll use a more general term currently en vogue: **embeddings**. Embedding methods can be used for more than feature extraction. For example, some are designed to convert non-numeric data (such as text) into a more usable numeric format. \n\nThis section will examine two primary classes of embedding methods that can achieve multiple purposes. First, we’ll consider linear methods that take a numeric input matrix $X$ that is $n \\times p$ and create a different, probably smaller set of features $X^*$ ($n \\times m$) using the transformation $X^* = XA$. After describing linear methods, consider a different class of transformations that focus on the distances between data points called _multidimensional scaling_. These tools create a new set of features that are not necessarily linear combinations of the original features but often use some of the same math as the linear techniques. \n\nBefore beginning, we'll introduce another data set that will be used here as well forthcoming chapters. \n\n### Example: Predicting Barley Amounts\n\n@larsen2019deep and @pierna2020applicability describe a data set where laboratory measurements are used to predict what percentage of a mixture was lucerne, soy oil, or barley oil^[Retreived from [`https://chemom2019.sciencesconf.org/resource/page/id/13.html`](https://chemom2019.sciencesconf.org/resource/page/id/13.html)]. An instrument is used to measure how much of particular wavelengths of light are absorbed by the mixture to help determine chemical composition. We will focus on using the lab measurements to predict the percentage of barley oil in the mixture. The distribution of these values is shown in @fig-barley-data(a). \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![(a) The distribution of the outcome for the entire data set. The bar colors reflect the percent barley distribution and are used in subsequent sections. (b) Selected training set spectra for four barley samples. Each line represents the set of 550 predictors in the data.](../figures/fig-barley-data-1.svg){#fig-barley-data fig-align='center' width=60%}\n:::\n:::\n\n\nNote that most of the data have very little barley oil. About 27% of the data are less than 1% and the median barley oil percentage is 5.96%.\n\nThe 550 predictors are the light absorbance for sequential values in the light region of interest (believed to be from wavelengths between 1300 to 2398). @fig-barley-data(b) shows a selection of four samples from the data. The darker lines represent samples with lower barley content. \n\nThese predictor values, called _spectra_, have a very high serial correlation between predictor values in a sample. The median correlation between the predictors was 0.98. This is a major factor for the analysis of these data and we use it here; a high degree of between-predictor correlation can easily undermine some types of models. However, some embedding methods are especially suited for decorrelating predictors. \n\nIn the computations that follow, each predictor was standardized using the orderNorm transformation mentioned earlier (unless otherwise noted). \n\nThe data originated from a modeling competition to find the most accurate model and specific samples were allocated to training and test sets. However, there were no outcome values for the test set; our analysis will treat the 6,915 samples in their training set as the overall pool of samples. This is enough data to split into separate training ($n_{tr} =$ 4,839) , validation ($n_{val} =$ 1,035), and test sets ($n_{te} =$ 1,041). Stratified sampling, using the outcome data, was used to split the initial data pool. \n\n\n### Linear Transformations\n\n### Distance-Based Methods\n\nMultidimensional scaling (MDS) [@torgerson1952multidimensional] is a feature extraction tool that creates embeddings that try to preserve the geometric distances between training set points. In other words, the distances between points in the smaller dimensions should be comparable to those in the original dimensions. Since the methods in this section use distances, the predictors should be standardized to equivalent units before the embedding is trained.  \n\nTake @fig-mds-example(a) as an example. There are ten points in two dimensions (colored by three outcome classes). If we were to project these points down to a single dimension, we'd like points that are close in two dimensions to remain close when projected down to a single dimension. Panel (c) shows two such solutions. Each does reasonably well with some exceptions (i.e., points five and six are not very close for Metric MDS). \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![(a) A collection of samples associated with three outcome classes. (b) A diagram of the two nearest neighbors. (c) A one-dimensional projection using two different methods: non-metric MDS and Isomap.](../figures/fig-mds-example-1.png){#fig-mds-example fig-align='center' width=70%}\n:::\n:::\n\n\n\nThere are many different MDS methods. @Ghojogh2023 has an excellent review of them and their nuances. \n\nSome MDS methods compute all pairwise distances between points and uses this as the input to the embedding algorithm. This is similar to how PCA can be estimated using the covariance or correlation matrices. One technique, _Non-Metric MDS_ [@kruskal1964multidimensional;@kruskal1964nonmetric;@sammon1969nonlinear], finds embeddings that minimize an objective function called \"stress\":\n\n$$\n\\text{Stress} = \\sqrt{\\frac{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}\\left(d(x_i, x_j) - d(x^*_i, x^*_j)\\right)^2}{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}d(x_i, x_j)^2}}\n$$\n\nThe denominator uses the squared difference between the pairwise distances in the original values ($x$) and the smaller embedded dimension ($x^*$). The summations only move along one diagonal of the distance matrices to reduce redundant computations. Panel (c, top row) has the resulting one dimensional projection of our two-dimensional data.\n\nThis can be an effective dimension reduction procedure, although there are a few issues. First, the entire matrix of distances is required (with $n_{tr}(n_{tr}-1)/2$ entries). For large training sets, this can be unwieldy and time-consuming.  Second, like PCA, it is a global method. We might be able to achieve more nuanced embeddings by focusing on local structures. Finally, metric MDS is not easily made to be projective; how can we apply the same process to new data? \n\n#### Modern Distance-Based Embeddings\n\nTo start, we'll focus on _Isomap_ [@tenenbaum2000global]. This nonlinear MDS method uses a specialized distance function to find the embedded features. First, the _K_ nearest neighbors are determined for each training set point using standard functions, such as Euclidean distance. @fig-mds-example(b) shows the _K_ = 2 nearest neighbors for our example data. Many nearest-neighbor algorithms can be very computational efficient and their use eliminates the need to compute all of the pairwise distances. \n\nThe connections between neighbors form a _graph structure_ that defines which data points are closely related to one another. From this, a new metric called _geodesic distance_ can be approximated. For a graph, we can compute the approximate geodesic distance using the shortest path between two points on the graph. With our example data, the Euclidean distance between points four and five is not large. However, its approximate geodesic distance is greater because the shortest path is through points nine, eight, and seven. @Ghojogh2023 use a wonderful analogy: \n\n> A real-world example is the distance between Toronto and Athens. The Euclidean distance is to dig the Earth from Toronto to reach Athens directly. The geodesic distance is to move from Toronto to Athens on the curvy Earth by the shortest path between two cities. The approximated geodesic distance is to dig the Earth from Toronto to London in the UK, then dig from London to Frankfurt in Germany, then dig from Frankfurt to Rome in Italy, then dig from Rome to Athens.\n\nThe Isomap embeddings are a function of the eigenvalues computed on the geodesic distance matrix. The $m$ embedded features are functions of the first $m$ eigenvectors. Although eigenvalues are associated with linear embeddings (e.g., PCA), nonlinear geodesic distance results in a global nonlinear embedding. @fig-mds-example(c, bottom row) shows the 1D results for the example data set. For a new data point, its nearest-neighbors in the training set are determined so that the approximate geodesic distance can be computed. The estimated eigenvectors and eigenvalues are used to project the new point into the embedding space. \n\nFor Isomap, the number of nearest neighbors and the number of embeddings are commonly optimized. @fig-barley-isomap shows a two-dimensional Isomap embedding for the barley data with varying numbers of neighbors. In each configuration, the higher barley values are differentiated from the small (mostly zero) barley samples. There does seem to be two clusters of data associated with small outcome values. \n\nassociated with smaller values of the first feature and larger values of the second Isomap feature. The new features become more densely packed as the number of neighbors increases.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Isomap for the barley data for different number of nearest neighbors. The training set was used to fit the model and these results show the projections on the validation set. Lighter colors indicate larger values of the outcome.](../figures/fig-barley-isomap-1.png){#fig-barley-isomap fig-align='center' width=80%}\n:::\n:::\n\n\nThere are many other approaches to preserve local distances. One is _Laplacian eigenmaps_ [@belkin2001laplacian]. Like Isomap, it uses nearest neighbors to define a graph of connected training set points. For each connected point, a weight between graph nodes is computed that becomes smaller as the distance between points in the input space increases. The radial basis kernel (also referred to as the \"heat kernel\") is a good choice for the weighting function: \n\n$$\nw_{ij} = \\exp\\frac{-||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2}{\\sigma}\n$$\n\nwhere $\\sigma$ is a scaling parameter that can be tuned. If two points are not neighbors, or if $i = j$, the weights $w_{ij} = 0$. Note that the equation above uses Euclidean distance. For the 2-nearest neighbor graph shown in @fig-mds-example(b) and $\\sigma = 1 / 2$, the weight matrix is roughly\n\n\n::: {.cell layout-align=\"center\"}\n$$\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\nW = \\begin{bmatrix}\n\\0 & 0.1 & \\0 & 0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n & \\0 & 0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  & \\0 & 0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  &  & \\0 & \\0 & \\0 & 0.1 & \\0 & \\0 & \\0\\\\\n &  &  &  & \\0 & 0.4 & \\0 & \\0 & 0.1 & \\0\\\\\n &  & sym &  &  & \\0 & \\0 & \\0 & 0.1 & \\0\\\\\n &  &  &  &  &  & \\0 & 0.4 & \\0 & 0.1\\\\\n &  &  &  &  &  &  & \\0 & 0.1 & 0.3\\\\\n &  &  &  &  &  &  &  & \\0 & \\0\\\\\n &  &  &  &  &  &  &  &  & \\0 \n\\end{bmatrix}\n$$\n:::\n\n\nThe use of nearest neighbors means that the matrix can be very sparse and the zero values help define locality for each data point. Recall that samples 2 and 3 are fairly close to one another while samples 1 and 2 are father away. The weighting scheme gives the former pair a 4-fold larger weight in the graph than the latter pair. \n\nLaplacian eigenmaps rely heavily on graph theory. This method computes a _graph Laplacian_ matrix, defined as $L = D - W$ where the matrix $D$ has zero non-diagonal entries and diagonals equal to the sum of the weights for each row. For our example data the matrix is: \n\n\n::: {.cell layout-align=\"center\"}\n$$\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\nL = \\begin{bmatrix}\n 0.3 & -0.1 & \\0 & -0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  0.5 & -0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  &  0.5 & -0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  &  &  0.4 & \\0 & \\0 & -0.1 & \\0 & \\0 & \\0\\\\\n &  &  &  &  0.5 & -0.4 & \\0 & \\0 & -0.1 & \\0\\\\\n &  & sym &  &  &  0.5 & \\0 & \\0 & -0.1 & \\0\\\\\n &  &  &  &  &  &  0.6 & -0.4 & \\0 & -0.1\\\\\n &  &  &  &  &  &  &  0.8 & -0.1 & -0.3\\\\\n &  &  &  &  &  &  &  &  0.3 & \\0\\\\\n &  &  &  &  &  &  &  &  &  0.4 \n\\end{bmatrix}\n$$\n:::\n\n\nThe eigenvalues and eignevectors of this matrix are used as the main ingredient for the embeddings. @Bengio2003advances shows that, since these methods eventually use eigenvalues in the embeddings, they be easily used to project new data. \n\nThe Uniform Manifold Approximation and Projection (UMAP) [@sainburg2020parametric] technique is one of the most popular distance-based methods. Its precursors, stochastic neighbor embedding (SNE) [@hinton2002stochastic] and  Student’s t-distributed stochastic neighbor embedding (t-SNE) [@van2008visualizing], redefined feature extraction, particularly for visualizations. UMAP borrows significantly from Laplacian eigenmaps and t-SNE but has a more theoretically sound motivation. \n \nAs with Laplacian eigenmaps, UMAP converts the training data points to a sparse graph structure. Given a set of nearest neighbors, it computes values similar to the previously shown weights, which we will think of as the probability that point $j$ is a neighbor of point $i$: \n\n$$\np_{j|i} = \\exp\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i}\n$$\n\nwhere $\\rho_i$ is the distance from $\\boldsymbol{x}_i$ to it's closest neighbor and $\\sigma_i$ is a scale parameter that now varies with each sample ($i$). To compute $\\sigma_i$, we can solve the equation   \n \n$$\n\\sum_{i=1}^K  \\exp\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i} = \\log_2(K)\n$$\n\nUnlike the previous weighting system, the resulting $n \\times n$ matrix may not be symmetric, so the final weights are computed using $p_{ij} = p_{j|i} + p_{i|j} - p_{j|i}p_{i|j}$. \n\nUMAP performs a similar calculation for the embedded values $x^*$. We'll denote the probability that embedded points $\\boldsymbol{x}_i^*$ and $\\boldsymbol{x}_j^*$ are connected as $q_{ij}$. \n\nNumerical optimization methods^[Specifically gradient descent with a user-defined learning rate.] used to estimate the $n \\times m$ values $x^*_{ij}$. The process is initialized using a very sparse Laplacian eigenmap, the first few PCA components, or random uniform numbers. The objective function is based on cross-entropy and attempts to make the graphs in the input and embedded dimensions as similar as possible by minimizing: \n\n$$\nCE = \\sum_{i=1}^{n_{tr}}\\sum_{j=i+1}^{n_{tr}} \\left[p_{ij}\\, \\log\\frac{p_{ij}}{q_{ij}} + (1 - p_{ij})\\log\\frac{1-p_{ij}}{1-q_{ij}}\\right]\n$$\n\nUnlike the other embedding methods shown in this section, UMAP can also create supervised embeddings so that the resulting features are more predictive of a qualitative or quantitative outcome value. See @sainburg2020parametric.\n\nBesides the number of neighbors and embedding dimensions, several more tuning parameters exist.  The optimization process's number of optimization iterations (i.e., epochs) and the learning rate can significantly affect the final results. A distance-based tuning parameter, often called _min-dist_, specifies how \"packed\" points should be in the reduced dimensions. Values typically range from zero to one. However, the original authors state:\n\n> We view min-dist as an essentially aesthetic parameter governing the appearance of the embedding, and thus is more important when using UMAP for visualization.\n\nAs will be seen below, the initialization scheme is an important tuning parameter. \n\nFor supervised UMAP, there is an additional weighting parameter (between zero and one) that is used to balance the importance of the supervised and unsupervised aspects of the results. \n\n@fig-umap shows an interactive visualization of how UMAP can change with different tuning parameters. Each combination was trained for 1,000 epochs and used a learning rate of 1.0. For illustrative purposes, the resulting embeddings were scaled to a common range. \n\n::: {#fig-umap}\n\n::: {.figure-content}\n\n```{shinylive-r}\n#| label: fig-umap\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\n\n# ------------------------------------------------------------------------------\n\nlight_bg <- \"#fcfefe\" # from aml4td.scss\ngrid_theme <- bs_theme(\n  bg = light_bg, fg = \"#595959\"\n)\n\n# ------------------------------------------------------------------------------\n\ntheme_light_bl<- function(...) {\n\n  ret <- ggplot2::theme_bw(...)\n\n  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n  ret$panel.background  <- col_rect\n  ret$plot.background   <- col_rect\n  ret$legend.background <- col_rect\n  ret$legend.key        <- col_rect\n\n  ret$legend.position <- \"top\"\n\n  ret\n}\n\n# ------------------------------------------------------------------------------\n\nui <- fluidPage(\n  theme = grid_theme,\n  fluidRow(\n\n    column(\n      width = 4,\n      sliderInput(\n        inputId = \"min_dist\",\n        label = \"Min Distance\",\n        min = 0.0,\n        max = 1.0,\n        value = 0.2,\n        width = \"100%\",\n        step = 0.2\n      )\n    ), # min distance\n    column(\n      width = 4,\n      sliderInput(\n        inputId = \"neighbors\",\n        label = \"Neighbors\",\n        min = 5,\n        max = 45,\n        value = 5,\n        width = \"100%\",\n        step = 10\n      )\n    ), # nearest neighbors\n\n    column(\n      width = 4,\n      sliderInput(\n        inputId = \"supervised\",\n        label = \"Amount of Supervision\",\n        min = 0.0,\n        max = 0.7,\n        value = 0,\n        width = \"100%\",\n        step = 0.1\n      )\n    ),\n    fluidRow(\n      column(\n        width = 4,\n        radioButtons(\n          inputId = \"initial\",\n          label = \"Initialization\",\n          choices = list(\"Laplacian Eigenmap\" = \"spectral\", \"PCA\" = \"pca\", \n                         \"Random\" = \"random\")\n        )\n      ),\n      column(\n        width = 6,\n        align = \"center\",\n        plotOutput('umap')\n      )\n    )\n  ) # top fluid row\n)\n\nserver <- function(input, output) {\n  load(url(\"https://raw.githubusercontent.com/aml4td/website/mds-start/RData/umap_results.RData\"))\n\n  output$umap <-\n    renderPlot({\n      \n      dat <-\n        umap_results[\n          umap_results$neighbors == input$neighbors &\n            umap_results$min_dist == input$min_dist &\n            umap_results$initial == input$initial &\n            # log10(umap_results$learn_rate) == input$learn_rate &\n            umap_results$supervised == input$supervised,\n        ]\n\n      p <-\n        ggplot(dat, aes(UMAP1, UMAP2, col = barley)) +\n        geom_point(alpha = 1 / 3, cex = 3) +\n        scale_color_viridis(option = \"viridis\") +\n        theme_light_bl() +\n        coord_fixed() +\n        labs(x = \"UMAP Embedding #1\", y = \"UMAP Embedding #2\") +\n        guides(col = guide_colourbar(barheight = 0.5))\n\n      print(p)\n\n    })\n}\n\napp <- shinyApp(ui = ui, server = server)\n```\n:::\n\nA visualization of UMAP results for the barley data using different values for several tuning parameters. The points are the validation set values. \n\n:::\n\nThere are a few notable patterns in these results: \n\n - The initialization method can heavily impact the patterns in the embeddings. \n - As with Isomap, there are two clusters of data points with small barley values. \n - When the amount of supervision is increased, one or more circular structures begin to form for data points with small outcome values. \n - The minimum distance parameter can drastically change the results. \n\nt-SNE and UMAP have become very popular tools for visualizing complex data. Visually, they often appear to show interesting patterns that linear methods such as PCA cannot. However, they are computationally slow and not very stable over different tuning parameter values. Also, it is easy to believe that the UMAP distances between embedding points are important or quantitatively predictive. That is not the case; the distances can be easily manipulated using the tuning parameters (especially the minimum distance). \n\n\n#### Other Methods\n\n\n## Chapter References {.unnumbered}\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}