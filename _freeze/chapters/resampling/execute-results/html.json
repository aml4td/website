{
  "hash": "3e66ebf5a052a97f8c09dffc1d139f37",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/resampling/\"\n---\n\n\n\n\n\n# Measuring Performance with Resampling {#sec-resampling}\n\n\n\n\n\n\n\n\n\n\n\nIn @sec-external-validation, it was shown that using the same data to estimate and evaluate our model can produce inaccurate estimates of how well it functions. It also described using separate partitions of the data to fit and evaluate the model. \n\nThere are two general approaches for using external data for model evaluation. The first is a validation set, which we’ve seen before in @sec-three-way-split This is a good idea if you have a lot of data on hand. The second approach is to **resample** the training set. Resampling is an iterative approach that reuses the training data multiple times using sound statistical methodology. We’ll discuss both validation sets and resampling in this chapter. \n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A general data usage scheme that includes resampling. The colors denote the data used to train the model (in tan) and the separate data sets for evaluating the model (colored periwinkle).](../premade/resampling.svg){#fig-resampling-scheme fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n@fig-resampling-scheme shows a standard data usage scheme that incorporates resampling. After an initial partition, resampling creates **multiple versions** of the training set. We'll use special terminology^[The names used here are not universally used. We only invent new terminology to avoid confusion; people often refer to the data in our analysis set as the \"training set\" because it has the same purpose.] for the data partitions within each resample. These partitions serve the same function as the training and test sets:\n\n- The **analysis set** (of size $n_{fit}$) estimates quantities associated with preprocessing, model training, and postprocessing. \n- The **assessment set** ($n_{prd}$) is only used for prediction so that we can compute measures of model effectiveness (e.g., RMSE, classification accuracy, etc).  \n\nLike the training and test sets, the analysis and assessment sets are mutually exclusive data partitions and are different for each of the $B$ iterations of resampling. \n\nThe resampling process fits a model to an analysis set and predicts the corresponding assessment set. One or more performance statistics are calculated from these held-out predictions and saved. This process continues for $B$ iterations, and, in the end, there is a collection of $B$ statistics of efficacy. These are averaged to produce the overall **resampling estimate** of performance.  @alg-resampling formalizes this process. \n\n:::: {#alg-resampling}\n\n::: {.columns}\n\n::: {.column width=\"5%\"}\n\n:::\n\n::: {.column width=\"65%\"}\n\n\n```pseudocode\n#| label: alg-resampling\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"//\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n\\begin{algorithm}\n\\begin{algorithmic}\n\\State $\\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$\n\\State $B$: number of resamples\n\\State $M(\\mathfrak{D}^{tr}, B)$: a mapping function to split the data for the $B$ iterations.\n\\State $f()$: model pipeline \n\\Procedure{Resample}{$\\mathfrak{D}^{tr}, f(), M(\\mathfrak{D}^{tr}, B)$}\n  \\For{$b =1$ \\To $B$}\n    \\State Partition $\\mathfrak{D}^{tr}$ into $\\mathfrak{D}_b^{fit}$ and $\\mathfrak{D}_b^{prd}$ using $M_b(\\mathfrak{D}^{tr}, B)$.\n    \\State Train model $f$ on the analysis set to produce $\\hat{f}_{b}(\\mathfrak{D}_b^{fit})$.\n    \\State Generate assessment set predictions $\\hat{y}_b$ by applying model $\\hat{f}_{b}$ to $\\mathfrak{D}_b^{prd}$.\n    \\State Estimate performance statistic $\\hat{Q}_{b}$.\n  \\EndFor \n  \\State Compute reampling estimate $\\hat{Q} = \\sum_{b=1}^B \\hat{Q}_{b}$.\n  \\Return $\\hat{Q}$.\n\\Endprocedure\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n:::\n\n::: {.column width=\"15%\"}\n\n:::\n\n::: \n\nResampling models to estimate performance. \n\n::::\n\n\n:::{.callout-note}\nIn @alg-resampling, $f()$ is the model pipeline described in @sec-model-pipeline. Any estimation methods, before, during, or after the supervised model, are executed on the analysis set $B$ times during resampling.\n:::\n\nThere are many different resampling methods, such as cross-validation and the bootstrap. They differ in how the analysis and assessment sets are created in line 6 of @alg-resampling. Subsequent sections below discuss a number of these. \n\nTwo primary aspects of resampling make it an effective tool. First, the separation of data used to create and appraise the model avoids the data reuse problem described in @sec-external-validation. \n\nSecond, using multiple iterations ($B > 1$) means that you are evaluating your model under slightly different conditions (because the analysis and assessment sets are different). This is a bit like a \"multiversal\" science fiction story: what would have happened if the situation were slightly different before I fit the model? This lets us directly observe the model variance; how stable is it when the inputs are slightly modified? An unstable model (or one that overfits) will have a high variance.\n\nIn this chapter, we'll describe some conceptual aspects of resampling. Then, we’ll define and discuss various methods. Finally, the last section lists frequently asked questions we often hear when teaching these tools. \n\nUp until @sec-time-series-resampling, we will assume that each row of the data is independent. We’ll see examples of non-independent in the later sections.\n\n## What is Resampling Trying to Do? \n\nIt's important to understand some of the philosophical aspects of resampling. It is a statistical estimation procedure that tries to estimate some true, unknowable performance value (let's call it $Q$) associated with the same population of data represented by the training set. The statistic $Q$ could be RMSE, the Brier score, or any other performance statistic. \n\nThere have been ongoing efforts to understand what resampling methods do on a theoretical level. @bates2023cross is an interesting work focused on linear regression models with ordinary least squares. Their conclusion regarding resampling estimates is that they\n\n> ... cannot be viewed as estimates of the prediction error of the final model fit on the whole data. Rather, the estimate of prediction error is an estimate of the average prediction error of the final model across other hypothetical data sets from the same distribution.\n\nIt is a good idea to think that this will also be true for more complex machine learning models. The important idea is that resampling is measuring performance on training sets similar to ours (and the same size). \n\nTo understand different methods, let’s examine some theoretical properties of resampling estimates that matter to applied data analysis. \n\nLet’s start with analogy; consider the usual sample mean estimator used in basic statistics. Based on assumptions about the data, we can derive an equation that optimally estimates the true mean value based on some criterion. Often, the statistical theory focuses on our estimators' theoretical mean and variance. For example, if the data are independent and follow the same Gaussian distribution, the sample mean estimates the actual population mean (i.e., it is an **unbiased** estimator). We can sometimes derive what the estimator's theoretical variance is, too. These properties, or some combination of them, such as mean squared error, help guide us to the best estimator. \n\nThe same is true for resampling methods. We can understand how their bias and variances change under different circumstances and choose a technique appropriately. We'll also focus on bias and variance as the main properties of interest^[This discussion is very similar to the variance-bias tradeoff discussed in @sec-variance-bias. The context here differs, but the themes are the same.]. \n\nFirst is bias: how accurately does a resampling technique estimate the true population value $Q$? We'd like it to be unbiased, but to our knowledge, that is nearly impossible. However, some things we do know.  @fig-resampling-scheme shows that the training set (of size $n_{tr}$) is split into two partitions of size $n_{fit}$ and $n_{prd}$. It turns out that, as $n_{prd}$ becomes larger, the bias increases in a pessimistic direction^[Meaning that performance looks worse than it should. For example, smaller $R^2$ or inflated Brier score.]. In other words, if our training set has 1,000 samples, a resampling method where the assessment set has $n_{prd} = 100$ data points has a smaller bias than one using $n_{prd} = 500$. Another thing that we know is that increasing the number of resamples ($B$) can't significantly reduce the bias. \n\nThe variance (often called precision) of resampling is also important. We want to get a performance estimate that gives us consistent results if we repeat it. The resampling precision is driven mainly by $B$ and $n_{tr}$. With some resampling techniques, if your results are too noisy, you can resample more and stabilize or reduce the estimated variance (at the cost of increased computational time). We'll examine the bias and precision of different methods as we discuss each method. \n\nLet's examine a few types of resampling methods, starting with the most simple: a single validation set. \n\n## Validation Sets {#sec-validation}\n\nWe’ve already described a validation set; it is typically created via a three-way initial split^[The split can be made using the same tools already discussed, such as completely random selection, stratified random sampling, etc.] of the data (as shown in @fig-validation). For time series data, it is common to use the most recent data for the test set. Similarly, the validation set would include the most recent data (once the test set partition is created). The training set would include everything else.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![An initial data splitting scheme that incorporates a validation set.](../premade/validation.svg){#fig-validation fig-align='center' width=45%}\n:::\n:::\n\n\n\n\n\nWhile not precisely the same, a validation set is extremely similar to an approach described below called Monte Carlo cross-validation (MCCV). If we used a single MCCV resample, the results would be effectively the same as those of a validation set. \n\nOne aspect of using the validation set is what to do with it after you’ve made your final decision about the model pipeline. Ordinarily, the entire training set is used for the final model fit.\n\nData are precious, and, in some cases, an argument can be made that the final model fit could include the training _and_ validation set. Doing this does add some risk; your validation set statistics are no longer completely valid since they measured how well the model works with similar training sets of size $n_{tr}$. If you have an abundance of data, the risk is low, but, at the same time, the model fit won’t change much by adding $n_{val}$ data points. However, if your training data is not large, adding more data could have a profound impact, and you risk using the wrong model for your data^[Also, if $n_{tr}$ is not “large,” you shouldn’t use a validation set anyway.]. \n\n## Monte Carlo Cross-Validation {#sec-cv-mc}\n\nMonte Carlo cross-validation emulates the initial train/test partition. For each one of $B$ resamples, it takes a random sample of the training set (say about 75%) to use as the analysis set^[Again, this could be accomplished using any of the splitting techniques described in @sec-data-splitting.]. The remainder is used for model assessment. Each of the $B$ resamples is independent of the others, so some of the training set points are included in multiple assessment sets. @fig-mccv-scheme shows a schematic of three MCCV resamples of a data set with $n_{tr} = 30$ data points and 80% of the data allocated to the analysis set. Note that samples 16 and 17 are in multiple assessment sets. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A schematic of three Monte Carlo cross-validation samples created from an initial pool of 30 data points.](../premade/mc-cv.svg){#fig-mccv-scheme fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n\n\nHow many resamples should we use, and what proportion of the data should be used for the analysis set? These choices are partly driven by computing power and training set size. Using more resamples means better precision. \n\nA thought experiment can be useful for any resampling method. Based on the proportion of data going into the assessment set, how confident would you feel in a performance metric being computed for this much data? Suppose that we are computing the RMSE for a regression model. If our assessment set contained $n_{prd} = 10$ (on average), we might think our metric’s mean is excessively noisy.   In that case, we could either increase the proportion held out (better precision, worse bias) or resample more (better precision, same bias). \n\nEach resampling method has different trade-offs between bias and variance. Let’s look at some simulation results to help understand the trade-off for MCCV.\n\n### Another Simulation Study  {.unnumbered}\n\n@sec-complexity described a simulated data set that included 200 training set points. In that section, @fig-two-class-overfit illustrates the effect of overfitting using a particular model (KNN). This chapter will use the same training set but with a simple logistic regression model where a four-degree of freedom spline was used with each of the two predictors. \n\nSince these are simulated data, an additional, very large data set was simulated and used to approximate the model's true performance, once again evaluated using a Brier score. Our logistic model has a Brier score value of $Q \\approx$ 0.0898, which is quite good. Using this estimate, the model bias can be computed by subtracting the resampling estimate from 0.0898.\n\nThe simulation created 500 realizations of this 200 sample training set^[The simulation details, sources, and results can be found at [`https://github.com/topepo/resampling_sim`](https://github.com/topepo/resampling_sim).]. We resampled the logistic model for each and then computed the corresponding Brier score estimate from MCCV. This process was repeated using the different analysis set proportions and values of $B$ shown in @fig-mccv-stats. The left panel shows that the bias^[The percent bias is calculated as $100(Q_{true} - \\hat{Q})/Q_{true}$ for metrics where smaller is better.] decreases as the proportion of data in the analysis set becomes closer to the amount in the training set. It is also apparent that the bias is unaffected by the number of resamples ($B$). The panel on the right shows the precision, estimated using the standard error, of the resampled estimates.  This decreases nonlinearly as $B$ increases; the cost-benefit ratio of adding more resamples shows eventual diminishing returns. The amount retained for the analysis set shows that values close to one have higher precision than the others.  Regarding computational costs, the time to resample a model increases with both parameters (amount retained and $B$). \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Variance and bias statistics for simulated data using different configurations for Monte Carlo cross-validation. The range of the y-axes are common across similar plots below for different resampling techniques.](../figures/fig-mccv-stats-1.svg){#fig-mccv-stats fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nThe results of this simulation indicate that, in terms of precision, there is little benefit in including more than 70% of the training set in the assessment set. Also, while more resamples always help, there is incremental benefit in using more than 50 or 60 resamples (for this size training set). Regarding bias, the decrease somewhat slows down near 80% of the training set retained. Holdout proportions of around 75% - 80% this might be a reasonable rule of thumb.\n\n## V-Fold Cross-Validation {#sec-cv}\n\n**V-fold cross-validation** (sometimes called K-fold cross-validation) [@stone1974cross] is the most well-known resampling method. It randomly allocates the training data to one of _V_ groups of about equal size, called a \"fold\" (a stratified allocation can also be used). There are $B = V$ iterations where the analysis set comprises _V_ -1 of the folds, and the remaining fold defines the assessment set. For example, for 10-fold cross-validation, the ten analysis sets consist of 90% of the data, and the remaining 10% are used to quantify performance. @fig-cv-scheme shows the process for _V_ = 3 (for brevity). \n\n::: {#fig-cv-scheme layout-ncol=1}\n\n![](../premade/three-CV.svg){width=45% fig-align=\"center\"}\n\n![](../premade/three-CV-iter.svg){width=60% fig-align=\"center\"}\n\nAn example of using V-fold cross-validation.\n:::\n\n@Arlot2010 is a comprehensive survey of cross-validation. \n\n\n_V_ is typically 5 or 10. Again, the choice can be driven by computational costs (smaller values of _V_ are faster) or more statistical considerations, specifically bias and variance again. _V_-fold cross-validation generally has a smaller bias than other methods but has relatively poor precision. As with MCCV, the degree of bias is determined by how much data are retained for the assessment sets (as defined by _V_). _V_ = 5 has substantial bias compared to the more common _V_ = 10. @fushiki2011estimation describes _post hoc_ methods for reducing the bias for this resampling method. The precision also improves when _V_ increases. \n\n\n### Repeated Cross-Validation {.unnumbered}\n\nOne way to improve the precision is to use **repeated V-fold cross-validation**. In this case, the same sample allocation scheme is used to create the folds and this is repeated multiple times using different random number seeds. For example, two repeats of 10-fold cross-validation^[This isn’t the same as using _V_ = 20. That scheme has very different bias properties than two repetitions of _V_ = 10.] create two sets of _V_ folds, which are treated as a collection of twenty resamples (e.g., $B = V \\times R$ for $R$ repeats). Again, increasing the number of resamples does not generally change the bias. \n\n### Leave-One-Out Cross-Validation {.unnumbered}\n\nLeave-one-out cross-validation (LOOCV, also called the jackknife) sets $V = n_{tr}$. A single sample is withheld and, over $n_{tr}$ iterations, a set of models is created, each with an analysis set of $n_{tr} - 1$ samples. The resampling estimate is created by applying the metric function to the resulting $n_{tr}$ predicted values. There are no replicate performance values; only a single $\\hat{Q}$. As one might expect, the bias is nearly zero for this method. Although it would be difficult to quantify, the standard error of the estimator should be very large. \n\nThis method is extremely computationally expensive^[However, for linear regression, the LOOCV predictions can be quickly computed without refitting a large number of models.] and is not often used in practice. \n\n### Variance and Bias for V-Fold Cross-Validation {.unnumbered}\n\n\n\n\n\n\n\n\n\n\n\n\n@fig-cv-stats shows bias and variance results from the simulation, where the x-axis is the total number of resamples $V\\times R$. As expected, the bias decreases with _V_ and is constant over the number of resamples. Five-fold cross-validation stands out as particularly bad with a percent bias of roughly 3.9%. Using ten folds decreases this to about 2.2%. The precision values show that smaller values of _V_ have better precision; 10-fold CV needs substantially more resamples to have the same standard error. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Variance and bias statistics for simulated data using different configurations for $V$-fold cross-validation.](../figures/fig-cv-stats-1.svg){#fig-cv-stats fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nIs there any advantage to using _V_ > 10? Not much. The decrease in bias is small and many more replicates are required to reach the same precision as _V_ = 10.  For example, twice as many resamples is required for 20-fold CV to match the variance of 10-fold CV. \n\nFor this simulation, the properties are about the same when MCCV and 10-fold CV are matched in terms of number of resamples and the amount allocated to the assessment set. \n\nOur recommendation is to almost always use _V_ = 10. The bias and variance gains are both good with 10 folds. Reducing bias in 5-fold cross-validation is difficult but the 10-fold precision can be improved by increased replication. \n\n## The Bootstrap {#sec-bootstrap}\n\nThe bootstrap [@Efron1979boot;@davison1997bootstrap;@efron2003second;@EfronHastie2016a] is a resampling methodology originally created to compute the sampling distribution of statistics using minimal probabilistic assumptions^[This application will be discussed in section TODO.]. In this chapter, we'll define a bootstrap sample and show how it can be used to measure fit quality. \n\nFor a training set with $n_{tr}$ data points, a bootstrap resample takes a random sample of the training set that is also size $n_{tr}$. It does this by sampling with replacement; when each of the $n_{tr}$ samples is drawn, it has no memory of the prior selections. This means that each row can be randomly selected again. For example, @fig-bootstrap-scheme shows another schematic for three bootstrap samples. In this figure, the thirteenth training set sample was selected to go into the first analysis set three times. For this reason, a bootstrap resample will contain multiple replicates of some training set points, while others will not be selected at all. The data that were never selected are used to create the assessment set.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A schematic of three bootstraps samples created from an initial pool of 30 data points.](../premade/bootstraps.svg){#fig-bootstrap-scheme fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nThe probability that a data point will be picked is $1 / n_{tr}$ and, from this, the probability that a training set point is _not selected at all_ is \n\n$$\\prod_{i=1}^{n_{tr}} \\left(1 - \\frac{1}{n_{tr}}\\right) = \\left(1 - \\frac{1}{n_{tr}}\\right)^{n_{tr}}\\approx e^{-1} = 0.368$$\n\nSince the bootstrap sample contains the _selected_ data, each training point is selected with probability $1 - e^{-1} \\approx 0.632$. The implication is that, on average, the analysis set, contains about 63.2% unique training set points and the assessment set includes, on average, 36.8% of the training data. If we were to compare the amount of unique training set points held out by the bootstrap to V-fold cross-validation, the bootstrap is comparable to using $V = 3$. \n\n@fig-boot-stats shows bias and variance for the simulated data. As one might expect, there is considerable bias in the bootstrap estimate of performance. In comparison to the corresponding plot for MCCV, the bootstrap bias is worse than the MCCV curve where 60% were held out. The curve is also flat; the bias does't go away by increasing $B$.\n\nHowever, the precision is extremely good, even with very few resamples. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Variance and bias statistics for simulated data using different configurations for the bootstrap.](../figures/fig-boot-stats-1.svg){#fig-boot-stats fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n### Correcting for Bias {.unnumbered}\n\n\n\n\n\n\n\n\n\n\n\nThere have been some attempts to de-bias the bootstrap estimates. The \"632 rule\" [@Efron1983p51] uses the standard bootstrap estimate ($\\hat{Q}_{bt}$) and the resubstitution estimate ($\\hat{Q}_{rsub}$) together: \n\n$$\\hat{Q}_{632} = e^{-1}\\, \\hat{Q}_{rsub} + (1 - e^{-1})\\, \\hat{Q}_{bt} = 0.368\\,\\hat{Q}_{rsub} + 0.632\\,\\hat{Q}_{bt}$$\n@fig-boot-stats shows that there is a significant drop in the bias when using this correction. The 632 estimator combines two different statistical estimates, and we only know the standard error of $\\hat{Q}_{bt}$. Therefore, the right-hand panel does not show a standard error curve.  \n\nLet's look at an average example from one of the simulated data sets with $B = 100$. @tbl-bootstrap-bias has the estimator values and their intermediate values. Let's first focus on the values for the column labeld \"Logistic.\" The standard bootstrap estimate was $\\hat{Q}_{bt}$ = 0.101 and repredicting the training set produced $\\hat{Q}_{rsub}$ = 0.0767. The 632 estimate shifts the Brier score downward to $\\hat{Q}_{632}$ = 0.0919. This reduces the pessimistic bias; our large sample estimate of the true Brier score is 0.0898 so we are closer to that value.\n\nAnother technique, the 632+ estimator [@Efron1997tc], uses the same blending strategy but uses dynamic weights based on how much the model overfits (if at all). It factors in a model’s “no-information rate”: the metric value if the predicted and true outcome values were independent. The author gives a formula for this, but it can also be estimated using a permutation approach where we repeatedly shuffle the outcome values and compute the metric.  We will denote this value as $\\hat{Q}_{nir}$. For our simulated data set $\\hat{Q}_{nir}$ = 0.427; we believe that this is the worst case value for our metric. \n\nWe compute the relative overfitting rate (ROR) as \n\n$$ ROR = \\frac{\\hat{Q}_{bt}-\\hat{Q}_{rsub}}{\\hat{Q}_{nir} -\\hat{Q}_{rsub}} $$\n\nThe denominator measures the range of the metric (from most optimistic to most pessimistic). The numerator measures the optimism of our standard estimator. A value of zero implies that the model does not overfit, and values of one indicate the opposite. For our logistic model, the ratio is 0.024 / 0.35 = 0.0686, indicating that the logistic model is not overinterpeting the training set. \n\nThe final 632+ estimator uses a different weighting system to combine estimates:  \n\n$$\n\\begin{align}\n\\hat{Q}_{632+} &= (1 - \\hat{w})\\, \\hat{Q}_{rsub} + \\hat{w}\\, \\hat{Q}_{bt} \\quad \\text{with}\\notag \\\\\n\\hat{w} &= \\frac{0.632}{1 - 0.368ROR} \\notag\n\\end{align}\n$$\nPlugging in our values, the final weight on the standard estimator ($w$ = 0.648) is very close to what is used by the regular 632 rule ($w$ = 0.632). The final estimate is also similar: $\\hat{Q}_{632+}$ = 0.0923. \n\n\n\n\n\n::: {#tbl-bootstrap-bias .cell layout-align=\"center\" tbl-cap='Bias correction values for two models on a simulated data set.'}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"mhsavldpxv\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#mhsavldpxv table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#mhsavldpxv thead, #mhsavldpxv tbody, #mhsavldpxv tfoot, #mhsavldpxv tr, #mhsavldpxv td, #mhsavldpxv th {\n  border-style: none;\n}\n\n#mhsavldpxv p {\n  margin: 0;\n  padding: 0;\n}\n\n#mhsavldpxv .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#mhsavldpxv .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#mhsavldpxv .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#mhsavldpxv .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#mhsavldpxv .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#mhsavldpxv .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#mhsavldpxv .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#mhsavldpxv .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#mhsavldpxv .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#mhsavldpxv .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#mhsavldpxv .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#mhsavldpxv .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#mhsavldpxv .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#mhsavldpxv .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#mhsavldpxv .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#mhsavldpxv .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#mhsavldpxv .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#mhsavldpxv .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#mhsavldpxv .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#mhsavldpxv .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#mhsavldpxv .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#mhsavldpxv .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#mhsavldpxv .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#mhsavldpxv .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#mhsavldpxv .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#mhsavldpxv .gt_left {\n  text-align: left;\n}\n\n#mhsavldpxv .gt_center {\n  text-align: center;\n}\n\n#mhsavldpxv .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#mhsavldpxv .gt_font_normal {\n  font-weight: normal;\n}\n\n#mhsavldpxv .gt_font_bold {\n  font-weight: bold;\n}\n\n#mhsavldpxv .gt_font_italic {\n  font-style: italic;\n}\n\n#mhsavldpxv .gt_super {\n  font-size: 65%;\n}\n\n#mhsavldpxv .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#mhsavldpxv .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#mhsavldpxv .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#mhsavldpxv .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#mhsavldpxv .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#mhsavldpxv .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#mhsavldpxv .gt_indent_5 {\n  text-indent: 25px;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\" \"> </th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\" \"> </th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"2\" scope=\"colgroup\" id=\"Brier Score\">\n        <span class=\"gt_column_spanner\">Brier Score</span>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"1 NN\">1 NN</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Logistic\">Logistic</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr class=\"gt_group_heading_row\">\n      <th colspan=\"4\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"estimates\">estimates</th>\n    </tr>\n    <tr class=\"gt_row_group_first\"><td headers=\"estimates  statistic\" class=\"gt_row gt_left\">simple mean</td>\n<td headers=\"estimates  blank\" class=\"gt_row gt_right\">   </td>\n<td headers=\"estimates  brier_class_knn\" class=\"gt_row gt_right\">0.174</td>\n<td headers=\"estimates  brier_class_logistic\" class=\"gt_row gt_right\">0.101</td></tr>\n    <tr><td headers=\"estimates  statistic\" class=\"gt_row gt_left\">resubstitution</td>\n<td headers=\"estimates  blank\" class=\"gt_row gt_right\">   </td>\n<td headers=\"estimates  brier_class_knn\" class=\"gt_row gt_right\">0.000</td>\n<td headers=\"estimates  brier_class_logistic\" class=\"gt_row gt_right\">0.077</td></tr>\n    <tr><td headers=\"estimates  statistic\" class=\"gt_row gt_left\">(truth)</td>\n<td headers=\"estimates  blank\" class=\"gt_row gt_right\">   </td>\n<td headers=\"estimates  brier_class_knn\" class=\"gt_row gt_right\">0.170</td>\n<td headers=\"estimates  brier_class_logistic\" class=\"gt_row gt_right\">0.090</td></tr>\n    <tr class=\"gt_group_heading_row\">\n      <th colspan=\"4\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"intermediates\">intermediates</th>\n    </tr>\n    <tr class=\"gt_row_group_first\"><td headers=\"intermediates  statistic\" class=\"gt_row gt_left\">no information rate</td>\n<td headers=\"intermediates  blank\" class=\"gt_row gt_right\">   </td>\n<td headers=\"intermediates  brier_class_knn\" class=\"gt_row gt_right\">0.496</td>\n<td headers=\"intermediates  brier_class_logistic\" class=\"gt_row gt_right\">0.427</td></tr>\n    <tr><td headers=\"intermediates  statistic\" class=\"gt_row gt_left\">relative overfitting rate</td>\n<td headers=\"intermediates  blank\" class=\"gt_row gt_right\">   </td>\n<td headers=\"intermediates  brier_class_knn\" class=\"gt_row gt_right\">0.351</td>\n<td headers=\"intermediates  brier_class_logistic\" class=\"gt_row gt_right\">0.069</td></tr>\n    <tr><td headers=\"intermediates  statistic\" class=\"gt_row gt_left\">weights</td>\n<td headers=\"intermediates  blank\" class=\"gt_row gt_right\">   </td>\n<td headers=\"intermediates  brier_class_knn\" class=\"gt_row gt_right\">0.726</td>\n<td headers=\"intermediates  brier_class_logistic\" class=\"gt_row gt_right\">0.648</td></tr>\n    <tr class=\"gt_group_heading_row\">\n      <th colspan=\"4\" class=\"gt_group_heading\" scope=\"colgroup\" id=\"final estimates\">final estimates</th>\n    </tr>\n    <tr class=\"gt_row_group_first\"><td headers=\"final estimates  statistic\" class=\"gt_row gt_left\">632</td>\n<td headers=\"final estimates  blank\" class=\"gt_row gt_right\">   </td>\n<td headers=\"final estimates  brier_class_knn\" class=\"gt_row gt_right\">0.110</td>\n<td headers=\"final estimates  brier_class_logistic\" class=\"gt_row gt_right\">0.092</td></tr>\n    <tr><td headers=\"final estimates  statistic\" class=\"gt_row gt_left\">632+</td>\n<td headers=\"final estimates  blank\" class=\"gt_row gt_right\">   </td>\n<td headers=\"final estimates  brier_class_knn\" class=\"gt_row gt_right\">0.126</td>\n<td headers=\"final estimates  brier_class_logistic\" class=\"gt_row gt_right\">0.092</td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n\n\nThe simulations studies shown here in @fig-boot-stats replicated what @Molinaro2005p47 found; the 632+ rule has bias properties about the same as 10-fold cross-validation. \n\nHow do these estimators work when there is extreme overfitting? One way to tell is to consider a 1-nearest neighbor model. In this case, re-predicting the training set predicts every data point with its outcome value (i.e., a “perfect” model). For the Brier score, this means $\\hat{Q}_{rsub}$ is zero. @tbl-bootstrap-bias shows the rest of the computations. For this model $Q\\approx$ 0.17 and the standard estimate comes close: $\\hat{Q}_{bt}$ = 0.174. \n\nThe 632 estimate is $\\hat{Q}_{632} = 0.632\\,\\hat{Q}_{bt} =$ 0.11; this is too much bias reduction as it overshoots the true value by a large margin. The 632+ estimator is slightly better. The ROR value is higher (0.351) leading to a higher weight on the standard estimator to produce $\\hat{Q}_{632+}$ = 0.126.\n\nShould we use the bootstrap to compare models? Its small variance property is enticing, but the bias remains an issue. The bias is likely to change as a function of the magnitude of the metric. For example, the bootstrap’s bias is probably different for models with 60% and 95% accuracy. If we think our models will have about the same performance, the bootstrap (with a bias correction) may be a good choice. \n\nLet's take a look at a few specialized resampling methods. \n\n## Time Series Data {#sec-time-series-resampling}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A schematic of rolling origin resampling for time series data. Each box corresponds to a unit of time (e.g., day, month, year, etc.).](../premade/rolling.svg){#fig-rolling-scheme fig-align='center' width=55%}\n:::\n:::\n\n\n\n\n\n\n\n## Spatial Data {#sec-spatial-resampling}\n\n## Grouped or Hierarchical Data {#sec-group-resampling}\n\n\n## Summarizing Methods {#sec-resampling-summary}\n\n\n## Frequently Asked Questions {#sec-resampling-faq}\n\nThese questions represent areas of confusion abot resampling. \n\n#### Is it bad to get different results within each resample?   {#sec-resampling-Q01 .unnumbered}\n\nThe point of resampling is to see how the model pipeline changes when the data changes. There is a good chance that some important estimates differ from resample to resample. For example, you might get $B$ different predictor lists when algorithmically filtering predictors, and that’s okay. It does give you a sense of how much noise is in the part of your model pipeline.\n\n#### What happens to the $B$ models? Which one should I keep? {#sec-resampling-Q02 .unnumbered}\n\nThe $B$ models fit within resampling are only used to estimate how well the model fits the data. You don’t need them after that.\n\nIf you are not using a single validation set, recall that none of these models are trained with the unaltered training set, so none would be the final model.  \n\nYou can retain them for diagnostic purposes though. As mentioned above, using the replicate models to see how parts of the model change can be helpful. \n\n#### Is this some sort of ensemble thingy?  {#sec-resampling-Q03 .unnumbered}\n\nNo. However, if you do end up using a stacking ensemble (@sec-ensembles), you could use the out-of-sample predictions to train that type of ensemble. \n\n#### Can I accidentally do it wrong?  {#sec-resampling-Q04 .unnumbered}\n\nUnfortunately yes. As previously mentioned, improper data usage is one of the most frequent ways that machine learning models silently fail; that is, you won’t know that there is a problem until the next set of labeled data. A good example discussed in @sec-removing-predictors is @Ambroise2002p1493. \n\nIf you are \n\n - training _every_ part of your model pipeline after the initial data split and using _only_ the training set and, \n\n- evaluating performance with external data\n\nthe risk of error is low.\n\n#### Why do I need a test set?  {#sec-resampling-Q05 .unnumbered}\n\nIt can be argued that resampling does an excellent job, and saving data for a single-shot assessment at the end is wasteful. Statistically speaking, that is true. \n\nHowever, machine learning models are complex; our pipeline could be very sophisticated for complex data.  For this reason, it is **extremely good scientific practice** to have a “control” that you can use to verify your work. If the model is important to you, you should use a test set. \n \n#### What is nested resampling? {#sec-resampling-Q06 .unnumbered}\n\nThis is a version of resampling where an additional layer of resampling occurs. For example, suppose you are using 10-fold cross-validation. Within each of the 10 iterations, you might add 20 bootstrapping iterations (that resamples the analysis set). That ends up training 200 distinct models. \n\nFor a single model, that’s not very useful. However, as discussed in @sec-nested-resampling, there are situations where the analysis set is being used for too many purposes. For example, during model tuning, you might use resampling to find the best model _and_ measure its performance. In certain cases, that can lead to bad results for the latter task.  \n\nAnother example is recursive feature selection, where we are trying to rank predictors, sequentially remove them, and determine how many to remove. In this case, using an outer resampling loop to determine where to stop and an inner loop for the other tasks is a good idea.\n\nWe will see nested resampling in later chapters. \n\n#### Are these methods related to permutation tests? {#sec-resampling-Q07 .unnumbered}\n\nNot really. Permutation methods are similar to resampling only in that they perform multiple calculations on different versions of the training set. \n\nWe used permutation methods in the bootstrapping section to estimate the no-information rate. Otherwise, they won’t be used to evaluate model performance. \n\n#### Are sub-sampling or down-sampling the same as resampling?  {#sec-resampling-Q08 .unnumbered}\n \nNo. These are techniques to modify your training set (or analysis set) to rebalance the data when a classification problem has rare events. They’ll be discussed in @sec-imbalance-sampling.\n \n#### Why do I mostly hear about validation sets?  {#sec-resampling-Q09 .unnumbered}\n\nAs discussed in the introduction chapter, stereotypical deep learning models are trained on _very_ large sets of data. There is very little reason to use multiple resamples in these instances, and deep learning takes up a lot of space in the media and social media. \n\nGenerally, every data set that is not massive could benefit from multiple resamples. \n\n\n\n\n\n\n\n\n\n\n\n\n## Chapter References {.unnumbered}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}