{
  "hash": "f5ec29546e57a24758d1cc6de68ad16b",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/cls-imbalance/\"\n---\n\n# Class Imbalances {#sec-cls-imbalances}\n\n\n\n\n\nWhen the outcome classes have very different rates of occurrence, we call this a _class imbalance_. Recalling the Wordle data shown in @sec-cls-distributions, we saw that the rates of correct word identification on the first and second guesses were very low relative to the other class values. When this is an imbalance, there are often only two class levels given the somewhat arbitrary designations of \"events\" and \"non-events.\" We'll focus on two class problems in this chapter, although most of the tools described here can handle data sets with three or more classes^[We'll use the terms \"majority\" and \"minority\" class here. For more than two classes, you could substitute \"non-minority\" for \"majority.\"]. Imbalances can frequently occur. For example, the rate of hospital laboratory specimens is very low [@mitani2020highly], as are most rates of infectious diseases and violent crimes. \n\nThis chapter will describe many modeling and statistical issues surrounding imbalances and how they can affect the interpretation of models and their estimates. Imbalances have been discussed at length in the literature and on social media. This chapter also tries to clear up many misconceptions of the \"right way\" to analyze such data. There is no global solution for modeling rate events, and our emphasis is on defining the purpose of models based on imbalanced data and choosing methods that are well-suited to that purpose. \n\nFirst, let's take a look at the main data set used in this chapter. \n\n## Example: Predicting Myopia {#sec-yyopia}\n\nFor demonstration, we'll use data from the myopia study as described in Section 1.6.6 of @hosmer2013applied. Myopia (i.e., nearsightedness) can be caused by the shape of one's eye, genetics, and environmental conditions, including the heavy use of computer or television screens. The data are from a medical study running from 1990 to 1995 and contained 302 girls and 316 boys ranging from 5 to 9 years old. These data are from the initial exam and include demographic factors, several physiological measurements of the eye from an ocular examination, and whether either parent is also myopic. The outcome is determined after a five-year follow-up visit. In this study, 81 (13.1%) of children were nearsighted. \n\nTo begin, a 3:1 stratified split was used to partition the data into training and testing sets. Stratification was based on the outcome. This results in a training set of 462 children, only 60 of whom are myopic. For the test set, 21 children out of 156 were myopic. Since the training set and the number of events are small, five repeats of 10-fold cross-validation were used with the same stratification strategy. The repeats will help increase the precision of our resampling statistics. \n\n::: {.important-box}\nIt is critical that the test set reflects the true event rate in the population of interest. While we may change the event rate in the training set during model development, the test set data should be as consistent as possible with reality. \n:::\n\nWe'll conduct some quick exploratory data analysis, then proceed to creating models based on traditional modeling methods, i.e., without making adjustments for class imbalance during model optimization. \n\n### Exploring the Data {#sec-myopia-eda}\n\n\n\nFirst, let's examine the age and sex of the participants. @fig-myopia-age-sex shows the class percentages for 10 subgroups of the data. The size of the point is based on the number of children in the subgroup, indicating that many of them are 6 years old, while very few are 9 years old. There were no nine-year-old myopic girls in the training set. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Training set rates of myopia by age and sex of the children. The error bars are based on 90% confidence intervals, and the size of the point indicates the number of children in each subgroup.](cls-imbalance_files/figure-html/fig-myopia-age-sex-1.svg){#fig-myopia-age-sex fig-align='center' width=80%}\n:::\n:::\n\n\nThe points suggest a slight increase in myopia rates among girls, but this is well within the error bars defined by the 90% confidence intervals. Due to the concentration of points in a single age group, it is difficult to determine whether there is an age trend. However, since the slight rate increase for girls is constant from ages six to eight, there is little indication of an interaction between these factors. \n\nSecond, do genetics matter? If neither parent is myopic, the rates for their children are low: 1.8% with 90% confidence interval (0.3%, 5.7%). When _both_ parents have the condition, the rate is  21.6%  (CI: 15.4%, 28.8%), indicating a strong signal. \nThe effects of single paternal and maternal genetics are in between, with rates 15.3% and 12.6%, respectively, with similar confidence intervals.    \n\nThe ocular exam yielded five different measurements of the eye. @fig-myopia-splom shows a scatterplot matrix of pairwise relationships, colored by the outcome class. With one exception,  there is little to no correlation between predictors. However, the axial length and the vitreous chamber depth have a high positive correlation. Additionally, most predictors have fairly symmetric distributions, although the spherical equivalent refraction data exhibit a moderately strong right skew. This predictor also shows some raw separation between the classes. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Scatterplot matrix of eye measurements.](cls-imbalance_files/figure-html/fig-myopia-splom-1.svg){#fig-myopia-splom fig-align='center' width=90%}\n:::\n:::\n\n\n## Underlying Mathematical Issues {#sec-imbalance-math}\n\nThere are several aspects of class imbalance that make it one of the most challenging problems in machine learning. \n\nFirst, most performance metrics are not robust to imbalances. Recall that in @sec-cls-metrics, accuracy was used as a prime example. For a two-class outcome with an event rate of $\\pi$, a model can automatically achieve an accuracy of $1 - \\pi$ _without any predictors_, simply by predicting the majority class. This is the case for most metrics. The areas under the ROC and PR curves are notably effective in the presence of an imbalance [@sec-compare-roc-pr], as is the Kappa statistic. In other words, while still being accurate, it will not treat the minority class data with the importance that it has to the modeling project. \n\nSecond, as one would expect, classes with the most data are most likely to be predicted well. For example, if the minority class is associated with the event of interest, the confidence intervals for specificity will reflect greater certainty (i.e., narrower intervals) than those for sensitivity. Another consequence of one class having most of the training data is that the model may be drawn to patterns in the majority class. This is especially true for models that have high complexity (e.g., low bias). To demonstrate, @fig-myopia-cart shows a simple CART model fit to the myopia training data (using automated cost-complexity pruning). The first split captures a large majority of the training set since these are easily predicted to be non-myopic children. The remainder of the tree is splitting small amounts of non-myopic data to create pure terminal nodes. \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A CART fit to the myopia training set. Most of the training set data are covered by the initial split, which focuses on the majority class.](cls-imbalance_files/figure-html/fig-myopia-cart-1.svg){#fig-myopia-cart fig-align='center' width=90%}\n:::\n:::\n\n\n\nThe model is doing what it is intended to do; it does not know that we might be most interested in our minority class samples. The default objective function doesn't account for class frequencies, so it optimizes over the entire training set, giving the majority class the most leverage. \n\nThis also brings up a pattern that can occur when dealing with class imbalances that is somewhat counterintuitive: models that are _less_ complex tend to work better than highly complex models. In essence, the more models can adapt to the data, the more likely they are to overfit to the majority class. The decision tree shown above is a good example. Tree-based models employ a greedy optimization approach to train the model and can carve out small subspaces of predictors to make local predictions. It would be very difficult for a logistic regression to do the same, at least without enormously complex feature engineering. As a result, these simpler models can have sufficient predictive power to be useful, but not so much as to discern minute patterns in the data, thereby correctly predicting the few events and risking overfitting.  We'll see this pattern in the example data, which is admittedly small, but it has also repeated in much larger datasets with class imbalances. \n\nThird, the event rate significantly affects our class probability estimates, even in non-Bayesian models. Recall from @sec-nonlinear-da that one of the prior distributions corresponds to the probability distribution of our outcome ($Pr[Y = k]$ in @eq-discrim). If our supervised model is represented by $Pr[\\boldsymbol{x}|Y = k]$, its predicted probabilities are shown by the event rate to produce the posterior probability. In other words, if the minority class event rate is 5% and our model predicts that a sample estimates the probability of being an event as 80%, the overall (posterior) probability of being an event is about 17%. This raises the bar for the supervised model, requiring exquisite performance to overcome the prevalence. With this event rate, the model would have to produce a class probability estimate of 95% or more to produce a posterior probability of at least 50%. We'll discuss this idea in the next section as well as in @sec-prevalence-again.\n\nIn future sections, we’ll use two predictors in the myopia data to visualize the effects of different techniques. In the upper-left panel of the lower diagonal of @fig-myopia-splom we see the spherical equivalent refraction and vitreous chamber depth predictor data from the training set, colored by the outcome class. A flexible discriminant analysis model using MARS basis functions was used to model these data. Interactions between hinge functions were allowed, and generalized cross-validation was used to automatically determine the optimal level of model complexity. The model was fit to the training set, and @fig-myopia-fda shows the class boundary, defined using the 50% probability cutoff. The events (in red) occupy the space to the left of the main data stream. The class boundary does its best to find the optimal partition of the data, but we can see that less than half of the training set events are not to the left of the boundary. \n\n\n\n:::: {#fig-myopia-fda}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](cls-imbalance_files/figure-html/myopia-fda-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\nA flexible discriminant fit to the myopia training set. Using the default probability cutoff of 50%, shown as the black contour, 24 of the 60 events are correctly predicted as events. \n::::\n\nBefore moving on to specific techniques, let's discuss a potential fallacy encountered when there are rare events: correctly predicting the events to the detriment of the overall model goals is often inappropriate. \n\n## Imbalances Make Us Focus on Our Goals {#sec-imbalance-goals}\n\nBefore starting the model development process, it is essential to consider how class balance impacts the project goals. It is often the case that the fact that events rarely occur means that the goal of the model is to identify the precious events in a large dataset (such as in information retrieval), even if it is at the cost of more false positives than we would ordinarily tolerate. In other words, finding the events is of paramount importance, and other considerations, such as interpretability, inference, and explanation of predictions, are not terribly important. This is often the case where the hard class prediction is used in an automated system that only takes the qualitative prediction as input. For example, @apm discusses a model used for a high-performance computing grid to route computational jobs to the correct queue based on their execution time. The system only needs to know where to send the job, and measures of uncertainty, such as estimated probabilities for how long the job will take, play no role in the process. \n\nA good example of such a project was worked on by one of the authors for clinical trials of a Parkinson's Disease (PD) therapy. Patients with PD have issues with movement, including tremors, slowness, and physical instability. One problem with neurological diseases is that they might be misdiagnosed. For example, @meara1999accuracy show data where several other diseases may present as PD, such as gait apraxia or dementia preceding motor symptoms. \n\nFor clinical trials, this can be a major issue. If a patient is enrolled who does not have Parkinson's Disease, any therapy for PD will not show improvements. Most regulatory bodies require an \"intention-to-treat\" analysis of clinical trial data, in which patients are grouped by their original diagnosis, regardless of what is learned after entry. This means that any misdiagnosed PD patients' results will count against the evaluation of how well the therapy works.\n\nFor this reason, a project was started to determine whether an ML model could be trained on external patient data to predict which patients originally diagnosed with PD actually had PD based on longer-term follow-up data. An effective model would be used to create an exclusion criterion for entry to the trial: if we predict that there is sufficient evidence that they might not have PD, they should not be included in the study. Even though the prevalence of PD is low, the company was willing to accept erroneously excluding true PD patients as long as they were extremely confident that only PD patients were enrolled. \n\nBased on this, we would pursue an objective function that focused on finding events and did not require reporting well-calibrated probabilities^[The author's manager, a dogmatic Bayesian, strongly discouraged spending time on the project since there was no way to overcome the low event rate. In the end, it was immaterial; the initial clinical data showed that the drug was ineffective, and the whole project was cancelled.]. The model would require significant clinical validation to demonstrate that our probability threshold for calling a patient \"truly PD\" meets its stated performance characteristics. Here, PR-curves and Gain Charts would be of paramount importance. \n\nHowever, it can also be the case that we need to understand why the model works or why a data point was predicted to belong to a specific class. In @sec-drug-interactions, we’ll discuss a project from drug development where a model is used to estimate the risk of a potential drug causing patient harm. We definitely want to find the events (e.g., problematic drugs), but we also need a good understanding of how certain we are that there is an issue. \n\n::: {.important-box}\nIn other words, we should not confuse what we will do to deal with extreme frequencies with the idea that we should just care about finding important events. \n:::\n\nThese issues are important to consider because, for class imbalances, several approaches can be taken to mitigate the disparity in class frequencies. Some of them can subvert the accuracy of the predicted class probabilities (i.e., result in poor calibration). If we require a model that predicts useful probabilistic outputs, we would avoid some modeling techniques. \n\nAdditionally, we should settle on what performance metrics are most important. We may need multiple metrics, including some to measure calibration (Brier scores), overall class separation (ROC curves), and/or the ability to detect events (PR curves). Additionally, knowing how well the hard class predictions work may require attention to sensitivity, specificity, recall, and other metrics. \n\nThese are all issues that should be thought about for every modeling project. We emphasize it here because imbalances, especially extreme ones, substantially raise the level of difficulty.\n\nThe next two sections will outline two general philosophies when modeling imbalanced data. The first is to accept that the imbalance and model it as-is. We can utilize all the tools and methods previously discussed to create a model driven by performance statistics based on probabilistic predictions, such as log-loss or the area under the receiver operator characteristic curve. Once we have an appropriate model for these criteria, we can focus on how to postprocess the predictions to produce the optimal hard class predictions. \n\nThe second philosophy is to circumvent the issue of the imbalance by either removing it or by making the performance metric aware that some training set points (e.g., the events) are more important than others. Both approaches bias the model to be less focused on the majority class.\n\nBoth philosophies have advantages and disadvantages. Generally, if finding the events is the overriding goal, the second philosophy will likely be the best choice.   \n\n## Strategy 1: Accepting the State of Nature {#sec-imbalance-acceptance}\n\nIn this case, we'll try to build ML models while generally ignoring the imbalance. The idea is that we should initially focus on optimizing metrics based on class probability estimates, such as the Brier score, cross-entropy, or the areas under the ROC or PR curves. Once we have a model with good calibration and/or discrimination, we can then focus on making the best hard class predictions. We'll show the advantages and disadvantages of using this strategy. \n\nTo get started, we'll fit a subset of models to these data and generally follow the process outlined in the previous chapters. There is little to no preprocessing required for these models. There is a single pair of highly correlated predictors, but that will be handled by the model in situations where multicorrelation is a concern. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nA set of five models was tested. First, a regularized logistic regression was used with glmnet penalties. The predictors included main effects and all two-factor interactions. Penalty values ranging from 10<sup>-4</sup> to 10<sup>-0.6</sup> were evaluated with four mixtures that ranged from 0% to 100% Lasso. A regular grid of 72 candidates was used to optimize these parameters^[We use the original implementation of the glmnet model, which features a capability where, for a specific mixture value, a single model fit contains results for multiple penalty values. For this reason, we can evaluate a regular grid with  72 candidates with only 4 model fits, one for each mixture value. A regular grid exploits this feature, but a space-filling design would nullify this advantage.].\n\nSingle-layer neural networks were evaluated and were tuned over: \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- The number of hidden units (2 to 50). \n- The L<sub>2</sub> penalty (10<sup>-10</sup> to 10<sup>-1</sup>).\n- The learning rate (10<sup>-4</sup> to 10<sup>-1</sup>).\n:::\n\n::: {.column width=\"50%\"}\n- The batch size (8 to 256 samples).\n- AdamW momentum (0.8 to 0.99).\n:::\n\n::::\n\nSGD via AdamW was used to optimize the model for up to 100 epochs, with early stopping after 5 consecutive poor results. ReLU activation was used. \n\nBoosted trees (via light GBM) were also trained for up to 1,000 iterations with early stopping after 5 bad iterations. The model was tuned over: \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Tree depth (1 to 15 splits). \n- Amount of data needed for further splitting (2 to 40 samples).\n:::\n\n::: {.column width=\"50%\"}\n- The learning rate (10<sup>-3</sup> to 10<sup>-0.5</sup>).\n- $m_{try}$ (1 to 16 predictors).\n:::\n\n::::\n\nAdditionally, random forests with 2,000 classification trees were tuned over: \n\n- Amount of data needed for further splitting (2 to 40 samples).\n- $m_{try}$ (1 to 16 predictors)..\n\nFinally, a RuleFit model was optimized over: \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- The number of boosting iterations (5 to 100)\n- Tree depth (2 to 4 splits). \n- Amount of data needed for further splitting (2 to 10 samples).\n\n:::\n\n::: {.column width=\"50%\"}\n- The learning rate (10<sup>-10</sup> to 10<sup>0</sup>).\n-  Proportional $m_{try}$ (10% to 100%).\n- The L<sub>1</sub> penalty (10<sup>-10</sup> to 10<sup>0</sup>).\n:::\n\n::::\n\nApart from the logistic regression, a space-filling design of 25 candidates was used for optimization. \n\n@fig-myopia-plain-roundup visualizes the results across and within models. Each point represents a candidate within a model. These are ranked by their resampled Brier score and are ordered accordingly. The vertical lines help highlight where the best candidate was located in the ranking. Interestingly, the logistic regression and RuleFit models had the best results and were very similar (see discussion below). The other three models had both good and bad performance across their candidates, but, in terms of model calibration, did not do as well. This may be due to the fact that this data set is better served by simpler models; the added complexity generated by large tree ensembles and neural networks is likely overfitting to some degree. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Cross-validated Brier scores for different modeling approaches without adjustments for class imbalance. The dashed vertical lines indicate the highest-ranked candidate for each model.](cls-imbalance_files/figure-html/fig-myopia-plain-roundup-1.svg){#fig-myopia-plain-roundup fig-align='center' width=80%}\n:::\n:::\n\n\nWhile the Rulefit and logistic regressions had the smallest Brier scores, their respective best candidates have issues. For the RuleFit model, the maximum estimated probability of a patient being myopic is 71.5% across all of the assessment sets. While the values are well-calibrated within the observed data range, this upper limit of probability is probably too small to be the final model. We would expect at least one of the held-out predictions to be closer to one when the patient is truly myopic. We also do not have a clear sense of how well-calibrated the predictions are when values are closer to one. \n\nFor the regularized logistic model, @fig-myopia-logistic illustrates the relationship between the tuning parameters and the Brier score. We can see that while the numerically best results are to use 100% Lasso regularization with a penalty of 10<sup>-2</sup>, there are several other combinations with nearly equivalent metrics. The model fitted on the entire training set could have a maximum of 136 model coefficients (apart from the intercept) since there are 16 main effects and 120 possible two-way interactions. However, the Lasso model can remove predictors, and the fitted model contains only 1 main effect and 16 interactions. For a model used purely for prediction, this is not a terrible situation. However, the lack of main effects is troubling since there are interaction terms in the model with no corresponding main effects. This is inconsistent with the heredity principle discussed in @sec-interactions-principles and suggests that these interactions are unlikely to be real. \n\nTo sidestep this issue, we can choose a different tuning parameter candidate. If we use _no_ Lasso regularization, the model will contain all parameters, but regularizes some of them to be close to zero. This would ensure that all of the interaction terms have corresponding main effects in the model. Taking this approach, the penalty associated with the smallest Brier score is 10<sup>-0.8</sup>. For this model, the resampling estimate of the Brier score is 0.0864%, which is 2.8 worse than the original choice, but still very good when compared to the other models. We'll use this candidate going forward. \n\nAdditionally, @fig-myopia-logistic shows some diagnostics for the logistic regression. These plots are constructed by obtaining the assessment set predictions across the 50 resamples. Note that since repeated cross-validation was used, each training set point has five replicates in the held-out data, totaling 23,100 rows across all resamples. To create the calibration and ROC curves, the five predictions for each training set point are averaged so that we have a set of 462 rows of predictions. Please note that these are approximate and may differ from the performance statistics generated by resampling. For example, @fig-myopia-logistic shows a single curve while the resampled ROC AUC is the average of 50 ROC curves created with 10% of the training set. Despite this, the visualizations can help us characterize the quality of the fit and detect any significant issues in the predictions. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Results of the logistic regression model optimization (without adjustments for the class imbalance). The top plot shows the performance profile for the two tuning parameters. Underneath are the calibration and ROC curves. The solid blue point on the ROC curve corresponds to the default cutoff. The pooled averaged holdout predictions were used to compute these visualizations.](cls-imbalance_files/figure-html/fig-myopia-logistic-1.svg){#fig-myopia-logistic fig-align='center' width=80%}\n:::\n:::\n\n\nThe calibration curve is not terrible; the lower range of the class probabilities is well-calibrated. The curve does deviate from the diagonal line when the probability midpoint on the x-axis is greater than about 30%. However, note from the shaded 90% confidence intervals that the variation around the curve is very large, largely due to the few samples with high probabilities of myopia. \n\nThe ROC curve is symmetric and fairly unremarkable. The visualization shows that the default 50% threshold for converting the probabilities to hard \"yes/no\" predictions is somewhat problematic. The sensitivity for this cutpoint is very poor (16.4%) while the specificity is exquisite (98.2%). This is fairly common for class imbalances. The model performs significantly better where the data are more abundant, and since most participants in the study are not myopic, high specificity is more easily achieved. \n\nWe quantified performance for these models using a variety of methods. @tbl-plain-metrics shows several statistics for both hard and soft prediction types. When assessing class probability estimates, the Brier scores for each model are all acceptably low, and most areas under the ROC curves have objectively large values. The areas under the precision-recall curves are mediocre, with the largest possible value being 1.0 and the worst value is 0.13. \n\n:::: {.columns}\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"90%\"}\n\n::: {#tbl-plain-metrics}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"leuxeszzit\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#leuxeszzit table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#leuxeszzit thead, #leuxeszzit tbody, #leuxeszzit tfoot, #leuxeszzit tr, #leuxeszzit td, #leuxeszzit th {\n  border-style: none;\n}\n\n#leuxeszzit p {\n  margin: 0;\n  padding: 0;\n}\n\n#leuxeszzit .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#leuxeszzit .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#leuxeszzit .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#leuxeszzit .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#leuxeszzit .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#leuxeszzit .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#leuxeszzit .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#leuxeszzit .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#leuxeszzit .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#leuxeszzit .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#leuxeszzit .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#leuxeszzit .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#leuxeszzit .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#leuxeszzit .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#leuxeszzit .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#leuxeszzit .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#leuxeszzit .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#leuxeszzit .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#leuxeszzit .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#leuxeszzit .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#leuxeszzit .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#leuxeszzit .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#leuxeszzit .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#leuxeszzit .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#leuxeszzit .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#leuxeszzit .gt_left {\n  text-align: left;\n}\n\n#leuxeszzit .gt_center {\n  text-align: center;\n}\n\n#leuxeszzit .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#leuxeszzit .gt_font_normal {\n  font-weight: normal;\n}\n\n#leuxeszzit .gt_font_bold {\n  font-weight: bold;\n}\n\n#leuxeszzit .gt_font_italic {\n  font-style: italic;\n}\n\n#leuxeszzit .gt_super {\n  font-size: 65%;\n}\n\n#leuxeszzit .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#leuxeszzit .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#leuxeszzit .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#leuxeszzit .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#leuxeszzit .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#leuxeszzit .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#leuxeszzit .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#leuxeszzit .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#leuxeszzit div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Model\">Model</th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"4\" scope=\"colgroup\" id=\"Hard Class Prediction Metrics\">\n        <div class=\"gt_column_spanner\">Hard Class Prediction Metrics</div>\n      </th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"3\" scope=\"colgroup\" id=\"Probability Prediction Metrics\">\n        <div class=\"gt_column_spanner\">Probability Prediction Metrics</div>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Accuracy\">Accuracy</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Sensitivity\">Sensitivity</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Specificity\">Specificity</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Kappa\">Kappa</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"ROC-AUC\">ROC AUC</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"PR-AUC\">PR AUC</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Brier-Score\">Brier Score</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Model\" class=\"gt_row gt_left\">Logistic (glmnet)</td>\n<td headers=\"Accuracy\" class=\"gt_row gt_right\">87.7%</td>\n<td headers=\"Sensitivity\" class=\"gt_row gt_right\">16.4%</td>\n<td headers=\"Specificity\" class=\"gt_row gt_right\">98.2%</td>\n<td headers=\"Kappa\" class=\"gt_row gt_right\">0.197</td>\n<td headers=\"ROC AUC\" class=\"gt_row gt_right\">0.849</td>\n<td headers=\"PR AUC\" class=\"gt_row gt_right\">0.536</td>\n<td headers=\"Brier Score\" class=\"gt_row gt_right\">0.0864</td></tr>\n    <tr><td headers=\"Model\" class=\"gt_row gt_left\">Neural Network</td>\n<td headers=\"Accuracy\" class=\"gt_row gt_right\">86.3%</td>\n<td headers=\"Sensitivity\" class=\"gt_row gt_right\">12.4%</td>\n<td headers=\"Specificity\" class=\"gt_row gt_right\">97.3%</td>\n<td headers=\"Kappa\" class=\"gt_row gt_right\">0.125</td>\n<td headers=\"ROC AUC\" class=\"gt_row gt_right\">0.800</td>\n<td headers=\"PR AUC\" class=\"gt_row gt_right\">0.401</td>\n<td headers=\"Brier Score\" class=\"gt_row gt_right\">0.0980</td></tr>\n    <tr><td headers=\"Model\" class=\"gt_row gt_left\">Random Forest</td>\n<td headers=\"Accuracy\" class=\"gt_row gt_right\">87.2%</td>\n<td headers=\"Sensitivity\" class=\"gt_row gt_right\">21.4%</td>\n<td headers=\"Specificity\" class=\"gt_row gt_right\">96.9%</td>\n<td headers=\"Kappa\" class=\"gt_row gt_right\">0.236</td>\n<td headers=\"ROC AUC\" class=\"gt_row gt_right\">0.852</td>\n<td headers=\"PR AUC\" class=\"gt_row gt_right\">0.501</td>\n<td headers=\"Brier Score\" class=\"gt_row gt_right\">0.0889</td></tr>\n    <tr><td headers=\"Model\" class=\"gt_row gt_left\">Boosted Tree</td>\n<td headers=\"Accuracy\" class=\"gt_row gt_right\">87.2%</td>\n<td headers=\"Sensitivity\" class=\"gt_row gt_right\">17.3%</td>\n<td headers=\"Specificity\" class=\"gt_row gt_right\">97.6%</td>\n<td headers=\"Kappa\" class=\"gt_row gt_right\">0.200</td>\n<td headers=\"ROC AUC\" class=\"gt_row gt_right\">0.864</td>\n<td headers=\"PR AUC\" class=\"gt_row gt_right\">0.508</td>\n<td headers=\"Brier Score\" class=\"gt_row gt_right\">0.0884</td></tr>\n    <tr><td headers=\"Model\" class=\"gt_row gt_left\">RuleFit</td>\n<td headers=\"Accuracy\" class=\"gt_row gt_right\">87.7%</td>\n<td headers=\"Sensitivity\" class=\"gt_row gt_right\">11.1%</td>\n<td headers=\"Specificity\" class=\"gt_row gt_right\">99.0%</td>\n<td headers=\"Kappa\" class=\"gt_row gt_right\">0.141</td>\n<td headers=\"ROC AUC\" class=\"gt_row gt_right\">0.869</td>\n<td headers=\"PR AUC\" class=\"gt_row gt_right\">0.541</td>\n<td headers=\"Brier Score\" class=\"gt_row gt_right\">0.0875</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\nResampled performance metrics for the best candidates across different models. The models were optimized based on the Brier score. \n \n:::\n\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n:::: \n\nThe hard class predictions in @tbl-plain-metrics use the default 50% probability cutoff. A few of the models have accuracies that are larger than the rate that the majority class occurs in the training set (87%). The Kappa statistics indicate a significant signal in the models for predicting qualitative class membership. However, with this cutoff, the model is effectively useless at predicting which people are myopic, as the best possible sensitivity is 21.4%. \n\nIf we are satisfied with the model's ability to discriminate between classes (as evidenced by the ROC curve) and that the probabilities are sufficiently calibrated, we can address the sensitivity problem by selecting an appropriate probability threshold. As with most optimization problems, we require an objective function to make the best decision regarding this tuning parameter. There are a few ways we can approach the problem: \n\n 1. Given what we know about how the model will be used, we could define specific costs for false negatives and false positives (with correctly predicted classes having zero cost). Using these values, we can choose a threshold that minimizes the cost of the decision. This is similar to a weighted Kappa approach, except the weights would not be symmetric. \n 2. Seek a constrained optimization. For example, we can try to maximize the sensitivity under the constraint that the specificity is at an acceptable value. The ROC curve provides the substrate for these calculations, as it includes sensitivity and specificity for various thresholds in the data. \n 3. Optimize an existing composite score that captures the right balance of sensitivity and specificity. The Matthews correlation coefficient and F-statistics are examples of such metrics. \n 4. Use a multiparameter optimization method, such as desirability functions from @sec-cls-multi-objectives, to define a jointly acceptable result (i.e., overall desirability). This would enable us to consider multiple performance metrics, as well as other variables, in the decision-making process. \n \nWe'll focus on the first two options. \n\nTo choose the threshold using a custom cost metric (the first option in the list above), we would have to determine an appropriate cost structure for false positives and false negatives, usually based on the consequences of either error. For our myopia example, this is a somewhat subjective choice that depends on some context-specific rationale. In other cases, the costs might be analytically calculated as a function of monetary costs [@zadrozny2001learning]. @fig-class-cost shows examples of cost curves if the false positive cost is fixed at 1.0 and the cost of a false negative varies in severity. The y-axis is the mean of the cost values averaged across the 50 assessment sets. \n \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Cost curves when the cost of a correct prediction is zero, the cost of a false positive is 1.0, and the cost of a false negative varies.](cls-imbalance_files/figure-html/fig-class-cost-1.svg){#fig-class-cost fig-align='center' width=60%}\n:::\n:::\n\n\nThe figure shows that increasing the cost of incorrectly predicting the events yields increasingly smaller thresholds. This is due to classifying more points as events, regardless of their actual class. The cost metric is punished less by incorrectly predicted non-events, so the overall cost statistic decreases substantially in the curves with higher false negative costs. \n\n::: {.important-box}\nIt is important to realize that this strategy only optimizes the probability threshold. The model fit (e.g., slope and intercept parameter estimates) has not been affected by the cost function.  In the next section, we will consider cost as the objective function that the model uses during training in @sec-cost-sensitive-learning. \n:::\n\nMoving on to constrained optimization, when choosing a threshold using an ROC curve, an immediate question arises: \"Which data should be used to compute the ROC curve?\" _Preferably_, the cutoff selection should use different data than the data used to fit the model. For example, diagnostic tests that require approval from a government agency (e.g., the Food and Drug Administration in the US) require a different data set to set the threshold. However, this may not be feasible if there is not an abundance of data or the resources to acquire such data. If we can't hold out another data set, we can use the assessment sets generated during resampling^[Or a validation set if multiple resamples are not used.]. There are at least two different approaches. \n\n - **Option 2A - _post hoc_ pooling**: After we have resampled our model configurations, we can pool the assessment set predictions for the best candidate. This ROC curve is computed based on the overall existing class probability estimate values. This approach generated the visualizations in @fig-myopia-logistic. \n\n- **Option 2B - threshold tuning**: During model tuning, we treat the probability threshold as a tuning parameter and include it in the grid search. For each threshold in the grid, conditional on any other tuning parameters, we have resampling estimates for sensitivity and specificity. These points are then used to create an ROC curve. \n\nThe primary difference between these approaches lies in data usage: do we compute one large ROC curve from pooled data or an ROC curve from resampled statistics? Statistically, the latter is more appropriate, but either might produce acceptable results.\n\nNote that there are a few options when we treat the threshold as a tuning parameter. A one-stage approach would include the threshold along with any other tuning parameters and use a single grid for all^[When adding the threshold to the optimization grid, it might be helpful to start with a space-filling design and _crossing_ it with a vector of thresholds. This allows us to have data on the thresholds for each of the candidate points. There is minimal overhead associated with this approach, as the thresholding computations occur after preprocessing and model fitting. For example, if an initial grid with 25 points is crossed with 10 threshold values,  there are still only 25 model configurations to train, which takes the longest time to compute.]. However, recall that the choice of this parameter does not affect any probability-based metrics, such as the Brier score, log-loss, or the area under the ROC curve. Instead, we could use a two-stage approach where the non-threshold parameters are optimized to find their best results, and then we can use a one-dimensional grid to find the best threshold (based on metrics appropriate for hard class predictions). We took this approach here: 20 threshold values ranging from 2% to 40% defined the second-stage grid. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Two approaches for optimizing the probability threshold via the ROC curve. Both curves use the regularized logistic regression's resampling results.](cls-imbalance_files/figure-html/fig-myopia-two-roc-1.svg){#fig-myopia-two-roc fig-align='center' width=50%}\n:::\n:::\n\n \nFor the logistic regression model, @fig-myopia-two-roc shows the ROC curves for both schemes. The curves significantly overlap for these data. Note that the curve associated with a second-stage grid contains 20 unique points since it is based on the values used in our second-stage grid. The curve for the _post hoc_ pooling method has 462 points, which represents how many unique predicted probabilities are contained in the _post hoc_ data pool. \n \nUsing either method for composing the ROC tool provides us with a tool to make trade-offs between sensitivity and specificity. For example, if we want to maximize sensitivity on the condition that specificity is at least 80%, the curve produced by threshold tuning indicates that the cutoff should be 16%. The estimated statistics associated with this value are a sensitivity of 75.7% and a specificity of 81.6%. \n\nThis overall approach involves finding a suitable ML model that does not allow class imbalance to alter our methodology, while deferring cutoff selection until the end of the process. This approach can lead to a well-calibrated model that appropriately classifies new samples. The downside becomes apparent when trying to explain individual results to the model's consumers. \n\nFor our myopia example, suppose we make a prediction for a patient where the probability of myopia is estimated to be 20%. The conversation with the patient might sound something like\n\n> \"We estimate that your child has a 20% chance of being severely nearsighted and, based on this, we would give them a diagnosis of myopia.\" \n\nIt is natural for someone to find the diagnosis counterintuitive since they are probably comparing the probability to the natural 50% threshold. It might lead the parent to ask:  \n\n> \"That probability seems pretty low to make that diagnosis. Is that a mistake?\"\n\nThe non-technical response could be:\n\n> \"We think that the diagnosis is appropriate because the overall chances of being myopic are low, and we have determined that our cutoff for making the diagnosis (16%) gives us the most effective diagnoses.\"\n\nOf course, this is paraphrasing, but the hypothetical conversation does illustrate the potential confusion. \n\nThis seeming disconnect can become worse as the prevalence becomes smaller. For very rare events (i.e., < 5%), the cutoff associated with the sensitivity-specificity trade-off can be very small, perhaps less than 1%. This is the price paid for having probability estimates that are consistent with the rate at which the event occurs in the wild. \n \n## Strategy 2: Correcting the Imbalance {#sec-imblance-correction}\n\nIn the previous section, the only operation used to account for the small minority class was to optimize the probability threshold _after_ the model had been chosen. In this section, we'll consider several methods for actively addressing the imbalance during model optimization. The two primary classes of techniques are: \n\n- Changing the composition of the training set data.\n- Upweighting the minority class's effect on the objective function.\n\nAs we'll see, these can have a profound effect on the class probability estimates (both good and bad). These techniques are also most effective when the primary goal of the model is to identify events in new data. \n\n### Sampling and Synthesis Techniques {#sec-sampling-synthesis}\n\nOne strategy for addressing a class imbalance is to eliminate it in the training set by adding or removing rows, depending on the class. A \"sampling\" method would remove existing rows from the data (likely from the majority class) to even the class frequencies. The removal may be random or based on characteristics of the existing data. Other sampling methods can oversample existing minority data. \"Synthesis\" methods invent new rows of the data in a way that makes them similar to or consistent with existing examples from the same class. \n\nRegardless of the method used, note that rebalancing occurs only on the data being used to train the model. If we are within a resampling loop, the analysis sets are affected, but the assessment sets are not. If we are going to fit a model to the entire data set, the training set is rebalanced, but the test set^[And validation set, if any] is unaffected. The reason for this is that the data used to measure how well the models work should be drawn from the same population as the data the model will eventually be used to predict. Rebalancing tools deliberately bias the sample to be unlike the population being predicted. \n\nNote that all the techniques mentioned here should be considered as yet another preprocessing method. As such, it is in the interest of the modeler to resample the models and include these steps within the resampling process. It would be inappropriate to apply a technique such as downsampling and then resample the data. Since these methods involve randomness, we need to account for it in our performance statistics. \n\nAlso, it is unclear when these techniques should be placed in the preprocessing order. This likely depends on what other operations are planned for the data. For example, if downsampling is used, a zero-variance filter is best applied after the sample size is reduced. \n\nWe'll describe each technique in the context of its use to modify the training set (or the analysis set when resampling). However, for models that resample the data, such as stochastic gradient boosting or random forests, these tools could also be applied within each resample. For example, our first technique described below, _downsamples_ the majority class to reduce its size. A random forest model could do the same by using a stratified bootstrap sample that produces the sample number of data points for each class [@chen2004using]. @chawla2003smoteboost and @galar2011review provide similar examples. This is highly dependent on the implementation, but it might be advantageous because the sampling/synthesis methods would be applied repeatedly, which can reduce model variance by exposing it to more unique data points. \n\n#### Downsampling {#sec-downsampling}\n\n\n\n\nThis approach is straightforward: every class with more data than the minority class is randomly sampled, ensuring that all classes have the same number of training set points. The minority class stays the same. For our myopia data, the majority class is randomly reduced from 402 to 60 patients. @fig-myopia-subsampling show examples for the two predictor subset including how the FDA fit changes with the reduced data set. The same class boundary is shown within each of the facets. We can see that the boundary is slightly more simplistic than @fig-myopia-fda but has shifted to the right, capturing more events with the default 50% cutoff (at the expense of increased false positives). \n\n\n\n::: {#fig-myopia-subsampling}\n\n::: {.figure-content}\n\n```{shinylive-r}\n#| label: fig-subsampling\n#| out-width: \"80%\"\n#| viewerHeight: 600\n#| standalone: true\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(munsell)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\n# source(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-sa.R\")\n\nui <- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 10, -1)),\n    column(\n      width = 10,\n      selectInput(\n        inputId = \"method\",\n        label = \" \",\n        choices = c(\n          \"Downsampled\",\n          \"SMOTE\",\n          \"ROSE\",\n          \"Near Miss\",\n          \"Tomek\"\n        ),\n        selected = c(\"Downsampled\")\n      )\n    )\n  ),\n  as_fill_carrier(plotOutput(\"plot\")),\n  br(),\n  textOutput(\"text\")\n)\n\nserver <- function(input, output) {\n  load(url(\n    \"https://raw.githubusercontent.com/aml4td/website/main/RData/imbalanced_sampled.RData\"\n  ))\n\n  xrng <- range(imbalanced_sampled$spherical_equivalent_refraction)\n  yrng <- range(imbalanced_sampled$vitreous_chamber_depth)\n\n  orig <- imbalanced_sampled %>%\n    dplyr::filter(Data == \"Original\")\n\n  output$plot <-\n    renderPlot(\n      {\n        dat <- imbalanced_sampled %>%\n          dplyr::filter(Data == input$method) |>\n          dplyr::bind_rows(orig) |>\n          dplyr::mutate(\n            Data = factor(Data, levels = c(\"Original\", input$method))\n          )\n\n        grid <- imbalanced_grid %>%\n          dplyr::filter(Data == input$method) |>\n          dplyr::select(-Data) |>\n          filter(!is.na(.pred_yes))\n        p <-\n          dat |>\n          ggplot(\n            aes(\n              spherical_equivalent_refraction,\n              vitreous_chamber_depth\n            )\n          ) +\n          geom_point(\n            aes(\n              col = class,\n              pch = class\n            ),\n            cex = 1.6,\n            alpha = 2 / 3\n          ) +\n          facet_wrap(~Data) +\n          coord_fixed(ratio = 1.4, xlim = xrng, ylim = yrng)\n\n        p <- p +\n          theme(legend.position = \"top\") +\n          scale_color_brewer(drop = FALSE, palette = \"Set1\", direction = 01) +\n          theme_light_bl()\n\n        p <-\n          p +\n          geom_tile(data = grid, aes(fill = .pred_yes), alpha = .1) +\n          geom_contour(\n            data = grid,\n            aes(z = .pred_yes),\n            breaks = 1 / 2,\n            col = \"black\"\n          ) +\n          scale_fill_gradient2(\n            low = \"#377EB8\",\n            mid = \"white\",\n            high = \"#E41A1C\",\n            midpoint = 0.5\n          ) +\n          labs(\n            x = \"spherical equivalent refraction\",\n            y = \"vitreous chamber depth\",\n            fill = \"Probability\"\n          )\n        print(p)\n      },\n      res = 100\n    )\n  \n  output$text <-\n    renderText(\n      paste(\n        \"Using the default 50% threshold, this method correctly identified\", \n        imbalanced_hits[input$method], \n        \"truly myopic subjects (out of 60).\") \n    )\n}\n\napp <- shinyApp(ui = ui, server = server)\napp\n```\n\n:::\n\nThe effect of a few sampling and synthesis techniques on two of the predictors in the myopia training set. The class boundary is based on a 50% probability cutoff.\n\n:::\n\nBesides rebalancing, the advantage of downsampling is that models can be trained faster. The downside is that important information might be lost in the majority class(es) due to random sampling. We are diminishing the influence of the majority class and creating a smaller training set. Another effect of this is that our models will tend to be more simplistic, as the reduced data cannot support excessive complexity. \n\n\n\n\n\n#### Upsamping {#sec-upsampling}\n\nUpsampling will create random replicates of the majority class(es) so that all classes have the same frequency in the training set. This increases the training set, sometimes to orders of magnitude larger, so training time can become excessive for some models. There is also the issue of having a potentially large number of replicates for each minority^[If there are more than two classes, you can substitute \"non-majority\" class here.] class value in the original training set. For example, for models such as random forest that use bootstrap samples to fit models, we could have even more replicates of these data points. Another complication occurs when a model such as _K_-nearest neighbors is used; all of the nearest neighbors might be the same data point. \n\nUpsampling is probably the least used of these techniques due to these issues. \n\n#### SMOTE {#sec-smote}\n\nThe Synthetic Minority Oversampling Technique (SMOTE) is a method that combines random sampling with the creation of novel data points, which are fusions and/or perturbations of existing points. There are a few flavors of this method, but we'll focus on the original technique by @chawla2002smote where all predictors are required to be numeric. \n\nFor each data point in the minority class, compute its K-nearest neighbors. The create a new artificial data point, we select a minority class point at random ($\\boldsymbol{x}_i$) as well as one of its neighbors. For each predictor, we compute the difference between the original data point and its chosen neighbor (call this $\\delta_j$ for predictor $j$). For each predictor, we compute a uniform random number $u_j \\sim U(0, 1)$ and modify the original point as $x^*_{ij} = x_{ij} + u_j\\delta_j$. Essentially, SMOTE generates a hyperrectangle between the selected point and its chosen neighbor, and then randomly places the new sample within the hyperrectangle. \n\n@fig-smote-rose illustrates the process using a selected point (large red circle) in the north-west portion of the previously shown predictor space. A neighbor was selected below and to the right of the original point (large red square). The dotted lines show the area where newly synthesized points can be placed, and twenty examples are shown (open circles). \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Demonstrations of how SMOTE and ROSE synthesize new samples. The chosen point is the large red circle, and SMOTE's selected nearest neighbor is the large red square. The ellipses in panel (b) represent Gaussian probability contours based on the kernel density estimate. Twenty points are synthesized for each panel.](cls-imbalance_files/figure-html/fig-smote-rose-1.svg){#fig-smote-rose fig-align='center' width=80%}\n:::\n:::\n\n\nNote that SMOTE does not center the new samples around the original point; they can only be adjacent to it, since newly created data points must lie between it and the neighbor. This has the effect of clumping or clustering points to some degree. This can be seen in @fig-myopia-sampling where there are gaps in the minority class distribution where no minority class data reside.  This can be resolved by increasing the number of neighbors, but adding too many would lead to overlap in predictor distributions for each class. One could tune the number of neighbors, but it may not produce a clear winner for this parameter.  \n\nUsing the default of five nearest neighbors, the SMOTE version of the data is shown on the right in @fig-myopia-sampling. Due to the tight clustering, the FDA class boundary appears to slightly overfit the new training set. If the real data only contained these two predictors, we would probably want to tune the number of SMOTE nearest neighbors along with any other modeling or preprocessing parameters. \n\nIn @chawla2002smote, they also outline a modified version of SMOTE that can use both quantitative and qualitative predictors.  Other authors have extended the method for high-dimensional data  [@blagus2013smote] and other situations. @han2005borderline also describes a version that concentrates the sampling along regions where the predictor distributions of the different classes overlap (a.k.a., the borderline). This is similar in spirit to the NearMiss algorithms described below. \n\n#### ROSE {#sec-rose}\n\nROSE (short for Random OverSampling Examples) generates new synthetic observations [@menardi2014training]. Instead of using nearest neighbors, ROSE computes kernel density estimates for continuous predictors and uses this information to generate new samples in the neighborhood of a randomly sampled point. As described in @sec-naive-bayes, kernel density estimates are flexible nonparametric tools for approximating the distributions of numeric predictors. For each numeric predictor, class-specific univariate KDEs are estimated and can be used to simulate a new random multivariate data point centered at the selected prototype. @RJ2014008 uses a covariance matrix that is the optimal smoothing matrix under the assumption that the data are multivariate Gaussian. The standard deviation for predictor $j$ used to simulate a new sample is \n\n$$\n\\sigma_j^* = h_j\\sigma_j\\left[4/((p + 2)n_{tr})\\right]^{-(p + 4)}\n$$\n\nwhere $p$ is the number of predictors, $\\sigma_j$ is the standard deviation for predictor $j$, and $h_j$ is the bandwidth multiper for that predictor. Since each predictor is estimated separately, the resulting multivariate sample assumes that predictors are independent. In a local area, this is a fairly reasonable assumption. As in the naive Bayes discussion, $h_j$ serves as an optional multiplier for the bandwidth estimator that controls the spread of the local distribution. This could be tuned and can also be a set of different adjustment factors for each predictor. Note that ROSE generates a completely synthetic data set; it does not augment the existing data with novel samples. \n\n@fig-smote-rose shows an example with $h_1 = h_2 = 0.5$ using the sample example point as the SMOTE visualization on the left. The shading and contours describe two Gaussian distributions that use $\\sigma_j^*$, and the open circles are twenty different examples sampled from this distribution. Compared to SMOTE, the new data are centred on the selected point and are not confined to a rigid (hyper)rectangle.\n\nFor the reduced myopia data set, @fig-myopia-sampling shows the FDA class boundary produced using a ROSE version of the training set using the default of $h_1 = h_2 = 1$. The new version of the data shows less clustering than SMOTE with its default parameters. Unlike SMOTE, the increased sample size produced a simple fit that moves the vertical class boundary to the right of the original class boundary produces by the original training set. \n\n#### Neighbor-Based Sampling {#sec-knn-sampling}\n\n\n\n@mani2003knn describe tools for undersampling the majority class using nearest neighbors. Their idea is to find points to remove based on how close (or far) they are from the minority class data. They outlined three procedures that use the name \"NearMiss\". We'll briefly describe each in turn and use a small \"toy\" data set to outline the details. These data are shown in @fig-near-miss. There are 10 data points in the simulated training. The first 6 samples are from the majority class, and the next 4 samples labeled are from the minority data. Each of the three NearMiss methods uses a $4 \\times 6$ distance matrix that captures the distances between the majority and minority class data (there is no need to find the neighbors within these two groups). \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Data to demonstrate Near Miss and Tomek Link methods.](cls-imbalance_files/figure-html/fig-near-miss-1.svg){#fig-near-miss fig-align='center' width=50%}\n:::\n:::\n\n\n\n\nThe technique called \"NearMiss-1\" retains the majority-class samples with the smallest average distances to their _K_ nearest neighbors. For example, using _K_ = 2, this table shows the distances between the minority and majority class samples:\n\n:::: {.columns}\n\n::: {.column width=\"8%\"}\n:::\n\n::: {.column width=\"84%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"cxyhclbrhd\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#cxyhclbrhd table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#cxyhclbrhd thead, #cxyhclbrhd tbody, #cxyhclbrhd tfoot, #cxyhclbrhd tr, #cxyhclbrhd td, #cxyhclbrhd th {\n  border-style: none;\n}\n\n#cxyhclbrhd p {\n  margin: 0;\n  padding: 0;\n}\n\n#cxyhclbrhd .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 80%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#cxyhclbrhd .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#cxyhclbrhd .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#cxyhclbrhd .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#cxyhclbrhd .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#cxyhclbrhd .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#cxyhclbrhd .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#cxyhclbrhd .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#cxyhclbrhd .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#cxyhclbrhd .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#cxyhclbrhd .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#cxyhclbrhd .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#cxyhclbrhd .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#cxyhclbrhd .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#cxyhclbrhd .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cxyhclbrhd .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#cxyhclbrhd .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#cxyhclbrhd .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#cxyhclbrhd .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cxyhclbrhd .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#cxyhclbrhd .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cxyhclbrhd .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#cxyhclbrhd .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cxyhclbrhd .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#cxyhclbrhd .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cxyhclbrhd .gt_left {\n  text-align: left;\n}\n\n#cxyhclbrhd .gt_center {\n  text-align: center;\n}\n\n#cxyhclbrhd .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#cxyhclbrhd .gt_font_normal {\n  font-weight: normal;\n}\n\n#cxyhclbrhd .gt_font_bold {\n  font-weight: bold;\n}\n\n#cxyhclbrhd .gt_font_italic {\n  font-style: italic;\n}\n\n#cxyhclbrhd .gt_super {\n  font-size: 65%;\n}\n\n#cxyhclbrhd .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#cxyhclbrhd .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#cxyhclbrhd .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#cxyhclbrhd .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#cxyhclbrhd .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#cxyhclbrhd .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#cxyhclbrhd .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#cxyhclbrhd .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#cxyhclbrhd div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" style=\"table-layout:fixed;width:80%;\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <colgroup>\n    <col style=\"width:90px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n  </colgroup>\n  <thead>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Minority\">Minority</th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"6\" scope=\"colgroup\" id=\"Majority Point Labels\">\n        <div class=\"gt_column_spanner\">Majority Point Labels</div>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a1\">1</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a2\">2</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a3\">3</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a4\">4</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a5\">5</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a6\">6</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">7</td>\n<td headers=\"1\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.46</td>\n<td headers=\"2\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.41</td>\n<td headers=\"3\" class=\"gt_row gt_right\">1.41</td>\n<td headers=\"4\" class=\"gt_row gt_right\">2.33</td>\n<td headers=\"5\" class=\"gt_row gt_right\">3.10</td>\n<td headers=\"6\" class=\"gt_row gt_right\">3.73</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">8</td>\n<td headers=\"1\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.83</td>\n<td headers=\"2\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.64</td>\n<td headers=\"3\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">1.22</td>\n<td headers=\"4\" class=\"gt_row gt_right\">1.62</td>\n<td headers=\"5\" class=\"gt_row gt_right\">2.26</td>\n<td headers=\"6\" class=\"gt_row gt_right\">2.94</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">9</td>\n<td headers=\"1\" class=\"gt_row gt_right\">1.77</td>\n<td headers=\"2\" class=\"gt_row gt_right\">2.00</td>\n<td headers=\"3\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">1.01</td>\n<td headers=\"4\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.12</td>\n<td headers=\"5\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.95</td>\n<td headers=\"6\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">1.53</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">10</td>\n<td headers=\"1\" class=\"gt_row gt_right\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">2.99</td>\n<td headers=\"2\" class=\"gt_row gt_right\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">3.06</td>\n<td headers=\"3\" class=\"gt_row gt_right\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">2.40</td>\n<td headers=\"4\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6; border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">1.29</td>\n<td headers=\"5\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6; border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">0.57</td>\n<td headers=\"6\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6; border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">0.89</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">2NN Mean</td>\n<td headers=\"1\" class=\"gt_row gt_right\">0.65</td>\n<td headers=\"2\" class=\"gt_row gt_right\">0.53</td>\n<td headers=\"3\" class=\"gt_row gt_right\">1.11</td>\n<td headers=\"4\" class=\"gt_row gt_right\">0.71</td>\n<td headers=\"5\" class=\"gt_row gt_right\">0.76</td>\n<td headers=\"6\" class=\"gt_row gt_right\">1.21</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"8%\"}\n:::\n\n:::: \n\nFor the first majority-class observation, its closest minority-class samples are #7 and #8 (each column's two nearest neighbors are highlighted with a background accent color). The average distance between its two neighbors, shown in the bottom row of the table, is 0.65. To balance the data, NearMiss-1 would retain four samples: #1, #2, #4, and #5.\n\nThe algorithm for the second NearMiss variant focuses on using the _K_ farthest neighbors, i.e., those with the largest distance between points. However, the instructions from @mani2003knn are somewhat ambiguous:\n\n> The second method (NearMiss-2) selects [majority] examples that are close to _all_ [minority] examples. In this method, examples are selected based on their average distances to the three farthest [minority] examples.  \n\nIt is unclear whether the largest or the smallest average distance is used to determine which majority class data to keep. We'll assume that the minimum average distance should be used here. \n\nThis table highlights the two furthest neighbors (i.e., \"2FN\") and their average distances: \n\n:::: {.columns}\n\n::: {.column width=\"8%\"}\n:::\n\n::: {.column width=\"84%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"jsewszanvf\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#jsewszanvf table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#jsewszanvf thead, #jsewszanvf tbody, #jsewszanvf tfoot, #jsewszanvf tr, #jsewszanvf td, #jsewszanvf th {\n  border-style: none;\n}\n\n#jsewszanvf p {\n  margin: 0;\n  padding: 0;\n}\n\n#jsewszanvf .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 80%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#jsewszanvf .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#jsewszanvf .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#jsewszanvf .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#jsewszanvf .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#jsewszanvf .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#jsewszanvf .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#jsewszanvf .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#jsewszanvf .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#jsewszanvf .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#jsewszanvf .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#jsewszanvf .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#jsewszanvf .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#jsewszanvf .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#jsewszanvf .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jsewszanvf .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#jsewszanvf .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#jsewszanvf .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#jsewszanvf .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jsewszanvf .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#jsewszanvf .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jsewszanvf .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#jsewszanvf .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jsewszanvf .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#jsewszanvf .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jsewszanvf .gt_left {\n  text-align: left;\n}\n\n#jsewszanvf .gt_center {\n  text-align: center;\n}\n\n#jsewszanvf .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#jsewszanvf .gt_font_normal {\n  font-weight: normal;\n}\n\n#jsewszanvf .gt_font_bold {\n  font-weight: bold;\n}\n\n#jsewszanvf .gt_font_italic {\n  font-style: italic;\n}\n\n#jsewszanvf .gt_super {\n  font-size: 65%;\n}\n\n#jsewszanvf .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#jsewszanvf .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#jsewszanvf .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#jsewszanvf .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#jsewszanvf .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#jsewszanvf .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#jsewszanvf .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#jsewszanvf .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#jsewszanvf div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" style=\"table-layout:fixed;width:80%;\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <colgroup>\n    <col style=\"width:90px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n  </colgroup>\n  <thead>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Minority\">Minority</th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"6\" scope=\"colgroup\" id=\"Majority Point Labels\">\n        <div class=\"gt_column_spanner\">Majority Point Labels</div>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a1\">1</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a2\">2</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a3\">3</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a4\">4</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a5\">5</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a6\">6</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">7</td>\n<td headers=\"1\" class=\"gt_row gt_right\">0.46</td>\n<td headers=\"2\" class=\"gt_row gt_right\">0.41</td>\n<td headers=\"3\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">1.41</td>\n<td headers=\"4\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">2.33</td>\n<td headers=\"5\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">3.10</td>\n<td headers=\"6\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">3.73</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">8</td>\n<td headers=\"1\" class=\"gt_row gt_right\">0.83</td>\n<td headers=\"2\" class=\"gt_row gt_right\">0.64</td>\n<td headers=\"3\" class=\"gt_row gt_right\">1.22</td>\n<td headers=\"4\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">1.62</td>\n<td headers=\"5\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">2.26</td>\n<td headers=\"6\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">2.94</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">9</td>\n<td headers=\"1\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">1.77</td>\n<td headers=\"2\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">2.00</td>\n<td headers=\"3\" class=\"gt_row gt_right\">1.01</td>\n<td headers=\"4\" class=\"gt_row gt_right\">0.12</td>\n<td headers=\"5\" class=\"gt_row gt_right\">0.95</td>\n<td headers=\"6\" class=\"gt_row gt_right\">1.53</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">10</td>\n<td headers=\"1\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6; border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">2.99</td>\n<td headers=\"2\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6; border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">3.06</td>\n<td headers=\"3\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6; border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">2.40</td>\n<td headers=\"4\" class=\"gt_row gt_right\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">1.29</td>\n<td headers=\"5\" class=\"gt_row gt_right\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">0.57</td>\n<td headers=\"6\" class=\"gt_row gt_right\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">0.89</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">2FN Mean</td>\n<td headers=\"1\" class=\"gt_row gt_right\">2.38</td>\n<td headers=\"2\" class=\"gt_row gt_right\">2.53</td>\n<td headers=\"3\" class=\"gt_row gt_right\">1.90</td>\n<td headers=\"4\" class=\"gt_row gt_right\">1.98</td>\n<td headers=\"5\" class=\"gt_row gt_right\">2.68</td>\n<td headers=\"6\" class=\"gt_row gt_right\">3.34</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"8%\"}\n:::\n\n:::: \n\nNote that the neighbors are the opposite of those in the previous table. In this instance, we would retain samples #1, #2, #3, and #4. \n\nNearMiss-3 is a two-step process. First, the nearest _K_<sub>1</sub> nearest neighbors are found for each minority sample. Any majority-class samples that were not on the list of neighbors are excluded. After this, we determine the _K_<sub>2</sub> nearest neighbors of the majority class samples. Of these, the majority class observations with the _largest_ average distance are retained. \n\nIn our example, we'll use two neighbors for both calculations. In the first stage, the majority sample #3 is not a neighbor of any minority-class point, so it is discarded. \n\nThis table shows the reduced distance matrix and the average distances: \n\n:::: {.columns}\n\n::: {.column width=\"8%\"}\n:::\n\n::: {.column width=\"84%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"empaxdpfnf\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#empaxdpfnf table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#empaxdpfnf thead, #empaxdpfnf tbody, #empaxdpfnf tfoot, #empaxdpfnf tr, #empaxdpfnf td, #empaxdpfnf th {\n  border-style: none;\n}\n\n#empaxdpfnf p {\n  margin: 0;\n  padding: 0;\n}\n\n#empaxdpfnf .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 80%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#empaxdpfnf .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#empaxdpfnf .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#empaxdpfnf .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#empaxdpfnf .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#empaxdpfnf .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#empaxdpfnf .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#empaxdpfnf .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#empaxdpfnf .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#empaxdpfnf .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#empaxdpfnf .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#empaxdpfnf .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#empaxdpfnf .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#empaxdpfnf .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#empaxdpfnf .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#empaxdpfnf .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#empaxdpfnf .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#empaxdpfnf .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#empaxdpfnf .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#empaxdpfnf .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#empaxdpfnf .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#empaxdpfnf .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#empaxdpfnf .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#empaxdpfnf .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#empaxdpfnf .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#empaxdpfnf .gt_left {\n  text-align: left;\n}\n\n#empaxdpfnf .gt_center {\n  text-align: center;\n}\n\n#empaxdpfnf .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#empaxdpfnf .gt_font_normal {\n  font-weight: normal;\n}\n\n#empaxdpfnf .gt_font_bold {\n  font-weight: bold;\n}\n\n#empaxdpfnf .gt_font_italic {\n  font-style: italic;\n}\n\n#empaxdpfnf .gt_super {\n  font-size: 65%;\n}\n\n#empaxdpfnf .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#empaxdpfnf .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#empaxdpfnf .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#empaxdpfnf .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#empaxdpfnf .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#empaxdpfnf .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#empaxdpfnf .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#empaxdpfnf .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#empaxdpfnf div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" style=\"table-layout:fixed;width:80%;\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <colgroup>\n    <col style=\"width:90px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n    <col style=\"width:80px;\"/>\n  </colgroup>\n  <thead>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Minority\">Minority</th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"5\" scope=\"colgroup\" id=\"Majority Point Labels\">\n        <div class=\"gt_column_spanner\">Majority Point Labels</div>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a1\">1</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a2\">2</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a4\">4</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a5\">5</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a6\">6</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">7</td>\n<td headers=\"1\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.46</td>\n<td headers=\"2\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.41</td>\n<td headers=\"4\" class=\"gt_row gt_right\">2.33</td>\n<td headers=\"5\" class=\"gt_row gt_right\">3.10</td>\n<td headers=\"6\" class=\"gt_row gt_right\">3.73</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">8</td>\n<td headers=\"1\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.83</td>\n<td headers=\"2\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.64</td>\n<td headers=\"4\" class=\"gt_row gt_right\">1.62</td>\n<td headers=\"5\" class=\"gt_row gt_right\">2.26</td>\n<td headers=\"6\" class=\"gt_row gt_right\">2.94</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">9</td>\n<td headers=\"1\" class=\"gt_row gt_right\">1.77</td>\n<td headers=\"2\" class=\"gt_row gt_right\">2.00</td>\n<td headers=\"4\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.12</td>\n<td headers=\"5\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">0.95</td>\n<td headers=\"6\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6;\">1.53</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">10</td>\n<td headers=\"1\" class=\"gt_row gt_right\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">2.99</td>\n<td headers=\"2\" class=\"gt_row gt_right\" style=\"border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">3.06</td>\n<td headers=\"4\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6; border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">1.29</td>\n<td headers=\"5\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6; border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">0.57</td>\n<td headers=\"6\" class=\"gt_row gt_right\" style=\"background-color: #B0E0E6; border-bottom-width: 2px; border-bottom-style: solid; border-bottom-color: #D3D3D3;\">0.89</td></tr>\n    <tr><td headers=\"Minority\" class=\"gt_row gt_left\">2NN Mean</td>\n<td headers=\"1\" class=\"gt_row gt_right\">0.65</td>\n<td headers=\"2\" class=\"gt_row gt_right\">0.53</td>\n<td headers=\"4\" class=\"gt_row gt_right\">0.71</td>\n<td headers=\"5\" class=\"gt_row gt_right\">0.76</td>\n<td headers=\"6\" class=\"gt_row gt_right\">1.21</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"8%\"}\n:::\n\n:::: \n\nBased on these distances, NearMiss-3 selects #1, #4, #5, and #6 to include in the majority class. \n\n\n\nTomek Links [@ivan1976two] is a filtering method that is the opposite of NearMiss. In this case, we will _remove_ data points that are close to some theoretical class boundary. The goal is to create a sort of gap between the predictor distributions for each class. For each class, we determine the (single) nearest neighbor from the opposite class. Two samples form a Tomek Link when they are each other's nearest neighbors and belong to opposing classes. We would remove both data points that form such a link. @kubat1997addressing describe algorithms with similar goals. \n\nAn easy example from @fig-near-miss is the closeness of samples #4 and #9. They are extremely close to one another, yet in different classes; this consitutes a Tomek Link. A counter-example is sample #3; its neighbor is #9, but the converse is not true since #9 and #4 are neighbors. For this data set, only samples #1, #3, #6, and #8 are _not_ Tomek Links and these are retained. \n\nWhen applied to the myopia data, the number of myopic samples is reduced from 60 to 39 and the number of non-myopic rows in the training set drops from 402 to 381. While the reduction in events might compromise the model's effectiveness, @fig-myopia-subsampling shows that the FDA class boundaries produced with the entire data set and the reduced version are extremely similar. \n\nAnother potential downside to removing Tomek Links is increased instability. Using a single nearest-neighbor approach yields high variance, since small changes in the data can lead to substantially different solutions. That added variance in the training set might be passed along to the classifier. \n\nRemoving Tomek Links is a process that can be applied to data sets with balanced or unbalanced class frequencies; the goal is to reduce overlap in predictor distributions so the model can find a clearer trend in the data. It can also be used in conjunction with other sampling or synthesis methods, preferably afterwards. \n\n#### Comparisons Using the Myopia Data {#sec-myopia-sampling}\n\nTo further investigate these methods, the regularized logistic regression and boosted tree pipelines from @sec-imbalance-acceptance were applied to the full myopia training set (i.e., all predictors). For the boosting model, the sampling/synthesis techniques were the only preprocessors used. However, for glmnet, these techniques were applied first, followed by a zero-variance filter, interaction creation, and normalization.  \n\nThe top panel of @fig-myopia-sampling shows the model configuration statistics for different sampling and synthesis methods. The visualization emulates an ROC curve by plotting the sensitivity versus the false positive rate (i.e., one minus specificity) to demonstrate the trade-offs between the two types of errors. The downsampling panel shows a tight cluster of model results for both types of models, located in the region where sensitivity and specificity are approximately equal. Unsurprisingly, this is due to the equal frequencies of the two classes in the training data (post-sampling). However, this will not always be the case for other data sets. Near-miss sampling has a similar pattern with tighter clustering of model configuration results. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Cross-validated statistics for the subsampling and synthesis methods. The top panel shows an ROC-like visualization with sensitivity and 1 - specificity. The bottom panel relates sensitivity ot the Brier Score.](cls-imbalance_files/figure-html/fig-myopia-sampling-1.svg){#fig-myopia-sampling fig-align='center' width=80%}\n:::\n:::\n\n\nThe two synthetic methods showed slightly different patterns. ROSE  with logistic regression yielded another tight cluster of results. For boosting, both SMOTE and ROSE demonstrated greater versatility in the tuning parameter results, enabling marginal trade-offs between sensitivity and specificity. \n\nThe bottom panel of @fig-myopia-sampling shows that, when sensitivity is improved by these tools, there can be a corresponding increase in the Brier scores. The boosting results show that a high sensitivity raises the Brier to values that would indicate a signficant lack of calibration in the predicted probability estimates. This is deliberate; the goal of rebalancing is to have parity in the class frequencies. This enables the model to increase the likelihood of identifying events in the data. The consequence of doing so is that our class probability distributions are biased towards overestimating the probability of an event. However, when assessing the model, we use unbalanced data, and the increased probability of an event is not consistent with the true event rate (as seen outside of the balanced training set^[This is directly related to the discussion in @sec-imbalance-math regarding how the event rate impacts the overall probabilities.]). \n\nIt is worth noting that we could also optimize the threshold to further fine-tune the sensitivity and specificity values. Unlike the analysis in the previous section, the custom thresholds are much less likely to be excessively small (or large). However, if this is required, we are probably better off using unsampled training sets or using cost-sensitive learning (shown in the next section).\n\n\n\nWe can see that these tools have an effect on the hard class prediction metrics, such as sensitivity and specificity. Although @fig-myopia-sampling indicates that the calibration properties are not good, meaning that probability estimates do not reflect the rate at which events occur. However, it is possible that the probabilities can help discriminate between the classes for some threshold. For the four sampling methods used on these data, their ROC AUC values for the best logistic regression models ranged from 0.821 to 0.851 The range for the boosted tree models was 0.835 to 0.864. Are these any better or worse than the models that were developed in @sec-imbalance-acceptance? Using the bootstrap techniques in @sec-compare-holdout, we linked the predictions by row for all of these models, created 2,000 bootstrap samples, and then computed the difference in the area under the ROC curves. @fig-myopia-sampling-comparison shows the results. For these data, there doesn’t appear to be any improvement in the ROC AUCs when subsampling and synthesis tools are used. Note that the x-axis scale is fairly small; these AUC values are very similar to one another. One comparison did not cover zero, indicating that using logistic regression with SMOTE had a slightly worse AUC than simply using the training set as is. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Bootstrap 90% confidence intervals to assess whether subsampling and synthesis methods improve the area under the ROC curve. Negative values indicate that the AUC values were higher for models developed in @sec-imbalance-acceptance.](cls-imbalance_files/figure-html/fig-myopia-sampling-comparison-1.svg){#fig-myopia-sampling-comparison fig-align='center' width=70%}\n:::\n:::\n\n\nThese results highlights the idea that rebalancing tools can help a model do better at finding events and that the user should only focus on hard class predictions (and perhaps avoid exposing the users to the probability estimates). \n\n### Cost-Sensitive Learning {#sec-cost-sensitive-learning}\n\nCost-sensitive learning is a technique that ensures the model places greater emphasis on certain classes or imposes a higher penalty for specific types of misclassification errors. @breiman1984classification (Section 4.4) is one of the earliest references to consider this point of view. We’ve previously talked about the differing consequences of making an incorrect prediction. For example, @sec-mushrooms showed a model that predicts whether mushrooms are poisonous or not. We could think about this in terms of the general cost of a prediction (not too dissimilar to the discussion related to SVM models). \n\nIf we have $C$ classes, we could create a cost matrix that defines the specific costs $\\mathbb{c}_{j|k}$ of incorrectly predicting a data point to be class $j$ when it is truly class $k$. In many instances, the diagonal terms of this matrix are zero since there is no real \"cost\" to making the correct prediction. There are cases where different types of benefits are associated with different types of correct predictions. See @elkan2001foundations for an example. However, for the purposes of our discussion here, we will assume that the diagonal is zero. \n\nIf we have this cost structure, we can use the predicted class probabilities to estimate the expected costs for a collection of data by averaging a set of individual costs. Let’s say that $C=3$ and the cost matrix is \n\n$$\n\\mathbb{C} = \n\\begin{bmatrix}\n0.00 & 1.00 & 3.00 \\\\\n0.50 & 0.00 & 2.00 \\\\ \n0.25 & 0.75 & 0.00\n\\end{bmatrix}\n$$\n\nwhere the true values are in columns, and the predicted class levels are rows. For example, the cost of predicting a data point that is Class 3 as Class 1 is $\\mathbb{c}_{1|3} = 3.00$. Here are some examples of probabilities and their individual costs:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"odrxqfpwwv\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#odrxqfpwwv table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#odrxqfpwwv thead, #odrxqfpwwv tbody, #odrxqfpwwv tfoot, #odrxqfpwwv tr, #odrxqfpwwv td, #odrxqfpwwv th {\n  border-style: none;\n}\n\n#odrxqfpwwv p {\n  margin: 0;\n  padding: 0;\n}\n\n#odrxqfpwwv .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#odrxqfpwwv .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#odrxqfpwwv .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#odrxqfpwwv .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#odrxqfpwwv .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#odrxqfpwwv .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#odrxqfpwwv .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#odrxqfpwwv .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#odrxqfpwwv .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#odrxqfpwwv .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#odrxqfpwwv .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#odrxqfpwwv .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#odrxqfpwwv .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#odrxqfpwwv .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#odrxqfpwwv .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#odrxqfpwwv .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#odrxqfpwwv .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#odrxqfpwwv .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#odrxqfpwwv .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#odrxqfpwwv .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#odrxqfpwwv .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#odrxqfpwwv .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#odrxqfpwwv .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#odrxqfpwwv .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#odrxqfpwwv .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#odrxqfpwwv .gt_left {\n  text-align: left;\n}\n\n#odrxqfpwwv .gt_center {\n  text-align: center;\n}\n\n#odrxqfpwwv .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#odrxqfpwwv .gt_font_normal {\n  font-weight: normal;\n}\n\n#odrxqfpwwv .gt_font_bold {\n  font-weight: bold;\n}\n\n#odrxqfpwwv .gt_font_italic {\n  font-style: italic;\n}\n\n#odrxqfpwwv .gt_super {\n  font-size: 65%;\n}\n\n#odrxqfpwwv .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#odrxqfpwwv .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#odrxqfpwwv .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#odrxqfpwwv .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#odrxqfpwwv .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#odrxqfpwwv .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#odrxqfpwwv .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#odrxqfpwwv .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#odrxqfpwwv div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Truth\">Truth</th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"3\" scope=\"colgroup\" id=\"Probability Estimates\">\n        <div class=\"gt_column_spanner\">Probability Estimates</div>\n      </th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Cost\">Cost</th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"A\">A</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"B\">B</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"C\">C</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Truth\" class=\"gt_row gt_center\">A</td>\n<td headers=\"A\" class=\"gt_row gt_right\">0.33</td>\n<td headers=\"B\" class=\"gt_row gt_right\">0.33</td>\n<td headers=\"C\" class=\"gt_row gt_right\">0.33</td>\n<td headers=\"Cost\" class=\"gt_row gt_right\">0.25</td></tr>\n    <tr><td headers=\"Truth\" class=\"gt_row gt_center\">B</td>\n<td headers=\"A\" class=\"gt_row gt_right\">0.33</td>\n<td headers=\"B\" class=\"gt_row gt_right\">0.33</td>\n<td headers=\"C\" class=\"gt_row gt_right\">0.33</td>\n<td headers=\"Cost\" class=\"gt_row gt_right\">0.58</td></tr>\n    <tr><td headers=\"Truth\" class=\"gt_row gt_center\">C</td>\n<td headers=\"A\" class=\"gt_row gt_right\">0.33</td>\n<td headers=\"B\" class=\"gt_row gt_right\">0.33</td>\n<td headers=\"C\" class=\"gt_row gt_right\">0.33</td>\n<td headers=\"Cost\" class=\"gt_row gt_right\">1.67</td></tr>\n    <tr><td headers=\"Truth\" class=\"gt_row gt_center\">A</td>\n<td headers=\"A\" class=\"gt_row gt_right\">0.10</td>\n<td headers=\"B\" class=\"gt_row gt_right\">0.50</td>\n<td headers=\"C\" class=\"gt_row gt_right\">0.40</td>\n<td headers=\"Cost\" class=\"gt_row gt_right\">0.35</td></tr>\n    <tr><td headers=\"Truth\" class=\"gt_row gt_center\">C</td>\n<td headers=\"A\" class=\"gt_row gt_right\">0.40</td>\n<td headers=\"B\" class=\"gt_row gt_right\">0.50</td>\n<td headers=\"C\" class=\"gt_row gt_right\">0.10</td>\n<td headers=\"Cost\" class=\"gt_row gt_right\">2.20</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\nFirst, consider the top three rows, which contain equivocal class probability estimates for each class. The result classification costs vary considerably due to the asymmetric nature of the different class costs. The Third row has the worst results because the cost-structure specifies that the third class is the most important to classify correctly (in elements $\\mathbb{C}_{13}$ and $\\mathbb{C}_{23}$). The fourth row contains an example that is mispredicted as class B when it is actually class A. The cost is not high since the elements in the first column of $\\mathbb{C}$ are not large relative to the others. The worst cost corresponds to the bottom row, where mispredicting class C as either class A or class B incurs the highest costs. This is compounded by the probability for class C being especially small which downweights the effect of $\\mathbb{C}_{13}$. \n\nIf our ML model can utilize custom objective functions, minimizing the expected cost is a fairly rational way to ensure your model is fit for purpose. If not, we can use the expected cost to help optimize the tuning parameters of our model (as we did in @fig-class-cost). \n\nAnother way some researchers have parameterized cost is to collapse the matrix, resulting in a general cost for misclassifying a sample, regardless of what it was misclassified as. In this case, we sum the cost matrix over columns, resulting in one cost per class. @ting1998inducing first describes these as _class weights_. In our example matrix shown above, the class weights are $\\mathbb{C}_1 = 0.75$,  $\\mathbb{C}_2 = 1.75$,  and  $\\mathbb{C}_3 = 5.00$. For a two-class outcome (e.g., myopia or not), the class weights are the same as the two off-diagonal entries in the cost matrix. \n\nIf we consider class-specific costs as weights, we can generalize many performance metrics to be cost-sensitive. For example, we can make the Brier score cost-sensitive using a modified version of @eq-brier\n\n$$\nBrier = \\frac{1}{WC}\\sum_{i=1}^n\\sum_{k=1}^C w_k(y_{ik} - \\hat{p}_{ik})^2\n$$ {#eq-brier-cost}\n\nwhere $w_k$ is the cost or weight for class $k$ and $W$ is the sum of the $n$ weights. If we have two classes and $w_1 = 5$ and $w_2 = 1$, every data point whose true outcome is the first class will have five-fold more influence over the objective function. In this way, we can train the model to ensure that it performs better in predicting specific classes than others. This is the basis for _cost-sensitive learning_^[Note that our previous use of costs in @fig-class-cost was _after_ the model was trained. In this section, we will use weights/costs so that the model adapts to the cost structure during training.]. \n\nIn practice, our implementations of ML model metrics may or may not enable cost-sensitive learning. If not, they may have the capacity to handle _case weights_. These are numeric values associated with each **row** of the data set that specify how it should affect the computations. There are many types of case weights for different purposes. Here are a few examples: \n\n- Frequency weights are integers that specify how often the specific predictor pattern occurs in the data. If there were two categorical predictions with a total of 25 combinations, a very large data set can be compressed by using a case weight that reflects how often each of the 25 patterns occurs. Frequency weights would be assigned to both the training and testing sets. \n\n- Importance weights are numbers (decimal and non-negative) that reflect how important the row is to the model fit. This is the type of weight that we’ve discussed above. Importance weights should only be assigned to the training set; the test set should not use them, as it should accurately reflect the state of the data in the real world. The importance weights would bias the data.  \n\nIf the model implementation allows for case weights, we can use importance weights to make them sensitive to different classes. \n\n::: {.note-box}\nWhile not related to class imbalances, importance weights are also useful for time-series data, where we would like more recent data to have a higher priority in the model fit. It can also be useful for dealing with problematic/anomalous data in a model. For example, data collected during the global COVID-19 pandemic can have a negative impact on fits, but it may still need to be included. Assigning these row small case weights may solve those issues. \n:::\n\nNote that the scale of class weights may differ across metrics and/or models (as we’ll see below). For one type of model, minority class weights ranging from 1 to 50 might have the same performance as another model/metric that ranges from 1 to 5. There is some trial and error associated with tuning the weights. \n\nIf your problem has a well-known cost structure, likely expressed in units of currency, there is probably little ambiguity about it. Otherwise, we might use class weights to either decrease the likelihood of tragic errors from occurring in the model or to help the model overcome a severe class imbalance. In other words, the \"importance\" in importance weights could mean many different things. \n\nTo visualize an example, @fig-myopia-cost-fda shows the same data as @fig-myopia-fda, but in this case, case weights were used so that the myopic data (in red) had a 15-fold larger impact on the objective function than the non-myopic children. The same FDA model was trained on the data, but it used the case weights. The result is a class boundary that bends more in the middle, allowing it to encompass a greater portion of the minority class than the original fit. \n\n\n\n:::: {#fig-myopia-cost-fda}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](cls-imbalance_files/figure-html/myopia-cost-grid-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\nA flexible discriminant fit to the myopia training set using cost-sensitive learning. The myopic subjects had a 15-fold higher cost in the objective function and 43 of the 60 events (in red) are identified using the default 50% threshold. This figure is comparable with @fig-myopia-fda and @fig-myopia-subsampling.\n\n::::\n\nCase weights based on the class can also approximate some of the subsampling methods shown in the previous section. For example, instead of dropping training set rows from the data, we can adjust the importance weights so that the sum of the weights in the majority class equals the sum of the weights in the minority class. By doing this, we may be able to obtain a better model since the majority of the majority class is not lost.  Similarly, an approximation to upsampling would only increase the minority class weights so that the class weight totals are in equilibrium. This would also result in faster training times than basic oversampling.\n\nIt is worth noting that some stochastic ensemble implementations (e.g., boosting and random forests) often confuse the terms \"case weights\" and \"sampling weights.\" In these instances, the weights are used to preferentially sample the training set while fitting the models (usually trees) without directly impacting the objective function. This process is more akin to downsampling and upsampling than to cost-sensitive learning. \n\nTo demonstrate with the full myopia training set, boosted trees and regularized logistic regression were used once again. The lightgbm implementation was used and, while their documentation is unclear about how the weights are used, it appears that the weights are applied to the objective function and not just the sampling mechanism^[A GitHub issue ([`https://github.com/microsoft/LightGBM/issues/1299`](https://github.com/microsoft/LightGBM/issues/1299)) has evidence that the weights are a \"multiplication applied to every positive label weight\" and shows some C++ code to that effect.].\n\n\n\n\n\n\n\nThe same grids were used as in the previous section for both models. For the case weights ranges for each model, some investigation was required to find appropriate changes in the performance metrics. While both parameter ranges were explored in log<sub>2</sub> units, starting at 1.0, the boosted model used a maximum weight of 2<sup>10</sup>, whereas logistic regression used maximum weights of 2<sup>4</sup>. The majority class weights were kept at a value of 1.0 for both models. For each model, a sequence of weights was created in a 1D grid, and these were crossed with the original grids.\n\n@fig-myopia-cost-sensitive shows the results where the ranges of the weights have been standardized across the two models to be within [0, 1]. In the top panel, another ROC-like visualization is used to demonstrate the effect of using higher weights for the myopic samples. We can see that both models tend to increase sensitivity as the weights increase^[As with the previous section, these statistics use the default 50% threshold.]. This is not always the case, as some tuning parameters for the model were suboptimal on their own, and adjusting the class weights did not yield a significant change. For the most part, the increase in sensitivity comes with the price of reduced specificity. The different class weights enable us to select the optimal trade-off between the two. \n\nAs with subsampling and synthesis methods, the class probabilities generated by this model become increasingly inconsistent with reality as the cost of the minority class increases. The bottom panel shows the degradation in the Brier score tracks with increased sensitivity. Once again, we caution against using these probability estimates with users, as they may no longer emulate the true likelihood of the event.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Cross-validated statistics for the cost-sensitive models. The top panel shows an ROC-like visualization with sensitivity and 1 - specificity. The bottom panel relates sensitivity ot the Brier Score.](cls-imbalance_files/figure-html/fig-myopia-cost-sensitive-1.svg){#fig-myopia-cost-sensitive fig-align='center' width=80%}\n:::\n:::\n\n\nWhen comparing subsampling/synthetic methods to cost-sensitive learning, the latter appears to be more versatile than the former. It enables the user to have more direct control over the trade-off between false-negative and false-positive rates. The downside to cost-sensitive learning is that it may be dependent on the implementation of the model, whereas subsampling/synthetic tools can be used anywhere, as they are specialized preprocessors. \n\n## Revenge of the Prevalence {#sec-prevalence-again}\n\nBefore we end the discussion of class imbalances, it is worth revisiting the discussion from @sec-cls-two-classes on how the event rate (i.e., prevalence) affects important statistics.  We'll continue to use the myopia data for discussion.  \n\nIf we assume that the rate of myopia in the population of interest is consistent with our data set (13.1%), we can gain a deeper understanding of how well the model performs.  Given our estimates of sensitivity and specificity, the unconditional performance statistics are: \n\n- positive predictive value (PPV): 38.2%\n- negative predictive value (NPV): 95.7%\n\nEven though our sensitivity and specificity ended up being fairly balanced, the statistics that convey to true ability to predict are not.  \n \nThis could exacerbate the potential confusion that can occur when explaining the results of a prediction. Continuing the hypothetical conversation from @sec-imbalance-acceptance, the explanation of the PPV might be: \n\n> \"We understand that this rule of thumb might seem overly stringent. \n> That said, from our data, we know that when we predict that a child is myopic, the probability that they are actually myopic is roughly 38%.\n> This is because _most children are not myopic_ but we do find some evidence to make the diagnosis.\" \n\nConversely, if the child had a very small predicted probability and a myopia diagnosis was not used, the explanation would be: \n\n> \"Even though the chances that your child is myopic are greater than zero, from our data we know that when we predict that a child is not myopic, the probability that they are actually not myopic is overwhelmingly high (roughly 96%).\n> This is because _most children are not myopic_ and there is not enough evidence to disprove this for your child.\" \n\nThis is much less ambiguous than the previous hypothetical conversation. \n\nWe can try to optimize the model for PPV and NPV in the same way as we did for sensitivity and specificity. However, to do this, the prevalence must be well known for the specific population that will be predicted by the model and should be stable over time. \n\nFor example, @covidcdc calculated COVID-19 cases and deaths, and estimated cumulative incidence for March and April 2020. The mortality rate varied geographically. For example, during that period, New York City had an estimated mortality rate of 5.3%, while the remainder of the state of New York had an estimated rate of 2.2%. Thankfully, in 2025, these rates are much lower. \n\nThis illustrates that computations that depend on the event rate can be challenging, as the values can systematically change under different conditions and over time. \n\n## Chapter References {.unnumbered}\n \n",
    "supporting": [
      "cls-imbalance_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}