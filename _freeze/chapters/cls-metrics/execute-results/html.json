{
  "hash": "4334959ffc2ddfeef959eb99ed0e9cdd",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/cls-metrics/\"\n---\n\n\n\n\n\n# Characterizing Classification Models {#sec-cls-metrics}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTODO\n\n-   update data description; maybe talk a little about the example data\n-   shinylive for roc/pr/gain\n-   mention bootstrapping and permutations\n-   desirability and other methods for a combined score?\n\n\nNature of classification predictions: hard and soft; information content; LLMs and no probs\n\nintroduce prevalence here?\n\n\n## Example Data\n\nFor this chapter, a training set of 6,107 data points, validation set of 6,107 data points, and a test set of 6,107 data points are used to illustrate various methods.\n\nA naive Bayes model (see @sec-naive-bayes) was used to model these data. \n\n## Confusion Matrices\n\n\n\n\n\n\n\n\n\n\n\n::::::: columns\n::: {.column width=\"37%\"}\n:::\n\n:::: {.column width=\"26%\"}\n::: {#tbl-confusion-matrix}\n\n\n\n\n::: {#tbl-confusion-matrix .cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"2\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Truth</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> Prediction </th>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> edible </th>\n   <th style=\"text-align:right;\"> poisonous </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> edible </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 2180 </td>\n   <td style=\"text-align:right;\"> 750 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> poisonous </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 564 </td>\n   <td style=\"text-align:right;\"> 2613 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\nConfusion matrix for validation set predictions from the naive Bayes model.\n:::\n::::\n\n::: {.column width=\"37%\"}\n:::\n:::::::\n\n\n\n## Assessing Hard Class Predictions\n\n$$\naccuracy = \\frac{1}{n}\\sum_{k=1}^Cn_{kk}\n$$\n\nwhere $n_{ij}$ are the number of counts in the *i* th row and the *j* th column of the confusion matrix, $n$ is the total number of data points, and $C$ is the number of classes. For the naive Bayes model, the accuracy computed on the validation set was 78.5{{< pct >}}.\n\n$$\nKappa = \\frac{accuracy - E}{1 - E}\n$$\n\nwhere $E$ is the expected accuracy based on the marginal totals of the confusion matrix. For two classes, this is:\n\n$$\nE =\\frac{1}{n^{2}}\\sum_{k=1}^2n_{k1}n_{k2}\n$$\n\nMatthews correlation coefficient [@MATTHEWS1975442; @GORODKIN2004367] (MCC) is another metric that can compare the observed and predicted classes. The equation for *C* classes is:\n\n$$\n{\\displaystyle {\\text{MCC}}={\\frac {\\displaystyle{\\sum_{k=1}^C\\sum_{l=1}^C\\sum _{m=1}^C(n_{kk}n_{lm}-n_{kl}n_{mk})}}{{\\sqrt {{\\displaystyle \\sum_{k=1}^C\\left(\\sum _{l=1}^Cn_{kl}\\right)\\left(\\sum _{\\substack{k'=1\\\\k'\\neq k}}^C\\sum _{l'=1}^Cn_{k'l'}\\right)}}}{\\sqrt {{\\displaystyle\\sum_{k=1}^C\\left(\\sum _{l=1}^Cn_{lk}\\right)\\left(\\sum _{\\substack{k'=1\\\\k'\\neq k}}^C\\sum _{l'=1}^Cn_{l'k'}\\right)}}}}}}\n$$\n\nThe statistic mirrors the correlation coefficient for numeric data in that it ranges from -1 to +1 and, like Kappa, has a similar interpretation. The MCC estimate, 0.569, is very close to the Kappa estimate.\n\n## Metrics for Two Classes\n\n\n::::::: columns\n::: {.column width=\"30%\"}\n:::\n\n:::: {.column width=\"40%\"}\n::: {#tbl-confusion-matrix-two-class}\n\n\n\n\n::: {#tbl-confusion-matrix-two-class .cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Truth</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> Prediction </th>\n   <th style=\"text-align:left;\"> event </th>\n   <th style=\"text-align:left;\"> non-event </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> event </td>\n   <td style=\"text-align:left;\"> TP </td>\n   <td style=\"text-align:left;\"> FP </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> non-event </td>\n   <td style=\"text-align:left;\"> FN </td>\n   <td style=\"text-align:left;\"> TN </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\nA general confusion matrix for outcomes with two classes.\n:::\n::::\n\n::: {.column width=\"30%\"}\n:::\n:::::::\n\n\n$$\nSensitivity = \\frac{\\text{correctly predicted events}}{\\text{total events}} = \\frac{TP}{TP + FN}\n$$\n\n\n\n$$\nSpecificity = \\frac{\\text{correctly predicted non-events}}{\\text{total non-events}} = \\frac{TN}{TN + FP}\n$$\n\n\n$$\nJ = Sensitivity + Specificity - 1\n$$\n\n$$\n\\begin{align}\nPPV&= \\frac{Sensitivity \\times Prevalence}{(Sensitivity \\times Prevalence) + ((1 - Specificity) \\times (1 - Prevalence))}  \\notag \\\\\n\\notag \\\\\nNPV&= \\frac{Specificity \\times (1 - Prevalence)}{(Prevalence \\times (1 - Sensitivity)) + (Specificity \\times (1 - Prevalence))} \\notag\n\\end{align}\n$$\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The effect of prevalence on the postive and negative predictive values using the sensitivity and specificity from the naive Bayes model.](../figures/fig-prevalence-1.svg){#fig-prevalence fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n\n\n$$\nPPV = \\frac{Sensitivity}{Sensitivity \\times (1 - Specificity)} = \\frac{TP}{TP+FP}\n$$\n\n$$\nPrecision = \\frac{\\text{relevant and retrieved}}{\\text{total retrieved}}=\\frac{TP}{TP + FP}\n$$ \n\nIn the special case when the classes are balanced, this is equal to the positive predictive value (but is not otherwise).\n\n\n$$\nRecall = \\frac{\\text{relevant and retrieved}}{\\text{total relevant}} = \\frac{TP}{TP + FN}\n$$ \n\nThis is the same as sensitivity.\n\nAdditionally, the $F_1$ score is the harmonic mean of precision and recall:\n\n$$\nF_{1}=2\\left(\\frac{precision \\times recall}{precision + recall}\\right)\n$$\n\nMore generally, the score can be written as\n\n$$\nF_{\\beta}=(1+\\beta_2)\\left(\\frac{precision \\times recall}{(\\beta^2 \\times precision) + recall}\\right)\n$$\n\n\n\n\n## Weighted performance metrics\n\nTwo class performance metrics are convenient summaries and are generally interpretable. As we saw earlier in this chapter, the metrics of accuracy and Kappa can directly be extended to more that two classes. Because of the desirable characteristics of other two-class performance metrics, conventions have been constructed to extend these metrics. These conventions can be applied to any two-class metric. We will use sensitivity, *S*, to illustrate the conventions.\n\nThe first approach, \"macro averaging\" computes $C$ versions of the statistic using a \"one-versus-all\" approach. For example, when $C = 3$, $S_1$ is the sensitivity when the first class is considered the event and the second and third classes are pooled into the non-event category. $S_2$ is the sensitivity when the second class is considered as the event, and so on. The macro estimate of sensitivity is computed as their average:\n\n$$\nS_{macro} = \\frac{1}{C}\\sum_{k=1}^C S_k\n$$\n\nwhere the $S_k$ are the one-versus-all metrics.\n\nA naive average of sensitivities is appropriate when the classes are balanced. When the classes are unbalanced, an unweighted average will bias this metric towards the sensitivity of the largest class. To account for class imbalance, *Weighted macro averaging* uses the class frequencies $n_k$ as weights\n\n$$\nS_{macro, wts} = \\sum_{k=1}^C \\frac{S_k}{n_k}\n$$\n\nFinally, *micro averaging* is a more complex approach because it aggregates the ingredients that go into the specific calculations. Sensitivity, for example, requires the number of true-positive and false negatives. These are summed and used in the regular computation of sensitivity:\n\n$$\nS_{micro} = \\frac{{\\displaystyle \\sum_{k=1}^C TP_k}}{\\left( {\\displaystyle \\sum_{k=1}^C TP_k} \\right) + \\left( {\\displaystyle \\sum_{k=1}^C FN_k} \\right) }\n$$ \n\nNote that there are many performance metrics that naturally work with three or more classes. There is little point in using these averaging tools for accuracy, Kappa, MCC, ROC AUC, and others.\n\n\n## Evaluating Probabilistic Predictions\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Cross-Entropy\n\n\n$$\nloss = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^Cy_{ik}\\log(\\hat{p}_{ik})\n$$\n\n### Brier Scores\n\nAnother performance metric that uses the predicted class probabilities is the Brier score [@brier1950verification; @bella2013effect]. Suppose that the value $y_{ik}$ is a 0/1 indicator for whether the observed outcome $i$ corresponds to class $k$. Using this the score is:\n\n$$\nBrier = \\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^C (y_{ik} - \\hat{p}_{ik})^2\n$$\n\n\n\n\n\n\n\n\n\n\n\nThis is the mean squared error of the predicted class probabilities $\\hat{p}_{ik}$. The Brier score can range between zero and two, but for two class problems, it is often rescaled to range between zero and one. A perfect model has a score of zero. If a performance metric achieves its smallest value when the probability estimate is perfect, it is called a *proper scoring rule*. There are theoretical benefits to using such a metric and, in the case of the Brier score, it also measures how well a model is calibrated (see @sec-calibration). In fact, the Brier score can be decomposed into different components, one of which specifically measures calibration. However, this can only be computed when the estimated probability values are not unique. For the naive Bayes model, the Brier score value was 0.15.\n\n\n\n### Reciever Operating Characteristic Curves\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Validation set ROC curves for the naive Bayes model The symbols represent different probability thresholds.](../figures/fig-roc-curve-1.svg){#fig-roc-curve fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\n\n### Precision-Recall Curves\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A precision-recall curve, estimated with the validation set, for the naive Bayes model.](../figures/fig-pr-curve-1.svg){#fig-pr-curve fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\n### Gain Charts\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A gain curve based on the validation set. The dotted linear indicates the percentage of samples to query in order to capture 30{{< pct >}} of the samples from the first class.](../figures/fig-gain-curve-1.svg){#fig-gain-curve fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n\n\n\n## Calibration {#sec-calibration}\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Calibration curves for naive Bayes model. The colored regions are individual 90{{< pct >}} confidence intervals.](../figures/fig-cal-curves-1.svg){#fig-cal-curves fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\nThe downside to this binning approach is that, for small data sets, the results can have very large variability or there may not be enough data within each bin to make accurate judgements regarding calibration. One alternative is to use moving windows. For example, for fixed bin width of 0.20, compute the required statistics and then move the bin by a small amount (say 0.02). This would result in larger bins capturing more data that would slide across the range of probability values.\n\nAnother alternative is to fit a model to the data. Logistic regression is a relatively simple classification model for binary outcomes and is discussed in @sec-logistic-regression. We can fit this model using our observed outcomes and use the predicted class probabilities ($\\hat{p}_i$) as the predictor. If the probabilities are well-calibrated, the model should show a linear relationship. One other feature that can be used in this diagnostic is to model the predictor with a smoothing spline. This enables the logistic model to reflect when the calibration is very poor. @fig-cal-curves (b) illustrates the same pattern with a much smaller degree of uncertainty since the model coefficients utilize all of the data.\n\n\n\n\n\n\n\n\n\n\n\nWhat can be done with poorly calibrated predictions? There are approaches for post-processing the predicted probabilities to have higher fidelity to the observed data. The customary method is to use a logistic regression fit, as seen in @fig-cal-curves (b), to re-calibrate the predictions. First a suitable logistic regression model is created in the same manner as the one used for diagnosis. Recall that the original predictions are used as predictors of the model. To re-calibrate a set of predictions, the original values are pushed through the model and the resulting logistic regression values are used as the new probability predictions.\n\n@platt1999probabilistic  proposed using logistic regression as a method of converting any quantitative score into a probability-like value. *Platt scaling* also adjusts the class indicators from binary 0/1 values to values slightly different from those values.\n\nThis brings up questions about which data should be used to fit the calibration model and which to evaluate its efficacy. Weâ€™ve previously mentioned that using simple re-predictions of the training set is a very bad idea and that is especially true here. If the model was resampled, the out-of-sample predictions can be used to fit the calibration model. In our example, a single validation set was used. Unfortunately, if all of the held-out data were used for re-calibration, the only unused data left would be the test set (which should not be compromised).\n\nTODO change below\n\nSince there are 6,107 samples in the validation set, we will randomly select half to fit the calibration model and will use the other half to evaluate how well the calibration process worked. @fig-re-cal-curves shows the results where panel (a) uses a sliding window to assess the smaller data set using a window size of 20{{< pct >}} that moves in 5{{< pct >}} increments. From these figures, the class probabilities appear to have been improved. Note that these values now encompass the entire probability range.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Calibration curves for naive Bayes model _after_ re-calibration using logistic regression in conjunction with a calibration set.](../figures/fig-re-cal-curves-1.svg){#fig-re-cal-curves fig-align='center' width=90%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\nUsing this approach to calibration, the new probability estimates have a Brier score of 0.147. While this is smaller than the value (0.15) prior to calibration, the exact same data were not used. This makes it difficult to say that the improvement is real.\n\nThere other approaches for re-calibrating. First, is isotonic regression [@isotonic]. This tool fits a model that will generate monotonic, non-decreasing, predictions: $\\hat{y}_i \\le \\hat{y}_{i+ 1}$ when $x_i \\le x_{i+ 1}$. There are multiple methods to create such models but the most straightforward approach is the pooled-adjacent violators algorithm (PAVA) of @pava. This method adjusts a sequence of data points that are not monotonically increasing to be so. For example, @tbl-isotonic shows an example where the rows are ordered by their model's predicted probabilities. One column shows a binary indicator for class `A`. The first three rows have the same outcome (`B`) but the fourth row is actually class `A`. The PAVA algorithm then takes the averages of the binary indicators that bracket the discordant value and all of these probability estimates are given a value of 1/3. From the table, it is clear that there are three possible probability estimates after isotonic regression is used.\n\n::::::: columns\n::: {.column width=\"25%\"}\n:::\n\n:::: {.column width=\"50%\"}\n::: {#tbl-isotonic}\n\n\n\n\n::: {#tbl-isotonic .cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"2\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Estimated Probability of Class A</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> Truth </th>\n   <th style=\"text-align:right;\"> Binary </th>\n   <th style=\"text-align:right;\"> Model </th>\n   <th style=\"text-align:right;\"> Isotonic </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.10 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.20 </td>\n   <td style=\"text-align:right;\"> 0.33 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.40 </td>\n   <td style=\"text-align:right;\"> 0.33 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> B </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 0.33 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n   <td style=\"text-align:right;\"> 1.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.70 </td>\n   <td style=\"text-align:right;\"> 1.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.80 </td>\n   <td style=\"text-align:right;\"> 1.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.90 </td>\n   <td style=\"text-align:right;\"> 1.00 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n\nAn example of calculations for calibration via isotonic regression.\n:::\n::::\n\n::: {.column width=\"25%\"}\n:::\n:::::::\n\nThe nice feature of this tool is that it is nonparametric and makes almost no distributional assumptions. However, one downside is that there are a small number of unique re-calibrated probability estimates. For this reason, it is advised to use isotonic regression for large data sets. For example, for the naive Bayes data used for calibration, there were originally 6100 unique probability estimates while, after isotonic regression is applied, there are only 30 possible values.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Calibration curves for naive Bayes model _after_ re-calibration using isoreg regression.](../figures/fig-re-cal-iso-curves-1.svg){#fig-re-cal-iso-curves fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\nThere are many other tools in the calibration toolbox, such as beta calibration [@betacal]. There are also methods for calibrating classification models with three or more classes [@NIPS2001abdbeb4d; @johansson21a; @NEURIPS2021bbc92a64].\n\nFinally, it is important to re-compute performance using the re-calibrated predictions. Also note that, since the class probabilities have been altered, any hard predictions should be recomputed since some values may have moved across the chosen probability threshold. Using the set of 6107 data points that have been re-calibrated, the re-estimated sensitivity and specificity were 79.4{{< pct >}} and 77.7{{< pct >}}, respectively and the area under the ROC curve was 0.87\n\n\n## Ordered Categories\n\nAs previously mentioned, our outcome classes can have an inherent ordering.\n\nexamples - wordle\n\nSince there is an ordering, it might be reasonable to think that misclassification severity relates to how \"far\" a predicted class is from its true class. For example, risk analysis systems, such as FMEA, involve enumerating different ways that a thing can fail and the corresponding consequences. Suppose that we have an ordinal scale for risk consisting of \"low,\" \"moderate,\" \"severe,\" and \"catastrophic\". It is reasonable to think that predicting that the risk is moderate when it is actually catastrophic is very bad. Predicting a moderate risk to be low may not be as dire.\n\nThere are a few metrics that use hard classification predictions. We previously mentioned the Kappa statistic. To penalize misclassification errors based on their distance to the true classification, [@Agresti2002vi] developed the *weighted Kappa* statistic where the errors' weights depend on their position differences. Let's look at a four-class example from @apm where computing jobs run on a server executed very fast (VF), fast (F), moderately long (M), or a long time (L).\n\nA linear weighting for the Kappa statistic is shown in @tbl-confusion-matrix-costs. There are bands of diagonal weights that are either 0, 1, 2, or 3. Weight Kappa statistics enable \"more incorrect\" errors more than others.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n::::::: columns\n::: {.column width=\"5%\"}\n:::\n\n:::: {.column width=\"90%\"}\n::: {#tbl-confusion-matrix-costs}\n\n\n\n\n::: {#tbl-confusion-costs .cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"2\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"4\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Truth</div></th>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"4\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Linear Weights</div></th>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"4\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Custom Costs</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> Prediction </th>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> VF </th>\n   <th style=\"text-align:right;\"> F </th>\n   <th style=\"text-align:right;\"> M </th>\n   <th style=\"text-align:right;\"> L </th>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> VF </th>\n   <th style=\"text-align:right;\"> F </th>\n   <th style=\"text-align:right;\"> M </th>\n   <th style=\"text-align:right;\"> L </th>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> VF </th>\n   <th style=\"text-align:right;\"> F </th>\n   <th style=\"text-align:right;\"> M </th>\n   <th style=\"text-align:right;\"> L </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> VF </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 1620 </td>\n   <td style=\"text-align:right;\"> 371 </td>\n   <td style=\"text-align:right;\"> 64 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 141 </td>\n   <td style=\"text-align:right;\"> 647 </td>\n   <td style=\"text-align:right;\"> 219 </td>\n   <td style=\"text-align:right;\"> 60 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 24 </td>\n   <td style=\"text-align:right;\"> 79 </td>\n   <td style=\"text-align:right;\"> 28 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> L </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 36 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 111 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\nA four class ordinal outcome with a confusion matrix, a linear set of weights for each potiental error, and a custom cost scheme used by @apm.\n:::\n::::\n\n::: {.column width=\"5%\"}\n:::\n:::::::\n\nWe computed Kappa under three scenareos: equal weights (0.508), linear scores (0.593), and quadratic scores (0.692).\n\nThe mean squared error for these data was 0.678. How bad is this? If we repeatedly permute the outcome classes and measure the RMSE we find that the worst case-scenario is about 1.22.\n\nFor the custom cost analysis, the average cost was 0.746 with a worst-case value of about 1.38.\n\n## Multi-objective Assessments\n\nPareto optimal; desirability functions. J statistic\n\n## Chapter References {.unnumbered}\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}