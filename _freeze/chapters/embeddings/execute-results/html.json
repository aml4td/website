{
  "hash": "3ffb9ec86b73491ecc192883a6668d02",
  "result": {
    "engine": "knitr",
    "markdown": "---\nknitr:\n  opts_chunk:\n    cache.path: \"../_cache/embeddings/\"\n---\n\n\n# Embeddings {#sec-embeddings}\n\n\n\n\n\n\n\nWhen there are a multitude of predictors, it might make sense to condense them into a smaller number of artificial features. To be useful, this smaller set should represent what is essential in the original data. This process is often called _feature extraction_ or _manifold learning_. We’ll use a more general term currently en vogue: **embeddings**. While this chapter focuses on feature extraction, embeddings  can be used for other purposes, such as converting non-numeric data (e.g., text) into a more usable numeric format. \n\nThis section will examine two primary classes of embedding methods that can achieve multiple purposes. First, we’ll consider linear methods that take a numeric input matrix $X$ that is $n \\times p$ and create a different, probably smaller set of features $X^*$ ($n \\times m$)^[With $m <<< p$.] using the transformation $X^* = XA$. \n\nAfter describing linear methods, we will consider a different class of transformations that focuses on the distances between data points called _multidimensional scaling_ (MDS). MDS creates a new set of $m$ features that are not necessarily linear combinations of the original features but often use some of the same math as the linear techniques. \n\nBefore beginning, we’ll introduce another data set that will be used here and in forthcoming chapters.\n\n## Example: Predicting Barley Amounts  {#sec-barley}\n\n@larsen2019deep and @pierna2020applicability describe a data set where laboratory measurements are used to predict what percentage of a liquid was lucerne, soy oil, or barley oil^[Retreived from [`https://chemom2019.sciencesconf.org/resource/page/id/13.html`](https://chemom2019.sciencesconf.org/resource/page/id/13.html)]. An instrument is used to measure how much of particular wavelengths of light are absorbed by the mixture to help determine chemical composition. We will focus on using the lab measurements to predict the percentage of barley oil in the mixture. The distribution of these values is shown in @fig-barley-data(a). \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![(a) The distribution of the outcome for the entire data set. The bar colors reflect the percent barley distribution and are used in subsequent sections. (b) Selected training set spectra for four barley samples. Each line represents the set of 550 predictors in the data.](../figures/fig-barley-data-1.svg){#fig-barley-data fig-align='center' width=60%}\n:::\n:::\n\n\nNote that most of the data have very little barley oil. About 27% of the data are less than 1%, and the median barley oil percentage is 5.96%.\n\nThe 550 predictors are the light absorbance for sequential values in the light region of interest (believed to be from wavelengths between 1300 and 2398 nm). @fig-barley-data(b) shows a selection of four samples from the data. The darker lines represent samples with lower barley content. \n\nThese predictor values, called _spectra_, have a very high serial correlation between predictors; median correlation between the predictors was 0.98. The high degree of between-predictor correlation can be a major complication for some models and can degrade predictive performance.  Therefore, we need methods that will simultaneously decorrelate predictors while extracting useful predictive information for the outcome. \n\nAnalyses of similar data sets can be found in [Section 9.1](https://bookdown.org/max/FES/illustrative-data-pharmaceutical-manufacturing-monitoring.html) of @fes and @wtf2024.\n\nIn the following computations, each predictor was standardized using the orderNorm transformation mentioned earlier (unless otherwise noted). \n\nThe data originated from a modeling competition to find the most accurate model and specific samples were allocated to training and test sets. However, there were no public outcome values for the test set; our analysis will treat the 6,915 samples in their training set as the overall pool of samples. This is enough data to split into separate training ($n_{tr} =$ 4,839), validation ($n_{val} =$ 1,035), and test sets ($n_{te} =$ 1,041). The allocation of samples to each of the three data sets utilized stratified sampling based on the outcome data. \n\n\n## Linear Transformations  {#sec-linear-embed}\n\nThe barley data set presents a common challenge for many machine learning techniques:  the features are highly correlated.  Techniques that require inverting the covariance matrix of the features, such as linear regression, will become numerically unstable or may not be able to be fit when features are highly correlated.  If we desire to use a modeling technique that is adversely affected by highly correlated features, then what can we do?  One option would be to use a filtering method that removes highly correlated features (@fes), leaving a subset of features that has sufficiently low among-feature correlation to allow the use of the desired models.  A second approach would be to use an embedding technique that extracts new features based on the original features such that the new features optimally summarize information from the original data while also minimizing the correlation among the new features making them well suited for the machine learning models we desire to use.\n\nIn this section we will review principal components analysis (PCA), independent component analysis (ICA), and partial least squares (PLS), which are fundamental linear embedding techniques that are effective at identifying meaningful embeddings.  Both PCA and ICA extract embedded features using only information in the original features.  Alternatively, PLS extracts embedded features using information from both the original features and the outcome.  For this reason PLS is called a supervised technique since it utilizes the outcome to guide the extraction of the embedded features.  Alternatively, PCA and ICA are called unsupervised techniques because the process of extracting the embedded features does not utilize the outcome.\n\nEach of these three embedding methods are _linear_ in a mathematical sense because they transform a matrix of numeric features ($X$) into new features that are linear combinations of the original features.  The new linear combinations are often called _scores_.  The original feature matrix $X$ can be transformed into a set of scores using the equation\n\n$$ X^* = X A $$\n\nwhere $A$ is a matrix that translates or embeds the original features into a lower dimensional space that summarizes the information contained in the original space.  It is easy to see why this matrix operation is linear when the elements of the $X$ and $A$ matrices are expanded.  For example, the first score for the _i^{th}_ sample would be:\n\n$$ x^*_{i1} = a_{11} x_{i1} + a_{21}x_{i2} + \\ldots +  a_{p1}x_{ip} $$\n\nBoth PCA and ICA have distinct objectives for how each embeds the original features into a lower dimensional subspace, thus leading to a different $A$ matrix.  By embedding the features, the correlation among the original features can be reduced and the relationship with the outcome can be better identified within the subspace. However utilizing an unsupervised embedding technique does not guarantee that the predictive performance of a model will improve.  The main assumption when using an unsupervised embedding method is that the condensed information will be effective at predicting the response.  While unsupervised techniques in practice can create embedded features that are predictive of the response, the assumption that they _will_ create new features that are predictive of the response is a logical leap because the objective of unsupervised techniques is completely disconnected with the response.  In fact, we often find that while unsupervised techniques create embedded features that have good characteristics for modeling, they do not create new features that most effectively improve a model's predictive performance.\n\nAlternatively, PLS is a technique that utilizes the response to identify an embedding that is optimally related to the response.  When compared to PCA, PLS will find a a smaller and more efficient embedding of the original predictors that is as or more predictive of the response.\n\n### Principal Component Analysis\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\nKarl Pearson introduced Principal Component Analysis (PCA) over a century ago, yet it remains a fundamental dimension reduction method [@jolliffe2016principal].  Its relevance stems from the underlying objective of the technique: to find linear combinations of the original predictors such that the combinations summarize the maximal amount of variation in the original feature space.  From a statistical perspective, variation is synonymous with information.  So, by finding combinations of the original features that capture variation, we find the embedded subspace of the data that optimally summarizes the information relevant to the features.  Simultaneously, the new dimensions  are required to be orthogonal to each other.  The property of orthogonality enables the feature space variability to be neatly partitioned in a way that does not overlap.  It turns out that the variability summarized across all of the principal components is exactly equal to the variability across the original features.  In practice, enough new score features are computed to account for a sufficient amount of the variability in the original features.\n\nAn important side benefit of this technique is that the resulting PCA scores are uncorrelated.  This property is very useful for modeling techniques (e.g., multiple linear regression, neural networks, support vector machines, and others) that need the predictors to be relatively uncorrelated.\n\nWhile PCA delivers new features with the desirable characteristics of uncorrelated scores and optimally summarizing variability, the technique must be used with care.  Specifically, we must understand that PCA seeks to find embeddings without regard to any further understanding of the original features such as their measurement scales or of the response.  Hence, PCA may generate embeddings that are focused on irrelevant features in the data and that are ignorant of the modeling objective.\n\nBecause PCA seeks linear combinations of predictors that maximize variability, it will naturally first be drawn to summarizing predictors that have more variation.  If the original predictors are on measurement scales that differ in order of magnitude, then the first components will focus on summarizing the higher magnitude predictors.  This means that the information contained in the features generated by PCA will focus on the scales of the measurements.  If, by chance, the most relevant feature for predicting the response also has the greatest variability, then the new features created by PCA will retain the important information for predicting the response.  However, if the most important features are on the smallest scales, then the top features created by PCA will be much less effective at predicting the response.\n\nIn most practical machine learning applications, features are on vastly different scales.  Additionally, the distributions of each feature are not bell-shaped or symmetric.  To enable PCA to perform well as an embedding technique, we recommend first transforming each feature's distribution to become more symmetric and bell-shaped using a technique such as Yeo-Johnson [TODO:  Add reference to this section].  Then the transformed features should be mean centered and scaled so that all features have a mean of zero and standard deviation of one.  Placing all features on the same scale prevents PCA from focusing dimension reduction simply on measurement scale.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A plot of the PCA component number versus the percent of variance explained for each component for the barley data.](../figures/fig-barley-pca-scree-1.svg){#fig-barley-pca-scree fig-align='center' width=60%}\n:::\n:::\n\n\nAfter the features have been individually transformed, PCA can then be used to extract new features.  The number of new features to retain is a parameter that must be determined.  In practice, the number of new features is often selected based on percentage of variability that the new features summarize relative to the original features.  @fig-barley-pca-scree displays the new PCA feature number (x-axis) versus the percent of total variability across the original features.  In this graph, 3 components summarize 99.4 percent of the total variability.  Often, we select the smallest number of new features that summarize the greatest amount of variability.  Instead of specifying the number of new features based on amount of variability summarized, the number of features can be, and _should_ be, a tuning parameter in the model tuning process.  The approach for tuning of this parameter will be addressed in Section [TODO:  Add reference to this section].  Since PCA is ignorant of the response, the new features that optimally summarize variability may not be the most predictive of the response.  It may be that there are new features that summarize less variability which are actually predictive of the response.  By making the number of PCA components to retain a tuning parameter, we enable the modeling process to better identify the predictive information from the original data.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A scatterplot of the first two components for PCA, ICA, and PLS, colored by the percentage of barley oil.](../figures/fig-barley-component-scatter-1.svg){#fig-barley-component-scatter fig-align='center' width=60%}\n:::\n:::\n\n\n\n@fig-barley-component-scatter (left facet) is a scatterplot of the first two principal components colored by the percentage of barley oil.  The figure reveals that the first component delineates the higher barley oil samples on the left with the lower barley oil samples on the right.  Likewise, there is a similar vertical gradient.  For this data, the unsupervised dimension reduction via PCA provides embeddings that are visually related to the response.  While the first two components are visually related the outcome, the correlation between these two components is 0.\n\n\n### Independent Component Analysis\n\nICA has roots in signal processing, where the observed signal that is measured is a combination of several different input signals.  A common example used to conceptualize the objective of ICA is the microphone problem.  Suppose that a voice recording is taken during a meeting where there is one primary presenter.  During the meeting, someone close to the microphone continues to shuffle papers which is also captured on the recording.  The final recording contains both the primary presenter's voice and the noise of the shuffled papers, which are two independent sources of audio information.  \nThe objective of independent component analysis (ICA) is to identify linear combinations of the original features that are statistically independent of each other [@nordhausen2018independent].  For the microphone problem, the goal would be to separate the two unique sources of sound in the recording.  The noise source would then be isolated and could be removed.  Because ICA's objective requires statistical independence among components, it will generate a different set of features than PCA.  This enables ICA to be able to model a different set of trends than PCA, which focuses on orthogonality and linear relationships.  Depending on the problem, ICA may generate new features that may be more predictive of the response than PCA.  Unlike PCA, which orders new components based on summarized variance, ICA's components have no natural ordering.  Thhis means that ICA may require more components than PCA in order to find the ones that are related to the outcome.\n\nTo compare PCA and ICA, let's return to Figure @fig-barley-component-scatter.  First, both techniques are similar in that the first and second components are uncorrelated.  However, while there are visual patterns between the first two components for PCA, there are no patterns for ICA.  This demonstrates the ICA creates independent components.  Third, while the outcome is related to the first two PCA components, it is not related to the first two ICA components.  For ICA to be used in this problem, more ICA components will need to be extracted to locate those components that are related to the outcome.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A pairwise scatterplot of the first four ICA components, colored by the percentage of barley oil.](../figures/fig-barley-ica-scatter-1.svg){#fig-barley-ica-scatter fig-align='center' width=60%}\n:::\n:::\n\n\n@fig-barley-ica-scatter is a scatterplot matrix of the first 4 ICA components colored by the four quartiles of the outcome.  In this figure, the first three ICA components do not distinguish separation in the outcome.  However, the fourth component begins to separate the lower barley oil percentage samples from the higher barley oil samples.  Therefore, at least 4 ICA components would be needed to predict barley oil percentage.  Like PCA, the number of ICA components should be a tuning parameter in the model building process.\n\nIn practice, satisfying the statistically independence requirement of ICA can be done several ways.  One way to do this is to maximize the \"non-Gaussianity\" of the resulting components. There are different ways to measure non-Gaussianity and the _fastICA_ approach uses an information theory called _negentropy_ [@hyvarinen2000independent].  ICA should create components that are dissimilar from PCA unless the predictors demonstrate significant multivariate normality or strictly linear trends. Also, unlike PCA, there is no unique ordering of the components. \n\n### Partial Least Squares {#numeric-pls}\n\nBoth PCA and ICA focus strictly on summarizing information based on the features exclusively.  When the outcome is related to the way that the unsupervised techniques extract the embedded features, then PCA and/or ICA can be effective dimension reduction tools.  However, when the qualities of variance summarization or statistical independence are not related to the outcome, then these techniques may struggle to identify appropriate embedded features that are useful for predicting the outcome.\n\nPartial least squares (PLS) is a dimension reduction technique that identifies embedded features that are optimally linearly related to the outcome [@stone1990continuum].  While the objective of PCA finds linear combinations of the original features that best summarize variability, the objective of PLS is to do the same _and_ to find the linear combinations that are correlated with the outcome.  This process can be thought of as a constrained optimization problem in which PLS is forced to compromise between optimal variability summary of the predictors and optimal prediction of the outcome.  This means that the outcome is used to focus the dimension reduction process such that the new features have the greatest correlation with the outcome.  Because one of the objectives of PLS is to summarize variance among the original features, the features should be pre-processed in the same way as PCA.  \n\nThe most common implementation of PLS produces new features that are uncorrelated, which is similar to PCA and is beneficial for many machine learning techniques.  This characteristic can be seen in the right facet of Figure @fig-barley-component-scatter.  Similar to PCA, the first two components are visually related the outcome, but the correlation is 0.  Because PLS utilizes the response to find the embedded features, it generally requires the same number components as PCA or, usually, fewer components to generate a model with similar predictive performance.  This implies that fewer features are necessary when using PLS, thus leading to a smaller input data set to the machine learning models.  As compared to PCA, PLS utilizes the percentage of barley oil to reorient the dimension reduction with higher barley oil samples in the upper right and lower barley oil samples in the lower left.  This reorientation is not surprising since PLS is simultaneously optimizing correlation with the response.  \n\n### Overall Comparisons\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A scatterplot matrix of the first component of each method versus percentage of barley oil.](../figures/fig-barley-method-correlation-comparison-1.svg){#fig-barley-method-correlation-comparison fig-align='center' width=60%}\n:::\n:::\n\n\nBecause PLS utilizes the outcome as part of the dimension reduction, its components will have as great or greater correlation with the outcome.  @fig-barley-method-correlation-comparison illustrates the relationship between the first component of each method with the outcome.  The correlation between the first score of PCA, ICA, and PLS with the percentage of barley oil is 0.55, 0.16, and 0.59.  Notice that PLS has the largest absolute correlation, which means that it has found a linear combination of the original features that is better than PCA or ICA.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A scatterplot matrix of the first component of each method compared to wavelengths 1300, 1900, and 2398, colored by the percentage of barley oil.](../figures/fig-barley-method-wavelength-comparison-1.svg){#fig-barley-method-wavelength-comparison fig-align='center' width=60%}\n:::\n:::\n\n\nAs a final comparison of the three linear embedding methods, let's examine how the first components of each method are related to three selected wavelengths at the beginning, middle, and end of the spectrum (@fig-barley-pca-pls-comparison).  The first characteristic that stands out is that PCA and PLS are nearly mirror images for each of the three selected wavelengths.  This is not surprising given the similarities we have seen between these methods for this data set.  The first ICA component does not provide separation for percentage of barley oil, regardless of the selected wavelength.  However, the first PCA and PLS components have a near linear relationship with wavelength 1900 which are also related to the outcome.  For PCA, smaller scores are associated with larger barley oil percentages.  While for PLS the opposite is true.  For PCA and PLS, the correlation between an original feature and a component is called a loading.  Features that load onto the same component are often related in some meaningful way. This information can be used to aid in interpreting the new components and why these are related to the outcome.  In this example, the chemical information contained in wavelength 1900 may be unique to barley oil.\n\nPCA and PLS will be discussed in more detail in Chapter [TODO:  Add reference to this chapter], where these methods are directly used as modeling techniques.\n\n## Multidimensional Scaling {#sec-mds}\n\nMultidimensional scaling [@torgerson1952multidimensional] is a feature extraction tool that creates embeddings that try to preserve the geometric distances between training set points. In other words, the distances between points in the smaller dimensions should be comparable to those in the original dimensions. Since the methods in this section use distances, the predictors should be standardized to equivalent units before the embedding is trained. We also recommend transformations to resolve skewness. \n\nTake @fig-mds-example(a) as an example. There are ten points in two dimensions (colored by three outcome classes). If we were to project these points down to a single dimension, we'd like points that are close in the original two dimensions to remain close when projected down to a single dimension. Panel (c) shows two such solutions. Each does reasonably well with some exceptions (i.e., points six and nine are too close for non-Metric MDS). \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![(a) A collection of samples associated with three outcome classes. (b) A diagram of the two nearest neighbors for each data point. (c) A one-dimensional projection using two different methods: non-metric MDS and Isomap.](../figures/fig-mds-example-1.png){#fig-mds-example fig-align='center' width=70%}\n:::\n:::\n\n\nHere we present two MDS methods, but there are many more. @Ghojogh2023 has an excellent review of an assortment of methods and their nuances. \n\nSome MDS methods compute all pairwise distances between points and use this as the input to the embedding algorithm. This is similar to how PCA can be estimated using the covariance or correlation matrices. One technique, _Non-Metric MDS_ [@kruskal1964multidimensional;@kruskal1964nonmetric;@sammon1969nonlinear], finds embeddings that minimize an objective function called \"stress\":\n\n$$\n\\text{Stress} = \\sqrt{\\frac{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}\\left(d(x_i, x_j) - d(x^*_i, x^*_j)\\right)^2}{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}d(x_i, x_j)^2}}\n$$\n\nThe numerator uses the squared difference between the pairwise distances in the original values ($x$) and the smaller embedded dimension ($x^*$). The summations only move along the upper triangle of the distance matrices to reduce redundant computations. @fig-mds-example(c, top row) has the resulting one dimensional projection of our two-dimensional data.\n\nThis can be an effective dimension reduction procedure, although there are a few issues. First, the entire matrix of distances is required (with $n_{tr}(n_{tr}-1)/2$ entries). For large training sets, this can be unwieldy and time-consuming.  Second, like PCA, it is a global method that uses all data in the computations.  We might be able to achieve more nuanced embeddings by focusing on local structures.  Finally, it is challenging to apply metric MDS to project new data onto the space in which the original data was projected.\n\n### Isomap  {#sec-isomap}\n\nTo start, we'll focus on _Isomap_ [@tenenbaum2000global]. This nonlinear MDS method uses a specialized distance function to find the embedded features. First, the _K_ nearest neighbors are determined for each training set point using standard functions, such as Euclidean distance. @fig-mds-example(b) shows the _K_ = 2 nearest neighbors for our example data. Many nearest-neighbor algorithms can be very computationally efficient and their use eliminates the need to compute all of the pairwise distances. \n\nThe connections between neighbors form a _graph structure_ that defines which data points are closely related to one another. From this, a new metric called _geodesic distance_ can be approximated. For a graph, we can compute the approximate geodesic distance using the shortest path between two points on the graph. With our example data, the Euclidean distance between points four and five is not large. However, its approximate geodesic distance is greater because the shortest path is through points nine, eight, and seven. @Ghojogh2023 use a wonderful analogy: \n\n> A real-world example is the distance between Toronto and Athens. The Euclidean distance is to dig the Earth from Toronto to reach Athens directly. The geodesic distance is to move from Toronto to Athens on the curvy Earth by the shortest path between two cities. The approximated geodesic distance is to dig the Earth from Toronto to London in the UK, then dig from London to Frankfurt in Germany, then dig from Frankfurt to Rome in Italy, then dig from Rome to Athens.\n\nThe Isomap embeddings are a function of the eigenvalues computed on the geodesic distance matrix. The $m$ embedded features are functions of the first $m$ eigenvectors. Although eigenvalues are associated with linear embeddings (e.g., PCA), nonlinear geodesic distance results in a global nonlinear embedding. @fig-mds-example(c, bottom row) shows the 1D results for the example data set. For a new data point, its nearest-neighbors in the training set are determined so that the approximate geodesic distance can be computed. The estimated eigenvectors and eigenvalues are used to project the new point into the embedding space. \n\nFor Isomap, the number of nearest neighbors and the number of embeddings are commonly optimized. @fig-barley-isomap shows a two-dimensional Isomap embedding for the barley data with varying numbers of neighbors. In each configuration, the higher barley values are differentiated from the small (mostly zero) barley samples. There does seem to be two or three clusters of data associated with small outcome values. The new features become more densely packed as the number of neighbors increases.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Isomap for the barley data for different numbers of nearest neighbors. The training set was used to fit the model and these results show the projections on the validation set. Lighter colors indicate larger values of the outcome.](../figures/fig-barley-isomap-1.png){#fig-barley-isomap fig-align='center' width=80%}\n:::\n:::\n\n\n### Laplacian Eigenmaps  {#sec-eigenmaps}\n\nThere are many other approaches to preserve local distances. One is _Laplacian eigenmaps_ [@belkin2001laplacian]. Like Isomap, it uses nearest neighbors to define a graph of connected training set points. For each connected point, a weight between graph nodes is computed that becomes smaller as the distance between points in the input space increases. The radial basis kernel (also referred to as the \"heat kernel\") is a good choice for the weighting function^[A note about some notation... We commonly think of the _norm_ notation as $\\|\\boldsymbol{x}\\|_p = \\left(|x_1|^p + |x_2|^p + \\ldots + |x_n|^p\\right)^{1/p}$. So what does the lack of a subscript in $||\\boldsymbol{x}||^2$ mean? The convention is the sum of squares:  $||\\boldsymbol{x}||^2 = x_1^2 + x_2^2 + \\ldots + x_n^2$.]: \n\n$$\nw_{ij} = \\exp\\left(\\frac{-||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2}{\\sigma}\\right)\n$$\n\nwhere $\\sigma$ is a scaling parameter that can be tuned. If two points are not neighbors, or if $i = j$, then $w_{ij} = 0$. Note that the equation above uses Euclidean distance. For the 2-nearest neighbor graph shown in @fig-mds-example(b) and $\\sigma = 1 / 2$, the weight matrix is roughly\n\n\n::: {.cell layout-align=\"center\"}\n$$\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\nW = \\begin{bmatrix}\n\\0 & 0.1 & \\0 & 0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n & \\0 & 0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  & \\0 & 0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  &  & \\0 & \\0 & \\0 & 0.1 & \\0 & \\0 & \\0\\\\\n &  &  &  & \\0 & 0.4 & \\0 & \\0 & 0.1 & \\0\\\\\n &  & sym &  &  & \\0 & \\0 & \\0 & 0.1 & \\0\\\\\n &  &  &  &  &  & \\0 & 0.4 & \\0 & 0.1\\\\\n &  &  &  &  &  &  & \\0 & 0.1 & 0.3\\\\\n &  &  &  &  &  &  &  & \\0 & \\0\\\\\n &  &  &  &  &  &  &  &  & \\0 \n\\end{bmatrix}\n$$\n:::\n\n\nThe use of nearest neighbors means that the matrix can be very sparse and the zero values help define locality for each data point. Recall that samples 2 and 3 are fairly close to one another, while samples 1 and 2 are farther away. The weighting scheme gives the former pair a 4-fold larger weight in the graph than the latter pair. \n\nLaplacian eigenmaps rely heavily on graph theory. This method computes a _graph Laplacian_ matrix, defined as $L = D - W$ where the matrix $D$ has zero non-diagonal entries and diagonals equal to the sum of the weights for each row. For our example data, the matrix is: \n\n\n::: {.cell layout-align=\"center\"}\n$$\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\nL = \\begin{bmatrix}\n 0.3 & -0.1 & \\0 & -0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  0.5 & -0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  &  0.5 & -0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n &  &  &  0.4 & \\0 & \\0 & -0.1 & \\0 & \\0 & \\0\\\\\n &  &  &  &  0.5 & -0.4 & \\0 & \\0 & -0.1 & \\0\\\\\n &  & sym &  &  &  0.5 & \\0 & \\0 & -0.1 & \\0\\\\\n &  &  &  &  &  &  0.6 & -0.4 & \\0 & -0.1\\\\\n &  &  &  &  &  &  &  0.8 & -0.1 & -0.3\\\\\n &  &  &  &  &  &  &  &  0.3 & \\0\\\\\n &  &  &  &  &  &  &  &  &  0.4 \n\\end{bmatrix}\n$$\n:::\n\n\nThe eigenvalues and eigenvectors of this matrix are used as the main ingredients for the embeddings. @Bengio2003advances shows that, since these methods eventually use eigenvalues in the embeddings, they can be easily used to project new data. \n\n### UMAP {#sec-umap}\n\nThe Uniform Manifold Approximation and Projection (UMAP) [@sainburg2020parametric] technique is one of the most popular distance-based methods. Its precursors, stochastic neighbor embedding (SNE) [@hinton2002stochastic] and  Student’s t-distributed stochastic neighbor embedding (t-SNE) [@van2008visualizing], redefined feature extraction, particularly for visualizations. UMAP borrows significantly from Laplacian eigenmaps and t-SNE but has a more theoretically sound motivation. \n \nAs with Laplacian eigenmaps, UMAP converts the training data points to a sparse graph structure. Given a set of nearest neighbors, it computes values similar to the previously shown weights ($W$ matrix), which we will think of as the probability that point $j$ is a neighbor of point $i$:  \n\n$$\np_{j|i} = \\exp\\left(\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i}\\right)\n$$\n\nwhere $\\rho_i$ is the distance from $\\boldsymbol{x}_i$ to its closest neighbor, and $\\sigma_i$ is a scale parameter that now varies with each sample ($i$). To compute $\\sigma_i$, we can solve the equation   \n \n$$\n\\sum_{i=1}^K  \\exp\\left(\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i}\\right) = \\log_2(K)\n$$\n\nUnlike the previous weighting system, the resulting $n \\times n$ matrix may not be symmetric, so the final weights are computed using $p_{ij} = p_{j|i} + p_{i|j} - p_{j|i}p_{i|j}$. \n\nUMAP performs a similar calculation for the embedded values $x^*$. We'll denote the probability that embedded points $\\boldsymbol{x}_i^*$ and $\\boldsymbol{x}_j^*$ are connected as $p_{ij}^*$. \n\nNumerical optimization methods^[Specifically gradient descent with a user-defined learning rate.] used to estimate the $n \\times m$ values $x^*_{ij}$. The process is initialized using a very sparse Laplacian eigenmap, the first few PCA components, or random uniform numbers. The objective function is based on cross-entropy and attempts to make the graphs in the input and embedded dimensions as similar as possible by minimizing:   \n\n$$\nCE = \\sum_{i=1}^{n_{tr}}\\sum_{j=i+1}^{n_{tr}} \\left[p_{ij}\\, \\log\\frac{p_{ij}}{p_{ij}^*} + (1 - p_{ij})\\log\\frac{1-p_{ij}}{1-p_{ij}^*}\\right]\n$$\n\nUnlike the other embedding methods shown in this section, UMAP can also create supervised embeddings so that the resulting features are more predictive of a qualitative or quantitative outcome value. See @sainburg2020parametric.\n\nBesides the number of neighbors and embedding dimensions, several more tuning parameters exist.  The optimization process's number of optimization iterations (i.e., epochs) and the learning rate can significantly affect the final results. A distance-based tuning parameter, often called _min-dist_, specifies how \"packed\" points should be in the reduced dimensions. Values typically range from zero to one. However, the original authors state:\n\n> We view min-dist as an essentially aesthetic parameter governing the appearance of the embedding, and thus is more important when using UMAP for visualization.\n\nAs will be seen below, the initialization scheme is an important tuning parameter. \n\nFor supervised UMAP, there is an additional weighting parameter (between zero and one) that is used to balance the importance of the supervised and unsupervised aspects of the results. \n\n@fig-umap shows an interactive visualization of how UMAP can change with different tuning parameters. Each combination was trained for 1,000 epochs and used a learning rate of 1.0. For illustrative purposes, the resulting embeddings were scaled to a common range. \n\n::: {#fig-umap}\n\n::: {.figure-content}\n\n```{shinylive-r}\n#| label: fig-umap\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\n\n# ------------------------------------------------------------------------------\n\nlight_bg <- \"#fcfefe\" # from aml4td.scss\ngrid_theme <- bs_theme(\n  bg = light_bg, fg = \"#595959\"\n)\n\n# ------------------------------------------------------------------------------\n\ntheme_light_bl<- function(...) {\n\n  ret <- ggplot2::theme_bw(...)\n\n  col_rect <- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n  ret$panel.background  <- col_rect\n  ret$plot.background   <- col_rect\n  ret$legend.background <- col_rect\n  ret$legend.key        <- col_rect\n\n  ret$legend.position <- \"top\"\n\n  ret\n}\n\n# ------------------------------------------------------------------------------\n\nui <- fluidPage(\n  theme = grid_theme,\n  fluidRow(\n\n    column(\n      width = 4,\n      sliderInput(\n        inputId = \"min_dist\",\n        label = \"Min Distance\",\n        min = 0.0,\n        max = 1.0,\n        value = 0.2,\n        width = \"100%\",\n        step = 0.2\n      )\n    ), # min distance\n    column(\n      width = 4,\n      sliderInput(\n        inputId = \"neighbors\",\n        label = \"Neighbors\",\n        min = 5,\n        max = 45,\n        value = 5,\n        width = \"100%\",\n        step = 10\n      )\n    ), # nearest neighbors\n\n    column(\n      width = 4,\n      sliderInput(\n        inputId = \"supervised\",\n        label = \"Amount of Supervision\",\n        min = 0.0,\n        max = 0.7,\n        value = 0,\n        width = \"100%\",\n        step = 0.1\n      )\n    ),\n    fluidRow(\n      column(\n        width = 4,\n        radioButtons(\n          inputId = \"initial\",\n          label = \"Initialization\",\n          choices = list(\"Laplacian Eigenmap\" = \"spectral\", \"PCA\" = \"pca\", \n                         \"Random\" = \"random\")\n        )\n      ),\n      column(\n        width = 6,\n        align = \"center\",\n        plotOutput('umap')\n      )\n    )\n  ) # top fluid row\n)\n\nserver <- function(input, output) {\n  load(url(\"https://raw.githubusercontent.com/aml4td/website/mds-start/RData/umap_results.RData\"))\n\n  output$umap <-\n    renderPlot({\n      \n      dat <-\n        umap_results[\n          umap_results$neighbors == input$neighbors &\n            umap_results$min_dist == input$min_dist &\n            umap_results$initial == input$initial &\n            # log10(umap_results$learn_rate) == input$learn_rate &\n            umap_results$supervised == input$supervised,\n        ]\n\n      p <-\n        ggplot(dat, aes(UMAP1, UMAP2, col = barley)) +\n        geom_point(alpha = 1 / 3, cex = 3) +\n        scale_color_viridis(option = \"viridis\") +\n        theme_light_bl() +\n        coord_fixed() +\n        labs(x = \"UMAP Embedding #1\", y = \"UMAP Embedding #2\") +\n        guides(col = guide_colourbar(barheight = 0.5))\n\n      print(p)\n\n    })\n}\n\napp <- shinyApp(ui = ui, server = server)\n```\n:::\n\nA visualization of UMAP results for the barley data using different values for several tuning parameters. The points are the validation set values. \n\n:::\n\nThere are a few notable patterns in these results: \n\n - The initialization method can heavily impact the patterns in the embeddings. \n - As with Isomap, there are two or three clusters of data points with small barley values. \n - When the amount of supervision increases, one or more circular structures form that are associated with small outcome values. \n - The minimum distance parameter can drastically change the results. \n\nt-SNE and UMAP have become very popular tools for visualizing complex data. Visually, they often show interesting patterns that linear methods such as PCA cannot. However, they are computationally slow and unstable over different tuning parameter values. Also, it is easy to believe that the UMAP distances between embedding points are important or quantitatively predictive. That is not the case; the distances can be easily manipulated using the tuning parameters (especially the minimum distance). \n \n\n## Centroid-Based Methods  {#sec-centroids}\n\nprototype-based methods\n\n## Embedding Qualitative Predictors  {#sec-qual-embedding}\n\n## Other Methods\n\nautoencoders? \n\n\n## Chapter References {.unnumbered}\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}