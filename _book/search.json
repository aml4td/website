[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "",
    "text": "Preface\nWelcome! This is a work in progress. We want to create a practical guide to developing quality predictive models from tabular data. We’ll publish materials here as we create them and welcome community contributions in the form of discussions, suggestions, and edits.\nWe also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).\nOur intention is to write these materials and, when we feel we’re done, pick a publishing partner to produce a print version.\nThe book takes a holistic view of the predictive modeling process and focuses on a few areas that are usually left out of similar works. For example, the effectiveness of the model can be driven by how the predictors are represented. Because of this, we tightly couple feature engineering methods with machine learning models. Also, quite a lot of work happens after we have determined our best model and created the final fit. These post-modeling activities are an important part of the model development process and will be described in detail.\nWe deliberately avoid using the term “artificial intelligence.” Eugen Rochko’s (@Gargron@mastodon.social) comment on Mastodon does a good job of summarizing our reservations regarding the term:\nTo cite this website, we suggest:\n@online{aml4td,\n  author = {Kuhn, M and Johnson, K},\n  title = {{Applied Machine Learning for Tabular Data}},\n  year = {2023},\n  url = { https://aml4td.org},\n  urldate = {2024-12-24}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "License",
    "text": "License\n\nThis work is licensed under CC BY-NC-SA 4.0\n\nThis license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, for noncommercial purposes only. If others modify or adapt the material, they must license the modified material under identical terms.\n\nBY: Credit must be given to you, the creator.\nNC: Only noncommercial use of your work is permitted. Noncommercial means not primarily intended for or directed towards commercial advantage or monetary compensation.\nSA: Adaptations must be shared under the same terms.\n\nOur goal is to have an open book where people can reuse and reference the materials but can’t just put their names on them and resell them (without our permission).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Intended Audience",
    "text": "Intended Audience\nOur intended audience includes data analysts of many types: statisticians, data scientists, professors and instructors of machine learning courses, laboratory scientists, and anyone else who desires to understand how to create a model for prediction. We don’t expect readers to be experts in these methods or the math behind them. Instead, our approach throughout this work is applied. That is, we want readers to use this material to build intuition about the predictive modeling process. What are good and bad ideas for the modeling process? What pitfalls should we look out for? How can we be confident that the model will be predictive for new samples? What are advantages and disadvantages of different types of models? These are just some of the questions that this work will address.\nSome background in modeling and statistics will be extremely useful. Having seen or used basic regression models is good, and an understanding of basic statistical concepts such as variance, correlation, populations, samples, etc., is needed. There will also be some mathematical notation, so you’ll need to be able to grasp these abstractions. But we will keep this to those parts where it is absolutely necessary. There are a few more statistically sophisticated sections for some of the more advanced topics.\nIf you would like a more theoretical treatment of machine learning models, then we recommend Hastie, Tibshirani, and Friedman (2017). Other books for gaining a more in-depth understanding of machine learning are Bishop and Nasrabadi (2006), Arnold, Kane, and Lewis (2019) and, for more of a deep learning focus, Goodfellow, Bengio, and Courville (2016) and/or Prince (2023).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#is-there-code",
    "href": "index.html#is-there-code",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Is there code?",
    "text": "Is there code?\nWe definitely want to decouple the content of this work from specific software. One of our other books on modeling had computing sections. Many people found these sections to be a useful resource at the time of the book’s publication. However, code can quickly become outdated in today’s computational environment. In addition, this information takes up a lot of page space that would be better used for other topics.\nWe will create computing supplements to go along with the materials. Since we use R’s tidymodels framework for calculations, the supplement currently in-progress is:\n\ntidymodels.aml4td.org\n\nIf you are interested in working on a python/scikit-learn supplement, please file an issue",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#are-there-exercises",
    "href": "index.html#are-there-exercises",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Are there exercises?",
    "text": "Are there exercises?\nMany readers found the Exercise sections of Applied Predictive Modeling to be helpful for solidifying the concepts presented in each chapter. The current set can be found at exercises.aml4td.org",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-can-i-ask-questions",
    "href": "index.html#how-can-i-ask-questions",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "How can I ask questions?",
    "text": "How can I ask questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like cross-validated. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what I call Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.\nIf you think there is a bug, please file an issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Can I contribute?",
    "text": "Can I contribute?\nThere is a contributing page with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list. It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto was used to compile and render the materials\n\nQuarto 1.6.39\n[✓] Checking environment information...\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.4.0: OK\n      Dart Sass version 1.70.0: OK\n      Deno version 1.46.3: OK\n      Typst version 0.11.0: OK\n[✓] Checking versions of quarto dependencies......OK\n[✓] Checking Quarto installation......OK\n      Version: 1.6.39\n[✓] Checking tools....................OK\n      TinyTeX: (external install)\n      Chromium: (not installed)\n[✓] Checking LaTeX....................OK\n      Using: TinyTex\n      Version: 2024\n[✓] Checking basic markdown render....OK\n[✓] Checking Python 3 installation....OK\n      Jupyter: 5.7.1\n      Kernels: python3\n[✓] Checking Jupyter engine render....OK\n[✓] Checking R installation...........OK\n      Version: 4.4.1\n      LibPaths:\n      knitr: 1.49\n      rmarkdown: 2.29\n[✓] Checking Knitr engine render......OK\n\nR version 4.4.1 (2024-06-14) was used for the majority of the computations. torch 2.0.1 was also used. The versions of the primary R modeling and visualization packages used here are:\n\n\n\n\n\n\n\n\n\naorsf (0.1.5)\napplicable (0.1.1)\naspline (0.2.0)\n\n\nbaguette (1.0.2)\nbestNormalize (1.9.1)\nbibtex (0.5.1)\n\n\nbonsai (0.3.1)\nbroom (1.0.7)\nbroom.mixed (0.2.9.6)\n\n\nbrulee (0.3.0)\nbslib (0.8.0)\nC50 (0.1.8)\n\n\nCubist (0.4.4)\nDALEXtra (2.3.0)\ndbarts (0.9-30)\n\n\nddalpha (1.3.16)\ndesirability2 (0.0.1)\ndials (1.3.0)\n\n\ndimRed (0.2.6)\ndiscrim (1.0.1)\ndoMC (1.3.8)\n\n\ndplyr (1.1.4)\ne1071 (1.7-16)\nearth (5.3.4)\n\n\nembed (1.1.4)\nfastICA (1.2-7)\nfinetune (1.2.0)\n\n\nforested (0.1.0.9000)\nGA (3.2.4)\ngganimate (1.0.9)\n\n\nggforce (0.4.2)\nggiraph (0.8.11)\nggplot2 (3.5.1)\n\n\nglmnet (4.1-8)\ngt (0.11.1)\nhardhat (1.4.0)\n\n\nhstats (1.2.1)\nipred (0.9-15)\nirlba (2.3.5.1)\n\n\njanitor (2.2.1)\nkernlab (0.9-33)\nkknn (1.3.1)\n\n\nklaR (1.7-3)\nleaflet (2.2.2)\nlightgbm (4.5.0)\n\n\nlme4 (1.1-35.5)\nMatrix (1.7-0)\nmda (0.5-5)\n\n\nmgcv (1.9-1)\nmixdir (0.3.0)\nmixOmics (6.25.1)\n\n\nmodeldata (1.4.0)\nmodeldatatoo (0.3.0)\nnaniar (1.1.0)\n\n\npamr (1.57)\nparsnip (1.2.1)\npartykit (1.2-23)\n\n\npatchwork (1.3.0)\nplsmod (1.0.0)\nprobably (1.0.3)\n\n\npROC (1.18.5)\npurrr (1.0.2)\nragg (1.3.3)\n\n\nranger (0.17.0)\nrecipes (1.1.0)\nrpart (4.1.23)\n\n\nrsample (1.2.1)\nRSpectra (0.16-2)\nrstudioapi (0.17.1)\n\n\nrules (1.0.2)\nsf (1.0-19)\nsfd (0.1.0)\n\n\nshinylive (0.3.0)\nsparsediscrim (0.3.0)\nsparseLDA (0.1-9)\n\n\nspatialsample (0.6.0)\nsplines2 (0.5.3)\nstacks (1.0.5)\n\n\nstopwords (2.3)\ntextrecipes (1.0.6)\nthemis (1.0.2)\n\n\ntidymodels (1.2.0)\ntidyposterior (1.0.1)\ntidyr (1.3.1)\n\n\ntidysdm (0.9.5)\ntorch (0.13.0)\ntune (1.2.1)\n\n\nusethis (3.1.0)\nuwot (0.2.2)\nVBsparsePCA (0.1.0)\n\n\nviridis (0.6.5)\nworkflows (1.1.4)\nworkflowsets (1.1.0)\n\n\nxgboost (1.7.8.1)\nxrf (0.2.2)\nyardstick (1.3.1)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#chapter-references",
    "href": "index.html#chapter-references",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nArnold, T, M Kane, and B Lewis. 2019. A Computational Approach to Statistical Learning. Chapman; Hall/CRC.\n\n\nBishop, C M, and N M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nHastie, T, R Tibshirani, and J Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/news.html",
    "href": "chapters/news.html",
    "title": "News",
    "section": "",
    "text": "Errata\nNo reported edits (yet).",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#changelog",
    "href": "chapters/news.html#changelog",
    "title": "News",
    "section": "Changelog",
    "text": "Changelog\n\n2024-10-01\nEnabled google analytics.\n\n\n2024-09-23\nChapter 4 is now on missing data, bumping subsequent chapter numbers.\nNew chapters on resampling and grid search are now online.\n\n\n2024-06-17\nSeveral new chapters on embeddings, splines/interactions/discretization, and overfitting. Also, shinylive is used for interactive visualizations of concepts.\n\n\n2024-01-02\nFixed various typos.\n\n\n2023-12-21\nTypo fix on main page.\n\n\n2023-12-19\nSmall updates.\nUpdated snapshot.\nThe background color of premade outputs has been changed to the pages background color (#16).\n\n\n2023-12-11\nNo new content.\nUpdated snapshot.\nIncludes new Google Scholar links for articles (#8)\n\n\n2023-11-17\nUpdated renv snapshot.\n\n\n2023-10-09\nFirst committed versions.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/contributing.html",
    "href": "chapters/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Software\nIf you plan to do anything beyond fixing a typo, the best thing you can do is to open an issue and discuss changes before you spend a lot of time doing them.\nIf you don’t have a lot of experience with git or GitHub, take a look at the wonderful Happy Git and GitHub for the useR.\nIf you want to contribute, some general advice is:\nA merged PR will make you appear in the contributor list (see below). It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.\nYou will mostly work with the *.qmd files in the chapters directory.\nHere is a list of the elements in the repo:\nRegarding R packages, the repository has a DESCRIPTION file as if it were an R package. This lets us specify precisely what packages and their versions should be installed. The packages listed in the imports field contain packages for modeling/analysis and packages used to make the website/book. Some basic system requirements are likely needed to install packages: Fortran, gdal, and others.\nThe main requirements are as follows.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#software",
    "href": "chapters/contributing.html#software",
    "title": "Contributing",
    "section": "",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing system. Quarto version 1.6.39 is used to compile the website.\nWe also use a few Quarto extensions. These should be installed from the project’s root directory via:\nquarto add quarto-ext/fontawesome\nquarto add quarto-ext/shinylive\nquarto add quarto-ext/fancy-text\nquarto add leovan/quarto-pseudocode\n\n\nR and renv\nR version 4.4.1 (2024-06-14) is what we are currently using. We suggest using rig to manage R versions. There are several IDEs that you can use. We’ve used RStudio (&gt;= 2023.6.1.524).\nThe current strategy is to use the renv (&gt;= version 1.0.11) package to make this project more isolated, portable and reproducible.\nTo get package dependencies installed…\n\nWhen you open the website.Rproj file, the renv package should be automatically installed/updated (if neded). For example:\n# Bootstrapping renv 1.0.3 ---------------------------------------------------\n- Downloading renv ... OK\n- Installing renv  ... OK\n\nThe following package(s) will be installed:\n- BiocManager [1.30.22]\nThese packages will be installed into \"~/content/aml4td/renv/library/R-4.3/x86_64-apple-darwin20\".\nif you try to compile the book, you probably get and error:\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nYou can get more information using renv::status() but you can get them installed by first running renv::activate(). As an example:\n&gt; renv::activate()\n\nRestarting R session...\n\n- Project '~/content/aml4td' loaded. [renv 1.0.3]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nSince we have package versions recorded in the lockfile, we can installed them using renv::restore(). Here is an example of that output:\n&gt; renv::restore() \nThe following package(s) will be updated:\n\n# Bioconductor ---------------------------------------------------------------\n- mixOmics         [* -&gt; mixOmicsTeam/mixOmics]\n\n# CRAN -----------------------------------------------------------------------\n- BiocManager      [1.30.22 -&gt; 1.30.21.1]\n- lattice          [0.21-9 -&gt; 0.21-8]\n- Matrix           [1.6-1.1 -&gt; 1.6-0]\n- nlme             [3.1-163 -&gt; 3.1-162]\n- rpart            [4.1.21 -&gt; 4.1.19]\n- survival         [3.5-7 -&gt; 3.5-5]\n- abind            [* -&gt; 1.4-5]\n\n&lt;snip&gt;\n\n- zip              [* -&gt; 2.2.0]\n- zoo              [* -&gt; 1.8-12]\n\n# GitHub ---------------------------------------------------------------------\n- BiocParallel     [* -&gt; Bioconductor/BiocParallel@devel]\n- BiocVersion      [* -&gt; Bioconductor/BiocVersion@devel]\n- modeldatatoo     [* -&gt; tidymodels/modeldatatoo@HEAD]\n- parsnip          [* -&gt; tidymodels/parsnip@HEAD]\n- usethis          [* -&gt; r-lib/usethis@HEAD]\n\n# RSPM -----------------------------------------------------------------------\n- bslib            [* -&gt; 0.5.1]\n- fansi            [* -&gt; 1.0.5]\n- fontawesome      [* -&gt; 0.5.2]\n- ggplot2          [* -&gt; 3.4.4]\n- htmltools        [* -&gt; 0.5.6.1]\n- withr            [* -&gt; 2.5.1]\n\nDo you want to proceed? [Y/n]: y\n\n# Downloading packages -------------------------------------------------------\n- Downloading BiocManager from CRAN ...         OK [569 Kb in 0.19s]\n- Downloading nlme from CRAN ...                OK [828.7 Kb in 0.19s]\n- Downloading BH from CRAN ...                  OK [12.7 Mb in 0.4s]\n- Downloading BiocVersion from GitHub ...       OK [826 bytes in 0.37s]\n\n&lt;snip&gt;\n\nDepending on whether you have to install packages from source, you may need to install some system dependencies and try again (I had to install libgit2 the last time I did this).\nOnce you have everything installed, we recommend installing the underlying torch computational libraries. You can do this by loading the torch package A download will automatically begin if you need one.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#contributor-list",
    "href": "chapters/contributing.html#contributor-list",
    "title": "Contributing",
    "section": "Contributor List",
    "text": "Contributor List\nThe would like to thank those who have made a contribution to the project: @amy-palmer, @bmreiniger, @coatless, @krz, @syclik, and @tomsing1.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Non-Tabular Data\nMachine learning (ML) models are mathematical equations that take inputs, called predictors, and try to estimate some future output value. The output, often called an outcome or target, can be numbers, categories, or other types of values.\nFor example, in the next chapter, we try to predict how long it takes to deliver food ordered from a restaurant. The outcome is the time from the initial order (in minutes). There are multiple predictors, including: the distance from the restaurant to the delivery location, the date/time of the order, and which items were included in the order. These data are tabular; they can be arranged in a table-like way (such as a spreadsheet or database table) where variables are arranged in columns and individual data points (i.e., instances of food orders) in rows, as shown in Table 1.11.\nNote that the predictor values are almost always known. For future data, the outcome is not; it is a machine learning model’s job to predict unknown outcome values.\nHow does it do that? A specific machine learning model has a defined mathematical prediction equation defining exactly how the predictors relate to the outcome. We’ll see two very different prediction equations shortly.\nThe prediction equation includes some unknown parameters. These are placeholders for values that help us best predict the outcomes. The process of model training (also called “fitting” or “estimation”) takes our existing predictor and outcome data and uses them to find the “optimal” values of these unknown parameters. Once we estimate these unknowns, we can use them and specific predictor values to estimate future outcome values.\nHere is a simple example of a prediction equation with a single predictor (the distance) and two unknown parameters that have been estimated:\n\\[\ndelivery\\:time = \\exp\\left[2.944 + 0.096 \\: distance\\right]\n\\tag{1.1}\\]\nWe could use this equation for new orders:\nand so on.\nThis is an incredibly reductive approach; it assumes that the relationship between the distance and time are log-linearly related and is not influenced by any other characteristics of the process. On the bright side, the exponential function ensures that there are no negative time predictions, and it is straightforward to explain.\nHow did we arrive at values of 2.944 and 0.096? They were estimated from data once we made a few assumptions about our model. We started off by proposing a format for the prediction equation: an additive, log-linear function2 that includes unknown parameters \\(\\beta_0\\) and \\(\\beta_1\\):\n\\[\n{ \\color{darkseagreen} \\underbrace{\\color{black} log(time)}_{\\text{the outcome}} } =\n{ \\color{darkseagreen} \\overbrace{\\color{black} \\beta_0+\\beta_1\\, distance}^{\\text{an additive function of the predictors}} } +  \n{ \\color{darkseagreen} \\underbrace{\\color{black} \\epsilon}_{\\text{the errors}} }\n\\]\nTo estimate the parameters, we chose some type of criterion to quantify what it means to be “good” values (called an objective function or performance metric). Here we will pick values of the \\(\\beta\\) parameters that minimize the squared error, with the error (\\(\\epsilon\\)) defined as the difference in the observed and predicted times (on the log scale).\nBased on this criterion, we can define intermediate equations that, if we solve them, will result in the best possible parameter estimates given the data and our choices for the prediction equation and objective function.\nSadly, Equation 1.1, while better than a random guess, is not very effective. It does the best it can with a the single predictor. It just isn’t very accurate.\nTo illustrate the diversity of different ML models, Equation 1.2 shows a completely different kind of prediction equation. This was estimated from a model called a regression tree. The function \\(I(\\cdot)\\) is one if the logical statement is true and zero otherwise. Two predictors (distance and day of the week) were used in this case.\n\\[\n\\begin{align}\ntime = \\:&23.0\\, I\\left(distance &lt;  4.465   \\text{ miles and } day \\in \\{Mon, Tue, Wed, Sun\\}\\right)  + \\notag \\\\\n       \\:&27.0\\, I\\left(distance &lt;  4.465   \\text{ miles and } day \\in \\{Thu, Fri, Sat\\}\\right)  + \\notag \\\\\n       \\:&29.8\\, I\\left(distance \\ge  4.465 \\text{ miles and } day \\in \\{Mon, Tue, Wed, Sun\\}\\right)  + \\notag \\\\\n       \\:&36.5\\, I\\left(distance \\ge  4.465 \\text{ miles and } day \\in \\{Thu, Fri, Sat\\}\\right)\\notag\n\\end{align}\n\\tag{1.2}\\]\nA regression tree determines the best predictors(s) to “split” the data, making smaller and smaller subsets of the data. The goal is to make the subsets within each split have about the same outcome value. The coefficients for each logical group are the average of the delivery times for each subset defined by the statement with the \\(I(\\cdot)\\) terms. Note that these logical statements, called rules, are mutually exclusive; only one of the four subsets will be true for any data point being predicted.\nLike the linear regression model that produced Equation 1.1, the performance of Equation 1.2 is quite poor. There are only four possible predicted delivery times. This particular model is not known for its predictive ability, but it can be highly predictive when combined with other regression trees into an ensemble model.\nThis book is focused on the practice of applying various ML models to data to make accurate predictions. Before proceeding, we’ll discuss different types of the data used to estimate models and then explore some aspects of ML models, particularly neural networks.\nThe columns shown in Table 1.1 are defined within a small scope. For example, the delivery hour column encapsulates everything about that order characteristic:\nNon-tabular data does not have the same properties. Consider images, such as the one shown Figure 1.1 that depicts four cells from a biological experiment (Yu et al. 2007). These cells that have been stained with pigments that fluoresce at different wavelengths to “paint” different parts of the cells. In this image, blue reflects the part of the cell called the cytoskeleton, which encompasses the entire cell body. The red color corresponds to a stain that only attaches itself to the contents of the cell nucleus, specifically DNA. This image3 is 371 pixels by 341 pixels with quantification of two colors.\nFigure 1.1: An example image of several cells using two colors. Image from Yu et al. (2007).\nFrom a data perspective, this image is represented as a three-dimensional array (371 rows, 341 columns, and 2 colors). Probably the most important attribute of this data is the spatial relationships between pixels. It is critical to know where each value in the 3D array resides, but knowing what the nearby values are is at least as important.\nWe could “flatten” the array into 371 \\(\\times\\) 341 \\(\\times\\) 2 = 253,022 columns to use in a table4. However, this would break the spatial link between the data points; each column is no longer as self-contained as the columns from the delivery data. The result is that we have to consider the 3D array as the data instead of each row/column/color combination.\nAdditionally, images often have different dimensions which results in different array sizes. From a tabular data perspective, a flattened version of such data would have a different number of columns for different-sized images.\nVideos are an extension of image data if we were to consider it a four-dimensional array (with time as the additional dimension). The temporal aspects of such data are critical to understanding and predicting values.\nText data are often thought of as non-tabular data. As an example, consider text from Sanderson (2017):\nWith text data, it is common to define the “token”: the unit of text that should be analyzed. This could be a paragraph, sentence, word, etc. Suppose that we used sentences as tokens. In this case, the table for this quote would have four rows. Where do we go from here? The sequence of words (or characters) is likely to be important, and this makes a case for keeping the sentences as strings. However, as will be seen shortly, we might be able to convert these four tokens to numeric columns in a way that preserves the information required for prediction.\nPerhaps a better text example relates to the structure of chemicals. A chemical is a three-dimensional molecule but is often described using a SMILES string (Engel and Gasteiger 2018, chap. 3). This is a textual description of molecule that lists its elements and how they relate to one another. For example, the SMILES string CC(C)CC1=CC=C(C=C1)C(C)C(=O)O defines the anti-inflammatory drug Ibuprofen. The letters describe the elements (C is carbon, O is oxygen, etc) and the other characters defines the bonds between the elements. The character sequence within this string is critical to understanding the data.\nOur motives for categorizing data as tabular and non-tabular are related to how we choose an appropriate model. Our opinion is that most modeling projects involve tabular data (or data that can be effectively represented as tabular). Models for non-tabular data are very specialized and, while these ML approaches are the most discussed in the social media, they tend not to be the best approach for tabular data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular",
    "href": "chapters/introduction.html#sec-nontabular",
    "title": "1  Introduction",
    "section": "",
    "text": "You don’t need any other columns to define it.\nA single number is all that is required to describe that information.\n\n\n\n\n\n\n\n\n\n“The most important step a man can take. It’s not the first one, is it? It’s the next one. Always the next step, Dalinar.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular-convert",
    "href": "chapters/introduction.html#sec-nontabular-convert",
    "title": "1  Introduction",
    "section": "1.2 Converting Non-Tabular Data to Tabular",
    "text": "1.2 Converting Non-Tabular Data to Tabular\nIn some situations, non-tabular data can be effectively converted to a tabular format, depending on the specific problem and data.\nFor some modeling projects using images, we might not be interested in every pixel. For Figure 1.1, we really care about the cells in the image. It is important to understand within-cell characteristics (e.g. shape or size) and some between-cell information (e.g., the distance between cells). The vast majority of the pixels don’t need to be analyzed.\nWe can pre-analyze the 3D arrays to determine which pixels in an image correspond to specific cells (or nuclei within a cell). Segmentation (Holmes and Huber 2018) is the process of estimating regions of an image that define some object(s). Figure 1.2 shows the same cells with lines generated from a simple segmentation.\n\n\n\n\n\n\n\n\nFigure 1.2: The cell segmentation results for the four neuroblastomas shown in Figure 1.1.\n\n\n\n\n\nOnce our cells have been segmented, we can compute various statistics of interest based on size, shape, or color. If a cell is our unit of data, we can create a tabular format where rows are cells and columns are cell characteristics (Table 1.2). The result is a data table that has more rows than the original non-tabular structure since there are multiple cells in an image. There are far fewer columns but these columns are designed to be more informative than the raw pixel data since the researchers define the cell characteristics that they desire.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\n\nEccentricity\n\n\n\nArea\n\n\n\nIntensity\n\n\n\nNucleus\nCell\nNucleus\nCell\nNucleus\nCell\n\n\n\n\n17\n0.494\n0.836\n\n3,352\n11,699\n\n0.274\n0.155\n\n\n18\n0.708\n0.550\n\n1,777\n4,980\n\n0.278\n0.210\n\n\n21\n0.495\n0.802\n\n1,274\n3,081\n\n0.326\n0.218\n\n\n22\n0.809\n0.975\n\n1,169\n3,933\n\n0.583\n0.229\n\n\n\n\n\n\n\n\nTable 1.2: Cells, such as those found in Figure 1.1, translated into a tabular format using three features for the nuclear and non-nuclear regions of the segmented cells.\n\n\n\n\nThis conversion to tabular data isn’t appropriate for all image analysis problems. If you want to know if an image contains a cat or not it probably isn’t a good idea.\nLet’s also consider the text examples. For the book quote, Table 1.3 shows a few simple predictors that are tabular in nature:\n\nPresence/absence columns for each word in the entire document. Table 1.3 shows columns corresponding to the words “always”, “can”, and “take”. These are represented as counts.\nSentence summaries, such as the number of commas, words, characters, first-person speech, etc.\n\nThere are many more ways to represent these data. More complex numeric embeddings that are built on separate large databases can reflect different parts of speech, text sequences (called n-grams), and others. Hvitfeldt and Silge (2021) and Boykis (2023) describe these and other tools for analyzing text data (in either format).\n\n\n\n\n\n\n\n\n\n\n\nSentence\n\"always\"\n\"can\"\n...\n\"take\"\nCharacters\nCommas\n\n\n\n\nThe most important step a man can take.\n0\n1\n...\n1\n32\n0\n\n\nIt's not the first one, is it?\n0\n0\n...\n0\n24\n1\n\n\nIt's the next one.\n0\n0\n...\n0\n15\n0\n\n\nAlways the next step, Dalinar.\n1\n0\n...\n0\n26\n1\n\n\n\n\n\n\n\n\nTable 1.3: An example of converting non-numeric data to a numeric, tabular format.\n\n\n\n\nFor the chemical structure of Ibuprofen, there is a rich field of molecular descriptors (Leach and Gillet 2003, chap. 3; Engel and Gasteiger 2018) that can be used to produce thousands of informative predictor columns. For example, we might be interested in the size of the molecule (perhaps measured by surface area), its electrical charge, or whether it contains specific sub-structures.\nThe process of determining the best data representations is called feature engineering. This is a critical task in machine learning and is often overlooked. Part 2 of this book spends some time looking at feature engineering methods (once the data are in a tabular format).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-types",
    "href": "chapters/introduction.html#sec-model-types",
    "title": "1  Introduction",
    "section": "1.3 Models and Machine Learning",
    "text": "1.3 Models and Machine Learning\nSince we have mentioned linear regression and regression trees, it makes sense to talk about possible machine learning scenarios. There are many, but we will focus on two types.\nRegression models have a numeric outcome (e.g., delivery time) and our goal is to predict that value. For some data sets we are mostly interested in ranking new data (as opposed to accurately predicting the value).\nThe other scenario that we discuss is classification models. In this case, the outcome is a qualitative categorical value (e.g., “cat” or “no cat” in an image). One interesting aspect of these models is that there are two main types of predictions:\n\nhard predictions correspond to the original outcome value.\nsoft predictions return the probability of each possible outcome (e.g., there is a 27% chance that a cat is in the image).\n\nThe number of categories is important; some models are specific to two classes, while others can accommodate any number of outcome categories. Most classification outcomes have mutually exclusive classes (one of all possible class values). There are also ordered class values where, while categorical, there is some type of ordering. For example, tumors are often classified in stages, with stage I being a localized malignancy and stage IV corresponding to one that has spread throughout the body.\nMulti-label outcomes can have multiple classes per row. For example, if we were trying to predict which languages that someone can speak, multilingual people would have multiple outcome values.\nWe are focused on prediction problems. This implies that our data will contain an outcome. Such scenarios falls under the broader class of supervised models. An unsupervised model is one where the is no true outcome value. Tools such as clustering, principal component analysis (PCA), and others look to quantify patterns in the data without some prediction goal. We’ll encounter unsupervised methods in our pre-model activities such as feature engineering described in Part 2.\nSeveral synonyms for machine learning include statistical learning, predictive modeling, pattern recognition, and others. While many specific models are designed for machine learning, we suggest that users think about machine learning as the problem and not the solution. Any model that can adequately predict the outcome values deserves the title. A commonly used argument for this position is that two models that are conventionally thought of as tools to make inferences (e.g., p-values) are linear regression and logistic regression. Equation 1.1 is a simplistic linear regression model. As will be seen in future chapters, linear and logistic regression models can produce highly accurate predictions for complex problems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-data-types",
    "href": "chapters/introduction.html#sec-data-types",
    "title": "1  Introduction",
    "section": "1.4 Data Characteristics",
    "text": "1.4 Data Characteristics\nIt’s helpful to discuss different nomenclature and types of data. With tabular data, there are a number of ways to refer to a row. The terms sample, instance, observation, and/or data point are commonly used. The first, “sample”, can be used in a few different contexts; we will also use it to describe a subset of a greater data collection (as in “we took a random sample of the data”.). The notation for the number of rows in a data set is \\(n\\) with subscripts for specificity (e.g., \\(n_{tr}\\)).\nFor predictors, there are also many synonyms. In statistical nomenclature, independent variable or covariate are used. More generic terms are attribute and descriptor. The term feature is generally equated to predictors, and we’ll use both. An additional adjective may also be assigned to “feature”. Consider the date and time of the orders shown in Table 1.1. The “original feature” is the data/time column in the data. The table includes two “derived features” in the form of the decimal hour and the day of the week. This distinction will be important when discussing feature engineering, importance scores, and explaining models. The number of overall predictors in a data set is symbolized with \\(p\\).\nOutcomes/target columns can also be called the dependent variable or response.\nFor numeric columns, we will make some distinctions. A real number is numeric with potentially fractional values (e.g., time to delivery). Integers are whole numbers, often reflecting counts. Binary data take two possible values that are almost always represented with zero and one.\nDense data describes a collection of numeric values with many possible values, also reflected by the delivery time and distances in Table 1.1. Sparse data have fewer possible values or when only a few values are contained in the observed data. The product counts in the delivery data are good examples. They are integer counts consisting mostly of zeros. The frequency of non-zero values decreases with the magnitiude of the counts.\nWhile numeric data are quantitative, qualitative data cannot be represented by a numeric scale5. The day of the week data are categories with seven possible values. Binary categorical data have two categories, such as alive/dead. The symbol \\(C\\) is used to describe the number of categories in a column.\nThe item columns in Table 1.1 are interesting too. If the order could only contain one item, we might configure that data with a qualitative single column. However, these data are multiple-choice and, as such, are shown in multiple integer columns with zero, reflecting that it was not in the order.\nvalue types, skewness, colinearity, distributions, ordinal",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-pipeline",
    "href": "chapters/introduction.html#sec-model-pipeline",
    "title": "1  Introduction",
    "section": "1.5 What Defines the Model?",
    "text": "1.5 What Defines the Model?\nWhen discussing the modeling process, it is traditional to think the parameter estimation steps occur only when the model is fit. However, as more complex techniques are used to prepare predictor values for the model, it is crucial to understand that important quantities are also being estimated before the model. Here are a few examples:\n\nImputation methods, discussed in Chapter 46, create sub-models that estimate missing predictor values. These are applied to the data before modeling to complete the data set.\nFeature selection tools (Kuhn and Johnson 2019) often compute measures of predictor importance to remove uninformative predictors before passing the data to the model.\nEffect encoding tools, discussed in Section 6.4.3, use the effect of the predictor on the outcome as a predictor.\nPost-model adjustments to predictions, such as model calibration (Section 2.6 and ?sec-cls-calibration), are examples of postprocessing operations.\n\n\nThese operations can profoundly affect the overall results, yet they are not usually considered part of “the model.” We’ll use the more expansive term “modeling pipeline” to describe any estimation for the model or operations before or after the model:\n\nmodel pipeline = preprocessing + supervised model + postprocessing\n\n\nThis is important to understand: a common pitfall in ML is only validating the model fitting step instead of the whole process. This can lead to performance statistics that can be extraordinarily optimistic and might deceive a practitioner into believing that their model is much better than it truly is.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-deep-learning",
    "href": "chapters/introduction.html#sec-deep-learning",
    "title": "1  Introduction",
    "section": "1.6 Deep Learning",
    "text": "1.6 Deep Learning\nWhile many models can be used for machine learning, neural networks are the elephant in the room. These complex, nonlinear models are frequently used in machine learning to the point where, in some domains, these models are synonymous with ML. One class of neural network models is deep learning (DL) models (Goodfellow, Bengio, and Courville 2016; Prince 2023). Here, “deep” means that the models have many structural layers, resulting in extraordinary amounts of complexity.\nFor example, Figure 1.3 shows a diagram of a type of deep learning model called a convolutional neural network that is often used for making predictions on images. It consists of several layers that manipulate the data. This particular model, dubbed VGG16, was proposed by Simonyan and Zisserman (2014).\nThe color image on the left is a data point. The first block has three layers. The two larger yellow boxes are convolutional layers. They move a small multidimensional array over the larger data array to compute localized features. For example, for an image that is 150 x 150 pixels, a convolution that uses a 3x3 filter will move this small rectangle across the array. At the (2,2) position, it takes the average of all adjacent pixel locations.\nAfter the convolutional operations, a smaller pooling layer is shown in orange. Pooling, usually by finding the maximum feature value across different image regions, compresses/downsamples the larger feature set so a smaller size without diminishing features with large signals.\nThe next three blocks consist of similar convolutional and pooling layers but of different dimensions. The idea behind these blocks is to reduce the image data to a smaller dimension width/height dimension in a way that extracts potentially important hierarchical features from the image.\nAfter block 5, the data are flattened into a single array, similar to the columns in a tabular data set. Up to this point, the network has been focused on preprocessing the image data. If our initial image were 150 pixels by 150 pixels, the flattening process would result in over 8 thousand predictors that would be used in a supervised ML model. The network would have over 14 million model parameters to estimate from the data to get to this point.\nAfter flattening, the final two layers correspond to the classic supervised neural network model (Bishop 1995). The original paper used four large supervised neural network layers; for simplicity, Figure 1.3 shows a single dense layer. This can add dozens of millions of additional parameters for a modest-sized dense layer.\n\n\n\n\n\n\n\n\nFigure 1.3: A simplified version of the convolutional neural network deep learning model proposed by Simonyan and Zisserman (2014). This version only includes a single dense layer (instead of four).\n\n\n\n\n\nThere are many types of deep learning models. Another example is recurrent neural networks, which create layers that are proficient at modeling data that occur in sequences such as time series data or sentences (i.e., a sequence of words).\nVery sophisticated deep learning models have generated, by far, the best predictive ability for images and similar data. It would seem natural to think these models would be just as effective for tabular data. However, this has not been the case. Machine learning competitions, which are very different from day-to-day machine learning work, have shown that many other models can consistently do as well or likely better on tabular data sets. This has generated an explosion of literature7 and social media posts that try to explain why this is the case. For example, see Borisov et al. (2022) and McElfresh et al. (2023).\nWe believe that exceptionally complex DL models are handicapped in several ways when taken out of the environments where they have been successful.\nFirst, the complexity of these models requires extreme amounts of data, and the constraints of such large data sets drive how the models are developed.\nFor example, keeping very large data sets in a computer’s memory is almost impossible. As discussed earlier, we use the data to find optimal values of our parameters for some objective functions (such as classification accuracy). Using traditional optimization procedures, this is very difficult if the data cannot be accessed simultaneously. Deep learning models use more modern optimization methods, such as stochastic gradient descent (SGD) methods, for estimating parameters. It does not simultaneously hold all of the data in memory and fits the model on small incremental batches of data. This is a very effective technique, but it is not an approach one would use with data set sizes that are more common (e.g., less than a million data points). SGD is less effective for smaller data sizes than traditional in-memory optimizers. Another consequence of huge data requirements is that model training can take an excruciatingly long time. As a result, efficient model development can require specialized hardware. It can also drive how models are optimized and compared. DL models are often optimized by sequentially adjusting the model. Other ML models can be optimized with a more methodical simultaneous design that is often more efficient, systematic, and effective than sequential optimizers used with deep learning.\nAnother issue with deep learning models is that their size and mathematical structure are associated with a very difficult optimization problem; their objective function is non-convex. We’d like to have an optimization problem that has some guarantees that a global optimal value exists (and can be found). That is often not the case for these models. For example, if we want to estimate the VGG16 model parameters with the goal of maximizing the classification accuracy for finding dogs in an image, it can be difficult to get a reliable estimate and, if we do, we don’t know if it is really the correct answer.\nBecause the optimization problem is so difficult, much of the user’s time is spent optimizing the model parameters or tuning the network structure to find the best results. When combined with how long it takes for a single model fit, deep learning models are very high maintenance compared to other models when used on tabular data. They can probably produce a model that is marginally better than the others, but doing so requires an inordinate amount of time, effort, and constraints. DL models are superior for certain hard problems with large data sets. For other situations, other ML models can go probably go further and faster8.\nFinally, there are some data-centric reasons that the successfulness of deep learning models doesn’t automatically translate to tabular data. There are characteristics of tabular data that don’t often occur in stereotypical DL applications. For example:\n\nIn some cases, a group of predictors might be highly correlated with one another. This can compromise some of the mathematical operations used to estimate parameters in neural networks (and many other models).\nUnless we have extensive prior experience with our data, we don’t know which are informative and which are irrelevant for predicting an outcome. As the number of non-informative predictors increases, the performance of neural networks decreases. See Section 19.1 of Kuhn and Johnson (2013).\nPredictors with “irregular” distributions, such as skewness or heavy tails, can harm performance (McElfresh et al. 2023).\nMissing predictor data are not naturally handled in basic neural networks.\n“Small” sample sizes or data dimensions with at least as many predictors as rows.\n\nas well as others. These issues can be overcome by adding layers to a network designed to counter specific challenges. However, in the end, this ends up adding more complexity. We propose that there are a multitude of other models, and many of them are resistant or robust to these types of data-specific characteristics. It is better to have different tools in our toolbox; making a more complex hammer may not help us put a screw in a wall.\nThe deep learning literature has resulted in many collateral benefits for the broader machine learning field. We’ll describe a few techniques that have improved machine learning in subsequent chapters,",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-aspects",
    "href": "chapters/introduction.html#sec-aspects",
    "title": "1  Introduction",
    "section": "1.7 Modeling Philosophies",
    "text": "1.7 Modeling Philosophies\ntalk about subject-matter knowledge/intuition vs “let the machine figure it out”. biased versus unbiased model development.\nassistive vs automated usage",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-outline",
    "href": "chapters/introduction.html#sec-outline",
    "title": "1  Introduction",
    "section": "1.8 Outline of These Materials",
    "text": "1.8 Outline of These Materials\nThis work is organized into parts:\n\nIntroduction: The next chapter shows an abbreviated example to illustrate important concepts used later.\nPreparation: These chapters discuss topics such as data splitting and feature engineering. These activities occur before the actual training of the machine learning model but are critically important.\nOptimization: To find the model that has the best performance, we often need to tune them so that they are effective but do not overfit. These chapters describe overfitting, methods to measure performance, and two different classes of optimization methods.\nClassification: Various models for classification are described in this part.\nRegression: These chapter describe models for numeric outcomes.\nCharacterization: Once you have a model, how do you describe it or its predictions? How do you know when you should question the results of your model? We’ll address these questions in this part.\nFinalization: Post-modeling activities such as monitoring performance are discussed.\n\nBefore diving into the process of creating a good model, let’s have a short case study. The next chapter is designed to give you a sense of the overall process and illustrate important ideas and concepts that will appear later in the materials.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#chapter-references",
    "href": "chapters/introduction.html#chapter-references",
    "title": "1  Introduction",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nBishop, C. 1995. Neural Networks for Pattern Recognition. Oxford: Oxford University Press.\n\n\nBorisov, V, T Leemann, K Seßler, J Haug, M Pawelczyk, and G Kasneci. 2022. “Deep Neural Networks and Tabular Data: A Survey.” IEEE Transactions on Neural Networks and Learning Systems.\n\n\nBoykis, V. 2023. “What are embeddings?” https://doi.org/10.5281/zenodo.8015029.\n\n\nEngel, T, and J Gasteiger. 2018. Chemoinformatics : A Textbook . Weinheim: Wiley-VCH.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nHolmes, S, and W Huber. 2018. Modern Statistics for Modern Biology. Cambridge University Press.\n\n\nHvitfeldt, E, and J Silge. 2021. Supervised Machine Learning for Text Analysis in R. CRC Press.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nLeach, A, and V Gillet. 2003. An Introduction to Chemoinformatics. Springer.\n\n\nMcElfresh, D, S Khandagale, J Valverde, V Prasad, G Ramakrishnan, M Goldblum, and C White. 2023. “When Do Neural Nets Outperform Boosted Trees on Tabular Data?” arXiv.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.\n\n\nSanderson, B. 2017. Oathbringer. Tor Books.\n\n\nSimonyan, K, and A Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv.\n\n\nYu, W, HK Lee, S Hariharan, WY Bu, and S Ahmed. 2007. “CCDB:6843, Mus Musculus, Neuroblastoma.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Non-tabular data are later in this chapter.↩︎\nThis is a linear regression model.↩︎\nThis is a smaller version of a larger image that is 1392 pixels by 1040 pixels.↩︎\nThe original image would produce 2,895,360 flattened features.↩︎\nAlso known as “discrete” or “nominal” data.↩︎\nA note to readers: these ?sec-* references are to sections or chapters that are planned by not (entirely) written yet. They will resolve to actual references as we add more content. ↩︎\nTo be honest, the literature in this area has been fairly poor (as of this writing).↩︎\nThat said, simple neural network models can be very effective. While still a bit more high maintenance than other models, it’s a good idea to give them a try (as we will see in the following chapter).↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html",
    "href": "chapters/whole-game.html",
    "title": "2  The Whole Game",
    "section": "",
    "text": "2.1 Predicting Delivery Time\nMany resources for teaching machine learning gloss over some crucial details that might appear to be tangential to the model. As a result, they omit parts that are critical to the overall project.\nThe notion of “the whole game” is to illustrate machine learning in a way that reflects the complexity of the entire process. Perkins (2010) uses this phrase in the context of learning baseball. Instead of focusing players on one or two key aspects of the game, it is better to be exposed (at some level) to everything. In his words, a smaller focus “was kind of like batting practice without knowing the whole game.”\nThis chapter is a self-contained case study.\nOur goal is to provide a somewhat abbreviated, high-level overview to help readers understand the overall strategy before we get into the specific set of tactics that we might use. This chapter previews what is to come and sets the stage to discuss some important themes that may not be obvious at first glance.\nWe’ll avoid many details in this chapter by copiously referencing upcoming chapters.\nThe illustrative example focuses on predicting the food delivery time (i.e., the time from the initial order to receiving the food). There is a collection of data containing 10,012 orders from a specific restaurant. The predictors, previously shown in Table 1.1, include:\nThe outcome is the time1 (in minutes).\nThe next section describes the process of splitting the overall data pool into different subsets and signifies the start of the modeling process. However, how we make that split depends on some aspects of the data. Specifically, it is a good idea to examine the distribution of our outcome data (i.e., delivery time) to determine how to randomly split the data2.\nFigure 2.1 (panel (a)) uses a histogram to examine the distribution of the outcome data. It is right-skewed and there is a subset of long delivery times that stand out from the mainstream of the data. The most likely delivery time is about 27 minutes, but there is also a hint of a second mode a few minutes earlier.\nFigure 2.1: Histrograms of the training set outcome data with and without a transformation.\nThis skewness might affect how we split the data and is discussed in the next section. Another important question is: should we model a transformed version of the delivery times? If we have large outlying values, the model might be inappropriately pulled to these values, often at the expense of the overall quality of fit. One approach is to model the logarithm of the times (as shown in Equation 1.1). Figure 2.1 (panel (b)) shows the distribution of the log (base 2) deliveries. It is more symmetric, and the potential second mode is more pronounced. This isn’t a bad idea, and the primary consequence is how we interpret some of the metrics that are used to quantify how well the model fits the data3. In the end, we decided to model the data in the original units but did try the analysis both ways (and did not find huge differences in the results).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-delivery-times",
    "href": "chapters/whole-game.html#sec-delivery-times",
    "title": "2  The Whole Game",
    "section": "",
    "text": "The time, in decimal hours, of the order.\nThe day of the week for the order.\nThe approximate distance between the restaurant and the delivery location.\nA set of 27 predictors that count the number of distinct menu items in the order.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-data-spending-whole-game",
    "href": "chapters/whole-game.html#sec-data-spending-whole-game",
    "title": "2  The Whole Game",
    "section": "2.2 Data Spending",
    "text": "2.2 Data Spending\nAn important aspect of machine learning is to use the right data at the right time. We don’t start modeling with the entire pool of data. As will be seen in Chapter 10, we need to reserve some data to evaluate the model and make sure that those data do not overlap with those used to fit the model.\nOften, our data spending strategy is to split the data into at least two subsets:\n\nThe training set is used to develop the model pipeline. It is used for investigating, fitting, and comparing them.\nThe test set is a smaller fraction of the data that is only used at the very end to independently validate how well we did.\n\nThe training set is used for a lot of our activities. We do need to evaluate multiple model pipelines along the way. We reserve the test set for the final assessment and should not use it for development. There are two main strategies for characterizing model efficacy during development.\nThe first is to resample the training set. This is an iterative computational tool to find good statistical estimates of model performance using the training set.\nThe other option is to initially split the data into three subsets: the training set, the testing set, and the validation set. Validation sets are small subset used for evaluating the model repeatedly during development.\nGenerally, we suggest using resampling unless the initial data pool is “sufficiently large.” For these data, 10,012 food orders is probably sufficient to choose a validation set over the resampling approach.\nTo split the data, we’ll allocate 60% for the training set and then use 20% for the validation set, and the remaining 20% for testing. The splitting will be conducted by stratification. To ensure that our delivery time distributions are approximately the same, it is temporarily “binned” into four quartiles. The three-way split is executed within each quartiles and the overall training/validation/testing sets are assembled by aggregating the corresponding data from the quartiles. As a result, the respective sample sizes are 6004/2004/2004 deliveries.\nAs previously stated, the majority of our time is spent with the training set samples. The first step in any model-building process is to understand the data, which can most easily be done through visualizations of the training set.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-eda-whole-game",
    "href": "chapters/whole-game.html#sec-eda-whole-game",
    "title": "2  The Whole Game",
    "section": "2.3 Exploratory Data Analysis",
    "text": "2.3 Exploratory Data Analysis\nOur goal now is to determine if there are aspects of predictors that would affect how we provide them to the model. We should look at individual variables as well as combinations multiple variables.\nTo start, for the dense numeric predictors, how would we characterize their relationship to the outcome? Is it linear, nonlinear, or random? Let’s start with the distance predictor. Figure 2.2(a) shows the values with the “scatter plot smoother” fit. The line is a flexible nonlinear approach that can adapt to any apparent pattern in the plot4. First, the plot shows that the distance distribution is also right-skewed (although a separate histogram would better illustrate this). The smoother shows a slightly nonlinear trend, although the nonlinearity is mainly in the right tail of the distances and might be an artifact of the small sample size in that region. Regardless, it appears to be an informative predictor.\n\n\n\n\n\n\n\n\nFigure 2.2: Visualizations of relationship between the outcome and several predictors using the training set.\n\n\n\n\n\nPanel (b) has a similar visualization for the order time. In this case, there is another nonlinear trend where the order increases with the hour, then peaks around 17:30, then begins to decrease. There is also an increase in delivery time variation as the order time approaches the peak.\nThe day of the week is visualized via box plots in panel (c). There is an increasing trend in both mean and variation as we move from Monday to Friday/Saturday, then a decrease on Sunday. Again, this appears to be an important predictor.\nThese three panels show relationships between individual predictors and delivery times. It is possible that multiple predictors can act in concert; their effect on the outcome occurs jointly. Panel (d) of Figure 2.2 shows that there is an interaction effect between the day and hour of the order. Mondays and Tuesdays have shorter delivery times that don’t change much over hours. Conversely, on Fridays and Saturdays, the delivery times are generally longer, with much higher peaks. This might explain the increase in delivery time variation that was seen in Panel (b). Now that we know of the existence of this interaction, we might add additional model terms to help the model understand and exploit this pattern in the training set.\nFor the 27 itemized frequency counts, we might want to know if there was a change in delivery time when a specific item was in the order (relative to those where it is not purchased). To do this, mean delivery times were computed for orders with and without the item. The ratio of these times was computed and then converted to the percent increase in time. This is used to quantify the effect of item on delivery time. We’d also like to know what the variation is on this statistic and a technique called the bootstrap (Davison and Hinkley 1997) was used to create 90% confidence intervals for the ratio. Figure 2.3 shows the results.\n\n\n\n\n\n\n\n\nFigure 2.3: Ranking the items by the increase in delivery time when ordered at least once.\n\n\n\n\n\nThere is a subset of items that increase the delivery time (relative to the experimental noise), many that seem irrelevant, and one that decreases the time.\nMore investigations into the data would be pursued to better understand the data and help us with feature engineering. For brevity, we’ll stop here and introduce three ML models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-development-whole-game",
    "href": "chapters/whole-game.html#sec-model-development-whole-game",
    "title": "2  The Whole Game",
    "section": "2.4 Model Development",
    "text": "2.4 Model Development\nWhich model to use? There is a multitude of tools that can be used to represent and predict these data. We’ll illustrate three in this section and the essential concepts to consider along the way.\nBefore proceeding, we need to pick a performance metric. This is used to quantify how well the model fits the data. For any model fit, the residual (usually denoted as \\(\\epsilon\\), also called the error) is the difference between the model prediction and the observed outcome. We’d like our model to produce residuals near zero. One metric5 is the mean absolute error (MAE). For a data set with \\(n\\) data points, the equation is\n\\[\nMAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| = \\frac{1}{n}\\sum_{i=1}^n |\\epsilon_i|\n\\tag{2.1}\\]\nwhere \\(\\hat{y}\\) is the predicted numeric outcome value and \\(\\epsilon\\) is the model error. The MAE units are the same as the outcome which, in this case, is minutes. We desire to minimize this statistic.\nTo get started, we’ll use a basic linear model for these data.\n\nLinear Regression\nLinear regression is probably the most well-known statistical model in history. It can predict a numeric outcome using one or more predictors using the equation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\ldots + \\beta_px_{ip} + \\epsilon_i\n\\tag{2.2}\\]\nwhere \\(y_i\\) is the outcome for the \\(i^{th}\\) data point (out of \\(n_{tr}\\)), and \\(x_{ij}\\) is value for the \\(j^{th}\\) predictor (out of \\(p\\)).\nFor Equation 2.2, there are many ways to estimate the model parameters (\\(\\beta\\)’s). Ordinary least squares is used most often. It finds the values of the \\(\\beta_j\\) that that minimize sum of squared errors (SSE). ?sec-reg-linear discusses several other ways to estimate the model parameters that use other criteria (variations of the SSE).\nThe main assumptions about the data for ordinary least squares are that they are independent and identically distributed. Given what we know about the problem, both assumptions are at least mildly incorrect. Orders are being processed simultaneously (affecting independence), and our visualizations have indicated that there is an increase in variation with the mean (i.e., not identically distributed). However, we are most interested in prediction here, so we can often overlook slight to moderate violations of these assumptions6\nWhat are our \\(x_{ij}\\) values in Equation 2.2? We can’t give the model equations values of \"Monday\" or \"Friday\"; OLS requires numbers in its equations. A simple workaround is to create binary indicator variables for six of the seven days of the week. For example, the indicator for Friday would have a value of one for orders placed on a Friday and zero otherwise7.\nAlso, for Equation 2.2, what do we do about the numeric predictors that appear to have nonlinear trends? We can augment the predictor by describing it with multiple columns. Equation 2.2 shows that linear regression is linear in the parameters (not the predictors). We can add terms such as \\(x_{ij} = distance_i^2\\) to allow the fit to be nonlinear. Alternatively, an effective tool for enabling nonlinear trends is spline functions8. These are special polynomial terms. For example, we could represent that original predictor with 10 spline terms (i.e., columns in the data) for the order hour. As we add more terms, the model fit has increased capacity to be nonlinear. We’ll use 10 separate splines for both distance and the order hour (in lieu of the original data columns.)\nHow do we encode the interaction seen in Figure 2.2(d)? We have six indicator columns for the order day and ten spline columns for the order hour. We can facilitate having different nonlinear hour trends for each day by making 60 cross-product terms. For example, the interactions for Fridays would multiply each of the ten spline terms by the binary indicator associated with Fridays.\nThese feature engineering operations result in a model with \\(p = 114\\) \\(\\beta\\) parameters. We fit this model to the training set to estimate the model parameters. To evaluate it, we use the trained model to predict the validation set, and these predictions are the substrate to compute the MAE. For this model, the validation set MAE is 1.609 (i.e., 1 minute and 36 seconds). If we think of MAE as an average error, this doesn’t seem too bad. However, what does this look like? Let’s plot the observed times against predicted values to get a better sense of how well this model did. Figure 2.4 has the results which also feature a scatter plot smoother.\n\n\n\n\n\n\n\n\nFigure 2.4: A scatter plot of the observed and predicted delivery times, via a linear regression model, for the validation set. The red line is a smooth trend estimate.\n\n\n\n\n\nThe best case would be that the points arrange themselves tightly around the diagonal line. The model slightly under-predicts for very rapid deliveries but substantially under-predicts for times greater than 40 minutes. However, overall, the model works well for the majority of the deliveries.\nFrom here, our next step would be to investigate poorly predicted samples and determine if there is some commonality to them. For example, are they short distances that are ordered late on a Friday, etc? If there appears to be a pattern, we would add additional model terms to compensate for the flaw and see if the validation set MAE decreases. We would iterate between exploratory analysis of the residuals, adding or subtracting features, then refitting the model.\nFor illustration, we stop here9 and take a look at very different model.\n\n\nRule-Based Ensemble\nCubist is a rule-based ensemble method. It creates a rule set by first generating a regression tree that is composed of a hierarchical set of if/then statements10. Figure 2.5 shows a simple tree for our data. A set of splits on different predictors results in six terminal nodes (at the bottom of the tree). A rule is a distinct path through the tree to a terminal node. For example, Node 11 in the figure has the corresponding rule: hour &gt;= 14.77 & day in {Fri, Sat} & distance &gt;= 4.045. For each rule, Cubist fits a linear regression to the data that the rule covers (\\(n = 261\\) for Node 11). It builds new rule sets sequentially and uses many rules/regression equations to predict new samples.\n\n\n\n\n\n\n\n\nFigure 2.5: An example of a regression tree used to create a set of six rules.\n\n\n\n\n\nThe method isolates different parts of the predictor space and, with these subsets, models the data using linear functions. The creation of multiple rules enables the model to emulate nonlinear functions easily. When a rule consists of multiple predictors, this constitutes an interaction effect. Additionally, the data preprocessing requirements are minimal: it can internally account for any missing predictor values, and there is no need to convert categorical predictors into binary indicator columns, and so on.\nWhen fit to the training set, the Cubist model created an ensemble with 105,327 rules. On average, the rules consisted of 2.7 variables and the the regression models contained 12.7 predictors. Two example rules, and their regression models, were:\n\nif\n   hour &lt;= 14.252\n   day in {Fri, Sat}\nthen\n   outcome = -23.039 + 2.85 hour + 1.25 distance + 0.4 item_24\n             + 0.4 item_08 + 0.6 item_01 + 0.6 item_10 + 0.5 item_21\n             + 0.3 item_09 + 0.4 item_23 + 0.2 item_03 + 0.2 item_06\n\nand\n\nif\n   hour &gt; 15.828\n   hour &lt;= 18.395\n   distance &lt;= 4.35\n   item_10 &gt; 0\n   day = Thu\nthen\n   outcome = 11.29956 + 4.24 distance + 14.3 item_01 + 4.3 item_10\n             + 2.1 item_02 + 0.03 hour\n\nThese two examples are reasonable and understandable. However, Cubist blends many rules together to the points where is becomes a black-box model. For example, the first data point in the validation set is affected by 5,916 rules (about 6% of the total ensemble).\nDoes this complexity help? The validation set estimated MAE at 1 minute and 24 seconds, which is an improvement over the linear regression model. Figure 2.6 visualizes the observed and predicted plot. The model under-predicts fewer points with long delivery times than the previous model but does contain a few very large residuals.\n\n\n\n\n\n\n\n\nFigure 2.6: A scatter plot of the observed and predicted delivery times from the Cubist model, for the validation set.\n\n\n\n\n\nWould this model have benefited from using the feature set from the linear regression analysis? Not really. Refitting with those predictors had an MAE of 1 minute and 26 seconds. In this particular case, the original Cubist was able to estimate nonlinear trends and interactions without additional feature engineering.\nThere is more that we can do with the Cubist model but, in this initial model screening phase, we should focus our efforts on evaluating a disparate set of models and features to understand what works, what doesn’t work, and the difficulty level of the problem. Let’s try one additional model.\n\n\nNeural Network\nA neural network is a highly nonlinear modeling technique. They relate the predictors to the outcome through intermediate substructures, called layers, that contain hidden units. The hidden units allow the predictors to affect the outcomes differently11 allowing the model to isolate essential patterns in the data. As the number of hidden units (or layers of hidden units) increases, so does the complexity of the model. We’ll only consider a single layer network with multiple hidden units for these data.\nNeural networks, like linear regression, require numbers as inputs. We’ll convert the day of the week predictor to indicators. Additionally, the model requires that all of the predictors be in the same units. There are a few ways to accomplish this. First, we will compute the training set mean and standard deviation of each feature. To standardize them we subtract the mean and divide the result by the standard deviation. As a result, all features have a zero mean and a unit standard deviation. A common scaling of the predictor columns is a necessary precondition for fitting neural networks\nOne question about this model is: how many hidden units should we use12? This decision will determine the complexity of the model. As models become model complex, they have more capacity to find complicated relationships between the predictors and response. However, added complexity increases the risk of overfitting. This situation, discussed in Section 9.1, occurs when a model fits exceptionally well to the training data but fails for new data when it finds patterns in the training data that are artifacts (i.e., do not generalize for other data sets). How should we choose the number of hidden units?\nUnfortunately, no equation can directly compute an estimate the number of units from the data. Values such as these are referred to as tuning parameters or hyperparameters. However, one solution is to extend the validation process previously shown for the linear regression and Cubist models. We will consider a candidate set of tuning parameter values ranging from 3 to 100 hidden units. We’ll fit a model to the training set for each of these 98 values and use the validation set MAE values to select an appropriate value. The results of this process is shown in Figure 2.7.\n\n\n\n\n\n\n\n\nFigure 2.7: The relationship between the number of hidden units and the mean absolute deviation from the validation set data.\n\n\n\n\n\nA small number of hidden units performs poorly due to underfitting (i.e., insufficient complexity). Adding more improves the situation and, while the numerically best result corresponds to a value of 61, there is appreciable noise in the MAE values13 . Eventually, adding too much complexity will either result in the MAE values becoming larger due to overfitting or will drastically increase the time to fit each model. The MAE pattern in Figure 2.7 shows a plateau of values with a similar MAE once enought hidden units are added. Given the noise in the system, we’ll select a value of 19 hidden units, by determining the least complex model that is within 1% of the numerically best MAE. The MAE associated with the selected candidate value14 was 1 minute and 28 seconds (but is likely to be closer to 1 minute and 30 seconds, given the noise in Figure 2.7).\nFigure 2.8 shows the familiar plot of the observed and predicted delivery times. There are fewer large residuals than the Cubist model but the underfitting for longer delivery times appears more pronounced. There is also a hint of under-predicted times for fast deliveries.\n\n\n\n\n\n\n\n\nFigure 2.8: A scatter plot of the validation set observed and predicted delivery times from a neural network with 19 hidden units.\n\n\n\n\n\nThese results suggest that the Cubist and neural network models have roughly equivocal performance15. Like Cubist, the finalized model is very complex; it is a highly nonlinear regression with 18,304 model parameters (i.e., slopes and intercepts).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-selection-whole-game",
    "href": "chapters/whole-game.html#sec-model-selection-whole-game",
    "title": "2  The Whole Game",
    "section": "2.5 Choosing Between Model",
    "text": "2.5 Choosing Between Model\nThere are a variety of criteria used to rank and compare model fits. Obviously, performance is important. Also, it is good practice to choose a model that is as simplistic as possible (subject to having acceptable performance). There are other practical factors too. If a model is to be deployed, the size of the files(s) required to automate predictions can be a constraint. There may also be data constraints; which predictors are easy or inexpensive to obtain? Does the number of predictors affect how quickly predictions can be made?\nWhile prediction is the focus for ML models, we often have to explain the model and/or predicted values to consumers. While we can develop explainers for any model, some types are easier to explain than others.\nFor this chapter, we have three models whose MAE values are within about 12 seconds of one another. Each has the same deficiency: they all significantly underpredict long delivery times. In fact, a few poor predictions may be a limiting factor for any regression metric; it might be impossible for one model’s MAE to really stand out from the others.\nWe’ll select the linear regression model as our final model due to its relative simplicity and linear structure.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-calibration-whole-game",
    "href": "chapters/whole-game.html#sec-calibration-whole-game",
    "title": "2  The Whole Game",
    "section": "2.6 Calibration",
    "text": "2.6 Calibration\nThere is one option for remedying the systematic underprediction of long delivery times. ?sec-reg-calibration describes calibration methods that might be able to correct such issues. To do so, we use the held out predictions to estimate the average unwanted trend and remove it from new predictions. For the linear regression results, the estimated average trend is almost identical to the smooth line shown in Figure 2.4.\nFor our test set predictions, we’ll remove this trend and, hopefully, this will recenter the long predictions to be closer to the diagonal line.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-test-results-whole-game",
    "href": "chapters/whole-game.html#sec-test-results-whole-game",
    "title": "2  The Whole Game",
    "section": "2.7 Test Set Results",
    "text": "2.7 Test Set Results\nWe can predict the test set using the linear regression fit from the training set. Recall that the validation set MAE was 1 minute and 36 seconds. Coincidentally, without the additional calibration, the test set value was also 1 minute and 36 seconds. The similarity of these results gives us confidence that the performance statistics we estimated during development are consistent with the test set. The test set plot of the predictions is shown in Figure 2.9(a).\nThe calibration results are shown in panel (b). There does appear to be an improvement in long delivery times; these are closer to the diagonal line. The MAE value is slightly improved: 1 minute and 32 seconds. The small difference in these results and the similar MAE values across models indicates that the few large residuals are probably the gating factor for these data.\n\n\n\n\n\n\n\n\nFigure 2.9: Observed versus predicted test set delivery times for the final model. Results are shown for pre-calibrated and post-calibration.\n\n\n\n\n\nAt this point, we should spend a fair amount of time documenting the model and our development process. It might be helpful to employ inferential analyses to explain to interested parties which model components were most important (e.g., how much the interaction or nonlinear terms improve the model, etc.). It is very important to document where the model does not work well, its intended use, and the boundaries of the training set. Finally, if the model is deployed, we should also monitor its effectiveness over time and also understand if the sample population is changing over time too.\nThe next section accentuates important aspects of this case study.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-themes-whole-game",
    "href": "chapters/whole-game.html#sec-themes-whole-game",
    "title": "2  The Whole Game",
    "section": "2.8 Themes",
    "text": "2.8 Themes\nAt a high level, let’s review the process that we just used to create a model; Figure 2.10 highlights several themes16. The overall process involves initial data splitting, model development, then model selection, and validation. We’ll look at each of these in turn to discuss important points.\n\n\n\n\n\n\n\n\nFigure 2.10: A general high level view of the process of creating a predictive model. The diagram reflects which steps use the test set, validation set, or the entire training set. Figure 2.11 describes the operations labeled “model development”.\n\n\n\n\n\n\nData Spending\nAlthough discussed in Chapter 3, how we allocate data to specific tasks (e.g., model building, evaluating performance, etc.) is an essential aspect of modeling. In our analysis, a minority of the data were explicitly allocated to measure how well the model worked. The test set is meant to be unbiased since it was never used to develop the model. This philosophy of empirical validation is a crucial feature for judging how models function.\nThe majority of the data ends up in the training set. The core activities of creating, optimizing, and selecting are accomplished with these data.\nFigure 2.10 highlights how we use the right data at the right time. A lack of a data usage methodology (or an inappropriate one) can lead to having a false sense of the quality of the model.\nFor example, when the linear regression model was calibrated, we chose to use the held-out samples as the substrate for estimating the unwanted trend. We could have used the re-predicted training set data but, as will be discussed in Section 9.1, these values can be unrealistically optimistic. Also, we chose not to compute the MAE on the held-out predictions after they were used to estimate the calibration. This dual use of the same data would result in an MAE value that is smaller than it should be. There are other ways of estimating the utility of the calibration prior to the test set; these are discussed in ?sec-reg-calibration.\n\n\nInvestigate Many Options\nFor the delivery data, three different models were evaluated. It is our experience that some modeling practitioners have a favorite model that is relied on indiscriminately (boosted trees immediately come to mind). The “No Free Lunch” Theorem (Wolpert 1996) argues that, without substantive information about the modeling problem and data, no single model will always do better than any other model. Because of this, a strong case can be made to try various techniques and then determine which to focus on. In our example, simple data visualizations elucidated the relationships between the outcome and the predictors. Given this knowledge, we might exclude some models from consideration, but there is still a wide variety of techniques to evaluate.\nFor individual models, Figure 2.11 summarizes the cyclic process of speculation, configuration, and validation of model parameters and features. For these data, we discovered most of the important trends prior to our first model fit. As previously mentioned, we would cycle through a few rounds where we try to make improvments for each of the models.\n\n\n\n\n\n\n\n\nFigure 2.11: The cycle of developing a specific type of model. EDA standard for exploratory data analysis.\n\n\n\n\n\nFor novel data sets, it is a challenge to know a priori what good choices are for tuning parameters, feature engineering methods, and predictors. The modeler must iterate using choices informed by their experience and knowledge of the problem.\n\n\nPredictor Representations\nThis example revolves around a small set of predictors. In most situations, there will be numerous predictors of different types (e.g., quantitative, qualitative, etc.). It is unclear which predictors should be included in the model for new modeling projects and how they should be represented.\nWhen considering how to approach a modeling project, the stereotype is that the user focuses on which new, fancy model to apply to the data. The reality is that many of the most effective decisions that users must confront are which predictors to use and in what format. Thoughtfully adding the right feature representations often obviates the need (and overhead and risks) for complex models. That is one of the lessons from this particular data set.\nWe’ll put the feature engineering process on equal footing with model training For this reason, Figure 2.11 shows that the choice of model features and parameters are both important.\nWhile Cubist and the neural network were able to internally determine the more elaborate patterns in the data, our discovery of these same motifs has a higher value. We learned something about the nature of the data and can relate them to the model (via feature engineering) and the end-users17. Later, in Section 8.1, we’ll see how a black-box model can be used to suggests additional interactions to add to our linear regression model. This has the effect of eliminating some of the large residuals seen in Figure 2.4.\n\n\nModel Selection\nAt some point in the process, a specific model pipeline must be chosen. This chapter demonstrated two types of selection. First, we chose some models over others: the linear regression model did as well as more complex models and was the final selection. In this case, we decided between model pipelines. There was also a second type of model selection shown. The number of hidden units for the neural network was determined by trying different values and assessing the results. This was also model selection, where we decided on the configuration of a neural network (as defined by the number of hidden units). In this case, we selected within different neural networks.\nIn either case, we relied on data separate from the training set to produce quantitative efficacy assessments to help choose. This book aims to help the user gain intuition regarding the strengths and weaknesses of different models to make informed decisions.\n\n\nMeasuring Performance\nBefore using the test set, two techniques were used to determine the model’s effectiveness. First, quantitative assessments of statistics (i.e., MAE) from the validation set help the user understand how each technique would perform on new data. The other tool was to create simple visualizations of a model, such as plotting the observed and predicted values, to discover areas of the data where the model does particularly well or poorly. This qualitative information is lost when the model is gauged only using summary statistics and is critical for improving models. There are many more visualization methods for displaying the outcomes, their predictions, and other data. The figures shown here are pretty simplistic.\nNote that almost all of our approaches for model evaluation are data-driven in a way where we determine if the predicted values are “close” to the actual values. This focus on empirical validation is critical in proving that a model pipeline is fit for purpose. Our general mantra is\n\nAlways have a separate piece of data that can contradict what you believe.\n\nFor models that require specific probabilistic assumptions, it is a good idea to check these too, primarily if a model will also be used for inference. However, before considering questions regarding theoretical assumptions, empirical validation should be used to ensure that the model can explain the data well enough to be worth our inferences. For example, if our validation set MAE was around 15 minutes, the model predictions would not show much fidelity to the actual times. Any inferences generated from such a model might be suspect.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-summary-whole-game",
    "href": "chapters/whole-game.html#sec-summary-whole-game",
    "title": "2  The Whole Game",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nAt face value, model building appears straightforward: pick a modeling technique, plug in the data, and generate a prediction. While this approach will yield a predictive model, it unlikely to generate a reliable, trustworthy model. To achieve this we must first understand the data and the objective of the modeling. We finally proceed to building, evaluating, optimizing, and selecting models after these steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#chapter-references",
    "href": "chapters/whole-game.html#chapter-references",
    "title": "2  The Whole Game",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nPerkins, D. 2010. Making Learning Whole. Wiley.\n\n\nWolpert, D. 1996. “The Lack of a Priori Distinctions Between Learning Algorithms.” Neural Computation 8 (7): 1341–90.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#footnotes",
    "href": "chapters/whole-game.html#footnotes",
    "title": "2  The Whole Game",
    "section": "",
    "text": "For event time data, it is possible to get incomplete data due to censoring. Suppose that, after 10 minutes you don’t have your order yet. You don’t know the exact delivery time but you do know that it is at least 10 minutes. There are specialized tools for analyzing and modeling censored data. Thankfully, all of our data are complete since we know the exact time of delivery.↩︎\nOverall, we advise looking at the other columns in the data after the splitting.↩︎\nThese are described more in ?sec-outcome-transformation.↩︎\nThe two strategies for creating flexible, smooth fits are splines and generalized additive models. These are described in detail in Sections 8.3 and ?sec-cls-gam, respectively. If you have used the ggplot2 function geom_smooth() function, you’ve used a smoother. ↩︎\nDiscussed in ?sec-reg-metrics↩︎\nIf we want to make inferences on the parameters, such as p-values and confidence intervals, we we’ll need to make more specific assumptions (such as normality of the errors).↩︎\nDifferent ways to convert categorical predictors to numeric predictors are detailed in Chapter 6.↩︎\nSection 8.3↩︎\nWe will re-examine the linear regression model for these data in Section 8.1.2, where we discover (spoilers!) an important term left out of this analysis.↩︎\nThe details for regression trees and Cubist are in ?sec-reg-trees↩︎\nSee Sections ?sec-cls-nnet and ?sec-reg-nnet for discussions and details.↩︎\nThis model description is deliberately vague (so as not to confuse readers who are new to this model). Other details: each model could be trained for up to 5,000 epochs. However, a small amount of the training set was randomly set aside to monitor the sums of squared errors at each epoch. Early stopping was triggered when this out-of-sample loss degraded for 10 consecutive epochs. A rectified linear unit activation function was used between the predictor and hidden layers. A cyclic learning rate scheduler was used with a maximum rate of 0.1 and a weight decay value of 0.01 was applied. In a more complete analysis, all of these values would also be tuned.↩︎\nThe noisy profile suggests that our validation set might not be large enough to differentiate between subtle differences within or between models. If the problem required more precision in the performance statistics, a more thorough resampling method would be appropriate, such as cross-validation. These are described in Chapter Chapter 10.↩︎\nTo some degree, this is an artificially optimistic estimate since we are using the same data to pick the best result and estimate the model’s overall performance. This is called optimization bias and it is discussed more in Section 11.4. For the characteristics of these data, the optimism is likely to be small, especially compared to the noise in the MAE results.↩︎\nWe will revisit this statement in ?sec-comparing-boot, where methods for estimating confidence intervals for validation and test results are presented.↩︎\nWhen resampling is used in place of a validation set, Chapter 10 has a similar diagram that reflects the subtle differences in data usage.↩︎\nIn fact, the day-to-day users of the data are the best resources for feature engineering.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html",
    "href": "chapters/initial-data-splitting.html",
    "title": "3  Initial Data Splitting",
    "section": "",
    "text": "3.1 The Ames Housing Data\nIn the previous chapter, Figures 2.10 and 2.11 described various operations for the development and evaluation of ML models. We’ve also emphasized that “the right data should be used at the right time.” If the same samples were used for many different purposes, we run the risk of overfitting. Illustrated in Chapter 9, this occurs when the model over-interprets irreproducible patterns in the modeling data that don’t happen in any other data set. As a result, the model performance statistics are likely to be very optimistic and give us a false sense of how well the model works. If the model were evaluated on a separate set of data (that does not have abnormal patterns), performance would look considerably worse. Because of potential overfitting, the modeler must decide how to best utilize their data across different operations.\nThis chapter will examine how we can appropriately utilize our data. Except in Section 3.8, we’ll assume that each data set row is statistically independent of the others. Before proceeding further, we’ll introduce an example data set used in multiple chapters.\nThese data, originally published by De Cock (2011), are an excellent teaching example. Data were collected for 2,930 houses in Ames, Iowa, via the local assessor’s office. A variety of different characteristics of the houses were measured. Chapter 4 of Kuhn and Silge (2022) contains a detailed examination of these data. For illustration, we will focus on a smaller set of predictors, summarized in Tables 3.1 and 3.2. The geographic locations of the properties are shown in Figure 3.2.\nColumn\n      Min\n      Median\n      Max\n      Std. Dev.\n      Skewness\n      Distribution\n    \n  \n  \n    Baths\n     0.0\n      2.0\n      5.0\n     0.64\n 0.3\n\n    Gross Living Area\n   334.0\n  1,442.0\n  5,642.0\n   505.51\n 1.3\n\n    Latitude\n    42.0\n     42.0\n     42.1\n     0.02\n-0.5\n\n    Longitude\n   -93.7\n    -93.6\n    -93.6\n     0.03\n-0.3\n\n    Lot Area\n 1,300.0\n  9,436.5\n215,245.0\n 7,880.02\n12.8\n\n    Sale Price\n12,789.0\n160,000.0\n755,000.0\n79,886.69\n 1.7\n\n    Year Built\n 1,872.0\n  1,973.0\n  2,010.0\n    30.25\n-0.6\n\n    Year Sold\n 2,006.0\n  2,008.0\n  2,010.0\n     1.32\n 0.1\n\n  \n  \n  \n\n\n\n\n\nTable 3.1: A summary of numeric predictors in the Ames housing data.\nColumn\n      # Values\n      Most Frequent (n)\n      Least Frequent (n)\n      Distribution\n    \n  \n  \n    Building Type\n5\nSingle-Family Detached (2425)\nTwo-Family Conversion (62)\n\n    Central Air\n2\nYes (2734)\nNo (196)\n\n    Neighborhood\n28\nNorth Ames (443)\nLandmark (1)\n\n  \n  \n  \n\n\n\n\n\nTable 3.2: A summary of categorical predictors in the Ames housing data.\nAs shown in Table 3.1, the sale price distribution is fairly right-skewed. For this reason, and because we do not want to be able to predict negative prices, the outcome is analyzed on the log (base-10) scale.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-train-test",
    "href": "chapters/initial-data-splitting.html#sec-train-test",
    "title": "3  Initial Data Splitting",
    "section": "3.2 Training and Testing Sets",
    "text": "3.2 Training and Testing Sets\nOne of the first decisions is to decide which samples will be used to evaluate performance. We should evaluate the model with samples that were not used to build or fine-tune it. An “external sample” will help us obtain an unbiased sense of model effectiveness. A selection of samples can be set aside to evaluate the final model. The training data set is the general term for the samples used to create the model. The remaining samples, or a subset of them, are placed in the testing data set. The testing data set is exclusively used to quantify how well the model works on an independent set of data. It should only be accessed once to validate the final model candidate.\nHow much data should be allocated to the training and testing sets? This depends on several characteristics, such as the total number of samples, the distribution of the response, and the type of model to be built. For example, suppose the outcome is binary and one class has far fewer samples than the other. In that case, the number of samples selected for training will depend on the number of samples in the minority class. Finally, the more tuning parameters required for a model, the larger the training set sample size will need to be. In general, a decent rule of thumb is that 75% could be used from training.\nWhen the initial data pool is small, a strong case can be made that a test set should be avoided because every sample may be needed for model building. Additionally, the size of the test set may not have sufficient power or precision to make reasonable judgments. Several researchers (J. Martin and Hirschberg 1996; Hawkins, Basak, and Mills 2003; Molinaro 2005) show that validation using a single test set can be a poor choice. Hawkins, Basak, and Mills (2003) concisely summarizes this point:\n\n“hold-out samples of tolerable size […] do not match the cross-validation itself for reliability in assessing model fit and are hard to motivate”.\n\nResampling methods (Chapter 10), such as cross-validation, are an effective tool that indicates if overfitting is occurring. Although resampling techniques can be misapplied, such as the example shown in Ambroise and McLachlan (2002), they often produce performance estimates superior to a single test set because they evaluate many alternate versions of the data.\n\nOverfitting is the greatest danger in predictive modeling. It can occur subtly and silently. You cannot be too paranoid about overfitting.\n\nFor this reason, it is crucial to have a systematic plan for using the data during modeling and ensure that everyone sticks to the program. This can be particularly important in cases where the modeling efforts are collaborations between multiple people or institutions. We have had experiences where a well-meaning person included the test set during model training and showed stakeholders artificially good results. For these situations, it might be a good idea to have a third party split the data and blind the outcomes of the test set. In this way, we minimize the possibility of accidentally using the test set (or people peeking at the test set results).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-leakage",
    "href": "chapters/initial-data-splitting.html#sec-leakage",
    "title": "3  Initial Data Splitting",
    "section": "3.3 Information Leakage",
    "text": "3.3 Information Leakage\nInformation leakage (a.k.a data leakage) is another aspect of data handling to consider at the onset of a modeling project. This occurs when the model has access to data that it should not. For example,\n\nUsing the distribution of the predictor data in the test set (or other future data) to inform the model.\nIncluding identical or statistically related data in training and test sets.\nExploiting inadvertent features that are situationally confounded with the outcome.\n\nAn example of the last item we experienced may be familiar to some readers. A laboratory was producing experimental results to evaluate the difference between two treatments for a particular disorder. The laboratory was under time constraints due to an impending move to another building. They prioritized samples corresponding to the new treatment since these were more interesting. Once finished, they moved to their new home and processed the samples from the standard treatment.\nOnce the data were examined, there was an enormous difference between the two treatment sets. Fortuitously, one sample was processed twice: before and after they moved. The two replicate data points for this biological sample also showed a large difference. This means that the signal seen in the data was potentially driven by the changes incurred by the laboratory move and not due to the treatment type.\nThis type of issue can frequently occur. See, for example, Baggerly, Morris, and Coombes (2004), Kaufman et al. (2012), or Kapoor and Narayanan (2023).\nAnother example occurs in the Ames housing data set. These data were produced by the local assessor’s office, whose job is to appraise the house and estimate the property’s value. The data set contains several quality fields for things like the heating system, kitchen, fireplace, garage, and so on. These are subjective results based on the assessor’s experience. These variables are in a qualitative, ordinal format: “poor”, “fair”, “good”, etc. While these variables correlate well with the sale price, they are actually outcomes and not predictors. For this reason, it is inappropriate to use them as independent variables.\nFinally, the test set must emulate the data that will be seen “in the wild”, i.e., in future samples. We have had experiences where the person in charge of the initial data split had a strong interest in putting the “most difficult” samples in the test set. The prevalence of such samples should be consistent with their prevalence in the population that the model is predicting.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "href": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.4 Simple Data Splitting",
    "text": "3.4 Simple Data Splitting\nWhen splitting the data, it is vital to think about the model’s purpose and how the predictions will be used. The most important issue is whether the model will predict the same population found in the current data collection. For example, for the Ames data, the purpose is to predict new houses in the town. This definition implies a measure of interpolation since we are primarily concerned with what is happening in Ames. The existing data capture the types of properties that might be seen in the future.\nAs a counter-example, Chapter 4 of Kuhn and Johnson (2019) highlights a prediction problem in which a model is used to predict the future ridership of commuters on the Chicago elevated trains. This data set has daily records of how many commuters ride the train, and temporal factors highly affect the patterns. In this case, the population we will predict is future ridership. Given the heavy influence of time on the outcome, this implies that we will be extrapolating outside the range of existing data.\nIn cases of temporal extrapolation, the most common approach to creating the training and testing set is to keep the most recent data in the test set. In general, it is crucial to have the data used to evaluate the model be as close to the population to be predicted. For times series data, a deterministic split is best for partitioning the data.\nWhen interpolation is the focus, the simplest way to split the data into a training and test set is to take a simple random sample. If we desire the test set to contain 25% of the data, we randomly generate an appropriately sized selection of row numbers to allocate sales to the test set. The remainder is placed in the training set.\nWhat is the appropriate percentage? Like many other problems, this depends on the characteristics of the data (e.g., size) and the modeling context. Our general rule of thumb is that one-fourth of the data can go into testing. The criticality of this choice is driven by how much data are available. The split size is not terribly important if a massive amount of data is available. When data are limited, deciding how much data to withhold from training can be challenging.\nT. Martin et al. (2012) compares different methods of splitting data, including random sampling, dissimilarity sampling, and other methods.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "href": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "title": "3  Initial Data Splitting",
    "section": "3.5 Using the Outcome",
    "text": "3.5 Using the Outcome\nSimple random sampling does not control for any data attributes, such as the percentage of data in the classes. When one class has a disproportionately small frequency compared to the others (discussed in ?sec-imbalances), the distribution of the outcomes may be substantially different between the training and test sets.\nWhen splitting the data, stratified random sampling (Kohavi 1995) applies random sampling within sub-groups (such as the classes) to account for the outcome. In this way, there is a higher likelihood that the outcome distributions will match. When an outcome is a number, we use a similar strategy; the numeric values are broken into similar groups (e.g., low, medium, and high) and execute the randomization within these groups.\nLet’s use the Ames data to demonstrate stratification. The outcome is the sale price of a house. Figure 3.1(a) shows the distribution of the outcomes with vertical lines that separate 20% partitions of the data. Panel (b) shows that the outcome distributions are nearly identical after partitioning into training and testing sets.\n\n\n\n\n\n\n\n\nFigure 3.1: (a) A density plot of the sale price of houses in Ames with vertical lines that indicate regions that cover 20% of the data. The ‘rug’ on the axis shows the individual data points. (b) Density plots of the training set outcomes (solid red) and test set outcomes (dashed blue) for the Ames data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "href": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "title": "3  Initial Data Splitting",
    "section": "3.6 Using the Predictors",
    "text": "3.6 Using the Predictors\nAlternatively, we can split the data based on the predictor values. Willett (1999) and Clark (1997) proposed data splitting based on maximum dissimilarity sampling. The dissimilarity between two samples can be measured in several ways. The simplest method uses the distance between the predictor values for two samples. If the distance is small, the points are nearby. Larger distances between points are indicative of dissimilarity. To use dissimilarity as a tool for data splitting, we should initialize the training set with a single sample. We calculate the dissimilarity between this initial sample and the unallocated samples. The unallocated sample that is most dissimilar is added to the training set. A method is needed to allocate more instances to the training set to determine the dissimilarities between groups of points (i.e., the two in the training set and the unallocated points). One approach is to use the average or minimum of the dissimilarities. For example, to measure the dissimilarities between the two samples in the training set and a single unallocated point, we can determine the two dissimilarities and average them. The third point added to the training is chosen as having the maximum average dissimilarity to the existing set. This process continues until we achieve the targeted training set size.\nFigure 3.2 illustrates this process for the Ames housing data. Starting with a data point near the middle of the town, dissimilarity sampling selected 25 data points using scaled longitude and latitude as predictors. As the sampling proceeds, the algorithm initially chooses samples near the outskirts of the data, especially if they are outliers. Overall, the selected data points cover the space with no redundancy.\n\n\n\n\n\n\n\n\nFigure 3.2: Maximum dissimilarity sampling of 25 points in the Ames data. The small black circles are individual properties. Larger, lighter colors indidicate earlier selection.\n\n\n\n\n\nFor this example, the two predictors used for splitting were numeric. In this case, we typically use simple distance functions to define dissimilarity. Many other functions are possible. The Gower distance (Gower 1971) is a good alternative when a data set has non-numeric predictors. ?sec-cls-knn discusses this metric in more detail.\n\nWhile this analysis nicely illustrates the dissimilarity sampling process, it is flawed since it ignores the issue of spatial autocorrelation. This is the idea that things close to one another act more similarly than objects farther away and will be discussed more in Section 3.9 below.\n\nThere are various other methods to split the data using the predictor set. For example, Kennard and Stone (1969) describes an algorithm that attempts to sequentially select points to be uniformly distributed in the space defined by the splitting variables. Similarly, Vakayil and Joseph (2022) proposed a data splitting method called twinning, where a split of the data is sought that minimizes an aggregate distance between points in the training and testing set. Twinning uses the energy distance of Székely and Rizzo (2013), which measures the equality of distributions, to make the two data sets similar. Any variables can be used in the distance calculations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-three-way-split",
    "href": "chapters/initial-data-splitting.html#sec-three-way-split",
    "title": "3  Initial Data Splitting",
    "section": "3.7 Validation Sets",
    "text": "3.7 Validation Sets\nAs previously discussed, validation sets are a separate partition of the data that function as a precursor for the testing set. It allows us to obtain performance estimates on our model(s) during the development cycle. These are commonly used in deep learning and other domains where the initial data sizes range from very large to massive. This additional partition is often created simultaneously with the training and testing sets.\nValidation sets serve the same purpose as resampling methods described in Section 10.2 and we can consider them single resamples of the training data. Methods like bootstrapping or cross-validation use many alternative versions of the training set to compute performance statistics. When our data are extensive, multiple resamples are computationally expensive without significantly improving the precision of our estimates.\nWithout loss of generalization, we will treat the validation set as a particular case of resampling where there is a single resample of the training set. This difference is not substantive and allows us to have a common framework for measuring model efficacy (before the testing set).\nWe’ll see validation sets discussed in Section 10.2 and used in Sections TODO and TODO.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "href": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.8 Multi-Level Data",
    "text": "3.8 Multi-Level Data\nThere are cases where the rows of a data set may not be statistically independent. This often occurs when multiple data points are collected on individual people, such as\n\nPatients in medical studies may have data collected over time.\nPurchase histories of individual customers in a retail database.\n\nIn these and other situations, the data within a person tend to be correlated. This means that the data from a specific person have a higher correlation than data between people. There are many names for this type of data: multi-level data, hierarchical data, longitudinal data, random effect data, profile data, functional data, and so on. In some cases, there are multiple layers of data hierarchies.\nNote that the variable that indicates the person is generally not a predictor; we would not be making predictions about individual people. People, in this example, are sampled from the broader population. In this case, we are more concerned with the population rather than the individuals sampled from that population.\nThis aspect of the data differentiates it from the neighborhood predictor in the Ames data. The houses within each neighborhood may be more similar to one another than houses between neighborhoods. However, the difference is that we want to make predictions using information from these specific neighborhoods. Therefore, we will include neighborhood as a predictor since the individual neighborhoods are not a selected subset of those in the town; instead, the data contain all of the neighborhoods currently in the city.1\nChapter 9 of Kuhn and Johnson (2019) has a broad discussion on this topic with an illustrative example.\nWhen splitting multi-level data into a training and test set, the data are split at the subject level (as opposed to the row level). Each subject would have multiple rows in the data, and all of the subject’s rows must be allocated to either the training or the test set. In essence, we conduct random sampling on the subject identifiers to partition the data, and all of their data are added to either the training or test set.\nIf stratification is required, the process becomes more complicated. Often, the outcome data can vary within a subject. To stratify to balance the outcome distribution, we need a way to quantify the outcome per subject. For regression models, the mean of each subject’s outcome might be an excellent choice to summarize them. Analogously, the mode of categorical outcomes may suffice as an input into the stratification procedure.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-spatial-splitting",
    "href": "chapters/initial-data-splitting.html#sec-spatial-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.9 Splitting Spatial Data",
    "text": "3.9 Splitting Spatial Data\nSpatial data reflect information that corresponds to specific geographic locations, such as longitude and latitude values on the earth2. Spatial data can have autocorrelation, with objects being more similar to closer objects than to objects further away. Tobler’s First Law of Geography (Bjorholm et al. 2008) is:\n\n“Everything is related to everything else, but near things are more related than distant things.”\n\nThink of an overhead color satellite image of a forest. Different types of foliage—or lack thereof—have different colors. If we focus on the color green, the distribution of “greeness” is not a random pattern of the x/y axes (random distribution would resemble static). In other words, pixels cannot be assumed to be independent of one another.\nSpatial autocorrelation will generally affect how we partition the data; we want to avoid having very similar data points in both the training and testing sets.\nThe example data that we’ll use is from the US Forest Service, who send experts to specific locations to make a determination about whether an area is forested. Couch, White, and Frick (2024) state:\n\nThe U.S. Department of Agriculture, Forest Service, Forest Inventory and Analysis (FIA) Program provides all sorts of estimates of forest attributes for uses in research, legislation, and land management. The FIA uses a set of criteria to classify a plot of land as “forested” or “non-forested,” and that classification is a central data point in many decision-making contexts. A small subset of plots in Washington State are sampled and assessed “on-the-ground” as forested or non-forested, but the FIA has access to remotely sensed data for all land in the state.\n\nSee Frescino et al. (2023) and White et al. (2024) for similar data sets and background on the original problem. The Washington State data contains 7,107 locations. The data are shown in Figure 3.3 where the points are colored by the outcome class (green representing forestation). This figure shows that locations across the state are not spread uniformly. Bechtold and Patterson (2015) describes the sampling protocols and related issues. Finley, Banerjee, and McRoberts (2008) and May, Finley, and Dubayah (2024) show examples of how models have been applied to US forestry data in other geographic regions.\nThe rate for forestation in these data is nearly even at 54.8%.\n\n\n\n\n\n\n\n\nFigure 3.3: Area locations in Washington State sampled for determining forestation. Green points reflect locations that were determined to be forested.\n\n\n\n\nHow should we split these data? A simple random sample is inadequate; we need to ensure that adjacent/nearby locations are not allocated to the training and testing sets. There are a few ways to compensate for this issue.\nFirst, we can group the data into local areas and iterate by removing one or more group at a time. If there is no rational, objective process for creating groups, we can use clustering methods or create a regular grid across the data range (Roberts et al. 2017). For example, Figure 3.4(a) shows a collection of data grouped by a grid of hexagons. We could randomly sample some groups, often referred to as blocks, to allocate to the testing set.\nHowever, notice that some points, while in different hexagonal blocks, are still close to one another. Buffering (Brenning 2005; Le Rest et al. 2014) is a method that excludes data around a specific data point or group. Panel (b) shows a buffer for one block. If this region was selected for the testing set, we would discard the points that are within the buffer (but are in different blocks) from being allocated to the training set.\n\n\n\n\n\n\n\n\nFigure 3.4: A plot of 65 spatial data points where (a) shows how they can be blocked into one of seven hexagonal regions. Panel (b) shows the same data with a circular buffer around the center of a single block.\n\n\n\n\n\nJ Pohjankukka T Pahikkala and Heikkonen (2017), Lyons et al. (2018), Meyer et al. (2019) Karasiak et al. (2022), and Mahoney et al. (2023) have good overviews of the importance of countering spatial autocorrelation and appropriate sampling methods.\nFor our example, we used blocking and buffering to allocate roughly 20% of the forestry data to the testing set. First, the locations were allocated to a 25 x 25 grid of hexagons across the state. After this, a buffer was created around the hexagon such that outside points within the buffer would be excluded from being used in the training or testing sets. Finally, roughly 20% of the locations were selected for the test set by moving through the space and selecting hexagons sequentially. Figure 3.5 shows the results of this process.\nIn the end, a training set of 4,834 locations and a test set of 1,379 locations (roughly a 22.2% holdout) were assembled. The buffer contained 894 locations which was about 12.6% of the original pool of data points. The event rates were similar between the data sets: 56.2% for the training set and 52.7% for the testing set.\n\n\n\n\n\n\n\n\nFigure 3.5: A split of the forestation data into training (magenta) and testing (purple) sets. Some locations, in black, correspond to the buffer and are not allocated to either set. Clicking on individual points will show their allocation results.\n\n\n\n\nFrom this map, we can see that the hexagons selected for the testing set cover the geographic area well. While the training set dominates some spaces, this should be sufficient to reduce the effects of spatial autocorrelation on how we measure model performance.\nWe will revisit these data in Section 10.7 to demonstrate how to resample them appropriately.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#chapter-references",
    "href": "chapters/initial-data-splitting.html#chapter-references",
    "title": "3  Initial Data Splitting",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBaggerly, K, J Morris, and K Coombes. 2004. “Reproducibility of SELDI-TOF Protein Patterns in Serum: Comparing Datasets from Different Experiments.” Bioinformatics 20 (5): 777–85.\n\n\nBechtold, W, and P Patterson. 2015. The Enhanced Forest Inventory and Analysis Program - National Sampling Design and Estimation Procedures. U.S. Department of Agriculture, Forest Service, Southern Research Station.\n\n\nBjorholm, S, JC Svenning, F Skov, and H Balslev. 2008. “To What Extent Does Tobler’s 1 St Law of Geography Apply to Macroecology? A Case Study Using American Palms (Arecaceae).” BMC Ecology 8: 1–10.\n\n\nBrenning, A. 2005. “Spatial Prediction Models for Landslide Hazards: Review, Comparison and Evaluation.” Natural Hazards and Earth System Sciences 5 (6): 853–62.\n\n\nClark, R. 1997. “OptiSim: An Extended Dissimilarity Delection Method for Finding Diverse Representative Subsets.” Journal of Chemical Information and Computer Sciences 37 (6): 1181–88.\n\n\nCouch, S, G White, and H Frick. 2024. Forested: Forest Attributes in Washington State. https://github.com/simonpcouch/forested.\n\n\nDe Cock, D. 2011. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19 (3).\n\n\nFinley, A, S Banerjee, and R McRoberts. 2008. “A Bayesian Approach to Multi-Source Forest Area Estimation.” Environmental and Ecological Statistics 15: 241–58.\n\n\nFrescino, T, G. Moisen, P Patterson, C Toney, and G White. 2023. “‘FIESTA’: A Forest Inventory Estimation and Analysis R Package’.” Ecography 2023 (7).\n\n\nGower, J. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71.\n\n\nHawkins, D, S Basak, and D Mills. 2003. “Assessing Model Fit by Cross-Validation.” Journal of Chemical Information and Computer Sciences 43 (2): 579–86.\n\n\nJ Pohjankukka T Pahikkala, P Nevalainen, and J Heikkonen. 2017. “Estimating the Prediction Performance of Spatial Models via Spatial k-Fold Cross Validation.” International Journal of Geographical Information Science 31 (10): 2001–19.\n\n\nKapoor, S, and A Narayanan. 2023. “Leakage and the Reproducibility Crisis in Machine-Learning-Based Science.” Patterns 4 (9).\n\n\nKarasiak, N, JF Dejoux, C Monteil, and D Sheeren. 2022. “Spatial Dependence Between Training and Test Sets: Another Pitfall of Classification Accuracy Assessment in Remote Sensing.” Machine Learning 111 (7): 2715–40.\n\n\nKaufman, S, S Rosset, C Perlich, and O Stitelman. 2012. “Leakage in Data Mining: Formulation, Detection, and Avoidance.” ACM Transactions on Knowledge Discovery from Data 6 (4): 1–21.\n\n\nKennard, R W, and L A Stone. 1969. “Computer Aided Design of Experiments.” Technometrics 11 (1): 137–48.\n\n\nKohavi, R. 1995. “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.” International Joint Conference on Artificial Intelligence 14: 1137–45.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nKuhn, M, and J Silge. 2022. Tidy Modeling with R. O’Reilly Media, Inc.\n\n\nLe Rest, K, D Pinaud, P Monestiez, J Chadoeuf, and V Bretagnolle. 2014. “Spatial Leave-One-Out Cross-Validation for Variable Selection in the Presence of Spatial Autocorrelation.” Global Ecology and Biogeography 23 (7): 811–20.\n\n\nLyons, M, D Keith, S Phinn, T Mason, and J Elith. 2018. “A Comparison of Resampling Methods for Remote Sensing Classification and Accuracy Assessment.” Remote Sensing of Environment 208: 145–53.\n\n\nMahoney, MJ, LK Johnson, J Silge, H Frick, M Kuhn, and CM Beier. 2023. “Assessing the Performance of Spatial Cross-Validation Approaches for Models of Spatially Structured Data.” arXiv.\n\n\nMartin, J, and D Hirschberg. 1996. “Small Sample Statistics for Classification Error Rates I: Error Rate Measurements.” Department of Informatics and Computer Science Technical Report.\n\n\nMartin, T, P Harten, D Young, E Muratov, A Golbraikh, H Zhu, and A Tropsha. 2012. “Does Rational Selection of Training and Test Sets Improve the Outcome of QSAR Modeling?” Journal of Chemical Information and Modeling 52 (10): 2570–78.\n\n\nMay, P, A Finley, and R Dubayah. 2024. “A Spatial Mixture Model for Spaceborne Lidar Observations over Mixed Forest and Non-Forest Land Types.” Journal of Agricultural, Biological and Environmental Statistics, 1–24.\n\n\nMeyer, H, C Reudenbach, S Wöllauer, and T Nauss. 2019. “Importance of Spatial Predictor Variable Selection in Machine Learning Applications–Moving from Data Reproduction to Spatial Prediction.” Ecological Modelling 411: 108815.\n\n\nMolinaro, A. 2005. “Prediction Error Estimation: A Comparison of Resampling Methods.” Bioinformatics 21 (15): 3301–7.\n\n\nRoberts, DR, V Bahn, S Ciuti, MS Boyce, J Elith, G Guillera-Arroita, S Hauenstein, et al. 2017. “Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.” Ecography 40 (8): 913–29.\n\n\nSzékely, G J, and M L Rizzo. 2013. “Energy Statistics: A Class of Statistics Based on Distances.” Journal of Statistical Planning and Inference 143 (8): 1249–72.\n\n\nVakayil, A, and V R Joseph. 2022. “Data Twinning.” Statistical Analysis and Data Mining: The ASA Data Science Journal 15 (5): 598–610.\n\n\nWhite, G, J Yamamoto, D Elsyad, J Schmitt, N Korsgaard, J Hu, G Gaines III, T Frescino, and K McConville. 2024. “Small Area Estimation of Forest Biomass via a Two-Stage Model for Continuous Zero-Inflated Data.” arXiv.\n\n\nWillett, P. 1999. “Dissimilarity-Based Algorithms for Selecting Structurally Diverse Sets of Compounds.” Journal of Computational Biology 6 (3): 447–57.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#footnotes",
    "href": "chapters/initial-data-splitting.html#footnotes",
    "title": "3  Initial Data Splitting",
    "section": "",
    "text": "If you are familiar with non-Bayesian approaches to multi-level data, such as mixed effects models, this is the same as the difference between random and fixed effects. ↩︎\nChapters 11 and ?sec-iterative-search will also look at spatial effects unassociated with real geographies.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html",
    "href": "chapters/missing-data.html",
    "title": "4  Missing Data",
    "section": "",
    "text": "4.1 Root Causes\nIt is not uncommon for some predictor values to be unknown. There can be a multitude of reasons. To illustrate, let’s consider a laboratory test for a respiratory disease. One or more results may be missing due to a failed control or an inappropriate database join. It is also possible that the test itself failed to produce a result. For example, the test might fail for diagnostics that use throat swabs because an interfering substance, such as food coloring from a lozenge, is on the swab.\nOne of our illustrative data, the Ames housing data, has missing values in 22 predictors. Figure 4.1 illustrates the occurrence of missingness for the these data. This figure presents the full rectangular data matrix where each property (sorted alphabetically by neighborhood) is on the x-axis, and each predictor is on the y-axis. Red indicates that the value was missing in the data.\nThe figure highlights several characteristics. The most noteworthy: missing data affects every property (i.e., row) in the data. Pool quality is missing for nearly every property, while the columns for types of alleys, basements, and fences as well as the quality of fireplaces are missing for the vast majority of properties. Second, there are specific patterns of missing information that are visually apparent. For example, when missing data occurs in one garage variable, it likely occurs across other garage variables (condition, finish, quality, type, and year built). The same is true for basement variables (condition, exposure, quality, type 1, and type 2). Other variables, like fence, fireplace quality, and lot frontage do not appear to have any visual structure to their missingness.\nAn “upset plot” is a method for visualizing high-dimensional Venn diagrams (Lex et al. 2014). We can use this to explore potential patterns of missingness across predictors. For example, Figure 4.2 shows that the majority of the properties in Ames are missing values for ally, fence, or pool.\nThe problem with missing data is that many models are not naturally equipped to deal with this lack of information. As a simple example, consider the multiple linear regression model. Deriving the regression coefficients depends on operations on the predictor values (e.g., computing the covariance matrix). These calculations cannot be performed if the predictors contain any missing values; this is also true for the majority of models. Consequently, it is crucial to address the presence of missing data to build any of these predictive methods1. In addition to addressing missing data, the pattern and nature of missingness can sometimes serve as a significant predictor of the response. In this chapter, we will explore the root causes of missing data, examine approaches for resolving the problem, and understand when it should be addressed in the modeling process.\nAllison (2002) has an excellent and succinct summary of relevant statistical concepts and methods. Emmanuel et al. (2021) and Hasan et al. (2021) provide literature surveys on methods for missing data, while Nijman et al. (2022) describes a survey on just how badly these topics are described/documented in specific studies in the literature.\nWhen encountering missing data, the primary question is, “Why are these values missing?” Knowing is a good idea, and the answer can substantially affect how we compensate for the problem. In some instances, the answer might be apparent or can be inferred from the data. In the example of the laboratory test, suppose a random defect in the test kit was to blame for missing measurements. In this instance, the value was missing completely at random (MCAR). MCAR means that the mechanisms that affect missingness in the predictor are known and unrelated to the columns in the data set. When this is the case, our complete case sample (achieved by using rows with no missing values) corresponds to sampling from the same distribution as the entire data set (i.e., includes the missing rows).\nWith MCAR, we have significant latitude in terms of how to handle the situation. A complete case analysis will not induce any systematic bias in our results but, due to the smaller sample size, the model will have increased variability.\nAs a counter-example, suppose that patients with more severe respiratory illnesses were more likely to deal with their symptoms by using a lozenge, possibly leading to missing values. If the likelihood of a missing predictor value is a function of some other variable(s) contained in the data, this is called missing at random (MAR). You can think of this as conditional MCAR: each each level of a column describing lozenge usage (yes/no), the data are MCAR. Bhaskaran and Smeeth (2014) contains practical examples that help distinguish MAR and MCAR.\nAnother example data set will be introduced in Section 6.1, where we try to predict the cost of a hotel room (known as the “average daily rate” or ADR). The data set includes data on the method of booking the room, such as the name of the travel agent, the travel company, and the customer’s country of origin. The agent variable in the hotel rate data was missing for 22.7% of the data. We don’t always know if the agent is missing because no agent was used or because the value was not recorded. Treating the missingness of the agent as a binary outcome, we can use a simple recursive partitioning model (?sec-cart-cls) to understand potential relationships with other predictors. This might suggest the mechanism(s) influencing the occurrence of missing data. Figure 4.3 illustrates the predictors that partition the missing agent data into increasing homogeneous groups. For example, non-missing company values are highly associated with missing values. This might mean that the company making the reservation used automated systems instead of a specific person in the company being associated with the reservation. Lead time also appears to be related to missingness, with shorter lead times occurring more frequently with missing agent values. We don’t know whether lead time influences missingness or vice-versus. However, this analysis would provide a direction to begin an investigation.\nFigure 4.3: A classification tree to predict the missing category of agent in the hotel rate dataset. “TA” stands for travel agent and “TO” means tour operators.\nWith some simple exploration, as demonstrated with the Ames and hotel rate data sets, we can understand the causes or potential reasons for missing data. However, determining the cause of missing data may be more challenging for many other data sets. We need a framework to understand and manage missing data in such cases.\nOne helpful framework involves examining the mechanisms behind missing data, including structural deficiencies, random occurrences, and specific causes.\nFirst, there can be structural deficiencies. This type of missing data arises when necessary information about a predictor is omitted. Moreover, it is often the easiest to address once the missing information has been identified. The agent variable in the hotel rate data is one example of a structural deficiency. As a second example, we saw earlier in the Ames housing data that several of the garage predictors were simultaneously missing across 5% of homes. The missing information was because these homes did not have a garage2. Some or all of the variables of finish, quality, type, and year built for garages will likely be important for determining a home’s value. Therefore, we will need an approach to address these structural deficiencies.\nThere can also be random occurrences. Missing values often occur at random with no defined root cause. For example, a patient in a clinical trial may miss a scheduled visit due to a scheduling mishap. As another example, sporadically occurring severe weather can create missing data for collection devices that depend on continuous power. Generally, missing data due to random occurrences can happen a small percentage of the time and can be remedied. However, if randomly missing data occurs a large percentage of the time, the measurement system should be evaluated, and the collected variables should be scrutinized before being included in the modeling process. For a detailed understanding of this type of problem, see Little and Rubin (2019).\nWe might be able to identify specific causes for missingness. As we will see later in this chapter (Section 4.2.4), several basement variables in the original Ames data have missing values. This information was not missing because there was some failure in recording the data. The explanation was much simpler: these homes had no basements. This type of missing data is the most challenging to manage, and the appropriateness of techniques can vary. Hence, understanding the nature of the missing data is crucial before applying any methods.\nThe worst situation is missing not at random (MNAR)3, where we do not know the factors that influence the probability that values are missing. The best approach to MNAR data is not to have MNAR data; it is essential to determine why the data are missing and, hopefully, relate that to columns in the data. Otherwise, we can use imputations and other methods described below to solve the issues. However, there will likely be significant ambiguity about the analysis and how well it works. With models built in the MNAR assumption, it would behoove us to conduct extensive sensitivity analyses to assess the robustness of our approach to the missing data problem.\nAs an example, Chapters 12 - 15 of Kuhn and Johnson (2013) showed multiple classification models to predict the probability that a grant would be accepted. Two categorical predictors contained encoded values of “unknown” and had very high empirical associated with the outcome classes:\nThis is not a good place to be: we cannot explain the existence of two of our main predictors. It would be that the populations of grants that had missing data for these variables correspond to very successful results or that the process that assembled the data contains a systematic flaw. We do know that we can’t induce success in a grant application by just labeling the value as unknown. Clearly, we would want to investigate this situation to determine what is happening.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-root-causes",
    "href": "chapters/missing-data.html#sec-root-causes",
    "title": "4  Missing Data",
    "section": "",
    "text": "Informative missingness seemed to occur in these data; unknown values of the contract value band and sponsor code were heavily used in many models. This itself is a likely surrogate or signal for some other piece of information.\n\n\n\nUnderstanding these mechanisms will guide us in choosing appropriate techniques for handling missing data appropriately.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#approaches-for-resolving-missing-data",
    "href": "chapters/missing-data.html#approaches-for-resolving-missing-data",
    "title": "4  Missing Data",
    "section": "4.2 Approaches for Resolving Missing Data",
    "text": "4.2 Approaches for Resolving Missing Data\nThere are three general ways to resolve missing values: removal, imputation, or encoding. Each approach has advantages and disadvantages, and which we choose should depend on the specific problem context and data set. We will review these approaches in the subsections below and provide guidance on when each would be appropriate.\n\n4.2.1 Models that Tolerate Missing Data\nInstead of directly addressing the missing values, we could sidestep it by using models that tolerate missingness. Let’s examine a few.\nThe CART decision tree model (Breiman et al. 1984) recursively identifies variables and split points that optimally partition the data into subsets whose outcome frequency distributions are more homogeneous. For each selected variable and split point, additional variables and split points are identified with the next best ability to partition the data into more pure subsets. These additional variables are called surrogate splits. When a sample has a missing value for a predictor in the tree, the surrogate predictors are then used to direct the sample toward the appropriate terminal node.\nWhile C5.0 (Quinlan 1993; Kuhn and Johnson 2013) is also a decision tree, it adopts a unique approach to addressing missing values. This method utilizes fractional counts in subsequent splits based on the frequency distribution of missing data for a predictor. This approach enables the model to estimate where the missing values might fall within the partitioning.\nBoosted trees, such as xgboost (Chen and Guestrin 2016), are also based on a recursive partitioning framework. However, xgboost’s approach to addressing missing data is more complex and is called sparsity-aware split finding. In the model-building process, the algorithm determines which direction would be more optimal for each node in the tree if a sample had a missing value for that predictor.\nRandom forest (Breiman 2001) has several approaches to handling missing values. The naive approach internally imputes using the median of non-missing values for continuous predictors or the most frequent value for categorical predictors. A more advanced approach identifies the nearest non-missing samples to the sample with the missing value and imputes the value based on a weighted distance score.\nThe primary benefit of these models is that we can use the original data as-is. They are eliminate the propagation of errors that can occur when imputation tools are used. The drawback is that the number of models that can be applied to the data is very limited. As we know from the No Free Lunch theorem, no one model will be optimal for all problems. Therefore, we must address the missing data problem head-on to adequately explore the predictive ability of a wide range of models.\n\nThese models only solve the logistical aspect of missing data. If the nature of missingness in your data causes a systematic bias, the models above will not correct this bias.\n\nFor example, naive Bayes (Webb, Keogh, and Miikkulainen 2010) builds models by analyzing each predictor independently of the others. When a predictor contains a missing value, this missing sample’s information is omitted from probability calculations for only the affected predictors. If a distribution is systematically affected by missingness, the probabilities computed during prediction will be biased and may result in poor results in new samples. If missing data informs our understanding of the response, omitting the missing samples will be detrimental to the model.\nFigure 4.4 shows a two-class classification example where one predictor is used. The top panel shows the probability of a missing predictor value is related to its own value (i.e., MCAR). The predictor value is less likely to be complete was its value increases.\nNaive Bayes would use the predictor data to compute conditional densities of the predictor for each outcome class. The middle plot shows what these densities should resemble if the dataset were complete. There is a small overlap between the predictor densities; decent model performance is achievable.\n\n\n\n\n\n\n\n\nFigure 4.4: An example of how missingness can cause bias in the training data and impair a model fit.\n\n\n\n\n\nThe bottom set of densities reflects the observed data. The density for class level B is more affected and would appear to have a tighter distribution. This induces more overlap in the densities, making the classification problem more difficult.\nThis demonstrates that a model being agnostic to missing values does not mean that the missing value problem does not go away.\n\n\n4.2.2 Removal\nThe simplest method for managing missing data is to eliminate the predictors or samples that contain them. However, the deletion of data requires careful consideration of several factors within the dataset. When deleting data, the order in which we assess the proportion of missing values (in columns or rows) is important to consider. In some scenarios, there are many more predictors than samples; samples are often difficult or expensive to collect. In this case, it would be wise to first identify predictors that should be removed due to excessive missing values, then proceed to identify and possibly remove samples that have excessive missing values. If, on the other hand, the data contain many more samples than predictors, then the removal procedure could be reversed. In any case, however, we need to remember that the samples are our currency for tuning models and assessing model performance. Therefore, we will often place a higher priority on preserving samples over predictors.\n\n\n\n\n\n\n\n\nFigure 4.5: The distribution of percent missing values across predictors for the Ames housing data set.\n\n\n\n\n\nLet’s return to the Ames data set. In this example, there are many more samples (\\(n\\)=2930) than predictors (\\(p\\)=73). While this is true, Figure 4.1 clearly illustrates that a few of the predictors were missing for the vast majority of the samples. The proportion of missing values across the predictors is shown in Figure 4.5. This figure reveals that 4 predictors have more than 20% missing sample values. Because the percentage of missing values is large, these predictors would be candidates for removal. In the hotel rate data, the agent and company predictors had missing values of 22.7% and 91%, respectively. In our experience, we have tended to remove predictors that have more than 20%-30% missing values. This percentage is simply a point of guidance and not a hard rule to be applied to every data set. After removing these predictors, the Ames data set now has 0.7% missing values and the hotel rate data set had 0.05% missing values.\nAfter removing predictors with excessive missing data, we can then consider removing samples with excessive missing data. No sample in the Ames data had more than 12.1% of missing predictors. The samples with the greatest percentage of missing predictors were those that had missing basement information. No sample in the hotel rate data had more than 3.6% of missing predictors. Neither of these percentages is large enough to merit the removal of these samples. Therefore, we will keep these samples and utilize an imputation or encoding procedure to fill in these gaps.\nThere are a couple of caveats to removing predictors or samples based on missing information. First, the missing information in a predictor may be informative for predicting the response. For the hotel rate data, the missing status of the agent variable is associated with the response. Therefore, it may be better to encode the missing status for this variable rather than eliminate the variable altogether. This approach will be discussed below. Second, as seen in Figure 4.4, removing samples may create a bias in the remaining data which would impact the predictive ability of the model on future samples.\n\n\n4.2.3 Imputation\nImputation is the process of using other existing information (i.e., the predictors) to estimate what each missing value might have been. In other words, we will build an imputation model to fill in the missing column(s) so that we can run the primary machine-learning model. A separate imputation model is required for each column that contains (or could contain) missing data.\nImputation is a well-researched statistical methodology. This technique has traditionally been used for inferential models, focusing on maintaining the validity of test statistics to support hypothesis testing. As mentioned in earlier chapters, there is an important distinction between the objectives of statistical inference and prediction accuracy. See Sperrin et al. (2020) for a discussion related to imputation.\nIn the statistical literature (D’Agostino McGowan, Lotspeich, and Hepler 2024), two terms can help us understand the distinction:\n\nDeterministic imputation creates a single model to estimate the missing predictor’s value using one or more predictors in the training set4. After imputation, the new values are treated as known (i.e., not random variables).\nStochastic imputation involves creating many imputation models with the goal of creating a distribution of possible values for the missing data. It is used primarily to conduct proper inferential analyses and includes traditional multiple imputation techniques.\n\nOne important concept heavily featured in subsequent chapters is resampling, where we’ll train our preprocessors and supervised models using slightly different data sets. The goal of resampling is to estimate model performance accurately. Although resampling will involve repeated re-imputation of missing data, it is not a stochastic imputation method.\nLet’s highlight a couple of key distinctions when considering imputation for the purpose of inference versus prediction. One is that inferential models generally make assumptions about the statistical distributions of the predictors. Conversely, many machine learning models, like support vector machines, tree-based models, and neural networks, do not make such assumptions. Therefore, stochastic imputation is less relevant for predictive models. A second difference is that multiple imputation methods focus on understanding relationships within the existing data, while predictive models aim for generalizable relationships to unseen samples. This difference has implications on when imputation should be applied to the data, which will be discussed later in Section 4.4. Finally, in some cases, including the outcome data in the imputation model might make sense. This is inappropriate for our goals here (i.e., deterministic imputation). While D’Agostino McGowan, Lotspeich, and Hepler (2024) explains the theoretical reasons why this is the case, our objections can be viewed as purely functional: if we require the outcome to make decisions, we cannot predict new samples where the outcome is unknown.\nWhat are the important characteristics of an imputation technique that will be used in a prediction model? There are a few that we will focus on:\n\nTolerate other missing data: as we saw in the Ames data, multiple variables within a sample may be missing. Therefore, an imputation technique should be feasible in the presence of other missing data.\nHandle different predictor types: many data sets have different variables, such as numeric, categorical, and ordinal. The method should be able to accommodate numeric and qualitative predictors seamlessly and without changing the nature of the data (e.g., categorical predictors should be be converted to indicator columns).\nProduce efficient prediction equations: each predictor with missing data will require an equation. Imputation for large data sets, when used with resampling approaches during model training, will increase computation time as well as the size of the imputation equation. Therefore, the more efficient the imputation approach, the less computation time will be needed to obtain the final model.\nEnsure robustness: the method should be stable and not overly affected by outliers.\nBe accurate: the results should be close to what the actual value would have been5.\n\nBy considering each of these characteristics, imputation can effectively alleviate missing data problems, enhancing models’ quality and predictive performance. There are several imputation methods that meet most or all of the above characteristics. These imputation techniques fall into two general categories: most likely value and model-based (i.e., “regression imputation”).\nTo illustrate how these methods work, we will use two simple, simulated two-predictor classification data sets. Figure 4.6 displays the simulated data. For these data sets, the optimal class separation is defined by the black lines. We will initially focus on the nonlinear data set. From these data set, 10% of the samples will be randomly selected. Of these samples, half will have the value of first predictor deleted, while the other half will have the second predictor deleted.\n\n\n\n\n\n\n\n\nFigure 4.6: Two complete two-class simulated datasets with 200 data points, one with a linear relationship between the predictors and the other nonlinear. The optimal partitions of the classes are represented by the black lines.\n\n\n\n\n\n\n\n4.2.4 Encoding Missing Data\nWhen a predictor takes categorical values, an alternative approach to imputation is encoding. Consider the basement exposure variable in the original Ames data set. Possible exposure values are: “good,” “average,” “minimum,” and “no exposure” and contains 83 missing values. The most likely value approach would impute the missing values with the “no exposure” category since this is the most frequent category. A model-based approach would utilize information across the rest of the variables to predict which of the 4 available categories would be best. Would imputing with one of the 4 available categories be a good approach for the missing samples? In this case, the answer is “no!”. With a little more investigation, we can see that the reason the exposure variable contains missing values for these houses is because these houses do not have basements.\nWhen a value is missing for a specific reason like we see with the basement exposure variable, encoding the missing information will be more informative to the models. In an encoding procedure, we simply acknowledge that the value is missing by creating a new category such as “unknown,” “unspecified,” or “not applicable.” In the case of basement exposure, a more appropriate categorization would be “no basement.”\nThe encoding procedure is mostly used for categorical variables. However, there are times when encoding can be applied to continuous variables. In many analytical procedures, an instrument cannot reliably provide measurements below a specified limit. Measurements of samples above the limit are returned as numeric values, but measurements below the limit are returned as either “BLQ” (below lower limit of quantitation) or “&lt; 3.2”, where 3.2 is the lower limit of quantitation. These values cannot be included with the continuous values. What can we do in this situation? One approach would be to impute the BLQ values with a reasonable numeric value less than lower limit of quantitation6. This information could also be encoded by creating a new variable that contains two values, “ALQ” and “BLQ”, which would identify samples that could and could not be measured. If this variable is related to the outcome, then the ability to measure the quantity may be predictively informative.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#specific-imputation-methods",
    "href": "chapters/missing-data.html#specific-imputation-methods",
    "title": "4  Missing Data",
    "section": "4.3 Specific Imputation Methods",
    "text": "4.3 Specific Imputation Methods\nTechnically, almost any model that can make a prediction is a potential candidate for imputing data. However, some are better than others and we summarize a few of the most used below.\n\n4.3.1 Most Likely Value\nThe simplest approach to imputation is to replace a missing value with its most likely value based on that predictor’s non-missing values. For numeric data, the most likely value can be summarized by the mean or median. For categorical data, the most likely value is the mode (or most frequent value). Most likely value imputation meets many of the desirable characteristics for imputation. They can tolerate missing values from other predictors (because they operate one-predictor-at-a-time) and can handle different predictor types while also producing efficient prediction equations. Furthermore, achieving a robust imputation can be done using either the median or the trimmed mean (Barnett and Lewis 1994). The trimmed mean is the mean of the middle-most samples. For example, we could compute the mean of the samples with 5% of the the most extreme (smallest and largest) samples removed. Doing this would minimize the impact of any extreme sample values on the computation of the mean. However, imputing with the most likely value for a single predictor may not produce an imputed value that is close to the true value.\nTo demonstrate, we have imputed the missing values in the simulated data using the mean and median imputation techniques for both data sets seen in Figure 4.6. Figure 4.7 shows the values of the original samples that were selected to have missing values along with the imputed values based on mean and median imputation.\nFor both data sets, the imputed values are towards the center over the overall plot and away from the parabolic/linear regions where we know the actual data are. The median imputed values are less affected by extreme values and are closer to the region of actual data. Both techniques, however, are unable to place most of the missing data near their true values.\nFor many data sets, and especially for sets with a minimal number of missing values, most likely value imputation may be sufficiently good for what we need. The procedure is fast but is terribly inaccurate. But there may be occasions when when we need the imputations to be closer to the true (yet unknown) values.\n\n\n\n\n#| label: fig-imputation-exploration\n#| out-width: \"80%\"\n#| viewerHeight: 425\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(bslib)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-imputation-exploration.R\")\n\napp\n\n\n\nFigure 4.7: A visualization of the four new features for different linear embedding methods. The data shown are the validation set results from the two simulations shown in Figure 4.6.\n\n\n\n\n\n4.3.2 Linear Methods\nWhen a data set has many predictors and the predictors have some correlation with each other, then multiple linear regression can be a more effective imputation technique than the most likely value methods. That is, multiple linear regression will utilize the information contained in other predictors to estimate an imputed value that will be closer to what the actual value would have been. Multiple linear regression also produces a straightforward, compact prediction equation for each predictor with missing values which is computationally efficient when applied to new samples. However, this technique cannot naturally handle missing values in the predictors used to develop the imputation model. In practice, therefore, the imputation approach utilizes a complete case analysis.\nReturning to the simulation illustration, the linear imputation method creates two simple linear regression models, where one model uses predictor 1 as the response and predictor 2 as the predictor, and the other model reverses the roles of the predictors.\nSelecting the nonlinear data set in Figure 4.7 illustrates the impact of this technique. Assuming that we have conducted some exploratory data analysis, we would be aware of the nonlinear relationship between the predictors. As such, the imputation model for the second predictor included an additional squared term. This greatly improves the imputation quality. Unfortunately, the same approach is not possible for imputing the first predictor (which is poorly predicted).\nHowever, if these predictors were linearly related, then this technique would generate values closer to the actual values. This can be seen by choosing the linear data set in Figure 4.7.\nSince this imputation method is basically linear regression, we can use it for slightly more sophisticated results. For example, if we want to impute a predictor’s values based on some other predictors categories, linear regression can achieve this. There is also the possibility of interactions, splines, and so on (as long as the “predictors” in the imputation model are not missing themselves).\nWhen the predictor that requires imputation is categorical, then logistic or multinomial regression can be used to generate an imputed category.\nFigure 4.8 provides another visual comparison of the imputation techniques. In this figure, each point represents the Euclidean distance from the imputed sample to its known location. The mean, median, and linear methods have similar distributions of distances.\n\n\n\n\n\n\n\n\nFigure 4.8: A comparison of the distribution of distances between the actual and imputed samples across imputation techniques for the nonlinear data shown in Figure 4.6. The mean, median, and linear techniques perform similarly, while K-NN and bagging generate imputed values that are modestly closer to the actual values.\n\n\n\n\n\n\n\n4.3.3 Nearest Neighbors\n\\(K\\)-nearest neighbor calculations will be discussed in Section 7.3, where it will be used in some multidimensional scaling methods, and in ?sec-knn-cls in the context of a supervised model. In short, when imputing a missing predictor value for a new sample, the \\(K\\) most similar samples from the training set are determined (using all complete predictor values). The \\(K\\) predictor values for the predictor of interest are summarized via the mean (for numeric outcomes) or the mode. This summary statistic is used to fill in the missing value. This is a localized version of the most likely value approach.\nFigure 4.7 shows that a nearest-neighbor approach generally increases the accuracy of the imputation, especially for the first predictor. For the linear case, both predictors have high-quality imputations.\n\n\n4.3.4 Trees\nAs mentioned earlier, tree-based models are a reasonable choice for imputation techniques because many types of trees do not require complete data themselves. They generally provide good accuracy and do not extrapolate values beyond the bounds of the training data. While a single tree can be used for imputation, it will likely have low bias but high variance. Ideally, we would like imputed values to have low bias as well as low variance.\nTree ensembles, such as bagging and random forests (Chapters ?sec-trees-cls and ?sec-trees-reg), help solve this issue since they blend the predictors from many individual tree models. However, these methods can be computationally taxing for moderate- to large-sized data sets. Specifically, random forests require many trees (hundreds to thousands) to achieve a stable and reliable imputation model. This comes at the cost of a large computational footprint, which may become a challenge as the number of predictors with missing data increases; a separate model must be trained and retained for each predictor. As discussed later, bagged tree models tend to be smaller and faster than their random forest counterparts. Typically, there is a marginal loss in accuracy from using bagging instead of random forests.\nLike nearest-neighbor imputation, the bagged tree places most of the imputed values close to the original data distribution, as seen in Figure 4.7. The distribution of distances (Figure 4.8) for bagged imputation is similar to that of nearest neighbors. Figure 4.7 provides a comparison of the imputation techniques discussed here with the nonlinear simulated data and a data set in which the two groups are optimally separated by a linear boundary. In this comparison, the percentage of missing data can be adjusted to demonstrate how each imputation method performs.\nWhich imputation technique we choose depends on the problem. If a few predictors have missing values, then using \\(K\\)-NN or bagged imputation may not add much computational burden to the modeling process. However, if many predictors have missing values and the data set is large, then the model-based imputation techniques may become cumbersome. In this case, beginning with a most likely value imputation approach may be prudent. If the predictive performance of the optimally tuned machine learning technique is high, then the imputation approach was sufficient. However, if predictive performance lags, we could implement a different imputation technique while training models.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-when-to-address-missing-data",
    "href": "chapters/missing-data.html#sec-when-to-address-missing-data",
    "title": "4  Missing Data",
    "section": "4.4 When to Address Missing Data",
    "text": "4.4 When to Address Missing Data\nWhen missing values are present, it may be tempting to immediately address this problem and create a complete data set prior to beginning the modeling or data splitting processes. We recommend that imputation be done as part of the model development process as discussed in Chapter 2. In the diagram presented in Figure 2.10, imputation would occur as part of the training process. Recall, the model based imputation techniques discussed earlier have parameters that can be tuned. Understanding how these parameters affect model performance would be done during the training process, and an optimal value can be selected.\nIt is also important to understand that imputation is fundamentally a preprocessing step–we must do this before commencing model building. Therefore, we must think about where imputation should be located in the order of preprocessing steps. Specifically, imputation should occur as the first step. It is advisable to impute qualitative predictors before creating indicator variables to maintain the binary nature of the resulting data. Moreover, imputation should precede parameter estimation steps. For example, if centering and scaling are done before imputation, the resulting means and standard deviations will reflect biases and issues from the missing data. This approach ensures the integrity and accuracy of subsequent data processing and analysis stages.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#chapter-references",
    "href": "chapters/missing-data.html#chapter-references",
    "title": "4  Missing Data",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAllison, P. 2002. Missing Data. SAGE Publications, Inc.\n\n\nBarnett, V, and T Lewis. 1994. Outliers in Statistical Data. Vol. 3. 1. Wiley New York.\n\n\nBhaskaran, K, and L Smeeth. 2014. “What Is the Difference Between Missing Completely at Random and Missing at Random?” International Journal of Epidemiology 43 (4): 1336–39.\n\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nBreiman, L, J Friedman, C Stone, and RA Olshen. 1984. Classification and Regression Trees. CRC Press.\n\n\nChen, T, and C Guestrin. 2016. “Xgboost: A Scalable Tree Boosting System.” In Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining, 785–94.\n\n\nD’Agostino McGowan, L, S Lotspeich, and S Hepler. 2024. “The \"Why\" Behind Including \"Y\" in Your Imputation Model.” Statistical Methods in Medical Research 33 (6): 996–1020.\n\n\nEmmanuel, T, T Maupong, D Mpoeleng, T Semong, B Mphago, and O Tabona. 2021. “A Survey on Missing Data in Machine Learning.” Journal of Big Data 8: 1–37.\n\n\nHasan, K, A Alam, S Roy, A Dutta, T Jawad, and S Das. 2021. “Missing Value Imputation Affects the Performance of Machine Learning: A Review and Analysis of the Literature (2010-2021).” Informatics in Medicine Unlocked 27: 100799.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\nLex, A, N Gehlenborg, H Strobelt, R Vuillemot, and H Pfister. 2014. “UpSet: Visualization of Intersecting Sets.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983–92.\n\n\nLittle, R, and D Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley & Sons.\n\n\nNijman, SWJ, AM Leeuwenberg, I Beekers, I Verkouter, JJL Jacobs, ML Bots, FW Asselbergs, KGM Moons, and TPA Debray. 2022. “Missing Data Is Poorly Handled and Reported in Prediction Model Studies Using Machine Learning: A Literature Review.” Journal of Clinical Epidemiology 142: 218–29.\n\n\nQuinlan, R. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers.\n\n\nSisk, R, M Sperrin, N Peek, M van Smeden, and G Martin. 2023. “Imputation and Missing Indicators for Handling Missing Data in the Development and Deployment of Clinical Prediction Models: A Simulation Study.” Statistical Methods in Medical Research 32 (8): 1461–77.\n\n\nSperrin, M, G Martin, R Sisk, and N Peek. 2020. “Missing Data Should Be Handled Differently for Prediction Than for Description or Causal Explanation.” Journal of Clinical Epidemiology 125: 183–87.\n\n\nWebb, G, E Keogh, and R Miikkulainen. 2010. “Naı̈ve Bayes.” Encyclopedia of Machine Learning 15 (1): 713–14.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#footnotes",
    "href": "chapters/missing-data.html#footnotes",
    "title": "4  Missing Data",
    "section": "",
    "text": "Exceptions are listed below.↩︎\nHowever, note that these properties have complete values for their size column (i.e., a zero ft2 garage). ↩︎\nAlso called not missing at random (NMAR), informative missingness, or nonignorable missing data.↩︎\nSisk et al. (2023) calls this approach, when using multiple predictors for imputation, as “regression imputation.” Their definition would exclude simple “most likely value” imputations.↩︎\nUnfortunately, we may not be able to judge the accuracy of the imputation for most data sets since the missing data will never be known.↩︎\nOne approach is to generate a random uniform predictor value between zero and the limit of quantitation).↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html",
    "href": "chapters/numeric-predictors.html",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "5.1 What are Problematic Characteristics, and When Should Transformations be Applied?\nData that are available for modeling are often collected passively without the specific purpose of being used for building a predictive model. As an example, the Ames Housing data contains a wealth of information on houses in Ames, Iowa. But this available data may not contain the most relevant measurements for predicting house price. This may be due to the fact that important predictors were not measured. Or, it may be because the predictors we have collected are not in the best form to allow models to uncover the relationship between the predictors and the response.\nAs mentioned previously, feature engineering is the process of representing your predictor data so that the model has to do the least amount of work to explain the outcome effectively. A tool of feature engineering is predictor transformations. Some models also need predictors to be transformed to meet the model’s mathematical requirements (i.e., pre-processing). In this chapter we will review transformations for quantitative predictors.\nWe will begin by describing transformations that are applied to one predictor at a time that yield a revised for of the predictor (one in, one out). After these, an example of a group transformation is described called the spatial sign. Later, Chapter 7 will describe different types of many-to-many transformations such as principal component analysis (PCA) and multidimensional scaling (MDS). Additionally, in Chapter 8, we will examine techniques for expanding a single numeric predictor to many predictors (one in, many out).\nLet’s begin by understanding some general data characteristics that need to be addressed via feature engineering and when transformations should be applied.\nCommon problematic characteristics that occur across individual predictors are:\nSome models, like those that are tree-based, are able to tolerate these characteristics. However, these characteristics can detrimentally affect most other models. Techniques used to address these problems generally involve transformation parameters. For example, to place the predictors on the same scale, we would subtract the mean of a predictor from a sample and then divide by the standard deviation. This is know as standardizing and will be discussed in the next section.\nWhat data should be used to estimate the mean and standard deviation? Recall, the training data set was used to estimate model parameters. Similarly, we will use the training data to estimate transformation parameters. When the test set or any future data set are standardized, the process will use the estimates from the training data set. Any model fit that uses these standardized predictors would want new samples being predicted to have the same reference distribution.\nSuppose that a predictor column had an underlying Gaussian distribution with a sample mean estimate of 5.0 and a sample standard deviation of 1.0. Suppose a new sample has a predictor value of 3.7. For the training set, this new value lands around the 10th percentile and would be standardized to a value of -1.3. The new value is relative to the training set distribution. Also note that, in this scenario, it would be impossible to standardize using a recomputed standard deviation for the new sample (which means we try to divide with a zero standard deviation).\nMany transformations that involve a single predictor change the data distribution. Most predictive models do not place specific parametric assumptions on the predictor variables (e.g., require normality), but some distributions might facilitate better predictive performance than others.\nTODO some based on convention or scientific knowledge. Others like the arc-sin (ref The arcsine is asinine: the analysis of proportions in ecology) or logit? See issue #10.\nThe next two sections will consider two classes of transformations for individual predictors: those that resolve distributional skewness and those that convert each predictor to a common distribution (or scale).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#what-are-problematic-characteristics-and-when-should-transformations-be-applied",
    "href": "chapters/numeric-predictors.html#what-are-problematic-characteristics-and-when-should-transformations-be-applied",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "skewed or unusually shaped distributions,\nsample(s) that have extremely large or small values, and\nvastly disparate scales.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-skewness",
    "href": "chapters/numeric-predictors.html#sec-skewness",
    "title": "5  Transforming Numeric Predictors",
    "section": "5.2 Resolving asymmetry and skewness",
    "text": "5.2 Resolving asymmetry and skewness\nAn asymmetric statistical distribution is one in which the probability of a sample occurring is not symmetric around the center of the distribution (e.g., the mean). For example, Figure 5.1 (panel a) shows the training set distribution of the lot area of houses in Ames. There is a much higher likelihood of the lot area being lower than the mean (or median) lot size. There are fewer large lots than there are proportionally smaller lots. And, in a few cases, the lot sizes can be extremely large.\nThe skew of a distribution indicates the direction and magnitude of the asymmetry. It can be quantified using the skewness statistic:\n\\[\\begin{align}\n  skewness &= \\frac{1}{(n-1)v^{3/2}} \\sum_{i=1}^n (x_i-\\overline{x})^3 \\notag \\\\\n  \\text{where}\\quad  v &= \\frac{1}{(n-1)}\\sum_{i=1}^n (x_i-\\overline{x})^2 \\notag\n\\end{align}\n\\]\nwhere values near zero indicate a symmetric distribution, positive values correspond a right skew, and negative values left skew. The lot size data are significantly right-skewed (with a skewness value of 13.5). As previously mentioned, there are 2 samples in the training set that sit far beyond the mainstream of the data.\n\n\n\n\n\n\n\n\nFigure 5.1: Lot area for houses in Ames, IA. The raw data (a) are shown along with transformed versions using the Yeo-Johnson (b), percentile (c), and ordered quantile normalization (d) transformations.\n\n\n\n\n\nOne might infer that “samples far beyond the mainstream of the data” is synonymous with the term “outlier”; The Cambridge dictionary defines an outlier as\n\na person, thing, or fact that is very different from other people, things, or facts […]\n\nor\n\na place that is far from the main part of something\n\nThese statements imply that outliers belong to a different distribution than the bulk of the data. For example, a typographical error or an incorrect merging of data sources could be the cause.\nThe Croarkin et al. (2012) describes them as\n\nan observation that lies an abnormal distance from other values in a random sample from a population\n\nIn our experience, researchers are quick to label (and discard) extreme data points as outliers. Often, especially when the sample size is not large, these data points are not abnormal but belong to a highly skewed distribution. They are ordinary in a distributional sense. That is the most likely case here; some houses in Ames have very large lot areas, but they certainly fall under the definition of “houses in Ames, Iowa.” These values are genuine, just extreme.\nThis, by itself, is okay. However, suppose that this column is used in a calculation that involves squaring values, such as Euclidean distance or the sample variance. Extreme values in a skewed distribution can influence some predictive models and cause them to place more emphasis on these predictors1. When the predictor is left in its original form, the extreme samples can end up degrading a model’s predictive performance.\nOne way to resolve skewness is to apply a transformation that makes the data more symmetric. There are several methods to do this. The first is to use a standard transformation, such as logarithmic or the square root, the latter being a better choice when the skewness is not drastic, and the data contains zeros. A simple visualization of the data can be enough to make this choice. The problem is when there are many numeric predictors; it may be inefficient to visually inspect each predictor to make a subjective judgment on what if any, transformation function to apply.\nBox and Cox (1964) defined a power family of transformations that use a single parameter, \\(\\lambda\\), for different methods:\n\n\n\n\n\nno transformation via \\(\\lambda = 1.0\\)\nsquare (\\(x^2\\)) via \\(\\lambda = 2.0\\)\nsquare root (\\(\\sqrt{x}\\)) via \\(\\lambda = 0.5\\)\n\n\n\nlogarithmic (\\(\\log{x}\\)) via \\(\\lambda = 0.0\\)\ninverse square root (\\(1/\\sqrt{x}\\)) via \\(\\lambda = -0.5\\)\ninverse (\\(1/x\\)) via \\(\\lambda = -1.0\\)\n\n\n\n\n\nand others in between. The transformed version of the variable is:\n\\[\nx^* =\n\\begin{cases} \\lambda^{-1}(x^\\lambda-1) & \\text{if $\\lambda \\ne 0$,}\n\\\\[3pt]\nlog(x) &\\text{if $\\lambda = 0$.}\n\\end{cases}\n\\]\nTheir paper defines this as a supervised transformation of a non-negative outcome (\\(y\\)) in a linear regression model. They find a value of \\(\\lambda\\) that minimizes the residual sums of squared errors. In our case, we can co-opt this method to use for unsupervised transformations of non-negative predictors (in a similar manner as Asar, Ilk, and Dag (2017)). Yeo and Johnson (2000) extend this method by allowing the data to be negative via a slightly different transformation:\n\\[\nx^* =\n\\begin{cases}\n\\lambda^{-1}\\left[(x + 1)^\\lambda-1\\right] & \\text{if $\\lambda \\ne 0$ and $x \\ge 0$,} \\\\[3pt]\nlog(x + 1) &\\text{if $\\lambda = 0$ and $x \\ge 0$.} \\\\[3pt]\n-(2 - \\lambda)^{-1}\\left[(-x + 1)^{2 - \\lambda}-1\\right] & \\text{if $\\lambda \\ne 2$ and $x &lt; 0$,} \\\\[3pt]\n-log(-x + 1) &\\text{if $\\lambda = 2$ and $x &lt; 0$.}\n\\end{cases}\n\\]\nIn either case, maximum likelihood is also used to estimate the \\(\\lambda\\) parameter.\nIn practice, these two transformations might be limited to predictors with acceptable density. For example, the transformation may not be appropriate for a predictor with a few unique values. A threshold of five or so unique values might be a proper rule of thumb (see the discussion in ?sec-near-zero-var). On occasion the maximum likelihood estimates of \\(\\lambda\\) diverge to huge values; it is also sensible to use values within a suitable range. Also, the estimate will never be absolute zero. Implementations usually apply a log transformation when the \\(\\hat{\\lambda}\\) is within some range of zero (say between \\(\\pm 0.01\\))2.\nFor the lot area predictor, the Box-Cox and Yeo-Johnson techniques both produce an estimate of \\(\\hat{\\lambda} = 0.15\\). The results are shown in Figure 5.1 (panel b). There is undoubtedly less right-skew, and the data are more symmetric with a new skewness value of 0.114 (much closer to zero). However, there are still outlying points.\nThere are numerous other transformations that attempt to make the distribution of a variable more Gaussian. Table 5.1 shows several more, most of which are indexed by a transformation parameter \\(\\lambda\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nEquation\nSource\n\n\n\n\nModulus\n\\[x^* = \\begin{cases} sign(x)\\lambda^{-1}\\left[(|x|+1)^\\lambda-1\\right] & \\text{if $\\lambda \\neq 0$,}\\\\[3pt]\nsign(x) \\log{(|x|+1)} &\\text{if $\\lambda = 0$}\n\\end{cases}\\]\nJohn and Draper (1980)\n\n\nBickel-Docksum\n\\[x^* = \\lambda^{-1}\\left[sign(x)|x| - 1\\right]\\quad\\text{if $\\lambda \\neq 0$}\\]\nBickel and Doksum (1981)\n\n\nGlog / Gpower\n\\[x^* = \\begin{cases} \\lambda^{-1}\\left[({x+ \\sqrt{x^2+1}})^\\lambda-1\\right]  & \\text{if $\\lambda \\neq 0$,}\\\\[3pt]\n\\log({x+ \\sqrt{x^2+1}}) &\\text{if $\\lambda = 0$}\n\\end{cases}\\]\nDurbin et al. (2002), Kelmansky, Martínez, and Leiva (2013)\n\n\nNeglog\n\\[x^* = sign(x) \\log{(|x|+1)}\\]\nWhittaker, Whitehead, and Somers (2005)\n\n\nDual\n\\[x^* = (2\\lambda)^{-1}\\left[x^\\lambda - x^{-\\lambda}\\right]\\quad\\text{if $\\lambda \\neq 0$}\\]\nYang (2006)\n\n\n\n\n\nTable 5.1: Examples of other families of transformations for dense numeric predictors.\n\n\n\n\n\n\n\nSkewness can also be resolved using techniques related to distributional percentiles. A percentile is a value with a specific proportion of data below it. For example, for the original lot area data, the 0.1 percentile is 4,726 square feet, which means that 10% of the training set has lot areas less than 4,726 square feet. The minimum, median, and maximum are the 0, 50th and 100th percentiles, respectively.\nNumeric predictors can be converted to their percentiles, and these data, inherently between zero and one, are used in their place. Probability theory tells us that the distribution of the percentiles should resemble a uniform distribution. This results from the transformed version of the lot area shown in Figure 5.1 (panel c). For new data, values beyond the range of the original predictor data can be truncated to values of zero or one, as appropriate.\nAdditionally, the original predictor data can be coerced to a specific probability distribution. Peterson and Cavanaugh (2020) define the Ordered Quantile (ORQ) normalization procedure. It estimates a transformation of the data to emulate the true normalizing function where “normalization” literally maps the data to a standard normal distribution. In other words, we can coerce the original distribution to a near exact replica of a standard normal. Figure 5.1 (panel d) illustrates the result for the lot area. In this instance, the resulting distribution is precisely what would be seen if the true distribution was Gaussian with zero mean and a standard deviation of one.\nIn Section 5.4 below, another tool for attenuating outliers in groups of predictors is discussed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-common-scale",
    "href": "chapters/numeric-predictors.html#sec-common-scale",
    "title": "5  Transforming Numeric Predictors",
    "section": "5.3 Standardizing to a common scale",
    "text": "5.3 Standardizing to a common scale\nAnother goal for transforming individual predictors is to convert them to a common scale. This is a pre-processing requirement for some models. For example, a K-nearest neighbors model computes the distances between data points. Suppose Euclidean distance is used with the Ames data. One predictor, the year a house was built, has training set values ranging between 1872 and 2010. Another, the number of bathrooms, ranges from 0 to 5. If these raw data were used to compute the distance, the value would be inappropriately dominated by the year variable simply because its values were large. See TODO appendix for a summary of which models require a common scale.\nThe previous section discussed two transformations that automatically convert predictors to a common distribution. The percentile transformation generates values roughly uniformly distributed on the [0, 1] scale, and the ORQ transformation results in predictors with standard normal distributions. However, two other standardization methods are commonly used.\nFirst is centering and scaling (as previously mentioned). To convert to a common scale, the mean (\\(\\bar{x}\\)) and standard deviation (\\(\\hat{s}\\)) are computed from the training data and the standardized version of the data are \\(x^* = (x - \\bar{x}) / \\hat{s}\\). The shape of the original distribution is preserved; only the location and scale are modified to be zero and one, respectively.\nIn Section 6.2, methods are discussed to convert categorical predictors to a numeric format. The standard tool is to create a set of columns consisting of zeros and ones called indicator or dummy variables. When centering and scaling, what should we do with these binary features? These should be treated the same as the dense numeric predictors. The result is that a binary column will still have two unique values, one positive and one negative. The values will depend on the prevalence of the zeros and ones in the training data. While this seems awkward, it is required to ensure each predictor has the same mean and standard deviation. Note that if the predictor set is only scaled, Gelman (2008) suggests that the indicator variables be divided by two standard deviations instead of one.\nFigure 5.2(b) shows the results of centering and scaling the gross living area predictor from the Ames data. Note that the shape of the distribution does not change; only the magnitude of the values is different.\n\n\n\n\n\n\n\n\nFigure 5.2: The original gross living area data and two standardized versions.\n\n\n\n\n\nAnother common approach is range standardization. Based on the training set, a predictor’s minimum and maximum values are computed, and the data are transformed to a [0, 1] scale via\n\\[\nx^* = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\]\nWhen new data are outside the training set range, they can either be clipped to zero/one or allowed to go slightly beyond the intended range. The nice feature of this approach is that the range of the raw numeric predictors matches the range of any indicator variables created from previously categorical predictors. However, this does not imply that the distributional properties are the same (e.g., mean and variance) across predictors. Whether this is an issue depends on the model being used downstream. Figure 5.2(c) shows the result when the gross living predictor is range transformed. Notice that the shape of the distributions across panels (a), (b), and (c) are the same — only the scale of the x-axis changes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-spatial-sign",
    "href": "chapters/numeric-predictors.html#sec-spatial-sign",
    "title": "5  Transforming Numeric Predictors",
    "section": "5.4 Spatial Sign",
    "text": "5.4 Spatial Sign\nSome transformations involve multiple predictors. An upcoming chapter describes a specific class of simultaneous feature extraction transformations. Here, we will focus on the spatial sign transformation (Serneels, Nolf, and Espen 2006). This method, which requires \\(p\\) standardized predictors as inputs, projects the data points onto a \\(p\\) dimensional unit hypersphere. This makes all of the data points equally distant from the center of the hypersphere, thereby eliminating all potential outliers. The equation is:\n\\[\nx^*_{ij}=\\frac{x_{ij}}{\\sum\\limits^{p}_{j=1} x_{ij}^2}\n\\]\nNotice that all of the predictors are simultaneously modified and that the calculations occur in a row-wise pattern. Because of this, the individual predictor columns become combinations of the other columns and now reflect more than the individual contribution of the original predictors. In other words, after this transformation is applied, if any individual predictor is considered important, its significance should be attributed to all of the predictors used in the transformation.\nFigure 5.3 shows predictors from the Ames data. In these data, we somewhat arbitrarily labeled 29 samples as being “far away” from most of the data in either lot area and/or gross living area. Each of these predictors may follow a right-skewed distribution, or there is some other characteristic that is associated with these samples. Regardless, we would like to transform these predictors simultaneously.\nThe second panel of the data shows the same predictors after an orderNorm transformation. Note that, after this operation, the outlying values appear less extreme.\n\n\n\n\n\n\n\n\nFigure 5.3: Lot area (x) versus gross living area (y) in raw format as well as with order-norm and spatial sign transformations.\n\n\n\n\n\nThe panel on the right shows the data after applying the spatial sign. The data now form a circle centered at (0, 0) where the previously flagged instances are no longer distributionally abnormal. The resulting bivariate distribution is quite jarring when compared to the original. However, these new versions of the predictors can still be important components in a machine-learning model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#chapter-references",
    "href": "chapters/numeric-predictors.html#chapter-references",
    "title": "5  Transforming Numeric Predictors",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAsar, O, O Ilk, and O Dag. 2017. “Estimating Box-Cox Power Transformation Parameter via Goodness-of-Fit Tests.” Communications in Statistics-Simulation and Computation 46 (1): 91–105.\n\n\nBickel, P, and K Doksum. 1981. “An Analysis of Transformations Revisited.” Journal of the American Statistical Association 76 (374): 296–311.\n\n\nBox, GEP, and D Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society. Series B (Methodological), 211–52.\n\n\nCroarkin, C, P Tobias, J Filliben, B Hembree, W Guthrie, L Trutna, and J Prins, eds. 2012. NIST/SEMATECH e-Handbook of Statistical Methods. NIST/SEMATECH. http://www.itl.nist.gov/div898/handbook/.\n\n\nDurbin, B, J Hardin, D Hawkins, and D Rocke. 2002. “A Variance-Stabilizing Transformation for Gene-Expression Microarray Data.” Bioinformatics 18.\n\n\nGelman, A. 2008. “Scaling Regression Inputs by Dividing by Two Standard Deviations.” Statistics in Medicine 27 (15): 2865–73.\n\n\nJohn, J, and N Draper. 1980. “An Alternative Family of Transformations.” Journal of the Royal Statistical Society Series C: Applied Statistics 29 (2): 190–97.\n\n\nKelmansky, D, E Martínez, and V Leiva. 2013. “A New Variance Stabilizing Transformation for Gene Expression Data Analysis.” Statistical Applications in Genetics and Molecular Biology 12 (6): 653–66.\n\n\nPeterson, R, and J Cavanaugh. 2020. “Ordered Quantile Normalization: A Semiparametric Transformation Built for the Cross-Validation Era.” Journal of Applied Statistics 47 (13-15): 2312–27.\n\n\nSerneels, S, E De Nolf, and P Van Espen. 2006. “Spatial Sign Preprocessing: A Simple Way to Impart Moderate Robustness to Multivariate Estimators.” Journal of Chemical Information and Modeling 46 (3): 1402–9.\n\n\nWhittaker, J, C Whitehead, and M Somers. 2005. “The Neglog Transformation and Quantile Regression for the Analysis of a Large Credit Scoring Database.” Journal of the Royal Statistical Society Series C: Applied Statistics 54 (5): 863–78.\n\n\nYang, Z. 2006. “A Modified Family of Power Transformations.” Economics Letters 92 (1): 14–19.\n\n\nYeo, I, and R Johnson. 2000. “A New Family of Power Transformations to Improve Normality or Symmetry.” Biometrika 87 (4): 954–59.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#footnotes",
    "href": "chapters/numeric-predictors.html#footnotes",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "The field of robust techniques is predicated on making statistical calculations insensitive to these types of data points.↩︎\nIf you’ve never seen it, the “hat” notation (e.g. \\(\\hat{\\lambda}\\)) indicates an estimate of some unknown parameter.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html",
    "href": "chapters/categorical-predictors.html",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "6.1 Example Data: Daily Hotel Rates\nThis chapter will discuss tools for handling predictor variables that are not measured on a strict numeric scale. For example, the building type and neighborhood variables are qualitative in the Ames housing data. The possible values of qualitative predictors are often called categories or levels (which will be used interchangeably here).\nWhy would we need to modify this type of predictor? First, most mathematical models require information to be numbers. We’ll need a method to convert qualitative data effectively to some quantitative format. This is an encoding problem; we are finding another representation of our predictor data. There are various tools for converting qualitative data, ranging from simple indicator variables (a.k.a. dummy variables) to more statistically sophisticated tools. These are the focus of this chapter.\nSecond, there are other circumstances where simplifying or transforming categorical predictors might be advantageous. These are usually related to exceptional situations, such as when there are many possible values (i.e., high cardinality) or infrequently occurring values. Techniques to address these issues are also discussed.\nAs implied above, some modeling techniques can take the categorical predictors in their natural form. These include tree- or rule-based models, naive Bayes models, and others. In this book’s third and fourth parts, we’ll discuss how certain models naturally handle categorical variables in their respective sections.\nTo begin, the primary data set used in this chapter is introduced.\nAntonio, de Almeida, and Nunes (2019) describe a data set with booking records from two Portuguese hotels. There are numerous variables in the data related to the\nIn addition, there are several qualitative variables: customer type, market segment, room type, etc.\nWe’ll analyze these data in various ways (especially in ?sec-reg-linear to ?sec-reg-summary where they are used as a case study). Our goal is to predict the average daily rate (ADR, i.e., the average cost in euros) from one of the two hotels in the original data.\nThere were 15,402 bookings for the resort hotel. An initial two-way split was used:\nThis chapter will use these data to demonstrate how qualitative data, such as the booking agent’s name or company, can be converted into quantitative information that many models require.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-hotel-rates",
    "href": "chapters/categorical-predictors.html#sec-hotel-rates",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "Reservation: room type, date, meal plan, etc.,\nBooking method: agent, distribution channel, etc.,\nCustomer: type, number of children staying, country of origin, etc.\n\n\n\n\n\nThe training set contains 11,551 (75%) bookings whose arrivals are between 2016-07-02 and 2017-05-15.\nThe remaining 25% were used as the test set with 3,851 bookings (between 2017-05-15 to 2017-08-31).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-indicators",
    "href": "chapters/categorical-predictors.html#sec-indicators",
    "title": "6  Working with Categorical Predictors",
    "section": "6.2 Simple Indicator Variables",
    "text": "6.2 Simple Indicator Variables\nAs a simple example, consider the customer type predictor with categories: “contract”, “group”, “transient”, and “transient (party)”. What would we do if a model required these data to be numeric? There is no natural ordering to these values, so giving the models integer values (e.g., contract = 1.0, group = 2.0, etc.) would be erroneous since that implies a scale for these values. Instead, it is common to create multiple binary columns (numeric with zeros or ones) representing the occurrence of specific values. These are called indicator columns or sometimes dummy variables.\nTable 6.1 shows how this works for the customer type. The rows depict the possible values in the data, while the columns are the resulting features used in place of the original column. This table uses the most common indicator encoding method called reference cell parameterization (also called a treatment contrast). First, a category is chosen as the reference value. In Table 6.1, the first alpha-numeric value is used (\"contract\"), but this is an arbitrary choice. After this, we create separate columns for all possible values except for the reference value. Each of these columns has a value of one when the data matches the column for that value (and is zero otherwise).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer type\n\nIndicator Columns\n\n\n\ngroup\ntransient\ntransient party\n\n\n\n\ncontract\n0\n0\n0\n\n\ngroup\n1\n0\n0\n\n\ntransient\n0\n1\n0\n\n\ntransient party\n0\n0\n1\n\n\n\n\n\n\n\n\nTable 6.1: Indicator columns produced from a categorical column using a reference cell parameterization.\n\n\n\n\nThe rationale for excluding one column is that you can infer the reference value if you know the values of all of the existing indicator columns1. Including all possible indicator columns embeds a redundancy in the data. As we will see shortly, data that contain this type of redundancy pose problems for some models like linear regression.\nBefore moving on, let’s examine how indicator columns shown in Table 6.1 affect an example data analysis. Suppose that we are modeling a numeric outcome using a linear regression model (discussed in ?sec-reg-linear) of the form:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_4x_{i4} + \\epsilon_i\n\\tag{6.1}\\]\nwhere the \\(\\beta\\) values are the parameters to be estimated and, in this application, the \\(x_{ij}\\) are the indicator columns shown in Table 6.1. The intercept term (\\(\\beta_0\\)) corresponds to a column of all ones. It is almost always the case that we include the intercept term in the model.\nTable 6.2 shows the encoding in Table 6.1 and adds columns for a numeric outcome and the intercept term. The outcome column shows the average daily rate (in €). Using a standard estimation procedure (called ordinary least squares), the bottom row of Table 6.2 shows the 4 parameter estimates. Since all of the indicators for the contract customer row are zero, the intercept column estimates the mean value for that level (\\(\\widehat{\\beta}_0\\) = 73). The variable \\(x_{i1}\\) only has an indicator for the “group” customers. Hence, its estimate corresponds to the difference in the average group outcome values (63.8) minus the effect of the reference cell: 73 - 63.8. From this, the resulting estimate (\\(\\widehat{\\beta}_1\\) = -9.23) is the effect of the group customers above and beyond the impact of the contract customers. The parameter estimates for the other possible values follow analogous interpretations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADR\n\nModel Terms\n\n\n\n(Intercept)\ngroup\ntransient\ntransient_party\n\n\n\n\ncontract\n73.0\n1\n0\n0\n0\n\n\ngroup\n63.8\n1\n1\n0\n0\n\n\ntransient\n94.6\n1\n0\n1\n0\n\n\ntransient party\n76.4\n1\n0\n0\n1\n\n\nestimate (€):\n\n\n73.0\n-9.2\n21.6\n3.4\n\n\n\n\n\n\n\n\nTable 6.2: An example of linear regression parameter estimates corresponding to a reference cell parameterization.\n\n\n\n\nAnother popular method for making indicator variables is called one-hot encoding (also known as a cell means encoding). This technique, shown in Table 6.3, makes indicators for all possible levels of the predictor and does not show an intercept column (for reasons described shortly). In this model parameterization, indicators are specific to each value in the data, and the linear regression estimates are the average response values for each customer type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADR\n\nModel Terms\n\n\n\ncontract\ngroup\ntransient\ntransient_party\n\n\n\n\ncontract\n73.0\n1\n0\n0\n0\n\n\ngroup\n63.8\n0\n1\n0\n0\n\n\ntransient\n94.6\n0\n0\n1\n0\n\n\ntransient party\n76.4\n0\n0\n0\n1\n\n\nestimate (€):\n\n\n73.0\n63.8\n94.6\n76.4\n\n\n\n\n\n\n\n\nTable 6.3: One-hot encoded indicator variables from a categorical column of data.\n\n\n\n\nOne-hot encodings are often used in nonlinear models, especially in neural networks and tree-based models. Indicators are not generally required for the latter but can be used2.\nOne issue with creating indicator variables from categorical predictors is the potential for linear dependencies. This occurs when two or more sets of predictors have a mathematically linear relationship with one another. For example, suppose that each agent only worked with a single customer type. If we were to create indicator variables for the agent names and the customer type, there would be a redundancy: if we know the agent, we know the customer type. Mathematically, the row-wise sum of the agent name indicators would equal the row-wise sum of the position indicators. For some types of predictive models, including linear or logistic regression, the linear dependencies would cause the mathematical estimation of the model parameters to fail.\nAn example of this is the one-hot encoding method shown in Table 6.3. If we were to include an intercept term with a one-hot encoding, the row-wise sum of the indicator columns equals the intercept column. The implication is that one of the model terms is not “estimable.” For this reason, this encoding method is mainly used with models that do not use specific linear algebra operations (e.g., matrix inversion) to estimate parameters. Some tools can identify linear combinations (Elswick et al. 1991), and from this information, the smallest possible set of columns can be found to delete and thereby eliminate the linear dependencies.\nNote that linear dependencies are exact linear relationships between predictors. There are other circumstances where two or more predictors are highly correlated but not perfectly related. This situation, called multicollinearity, is discussed throughout the book but mostly in ?sec-colinearity. Multicollinearity can be a problem for many models and needs to be resolved prior to modeling.\nIn addition to reference cell and one-hot encodings, there are various tools for creating indicator columns. Some, called contrast methods, are associated with more niche strategies, and many produce fractional values instead of binary indicators. For example, Helmert contrasts (Ruberg 1989) are configured so that each category is compared to the average of the preceding categories. No encoding method is uniformly better than the others; the choice is often driven by how the user will interpret the parameter estimates.\nThe situation becomes slightly more complex when a model contains multiple categorical predictors. The reference value combines levels across predictors for reference value parameterizations. For example, if a model included the customer’s country of residence, the reference would include that factor. Suppose Canadians were the reference nationality. Then the overall reference cell would be Canadian contract customers.\nNote that these encoding techniques are basically look-up tables that map individual categories known in the training set to specific numeric values. As discussed in the next section, mapping categories can be problematic when novel values are seen in new data (such as the test set).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-novel-categories",
    "href": "chapters/categorical-predictors.html#sec-novel-categories",
    "title": "6  Working with Categorical Predictors",
    "section": "6.3 Novel Categories",
    "text": "6.3 Novel Categories\nWhen creating indicator variables, there is the assumption that the categories in the training data reflect all possible categories. This may not be the case. For example, the agents observed in our data are not the only agents that will be booking rooms in the future. After we create a model with a specific set of agents, what happens when we predict future data that involves new agents?\nOne approach we could take is to prospectively create a new value for the predictor (let’s give it a value of \"new\"). When a future sample has an agent that was not in the original data, we could assign this sample to the general category so that the model can anticipate this new level. In general, this won’t help since the model estimation process will never have any “new” values on hand when the model is fit. For some models, this is innocuous and has no real harm (but offers no benefit either).\nLet’s revisit linear regression results from the reference cell parameterization again. Suppose there is a new customer type of “influencer.” Our reference cell encoding method could add an indicator column for that customer type, but since no influencers currently exist in the training data, it will be a column filled with zeros. We can call this a zero-variance predictor since it has a single unique value. It is entirely non-informative. For linear regression, this column of zeros creates a linear dependency with the intercept column because all rows have the same numeric value. Most regression software would detect zero-variance columns and remove them before the model is fit.\nSince there would be no influencer column in the data, all of the indicator columns would be zero when an influencer was observed in new data being predicted. However, from Table 6.2 we know that such a pattern would map to the reference level (which is the \"contract\" customer in our data). The new levels would be mapped to the reference level for this parameterization. This would occur more by happenstance and would not be a deliberate strategy.\nTraditional indicator encoding methods should be avoided if new predictor categories are anticipated. Instead, the techniques used in the next section (effect encodings or feature hashing) will likely be more effective at dealing with this situation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-high-cardinality",
    "href": "chapters/categorical-predictors.html#sec-high-cardinality",
    "title": "6  Working with Categorical Predictors",
    "section": "6.4 Predictors with Many Values",
    "text": "6.4 Predictors with Many Values\nSome predictors have high cardinality; they can take many possible values relative to the number of data points. For example, in the entirety of the hotel rate data, there are 122 unique agents (see Figure 6.1). Should we create so many indicator variables for this predictor? In the data, 49 agents had fewer than six bookings. This means there will be many sparse columns with almost all zero entries. Additionally, 71 of the agents have no data in the training set; they only occur in the test set. Their indicator columns would contain all zeros.\n\n\n\n\n\n\n\n\nFigure 6.1: The frequency distribution of agents in the training set.\n\n\n\n\n\nThis section will consider several methods for effectively representing high cardinality predictors. The first two techniques are unsupervised (i.e., they do not use the outcome), while the others are supervised. Cerda and Varoquaux (2022) surveys different methods for dealing with these types of predictors.\n\n6.4.1 “Othering”\nIt is possible to isolate infrequently occurring categories into a new “other” category. For example, there are 49 agents with fewer bookings than 0.05% of the training set. It may make sense to group these agents into their own categories as long as the predictive ability of the variable is maintained. There is no magic number for the training set frequency cutoff that decides which categories to pool. This pre-processing value requires optimization since each data set will have its own nuances with infrequent categories.\n\n\n6.4.2 Feature Hashing\nFeature hashing (Weinberger et al. 2009) is a technique where the user specifies how many indicator columns should be used to represent the data. Using cryptographic principles, an algorithm assigns each row to an indicator column. This assignment only uses the value of the category in the calculations and, unlike the traditional methods discussed in Section 6.2, does not require a dictionary that maps pre-defined categories to indicator columns.\nSuppose our agent column had a value of \"nobody owens\". The algorithm translates this value to a fixed-length string using a hash function. Hash functions are designed to be non-random and, effectively, irreversible. Once the original value is converted to the hash value, it is very difficult to reverse engineer. Also, the frequency distributions of the hash values are designed to appear uniform in distribution, as seen below.\nThe hashed value of the predictor’s level can be converted to an integer, and, using modular arithmetic, the integer is mapped to a feature column. Suppose that we create 256 hashing features and nobody owens maps to an integer of 123,456. To assign this agent to one of 256 feature columns, we compute 123456 mod 256 = 64. For this row of data, we put a value of one in column 64.\nPresumably, the number of possible categories is larger than the number of indicator columns we request. This means multiple predictor values will map to the same feature column. In cryptography, this is called a collision, the same as aliasing in statistical experimental design (Box, Hunter, and Hunter 1978). There is a trade-off between the number of feature columns and the number of collisions. As the number of columns increases, the probability of collisions decreases. However, the time required to train a model increases as the number of columns increases. Also, it is entirely possible that there are predictor columns where no corresponding categories, yielding all zeros.\nTo reduce collisions, the hashing function can produce integers that can be both positive and negative. When assigning the integer to a feature column, we can assign a value of -1 when the hashed integer value is negative. In this way, columns can have values of -1, 0, or 1. This means fewer collisions.\nTable 6.4 shows the results for several agents3 when ten signed feature hash columns are requested. The last row of the table shows the rate of non-zero values in each column. The average density is close to 10%, demonstrating that the hash function produces frequencies that emulate a uniform distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgent\n\nFeatures from Hashing\n\n\n\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n\n\n\n\nAaron Marquez\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n\n\nAlexander Drake\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n\n\nAllen Her\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n\n\nAnas El Bashir\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n\n\n:\n:\n:\n:\n:\n:\n:\n:\n:\n:\n:\n\n\nYa Eesh El Sadiq\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nYoonus Al Sultana\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nYoujia Tsuchiya\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nZachary Leedholm\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nDensity (%)\n10.7\n13.1\n7.4\n13.1\n7.4\n6.6\n9.8\n10.7\n9.8\n11.5\n\n\n\n\n\n\n\n\nTable 6.4: Signed indicators for agent via feature hashing.\n\n\n\n\nThe main downside of this method is that the use of hash values makes it impossible to explain the model. If the tenth feature column is critical, we can’t explain why this is the case for new data (since the hash function is practically non-reversible and may include collisions). This may be fine if the primary objective is prediction rather than interpretation. When the goal is to optimize predictive performance, then the number of hashing columns to use can be included as a tuning parameter. The model tuning process can then determine an optimal value of the number of hashing columns.\nOne unanticipated side effect of using feature hashing is that it might remove linear dependencies with other columns. This is because hashing distributes the predictor levels to hashing columns roughly uniformly. There is a high probability that this eliminates any existing relationship between other predictor values.\n\n\n6.4.3 Effect Encodings\nWe can also use a different supervised tool called effect encoding4 (Micci-Barreca 2001; Pargent et al. 2022). Recall the discussion of one-hot encodings for indicator variables. Table 6.3 showed that the model terms for such an encoding estimate the cell means for each category (when the outcome is numeric). For example, the effect on the outcome for contract customers in that table was 73€.\nEffect encodings use a simple, separate model to estimate the effect of each predictor category on the outcome and then use that value in place of the original data. For the example in Table 6.3, the rows corresponding to the group customers would be replaced with 63.8, transient customers with 94.6, and so on. Instead of making multiple binary columns out of the categorical predictor, a single numeric column would take its place.\nThis may seem somewhat problematic though; we are using the training data to estimate the effect of a predictor, putting the resulting effects back into the data, and using our primary model to (re)estimate the effect of the predictor. Since the same data are used, is this a circular argument? Also, there should be a worry about data quality. As previously mentioned, the number of bookings per agent can vary greatly. For example:\n\nAgent Chelsea Wilson had a single booking with an associated ADR of 193.6€. Using this single data point as the effect estimate is unsatisfactory since we suspect that the estimated ADR would not stay the same if the number of total bookings were larger5.\nAgent Alexander Drake had 967 bookings with a simple mean ADR estimate of 120.3€. We would judge the quality of this agent’s data to be better than cases with a handful of bookings.\n\nHow well can we estimate an effect if there are only one or two bookings for an agent in the data set?\nBoth of these concerns can be addressed by how we estimate the effect of the predictor. Recall that Table 6.3 showed how simple linear regression was used to calculate the mean for each customer type. The way that the indicator variables impact the parameter estimates is such that the rows for each customer type are the only data used to estimate that customer type’s effect. For example, the contract data do not directly affect the parameter estimate for the transient customers, and so on. This is often called a no pooling method of estimating parameters. In this context, “pooling” means combining data across multiple categories to estimate parameters.\nWe could compute the naive ADR estimate using sample means and produce a similar table. However, we can create better mean estimates for each agent by considering the data from all agents using a technique called partial pooling (Gelman et al. 2013; Johnson, Ott, and Dogucu 2022, chap. 15).\n\nIn the following two sub-sections, we’ll describe this technique in detail for cases where the outcome is a numeric value assumed to be normally distributed or or non-Gaussian. These sections will rely more on probability theory and statistical details than most others.\n\n\nNumeric outcomes\nFor our ADR analysis, the outcome is a quantitative value. A linear regression model (see ?sec-linear-regression) is the most commonly used approach to model this type of outcome. Linear regression models the outcome as a function of a slope and intercept model. For our example, where we only consider estimating the effect of the agents using indicator columns, the model is:\n\\[\ny_{ij} =\\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_{121}x_{i 121} + \\epsilon_{ij} = \\mu_i  + \\epsilon_{ij}\n\\]\nwhere \\(y_{ij}\\) is the outcome (in this case, the ADR of booking \\(i\\) by agent \\(j\\)), \\(x_{ij}\\) is an indicator for reservation \\(i\\) from agent \\(j\\), \\(\\beta_0\\) is the estimated outcome of the reference cell, and we assume \\(\\epsilon_i \\sim N(0, \\sigma^2_\\epsilon)\\). This leads to the assumption that \\(y_{ij} \\sim N(\\mu_i, \\sigma^2_\\epsilon)\\). The \\(\\hat{\\beta}_{j}\\) parameters are the effect of the agents above and beyond the reference cell effect. The index \\(j\\) is from 1 to 121 since there were 122 agents in the training set with at least one booking.\nAgain, this approach is called no pooling since it only uses each agent’s data to estimate their effect. It treats the agent as a known set of values and cannot generalize to agents outside of those in the training set6.\nPartial pooling takes the viewpoint that the agents in the training set are a sample of a population of agents. From this, we might posit a probability distribution should be assigned to the agent-to-agent effects. Since the range of the \\(\\beta\\) parameters is the real line, we might propose a random intercept model:\n\\[\ny_{ij} = { \\color{darkseagreen} \\underbrace{\\color{black} (\\beta_0 + \\beta_{0j})}_{\\text{intercept(s)}} }  + \\epsilon_{ij} = \\mu_j + \\epsilon_{ij}\n\\tag{6.2}\\]\nwhere the \\(\\beta_{0j}\\) are per-agent differences from the population effect (\\(\\beta_{0}\\)) (\\(j = 1 \\ldots 122\\) this time). Note that the model has no \\(x_{ij}\\) terms. This is simply a notation difference between the models since most methods that estimate this model do not treat that variable as a traditional regression covariate.\nSeveral techniques can fit this model. For a linear mixed model approach to fitting a random intercept model (Laird and Ware 1982; Demidenko 2013), we assume that the model parameters have normal distributions with \\(\\beta_{0} \\sim N(\\beta, \\sigma^2)\\) and \\(\\beta_{0j} \\sim N(0, \\sigma_0^2)\\) and that residuals (\\(\\epsilon_{ij}\\)) follow a Gaussian distribution. When used in Bayesian analysis, such distributions on parameters are referred to as prior distributions. We’ll stick with this terminology for Bayesian and non-Bayesian techniques. There are many strategies for estimating this model type, including maximum likelihood, restricted maximum likelihood, or Bayesian methods. See Stroup (2012) or Gelman et al. (2013) for more specifics.\nHowever, we might gain some intuition if we pause and consider a less complex version of this model. For simplicity, we’ll also make the completely unrealistic assumptions that all agents had the same number of bookings (\\(n_j = n\\)) and that we know that \\(\\sigma^2_\\epsilon = 1\\). From this, we can also say that sample means of the outcome data for each category are also normal with \\(\\bar{y}_{j} \\sim N(\\mu_i, 1/n)\\). These assumptions greatly simplify the math required to produce a formula for the estimates of the per-category means:\n\\[\n\\hat{y}_j = \\mu_0 + (\\bar{y}_j - \\mu_0) \\frac{\\sigma^2_0}{\\frac{1}{n} + \\sigma^2_0}\n\\tag{6.3}\\]\nThere are a few aspects of this equation7 to note. First, the effect for a specific category is estimated by a combination of the sample mean for each category (\\(\\bar{y}_j\\)) as well as our prior mean (\\(\\mu_0\\)). The difference demonstrates the possible shrinkage towards the prior mean.\nAlso, the shrinkage coefficient depends on \\(\\sigma^2_0\\) and the amount of data in the category. Suppose we choose a non-informative prior by making it extremely wide8 via \\(\\sigma^2_0 = 100\\). No matter how much data are observed in the category, the fraction will be close to 1, and our partial pooling estimate is essentially \\(\\bar{y}_j\\). If we had a highly opinionated prior9, \\(\\sigma^2_0 = 1\\), the weighting factor becomes \\(n / (n + 1)\\). For \\(n = 1\\), the partial pooling estimate has an even share of the sample mean and the prior mean. The effect of the prior mean decreases as \\(n\\) increases.\nThese example computations assumed that each category has the same number of rows in the training set (i.e., \\(n_j = n\\)). In practice, this is unrealistic. However, the insights into how the sample size changes the amount of shrinkage demonstrate the previous idea of how “poor data” can affect the estimates with partial pooling.\nWhile Equation 6.3 helps illustrate how partial pooling works, we should understand that the specific results hinge on knowing \\(\\sigma^2_\\epsilon = 1\\). We would seldom make such a critical assumption in practice.\nWe’ll use the non-Bayesian mixed model approach to estimate effects for our data since it is more computationally efficient. Using the training set data, maximum likelihood estimation produced \\(\\hat{\\beta}_0\\) = 76.9. Figure 6.2(a) shows the distribution of the 122 random intercept estimates \\(\\hat{\\beta}_{0j}\\). The estimated standard deviation of this distribution was \\(\\hat{\\sigma_0}\\) = 22.9€. Even though we assume that the prior distribution of the individual effects was symmetric, the histogram of the estimates shows a lack of symmetry. This may be due to a few outliers or indicate that our prior should not be symmetric.\n\n\n\n\n\n\n\n\nFigure 6.2: The effect encoding results for the hotel rate data. Panel (a) shows the distribution of the estimated per-agent effects. Panel (b) compares the two estimation methods for all unique data patterns in the training set.\n\n\n\n\n\nThe benefit of using this model, called a hierarchical model, is that it views the per-agent effects as a collection of values from a distribution. The mathematical consequence of this formulation is that the resulting parameter estimates use the data from all agents to estimate each agent’s estimates (hence the use of the phrase “partial pooling”). The intuition for that statistical method applies here: the estimates for agents with “poor data quality” have their results shrunken towards the overall population mean across all agents. Agents with “better” data are shrunken less during estimation. What does “poor data quality” mean? It primarily means high variance, which is usually a function of sample size and some baseline, the irreducible noise level in the system.\nRecall the previous discussion regarding agents Chelsea Wilson and Alexander Drake. Using the partial pooling approach:\n\nAgent Chelsea Wilson: partial pooling shrinks the single value (193.6€) back towards the estimated population mean (76.9€) to produce a more modest estimated effect of 95.3€. Because there is a single value for this agent, the prior distribution greatly influences the final estimate.\nAgent Alexander Drake: partial pooling only slightly adjusts the original sample mean (120.3€) to the final estimate (120.1€) since there are more data available. The prior has almost no influence on this estimate since the agent had 967 bookings in the training set.\n\nThe amount of shrinkage was driven mainly by the number of bookings per agent. Figure 6.2(b) shows the collection of effect estimates for each unique data pattern in the training set. The dotted line indicates the overall ADR sample mean, and the size of the points indicates the total number of bookings by an agent. The distribution of the shrunken effects (y-axis) is significantly attenuated compared to the no pooling effects estimates (x-axis). However, note that the points with large numbers of bookings line up along the diagonal green line (i.e., minimal shrinkage to the center); regardless of the value of their agent-specific ADR estimate (\\(\\bar{y}_j\\)).\nTo reiterate how these values are used for pre-processing this type of predictor, Table 6.5 shows the linear mixed model analysis results. The numeric column is our primary model’s data for representing the agent names. This avoids creating a large number of indicator variables for this predictor.\n\n\n\n\n\n\n\n\n\n\n\nAgent\nEffect Estimate (€)\n\n\n\n\nAaron Marquez\n92.2\n\n\nAlexander Drake\n120.1\n\n\nAllen Her\n73.7\n\n\n:\n:\n\n\nYoonus Al Sultana\n58.8\n\n\nYoujia Tsuchiya\n74.8\n\n\nZachary Leedholm\n85.6\n\n\n\n\n\n\n\n\nTable 6.5: Examples of the numeric values that are used in place of each agent’s data when effect encodings are used.\n\n\n\n\nAs an alternative to generalized linear mixed models to estimate the effects, a more general approach uses Bayesian analysis (Kruschke 2014; McElreath 2015). Here, we give all of the model parameters prior probability distributions, which reflect our prior knowledge of the parameters’ shape and range. “Priors” can be pretty vague or highly opinionated, depending on what we feel we know about the situation10. For example, to continue with the random intercept formulation, we might specify the following:\n\\[\\begin{align}\n\\beta_0 &\\sim N(0, \\sigma_0^2) \\notag \\\\\n\\beta_{0j} &\\sim t(2) \\notag \\\\\n\\sigma_0 &\\sim U(0, \\infty)\n\\end{align}\\]\nThe uniform prior in the standard deviation is incredibly vague and is considered a non-informative prior. Conversely, the t distribution (with two degrees of freedom) for the random effects reflects that we might expect that the distribution is symmetric but has heavy tails (e.g., potential outliers such as those seen in Figure 6.2(a)). Priors can be vague or highly opinionated.\nWhen estimating the Bayesian model, the prior distributions are combined with the observed data to produce a posterior distribution for each parameter. Like the previous mixed-model technique, the posterior mimics the prior when there is very little (or noisy) data. The posterior distribution favors the observed results when high-quality data are abundant. Using Bayesian methods to estimate the effects enables much more versatility in specifying the model but is often more computationally intensive.\n\n\nNon-Guassian outcomes\nIn many of these cases, effect encodings can be computed using Bayesian methods or via the generalized linear mixed model approach (Bolker et al. 2009; Stroup 2012) approach fitting a random intercept model.\nThe latter model has a similar right-hand side as Equation 6.2. For example, when the outcome (\\(y_{ij}\\)) is a count, the standard Poisson model would be\n\\[\n\\log(\\mu_{ij}) = \\beta_0 + \\beta_{0j}\n\\] where \\(\\mu_{ij}\\) is the theoretical effect in the original units.\nSuppose the data are binomial proportions or Bernoulli (i.e., binary 0/1 values). The canonical effects model is:\n\\[\n\\log\\left(\\frac{\\pi_{ij}}{1-\\pi_{ij}}\\right) = \\beta_0 + \\beta_{0j}\n\\] with \\(\\pi_{ij}\\) being the parameter value for the effects. This is a basic logistic regression formulation of the problem.\nSimilar Bayesian approaches can be used. However, a completely different technique for binary outcomes is to assume again that the original data are binomial with rate parameter \\(\\pi\\) but, instead of the logistic regression formulation, assume that the actual rates come from a Beta distribution with parameters \\(\\alpha\\) and \\(\\gamma\\). With the Beta distribution as the prior, users can specify distributions with different shapes, such as uniform, bell-shaped, or “U” shaped, depending on the values chosen for \\(\\alpha\\) and \\(\\gamma\\). See Chapter 3 of Johnson, Ott, and Dogucu (2022). The Bayesian approach can be very flexible.\n\n\nConsequences of using effect encodings\nAs with feature hashing, effect encoding methods can naturally handle novel categories in future data. Without knowing anything about the new category, the most logical choice for this method is to assign the overall population effect (e.g., the mode of the posterior distribution) as the value.\nAs with feature hashing, effect encodings can eliminate linear dependencies between sets of indicators. Since the categorical data are being replaced with numeric summaries of the outcome data, it may be likely that there is no longer a redundancy in the data. Indicator columns reflect the identities of the predictor categories, while the effect encoding column measures the effectiveness of the categories. For example, a hypothetical scenario was mentioned where the agent was aliased with the customer type, yielding a linear dependency. Once the agent name column is converted to effect estimates, there would no longer be a relationship between the two predictors.\nLastly, applying effect encodings requires a robust data usage strategy. The risk of embedding over-fitted statistics into the model could cause problems that may not be observed until a new data set is predicted (see Chapter 9). Using resampling methods or a validation set (Chapter 10) is critical for using this tool. Also, when there is a temporal aspect to the data, we should try to ensure that historical data are not being predicted with future data (i.e., information leakage). For example, when an agent has multiple bookings, we kept the oldest data for an agent in the training set and newer bookings in the test set. Simple random sampling may result in the opposite occurring.\n\n\n\n6.4.4 Supervised Combining of Categories\nAnother approach for dealing with predictors with large numbers of categories is to collapse them into smaller groups that similarly affect the outcome data. For example, a cluster of agents may have similar performance characteristics (i.e., mean ADRs). If we can derive clusters of categories from the training set, a smaller group of cluster-level indicators might be a better approach than many indicator columns.\nThis approach closely mirrors the splitting process in tree-based models (discussed in Sections ?sec-cart-cls and ?sec-cart-reg), and we can use these models to find suitable groupings of predictor values. In summary, the categories are ordered by their estimated effects and then partitioned into two groups by optimizing some performance measures. For classification models, this is usually a measure of the purity of the classes in the resulting partition11. A measure of error is used for numeric outcomes, such as RMSE. Once the initial partition is made, the process repeats recursively within each division until no more splits can be made, perhaps due to insufficient data.\nFor the agent data, Figure 6.3 shows this process. The 122 agent are categorized into 12 groups. The top panel shows the ordered mean ADR values for each agent. The vertical lines indicate how the different agents are grouped at each step of the splitting process. The bottom panel shows a scaled estimate of the RMSE that uses the group-level mean ADR to predict the individual outcome results.\n\n\n\n\n\n\n\n\nFigure 6.3: The progression of using a tree-based method to collapse predictor categories into smaller sets.\n\n\n\n\n\nFor these data, there is no rational grouping or clustering of the agents (regarding their ADR) for the tree-based model to discover. The process produces increasingly granular groups, and the stopping point is somewhat arbitrary. For these reasons, it is probably not the best approach for this predictor in this particular data set.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#encodings-for-ordinal-predictors",
    "href": "chapters/categorical-predictors.html#encodings-for-ordinal-predictors",
    "title": "6  Working with Categorical Predictors",
    "section": "6.5 Encodings for Ordinal Predictors",
    "text": "6.5 Encodings for Ordinal Predictors\nIn some cases, a categorical predictor will have levels with an inherent order. For example, in the Ames data, some house amenities were ranked in terms of quality with values ‘excellent’, ‘fair’, ‘good’, ‘typical’, and ‘poor’. These values do not imply a specific numerical translation (e.g., ‘excellent’ = 2 times ‘poor’). For this reason, there are different options for converting them to numeric features.\nFirst, the analysis could treat them as unordered categories and allow the model to try to determine how the values relate to the outcome. For example, if there is the belief that, as the quality of the heating system improves, the house price should increase. Using unordered categories would be agnostic to this assumption and the model would estimate whatever trend it saw in the data. This approach would essentially be treating the quality variable as a categorical variable with 5 levels.\nAnother choice is to use domain-specific knowledge to imply relationships between levels. For example, the quality values could be replaced with integers one through five. This does bake the assumed relationship into the features, and that is advantageous when the assumed pattern is correct.\nThere are also contrast methods, similar to the one discussed in Section 6.2, that can automatically create potential patterns using polynomial expansions (more on these in Section 8.2). For our quality example, there are five possible levels. Mathematically, we can create up to the number of unique levels (minus one) polynomial function. In this example, linear, quadratic, cubic, and quartic (\\(x^4\\)) polynomial functions can be used. Figure 6.4 shows these patterns using orthogonal polynomial values12.\n\n\n\n\n\n\n\n\nFigure 6.4: Polynomial contrasts for an ordered categorical predictor with five possible levels.\n\n\n\n\n\nIn many applications, higher-degree polynomials (&gt; 2) are unlikely to be effective predictors. For example, the cubic pattern in Figure 6.4 (“Feature 3”) implies that houses in Ames with ‘excellent’ quality heating systems would have lower sale prices than those with ‘fair’ or ‘poor’ systems. Utilizing higher order polynomials usually requires some specific knowledge about the problem at hand.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#chapter-references",
    "href": "chapters/categorical-predictors.html#chapter-references",
    "title": "6  Working with Categorical Predictors",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAntonio, N, A de Almeida, and L Nunes. 2019. “Hotel Booking Demand Datasets.” Data in Brief 22: 41–49.\n\n\nBolker, B, M Brooks, C Clark, S Geange, J Poulsen, H Stevens, and JS White. 2009. “Generalized Linear Mixed Models: A Practical Guide for Ecology and Evolution.” Trends in Ecology and Evolution 24 (3): 127–35.\n\n\nBox, GEP, W Hunter, and J. Hunter. 1978. Statistics for Experimenters. New York: Wiley.\n\n\nCerda, P, and G Varoquaux. 2022. “Encoding High-Cardinality String Categorical Variables.” IEEE Transactions on Knowledge and Data Engineering 34 (3): 1164–76.\n\n\nDemidenko, E. 2013. Mixed Models: Theory and Applications with R. John Wiley & Sons.\n\n\nEfron, B, and T Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.\n\n\nElswick, RK, C Gennings, V Chinchilli, and K Dawson. 1991. “A Simple Approach for Finding Estimable Functions in Linear Models.” The American Statistician 45 (1): 51–53.\n\n\nGelman, A, J Carlin, H Stern, and D Rubin. 2013. Bayesian Data Analysis. Chapman; Hall/CRC.\n\n\nJohnson, A, M Ott, and M Dogucu. 2022. Bayes Rules!: An Introduction to Applied Bayesian Modeling. Chapman; Hall/CRC.\n\n\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press.\n\n\nLaird, N, and J Ware. 1982. “Random-Effects Models for Longitudinal Data.” Biometrics, 963–74.\n\n\nMcElreath, R. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman; Hall/CRC.\n\n\nMicci-Barreca, D. 2001. “A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems.” ACM SIGKDD Explorations Newsletter 3 (1): 27–32.\n\n\nPargent, F, F Pfisterer, J Thomas, and B Bischl. 2022. “Regularized Target Encoding Outperforms Traditional Methods in Supervised Machine Learning with High Cardinality Features.” Computational Statistics, 1–22.\n\n\nRuberg, S. 1989. “Contrasts for Identifying the Minimum Effective Dose.” Journal of the American Statistical Association 84 (407): 816–22.\n\n\nStroup, W. 2012. Generalized Linear Mixed Models: Modern Concepts, Methods and Applications. CRC press.\n\n\nWeinberger, K, A Dasgupta, J Langford, A Smola, and J Attenberg. 2009. “Feature Hashing for Large Scale Multitask Learning.” In Proceedings of the 26th Annual International Conference on Machine Learning, 1113–20. ACM.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#footnotes",
    "href": "chapters/categorical-predictors.html#footnotes",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "In other words, we know that the vector of indicators (0, 0, 0) must represent the contract customers.↩︎\nThis is discussed in greater detail for one of the case studies in ?sec-reg-summary.↩︎\nThe original data uses codes to anonymize the agent names. The names shown here are randomly fabricated. The same is true of the company names.↩︎\nAlso called target encoding or likelihood encoding.↩︎\nThis demonstrates the concept of overfitting, discussed in Chapter 9 and subsequent chapters.↩︎\nIn the language of mixed effects models, the agent is a fixed effect in this parameterization.↩︎\nEquation 6.3 is related to the well-known James-Stein estimator (Efron and Hastie 2016, chap. 7).↩︎\nMaking the range of the prior exceedingly wide indicates that almost any value is possible.↩︎\nIn other words, a very narrow distribution. There are other ways to create opinionated priors though. For instance, a skewed prior distribution for one of the \\(\\beta\\) regression slopes would be considered opinionated.↩︎\nThe linear mixed model approach mentioned above is often considered as an empirical Bayes approach with more restrictive options for the prior distributions. For empirical Bayes, one might say that you make your priors for your regression parameters any distribution you want, as long as they are Gaussians.↩︎\nThe notion of “purity” can mean a very tight distribution of numerical data (for regression models) or when the frequency distribution of qualitative has a very high probability for one category.↩︎\nThese features have characteristics that their values sum to zero and there is zero correlation between features.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html",
    "href": "chapters/embeddings.html",
    "title": "7  Embeddings",
    "section": "",
    "text": "7.1 Example: Predicting Barley Amounts\nWhen there are a multitude of predictors, it might be advantegous to condense them into a smaller number of artificial features. To be useful, this smaller set should represent what is essential in the original data. This process is often called feature extraction, dimension reduction, or manifold learning. We’ll use a more general term currently en vogue: embeddings. While this chapter focuses on feature extraction, embeddings can be used for other purposes, such as converting non-numeric data (e.g., text) into a more usable numeric format.\nThis chapter will examine several primary classes of embedding methods that can achieve multiple purposes. First, we’ll consider linear methods that take a numeric input matrix \\(\\boldsymbol{X}\\) that is \\(n \\times p\\) and create a different, probably smaller set of features \\(\\boldsymbol{X}^*\\) (\\(n \\times m\\)) using the transformation \\(\\boldsymbol{X}^* = \\boldsymbol{X}\\boldsymbol{Q}\\). We hope that we can find appropriate embeddings so that \\(m &lt;&lt; p\\).\nAfter describing linear methods, we will consider a different class of transformations that focuses on the distances between data points called multidimensional scaling (MDS). MDS creates a new set of \\(m\\) features that are not necessarily linear combinations of the original features but often use some of the same math as the linear techniques.\nFinally, some embedding techniques specific to classification are discussed. These are based on class centroids.\nBefore beginning, we’ll introduce another data set that will be used here and in forthcoming chapters.\nLarsen and Clemmensen (2019) and Fernández P et al. (2020) describe a data set where laboratory measurements are used to predict what percentage of a liquid was lucerne, soy oil, or barley oil1. An instrument is used to measure how much of particular wavelengths of light are absorbed by the mixture to help determine chemical composition. We will focus on using the lab measurements to predict the percentage of barley oil in the mixture. The distribution of these values is shown in Figure 7.1(a).\nFigure 7.1: (a) The distribution of the outcome for the entire data set. The bar colors reflect the percent barley distribution and are used in subsequent sections. (b) Selected training set spectra for four barley samples. Each line represents the set of 550 predictors in the data.\nNote that most of the data have very little barley oil. About 27% of the data are less than 1%, and the median barley oil percentage is 5.96%.\nThe 550 predictors are the light absorbance for sequential values in the light region of interest (believed to be from wavelengths between 1300 and 2398 nm). Figure 7.1(b) shows a selection of four samples from the data. The darker lines represent samples with lower barley content.\nThese predictor values, called spectra, have a very high serial correlation between predictors; median correlation between the predictors was 0.98. The high degree of between-predictor correlation can be a major complication for some models and can degrade predictive performance. Therefore, we need methods that will simultaneously decorrelate predictors while extracting useful predictive information for the outcome.\nAnalyses of similar data sets can be found in Section 9.1 of Kuhn and Johnson (2019) and Johnson and Kuhn (2024).\nIn the following computations, each predictor was standardized using the orderNorm transformation mentioned earlier (unless otherwise noted).\nThe data originated from a modeling competition to find the most accurate model and specific samples were allocated to training and test sets. However, there were no public outcome values for the test set; our analysis will treat the 6,915 samples in their training set as the overall pool of samples. This is enough data to split into separate training (\\(n_{tr} =\\) 4,839), validation (\\(n_{val} =\\) 1,035), and test sets (\\(n_{te} =\\) 1,041). The allocation of samples to each of the three data sets utilized stratified sampling based on the outcome data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-linear-embed",
    "href": "chapters/embeddings.html#sec-linear-embed",
    "title": "7  Embeddings",
    "section": "7.2 Linear Transformations",
    "text": "7.2 Linear Transformations\nThe barley data set presents two common challenges for many machine learning techniques:\n\nThe number of original predictors (550) is fairly large .\nThe features are highly correlated. Techniques that require inverting the covariance matrix of the features, such as linear regression, will become numerically unstable or may not be able to be fit when features are highly correlated.\n\nWhat can we do if we desire to use a modeling technique that is adversely affected by either (or both) of these characteristics?\nDimensionality reduction is one technique that can help with the first issue (an abundance of columns). As we’ll see below, we might be able to extract new features (\\(X^*\\)) such that the new features optimally summarize information from the original data (\\(X\\)).\nFor the problem of highly correlated predictors, some embedding methods can additionally decorrelated the predictors by deriving embedded features with minimal correlation.\nThe three embedding methods we’ll discuss first are linear in a mathematical sense because they transform a table of numeric features into new features that are linear combinations of the original features. These new linear combinations are often called scores. This transformation uses the equation.\n\\[ \\underset{n\\times m}{\\boldsymbol{X}^*} = \\underset{n\\times p}{\\boldsymbol{X}}\\ \\underset{p\\times m}{\\boldsymbol{Q}} \\]\nwhere \\(\\boldsymbol{Q}\\) is a matrix that translates or embeds the original features into a potentially lower dimensional space (\\(m &lt; p\\)) without losing much information. It is easy to see why this matrix operation is linear when the elements of the \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Q}\\) matrices are expanded. For example, the first score for the \\(i^{th}\\) sample would be:\n\\[\nx^*_{i1} = q_{11} x_{i1} + q_{21}x_{i2} + \\ldots +  q_{p1}x_{ip}\n\\tag{7.1}\\]\nThe \\(q_{ij}\\) values are often referred to as the loadings for each predictor in the linear combination.\nIn this section, we will review principal components analysis (PCA), independent component analysis (ICA), and partial least squares (PLS), which are fundamental linear embedding techniques that are effective at identifying meaningful embeddings but estimate \\(A\\) in different ways. PCA and ICA extract embedded features using only information from the original features; they are unsupervised. Conversely, PLS uses the predictors and outcome; it is a supervised technique.\nThere are many other linear embedding methods that can be used. For example, non-negative matrix factorization (Lee and Seung 1999, 2000)is an embedding method that is useful when the data in \\(X\\) are integer counts or other values that cannot be negative.\nTo start, we’ll focus on the most often used embedding method: PCA.\n\n7.2.1 Principal Component Analysis\nKarl Pearson introduced PCA over a century ago, yet it remains a fundamental dimension reduction method (Jolliffe and Cadima 2016). Its relevance stems from the underlying objective of the technique: to find linear combinations of the original predictors that maximize amount of variation in the new features2.\n\nFrom a statistical perspective, variation is synonymous with information.\n\nSimultaneously, the scores produced by PCA (i.e., \\(X^*\\)) are required to be orthogonal to each other. The important side benefit of this technique is that PCA scores are uncorrelated. This is very useful for modeling techniques that need the predictors to be relatively uncorrelated: multiple linear regression, neural networks, support vector machines, and others.\nIn a way, PCA components have an order or hierarchy. The first principal component has a higher variance than any other component. The second component has a higher variance than subsequent components, and so on. We can think of this process as one of deflation; the first component extracts the largest sources of information/variation in the predictors set. The second component extracts as much as possible for whatever is left behind by the first, and so on. For this reason, we can track how much variation in the original data each component accounts for. We often use the phrase that “PCA chases variation” to achieve its goals.\nSuppose we start with \\(p\\) columns3 in our data. In that case, we can produce up to \\(p\\) PCA components, and their accumulated variation will eventually add up to the total variation in the original \\(X\\).\nWhile PCA can deliver new features with the desirable characteristics, the technique must be used with care. Specifically, we must understand that PCA seeks to find embeddings without regard to any further understanding of the original features (such as the response). Hence, PCA may generate embeddings that are ignorant of the modeling objective (i.e., they lack predictive information).\n\nPreparing for PCA\nBecause PCA seeks linear combinations of predictors that maximize variability, it will naturally first be drawn to summarizing predictors with more variation. If the original predictors are on measurement scales that differ in magnitude, then the first components will focus on summarizing the higher magnitude predictors. This means that the PCA scores will focus on the scales of the measurements (rather than their intrinsic value). If, by chance, the most relevant feature for predicting the response also has the largest numerical values, the new features created by PCA will retain the essential information for predicting the response. However, if the most important features are in the smallest units, then the top features created by PCA will be much less effective.\nIn most practical machine learning applications, features are on vastly different scales. We suggest using the tools from Section 5.3 to transform the data so that they have the same units. This prevents PCA from focusing dimension reduction simply on the measurement scale.\nAdditionally, the distributions of each feature may show considerable skewness or outliers. While there are no specific distributional requirements to use PCA, it is focused on variance and variance calculations often involve squared terms (e.g., \\((x_i - \\bar{x})^2\\)). This type of computation can be very sensitive to outliers and skewness; resolving these issues prior to PCA is recommended. See the methods previously described in Section 5.2. When outliers are a specific concern, the spatial-sign transformation might be a good idea (Visuri, Koivunen, and Oja 2000; Croux, Ollila, and Oja 2002).\n\n\nHow does PCA work?\nPrincipal component analysis is focused on understanding the relationships between the predictor columns. PCA is only effective when there are correlations between predictors. Otherwise, each new PCA feature would only highlight a single predictor, and you would need the full set of PCA features to approximate the original columns. Conversely, data sets with a handful of intense between-predictor relationships require very few predictors to represent the source columns.\nThe barley data set has abnormally high correlations between each predictor. These tend to be autoregressive; the correlations between predictors at adjacent wavelengths are very high, and the correlation between a pair of predictors diminishes as you move away from their locations on the spectrum. For example, the correlation between the first and second predictor (in their raw form) is essentially 1 while the corresponding correlation between the first and fiftieth columns is 0.67. We’ll explore the latter pair of columns below.\nBefore proceeding, let’s go on a small “side-quest” to talk about the mathematics of PCA.\n\nThe remainder of this subsection discussed a mathematical operation called the singular value decomposition (SVD). PCA and many other computations rely on on the SVD. It is a fundamental linear algebra calculation.\nWe’ll describe it loosely with a focus on how it is used. The concept of eigenvalues will come up again in the next section and in subsequent chapters. Banerjee and Roy (2014) and Aggarwal (2020) are thorough but helpful resources for learning more.\n\nThe SVD process is intended to find a way to rotate the original data in a way that the resulting columns are orthogonal to one another. It takes a square matrix as an input and can decompose it into several pieces:\n\\[\\underset{p\\times p}{A} = \\underset{p\\times p}{Q}\\quad\\underset{p\\times p}{\\Lambda}\\quad \\underset{p\\times p}{Q'}\\]\nThe results of this computation are the \\(p\\) eigenvectors \\(\\mathbf{q}_j\\) and eigenvalues \\(\\lambda_j\\). The eigenvectors tell you about the directions in which values of \\(A\\) are moving and the eigenvalues describe the corresponding magnitudes (e.g. how far they go in their corresponding direction).\nThe transformation works with PCA by using the covariance matrix as the input \\(A\\). We are trying to find trends in the relationships between variables and the SVD is designed to translate the original matrix to one that is orthogonal (that is, uncorrelated). This aspect of the SVD is how it connects to PCA. If we want to approximate our original data with new features that are uncorrelated, we need a transformation to orthogonality.\nThe matrix \\(Q\\) houses the \\(p\\) possible eigenvectors. These are the values by which you multiply the original matrix \\(A\\) to achieve an orthogonal version. For PCA, they are the loading values shown in Equation 7.1. The matrix \\(\\Lambda\\) is a diagonal matrix whose \\(p\\) values are the eigenvalues. It turns out that the eigenvalues represent the amount of variation (i.e., information) captured by each component.\nIf we conduct the SVD (and PCA) on the covariance matrix of the data, we are again subject to issues around potentially different units and scales of the predictors. Previously, we suggested to center and scale the data prior to PCA. The equivalent approach here is to conduct the SVD on the correlation matrix of the data. Similar to our recommendations on standardization, we recommend using the correlation as the input to PCA.\nOne interesting side-effect of using the covariance or correlation matrix as the input to PCA is related to missing data (discussed in more detail in Chapter 4). The conventional sample covariance (or correlation) matrices can be computed for each matrix element (i.e., without using matrix multiplication). Each covariance or correlation can be computed with whatever rows of the two predictors have pairwise-complete results. This means that we do not have to globally drop specific rows of the data when a small number of columns contain missing data.\n\n\nA two dimensional example\nTo demonstrate, we chose two predictors in the barley data. They correspond to wavelengths4 1,300 and 1,398 shown in Figure 7.1(b). In the data, these two predictors were preprocessed using the ORQ procedure (Section 5.2). The standardized versions of the predictors have a correlation of 0.68 and are shown in Figure 7.2(a).\nPCA was performed on these data, and the results are animated in Figure 7.2(b). This visualization shows the original data (colored by their outcome data) and then rotates it around the center position. The PCA transformation corresponds to the rotation where the x-axis has the largest spread. The variance is largest at two angles (135\\(^{\\circ}\\) and 315\\(^{\\circ}\\)) since the solution for PCA is unique up to sign. Both possible solutions are correct, and we usually pick one. For one solution, the PCA loading coefficients are:\n\\[\\begin{align}\nx^*_{i1} &= -0.71 x_{i1} -0.71 x_{i2} \\notag \\\\\nx^*_{i2} &= -0.71 x_{i1} + 0.71 x_{i2} \\notag\n\\end{align}\\]\nBecause of this illustrative example’s low-dimensional aspect, this solution is excessively simple. Ordinarily, the loading values take a great many values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: A demonstration that PCA is just a rotation of the data. The original predictors (those at the first and fiftieth wavelengths) are in Panel (a). The animation in (b) shows the rotation of two of the predictors, which stops at the two optimal rotations corresponding to the principal components and the original data.\n\n\n\nAgain,the remarkable characteristic of this specific set of linear combinations is that they are orthogonal (i.e., uncorrelated) with one another.\n\nUsing PCA to transform the data into a full set of uncorrelated variables is often called “whitening the data.” It retains all of the original information and is more pliable to many computations. It can be used as a precursor to other algorithms.\n\nNow that we’ve seen PCA on a small scale, let’s look at an analysis of the full set of predictors\n\n\nPCA on the barley data\nWe’ll repeat the analysis in the preceding section; PCA was calculated on the training set using all predictors. There are 550 feature columns and the same number of possible principal components. The number of new features to retain is a parameter that must be determined. In practice, the number of new features is often selected based on the percentage of variability that the new features summarize relative to the original features. Figure 7.3 displays the new PCA feature number (x-axis) versus the percent of total variability across the original features (commonly called a “scree plot”). In this graph, three components summarize 99.4% of the total variability. We often select the smallest number of new features that summarize the greatest variability. Again, this is an unsupervised optimization. To understand how the number of components affects supervised predictors (e.g., RMSE), the number of features can be treated as a tuning parameter .\n\n\n\n\n\n\n\n\nFigure 7.3: A scree plot of the PCA component number versus the percent of variance explained for the first 10 components of the barley data.\n\n\n\n\n\nNote that the first component alone captured 92.3% of the variation in the 550 training data columns. This reflects the extreme correlation seen in the predictors.\nFigure 7.4 shows a scatterplot of the principal components colored by the percentage of barley oil. The figure reveals that the first PCA component delineates the higher barley oil samples from those with less oil. There also appear to be three different clusters of samples with very low (if any) barley oil. It might be helpful to investigate these samples to ascertain if they are artifacts or caused by some systematic, underlying factors not in the current set of columns. The pairing of the second and fourth components appears to help differentiate lower barely samples from the broader set. The pairing of the third and fourth components doesn’t offer much predictive power on their own.\n\n\n\n\n#| label: fig-linear-scores\n#| out-width: \"80%\"\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggforce)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-linear-scores.R\")\n\napp\n\n\n\nFigure 7.4: A visualization of the four new features for different linear embedding methods. The data shown are the validation set results.\n\n\n\nOne note about this visualization. The axis scales are not common across panels (for ease of illustration). If we were to keep a common scale, the later components would appear to have very little effect due to the flatness of the resulting figures. When looking at the scores, we suggest keeping a common scale, which will most likely be the scale of the first component. This will help avoid over-interpreting patterns in the later components.\nFor PCA, it can be very instructive to visualize the loadings for each component. This can help in several different ways. First, it can tell which predictors dominate each specific new PCA component. If a particular component ends up having a strong relationship with the outcome, this can aid our explanation of how the model works. Second, we can examine the magnitude of the predictors to determine some of the relationships between predictors. If a group of predictors have approximately the same loading value, this implies that they have a common relationship with one another. This, on its own, can be a significant aid when conducting exploratory data analysis on a high-dimensional data set. Figure 7.5 shows the relationship between the PCA loadings and each predictor’s position on the spectrum.\n\n\n\n\n#| label: fig-linear-loadings\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(dplyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-linear-loadings.R\")\n\napp\n\n\n\nFigure 7.5: The loadings for the first four components of each linear embedding method as a function of wavelength.\n\n\n\nFor the first component, wavelengths at the low ends of the spectrum have a relatively small impact that increases as you get to about 1,400 nm. At this point, the predictors have a constant effect on that component. The second PCA component is almost the opposite; it emphasizes the smaller wavelengths while predictors above 1,400 nm fluctuate around zero. The third and fourth components emphasize different areas of the spectrum and are generally different than zero.\n\n\nHow to incorporate PCA into the model\nThe barley data demonstrates that PCA can effectively capture the training set information in a smaller predictor set when there is a correlation between predictors. The number of components to retain depends on the data. It is a good idea to optimize the number of components to retain via model tuning.\n\nNote that “dimension reduction” is not the same as feature selection. The PCA components described above are functions of all of the predictors. Even if you only need a handful of PCA components to approximate the training set, the original predictor set is still required when predicting new samples.\n\nAnother aspect of using PCA for feature extraction is the scale of the resulting scores. Figure 7.4 demonstrates that the first component has the largest variance, resulting in a wider range of training set values. For the validation set scores, the variance of PC1 is 14-fold larger than the variance of PC2. Even though the scores are unitless, there may be issues for models that expect the predictors to have a common scale, similar to the requirement for PCA itself. In these cases, it may be advisable to standardize the PCA components to have the same range before serving them to the supervised ML model.\n\n\nNon-standard PCA\nVarious methods exist for estimating the PCA loadings. Some are well-suited to different dimensions of the predictor’s data. For example, “small \\(n\\), large \\(p\\)” data sizes can make PCA computations difficult. Wu, Massart, and De Jong (1997) and others describe a kernel method more appropriate for very wide data sets. Schölkopf, Smola, and Müller (1998) extended this even further but using nonlinear kernel methods5. Still, others use modified objective functions to compensate for some potential deficiencies of the canonical technique.\nFor example, the standard loading matrix (i.e., the eigenvectors) are dense. It is possible that the loadings for some predictors shouldn’t affect specific components. The SVD might estimate these to be close to zero but not exactly zero. Sparse PCA techniques can, through various means, estimate some loading values to be exactly zero, indicating that the predictor has no functional effect on the embedding.\nFor example, Shen and Huang (2008) take a penalization approach that puts a high price for loadings to have very large values. This regularization method can shrink the values towards zero and make some absolute zero. This is similar to the regularization approach seen in Section 6.4.3 and is directly related to techniques discussed in ?sec-logistic-reg, ?sec-nnet-cls, and ?sec-linear-reg. This option can coerce values across the loadings to be zero. It is unlikely to force a particular predictor’s values to be zero across all components. In other words, it may not completely erase the effect of a predictor from the embedding altogether.\nAnother method is based on a Bayesian approach (Ning and Ning 2021), where the loading values are assumed to come from a mixture of two different distributions. One has a sharp “spike” around zero, and the other is a flat, wide distribution that encompasses a wide range of values (called the “slab”). The method then estimates the parameters to favor one distribution or the other and sets some proportion of the loadings to zero.\nAdditionally, there are PCA variants for non-numeric data, such as categorical predictors. Probabilistic PCA (Tipping and Bishop 1999; Schein, Saul, and Ungar 2003) uses a PCA generalization to reduce the dimensions of qualitative data.\n\n\n\n7.2.2 Independent Component Analysis\nICA has roots in signal processing, where the observed signal that is measured is a combination of several different input signals.\nFor example, electroencephalography (EEG) uses a set of electrodes on a patient’s scalp to noninvasively measure the brain’s electrical activity over time. The electrodes are placed on specific parts of the scalp and measure different brain regions (e.g., prefrontal cortex etc.). However, since the electrodes are near others, they can often measure mixtures of multiple underlying signals.\nThe objective of independent component analysis is to identify linear combinations of the original features that are statistically independent of each other (Hyvärinen and Oja 2000; Nordhausen and Oja 2018).\n\nTwo variables can be uncorrelated but still have a relationship (most likely nonlinear). For example, suppose we have a variable \\(z\\) that is a sequence of evenly spaced values between -2 and 2. The correlation between \\(z\\) and \\(z^2\\) is very close to zero but there is a clear and precise relationship between them, therefore the variables are not independent.\n\nIn practice, satisfying the statistically independence requirement of ICA can be done several ways. One approach maximizes the “non-Gaussianity” of the resulting components. As shown by Hyvärinen and Oja (2000),\n\nThe fundamental restriction in ICA is that the independent components must be nongaussian for ICA to be possible.\n\nThere are different ways to measure non-Gaussianity. The fastICA algorithm uses an information theory statistic called negentropy (Hyvärinen and Oja 2000). Another approach called InfoMax uses neural networks to estimate a different information theory statistic.\nBecause ICA’s objective requires statistical independence among components, it will generate features different from those of PCA. Unlike PCA, which orders new components based on summarized variance, ICA’s components have no natural ordering. This means that ICA may require more components than PCA to find the ones related to the outcome.\nThe predictors should be centered (and perhaps scaled) before estimating the ICA loadings. Additionally, some ICA algorithms internally add a PCA step to “whiten” the data before computing the ICA loadings (using all possible PCA components). Since independent component analysis thrives on non-Gaussian data, we should avoid transformations that induce a more symmetric distribution. For this reason, we will not use the ORD transformation to preprocess the predictors. Finally, the ICA loadings are often initialized before training using random values. If this randomness is not controlled, different results will likely occur from training run to training run.\nFigure 7.4 contains a scatterplot matrix of the first 4 ICA components colored by the outcome. In this figure, Two components that appear to differentiate levels of barley by themselves (components two and three). Unsurprisingly, their interaction appears to be the one that visually differentiates the different amounts of barley oil in the validation set.\nThe loadings are shown in Figure 7.5 and tell an interesting story. The pattern of loadings one and three have similar patterns over the wavelengths. The same situation is the case for components two and four. These two pairs of trends in the loadings are somewhat oppositional to one another. Also, the patterns have little in common with the PCA loadings. For example, the first PCA loading was relatively constant across wavelengths. None of the ICA components show a constant pattern.\n\n\n7.2.3 Partial Least Squares\nBoth PCA and ICA focus strictly on summarizing information based on the features exclusively. When the outcome is related to the way that the unsupervised techniques extract the embedded features, then PCA and/or ICA can be effective dimension reduction tools. However, when the qualities of variance summarization or statistical independence are not related to the outcome, then these techniques may struggle to identify appropriate embedded features that are useful for predicting the outcome.\nPartial least squares (PLS) (Wold, Martens, and Wold 1983) is a dimension reduction technique that identifies embedded features that are optimally linearly related to the outcome. While the objective of PCA is to find linear combinations of the original features that best summarize variability, the objective of PLS is to do the same and to find the linear combinations that are correlated with the outcome (Stone and Brooks 1990). This process can be thought of as a constrained optimization problem in which PLS is forced to compromise between maximizing information from the predictors and simultaneously maximizing the prediction of the outcome. This means that the outcome is used to focus the dimension reduction process such that the new features have the greatest correlation with the outcome. For more detailed information, we suggest Geladi and Kowalski (1986) and Esposito Vinzi and Russolillo (2013).\nBecause one of the objectives of PLS is to summarize variance among the original features, the features should be pre-processed in the same way as PCA.\nFor the barley data, the first few sets of PLS loads are fairly similar to those generated by PCA. The first three components have almost identical values as their PCA analogs. The fourth component has different loadings, and the correlation between the fourth PCA and PLS scores is -0.63. The similarities in the first three scores and loadings between the methods can be seen in Figure 7.4 and Figure 7.5, respectively.\nIf our goal is to maximize predictive performance, fewer features are necessary when using PLS. As we’ll see below, we can sometimes achieve similar performance using PCA or ICA but we will need more features to match what PLS can do with less.\nLike principal component analysis, there are many modified versions of partial least squares. For example, Lê Cao et al. (2008) and Lê Cao et al. (2008) describe a sparse estimation routine that can set some loadings equal to absolute zero. Also, Barker and Rayens (2003) and Y. Liu and Rayens (2007) adapt the algorithm for classification problems (i.e., qualitative outcomes).\n\n\n7.2.4 Overall Comparisons\nHow do these three approaches differ in terms of predictive performance? To evaluate this, these embeddings were computed with up to 25 new features and used as the inputs for a linear regression model. After the embeddings and regression parameters were estimated, the validation set RMSE was computed. Figure 7.6 has the results. The bands around each curve are 90% confidence intervals for the RMSE.\n\n\n\n\n\n\n\n\nFigure 7.6: Results for linear regressions using three different linear embeddings.\n\n\n\n\n\nOverall, the patterns are similar. By the time 25 components are added, there is parity between the methods. However, PLS appears to be more efficient at finding predictive features since its curve has uniformly smaller RMSE values than the others . This isn’t surprising since it is the only supervised embedding of the three.\nPCA and PLS will be discussed in more detail in ?sec-colinearity.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-mds",
    "href": "chapters/embeddings.html#sec-mds",
    "title": "7  Embeddings",
    "section": "7.3 Multidimensional Scaling",
    "text": "7.3 Multidimensional Scaling\nMultidimensional scaling (Torgerson 1952) is a feature extraction tool that creates embeddings that try to preserve the geometric distances between training set points. In other words, the distances between points in the smaller dimensions should be comparable to those in the original dimensions.\nMany different distance metrics can be used, but we’ll postpone that discussion until ?sec-cls-knn. That said, we’ll denote a distance value between two vectors of predictors as \\(d(x_i, x_j)\\) and we’ll assume basic Euclidean distance unless otherwise noted. With distance metrics, the predictors should be standardized to equivalent units before those computations. As with PCA, we also recommend transformations to resolve skewness.\nTake Figure 7.7(a) as an example. There are ten points in two dimensions (colored by three outcome classes). If we were to project these points down to a single dimension, we’d like points that are close in the original two dimensions to remain close when projected down to a single dimension. Panel (c) shows two such solutions. Each does reasonably well with some exceptions (i.e., points six and nine are too close for non-Metric MDS).\n\n\n\n\n\n\n\n\nFigure 7.7: (a) A collection of samples associated with three outcome classes. (b) A diagram of the two nearest neighbors for each data point. (c) A one-dimensional projection using two different methods: non-metric MDS and Isomap.\n\n\n\n\n\nHere we present a few MDS methods, but there are many more. Ghojogh et al. (2023) has an excellent review of an assortment of methods and their nuances.\nSome MDS methods compute all pairwise distances between points and use this as the input to the embedding algorithm. This is similar to how PCA can be estimated using the covariance or correlation matrices. One technique, Non-Metric MDS (Kruskal 1964a, 1964b; Sammon 1969), finds embeddings that minimize an objective function called “stress”:\n\\[\n\\text{Stress} = \\sqrt{\\frac{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}\\left(d(x_i, x_j) - d(x^*_i, x^*_j)\\right)^2}{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}d(x_i, x_j)^2}}\n\\]\nThe numerator uses the squared difference between the pairwise distances in the original values (\\(x\\)) and the smaller embedded dimension (\\(x^*\\)). The summations only move along the upper triangle of the distance matrices to reduce redundant computations. Figure 7.7(c, top row) has the resulting one dimensional projection of our two-dimensional data.\nThis can be an effective dimension reduction procedure, although there are a few issues. First, the entire matrix of distances is required (with \\(n_{tr}(n_{tr}-1)/2\\) entries). For large training sets, this can be unwieldy and time-consuming. Second, like PCA, it is a global method that uses all data in the computations. We might be able to achieve more nuanced embeddings by focusing on local structures. Finally, it is challenging to apply metric MDS to project new data onto the space in which the original data was projected.\nFor these reasons, let’s take a look at more modern versions of multidimensional scaling.\n\n7.3.1 Isomap\nTo start, we’ll focus on Isomap (Tenenbaum, Silva, and Langford 2000). This nonlinear MDS method uses a specialized distance function to find the embedded features. First, the K nearest neighbors are determined for each training set point using standard functions, such as Euclidean distance. Figure 7.7(b) shows the K = 2 nearest neighbors for our example data. Many nearest-neighbor algorithms can be very computationally efficient and their use eliminates the need to compute all of the pairwise distances.\nThe connections between neighbors form a graph structure that qualitatively defines which data points are closely related to one another. From this, a new metric called geodesic distance can be approximated. For a graph, we can compute the approximate geodesic distance using the shortest path between two points on the graph. With our example data, the Euclidean distance between points four and five is not large. However, its approximate geodesic distance is greater because the shortest path is through points nine, eight, and seven. Ghojogh et al. (2023) use a wonderful analogy:\n\nA real-world example is the distance between Toronto and Athens. The Euclidean distance is to dig the Earth from Toronto to reach Athens directly. The geodesic distance is to move from Toronto to Athens on the curvy Earth by the shortest path between two cities. The approximated geodesic distance is to dig the Earth from Toronto to London in the UK, then dig from London to Frankfurt in Germany, then dig from Frankfurt to Rome in Italy, then dig from Rome to Athens.\n\nThe Isomap embeddings are a function of the eigenvalues computed on the geodesic distance matrix. The \\(m\\) embedded features are functions of the first \\(m\\) eigenvectors. Although eigenvalues are associated with linear embeddings (e.g., PCA), nonlinear geodesic distance results in a local nonlinear embedding. Figure 7.7(c, bottom row) shows the 1D results for the example data set. For a new data point, its nearest-neighbors in the training set are determined so that the approximate geodesic distance can be computed. The estimated eigenvectors and eigenvalues are used to project the new point into the embedding space.\nFor Isomap, the number of nearest neighbors and the number of embeddings are commonly optimized. Figure 7.8 shows a two-dimensional Isomap embedding for the barley data with varying numbers of neighbors. In each configuration, the higher barley values are differentiated from the small (mostly zero) barley samples. We again see the two or three clusters of data associated with small outcome values. The new features become more densely packed as the number of neighbors increases.\n\n\n\n\n\n\n\n\nFigure 7.8: Isomap for the barley data for different numbers of nearest neighbors. The training set was used to fit the model and these results show the projections on the validation set. Lighter colors indicate larger values of the outcome.\n\n\n\n\n\n\n\n7.3.2 Laplacian Eigenmaps\nThere are many other approaches to preserve local distances. One is Laplacian eigenmaps (Belkin and Niyogi 2001). Like Isomap, it uses nearest neighbors to define a graph of connected training set points. For each connected point, a weight between graph nodes is computed that becomes smaller as the distance between points in the input space increases. The radial basis kernel (also referred to as the “heat kernel”) is a good choice for the weighting function6:\n\\[\nw_{ij} = \\exp\\left(\\frac{-||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2}{\\sigma}\\right)\n\\]\nwhere \\(\\sigma\\) is a scaling parameter that can be tuned. If two points are not neighbors, or if \\(i = j\\), then \\(w_{ij} = 0\\). Note that the equation above uses Euclidean distance. For the 2-nearest neighbor graph shown in Figure 7.7(b) and \\(\\sigma = 1 / 2\\), the weight matrix is roughly\n\n\\[\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\n\\boldsymbol{W} = \\begin{bmatrix}\n\\0 & 0.1 & \\0 & 0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n& \\0 & 0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  & \\0 & 0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  &  & \\0 & \\0 & \\0 & 0.1 & \\0 & \\0 & \\0\\\\\n&  &  &  & \\0 & 0.4 & \\0 & \\0 & 0.1 & \\0\\\\\n&  & sym &  &  & \\0 & \\0 & \\0 & 0.1 & \\0\\\\\n&  &  &  &  &  & \\0 & 0.4 & \\0 & 0.1\\\\\n&  &  &  &  &  &  & \\0 & 0.1 & 0.3\\\\\n&  &  &  &  &  &  &  & \\0 & \\0\\\\\n&  &  &  &  &  &  &  &  & \\0\n\\end{bmatrix}\n\\]\n\nThe use of nearest neighbors means that the matrix can be very sparse and the zero values help define locality for each data point. Recall that samples 2 and 3 are fairly close to one another, while samples 1 and 2 are farther away. The weighting scheme gives the former pair a 4-fold larger weight in the graph than the latter pair.\nLaplacian eigenmaps rely heavily on graph theory. This method computes a graph Laplacian matrix, defined as \\(\\boldsymbol{L} = \\boldsymbol{D} - \\boldsymbol{W}\\) where the matrix \\(\\boldsymbol{D}\\) has zero non-diagonal entries and diagonals equal to the sum of the weights for each row. For our example data:\n\n\\[\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\nL = \\begin{bmatrix}\n0.3 & -0.1 & \\0 & -0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  0.5 & -0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  &  0.5 & -0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  &  &  0.4 & \\0 & \\0 & -0.1 & \\0 & \\0 & \\0\\\\\n&  &  &  &  0.5 & -0.4 & \\0 & \\0 & -0.1 & \\0\\\\\n&  & sym &  &  &  0.5 & \\0 & \\0 & -0.1 & \\0\\\\\n&  &  &  &  &  &  0.6 & -0.4 & \\0 & -0.1\\\\\n&  &  &  &  &  &  &  0.8 & -0.1 & -0.3\\\\\n&  &  &  &  &  &  &  &  0.3 & \\0\\\\\n&  &  &  &  &  &  &  &  &  0.4\n\\end{bmatrix}\n\\]\n\nThe eigenvalues and eigenvectors of this matrix are used as the main ingredients for the embeddings. Bengio et al. (2003) shows that, since these methods eventually use eigenvalues in the embeddings, they can be easily used to project new data.\n\n\n7.3.3 UMAP\nThe Uniform Manifold Approximation and Projection (UMAP) (Sainburg, McInnes, and Gentner 2020) technique is one of the most popular distance-based methods. Its precursors, stochastic neighbor embedding (SNE) (Hinton and Roweis 2002) and Student’s t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton 2008), redefined feature extraction, particularly for visualizations. UMAP borrows significantly from Laplacian eigenmaps and t-SNE but has a more theoretically sound motivation.\nAs with Laplacian eigenmaps, UMAP converts the training data points to a sparse graph structure. Given a set of K nearest neighbors, it computes values similar to the previously shown weights (\\(\\boldsymbol{W}\\) matrix), which we will think of as the probability that point \\(j\\) is a neighbor of point \\(i\\):\n\\[\np_{j|i} = \\exp\\left(\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i}\\right)\n\\]\nwhere \\(\\rho_i\\) is the distance from \\(\\boldsymbol{x}_i\\) to its closest neighbor, and \\(\\sigma_i\\) is a scale parameter that now varies with each sample (\\(i\\)). To compute \\(\\sigma_i\\), we can solve the equation\n\\[\n\\sum_{i=1}^K  \\exp\\left(\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i}\\right) = \\log_2(K)\n\\]\nUnlike the previous weighting system, the resulting \\(n \\times n\\) matrix may not be symmetric, so the final weights are computed using \\(p_{ij} = p_{j|i} + p_{i|j} - p_{j|i}p_{i|j}\\).\nUMAP performs a similar weight calculation for the embedded values \\(\\boldsymbol{x}^*\\). We’ll denote the probability that embedded points \\(\\boldsymbol{x}_i^*\\) and \\(\\boldsymbol{x}_j^*\\) are connected as \\(p_{ij}^*\\). We’d like the algorthim to estimate these values such that \\(p_{ij} \\approx p_{ij}^*\\).\nNumerical optimization methods7 used to estimate the \\(n \\times m\\) values \\(x^*_{ij}\\). The process is initialized using a very sparse Laplacian eigenmap, the first few PCA components, or random uniform numbers. The objective function is based on cross-entropy and attempts to make the graphs in the input and embedded dimensions as similar as possible by minimizing:\n\\[\nCE = \\sum_{i=1}^{n_{tr}}\\sum_{j=i+1}^{n_{tr}} \\left[p_{ij}\\, \\log\\frac{p_{ij}}{p_{ij}^*} + (1 - p_{ij})\\log\\frac{1-p_{ij}}{1-p_{ij}^*}\\right]\n\\]\nUnlike the other embedding methods shown in this section, UMAP can also create supervised embeddings so that the resulting features are more predictive of a qualitative or quantitative outcome value. See Sainburg, McInnes, and Gentner (2020).\nBesides the number of neighbors and embedding dimensions, several more tuning parameters exist. The optimization process’s number of optimization iterations (i.e., epochs) and the learning rate can significantly affect the final results. A distance-based tuning parameter, often called min-dist, specifies how “packed” points should be in the reduced dimensions. Values typically range from zero to one. However, the original authors state:\n\nWe view min-dist as an essentially aesthetic parameter governing the appearance of the embedding, and thus is more important when using UMAP for visualization.\n\nAs will be seen below, the initialization scheme is an important tuning parameter.\nFor supervised UMAP, there is an additional weighting parameter (between zero and one) that is used to balance the importance of the supervised and unsupervised aspects of the results. A value of zero specifies a completely unsupervised embedding.\nFigure 7.9 shows an interactive visualization of how UMAP can change with different tuning parameters. Each combination was trained for 1,000 epochs and used a learning rate of 1.0. For illustrative purposes, the resulting embeddings were scaled to a common range.\n\n\n\n\n#| label: fig-umap\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(dplyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-umap.R\")\n\napp\n\n\n\nFigure 7.9: A visualization of UMAP results for the barley data using different values for several tuning parameters. The points are the validation set values.\n\n\n\nThere are a few notable patterns in these results:\n\nThe initialization method can heavily impact the patterns in the embeddings.\nAs with Isomap, there are two or three clusters of data points with small barley values.\nWhen the amount of supervision increases, one or more circular structures form that are associated with small outcome values.\nThe minimum distance parameter can drastically change the results.\n\nt-SNE and UMAP have become very popular tools for visualizing complex data. Visually, they often show interesting patterns that linear methods such as PCA cannot. However, they are computationally slow and unstable over different tuning parameter values. Also, it is easy to believe that the UMAP distances between embedding points are important or quantitatively predictive. That is not the case; the distances can be easily manipulated using the tuning parameters (especially the minimum distance).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-centroids",
    "href": "chapters/embeddings.html#sec-centroids",
    "title": "7  Embeddings",
    "section": "7.4 Centroid-Based Methods",
    "text": "7.4 Centroid-Based Methods\nAnother common approach to creating features is to calculate distances between landmarks or pairs of predictors. For example, the distance to desirable grade schools (or the university campus) could affect house prices in the Ames housing data. Potential predictors can be created that measure how close each home is to these points of interest. To calculate distance, a simple Euclidean metric can be used. However, for spatial data across longer distances, the Haversine metric (Sinnott 1984) is a better alternative because it takes into account the curvature of the earth.\nNote that distance-based features are often right skewed. When a metric produces a right-skewed distribution, the log-transformation often helps improve predictive performance when the predictor is truly informative.\nDistance-based features can also be effective for classification models. A centroid is another name for the multivariate mean of a collection of data. It is possible to compute class-specific centroids using only the data from each class. When a new sample is predicted, the distance to each class centroid can be used as a predictor. These features would be helpful when a model could be better at detecting/emulating linear class boundaries. An example of this would be tree-based models; these models have to work hard to approximate linear trends in the data. Supplementing the data with simple centroid features might improve performance.\nAgain, there are several choices for the distance metric. Mahalanobis distance is a good choice when there is not an overwhelming number of predictors:\n\\[\nD_c(\\boldsymbol{x}_0) = (\\boldsymbol{x}_0 - \\boldsymbol{\\bar{x}}_{c})' \\boldsymbol{S}^{-1}_c (\\boldsymbol{x}_0 - \\boldsymbol{\\bar{x}}_{c})\n\\]\nwhere \\(\\boldsymbol{x}_0\\) is the new data point being predicted, \\(\\boldsymbol{\\bar{x}}\\) is a vector of sample means, \\(\\boldsymbol{S}\\) is the estimated covariance matrix (the subscript of \\(c\\) denotes the class-specific statistics). This metric requires more data points within each class than the number of predictors being used. It also assumes that there are no linear dependencies between the predictors.\nWhen the model has many features, regularizing the centroid distances can be a good approach. This approach, similar to the tools described in Section 6.4.3, will shrink the class-specific centroids towards the overall (class-nonspecific) centroid at different rates. If a predictor does have any discriminative ability in the training set, its contribution to the class-specific centroids can be removed.\nWe’ll let \\(x_{ij}\\) denote sample \\(i\\) (\\(i=1\\ldots n_{tr}\\)) for predictor \\(j\\) (\\(j=1\\ldots p\\)). The approach by Tibshirani et al. (2003) estimates the standardized difference between the class-specific centroid and the global centroid using the following:\n\\[\\begin{align}\n\\delta_{jc} &= \\frac{\\bar{x}_{jc} - \\bar{x}_j}{w_c s_j} &&\\text{ where } \\notag \\\\\n\\bar{x}_{jc} &= \\frac{1}{{n_{tr}^c}}\\sum_{i=1}^{{n_{tr}^c}} x_{ij}\\, I(y_i = c) && \\text{\\textcolor{grey}{(class-specific centroid elements)}} \\notag \\\\\n\\bar{x}_{j} &= \\frac{1}{n_{tr}}\\sum_{i=1}^{n_{tr}} x_{ij}  && \\text{\\textcolor{grey}{(global centroid elements)}}\\notag \\\\\nw_c &= \\sqrt{\\frac{1}{{n_{tr}^c}}  - \\frac{1}{n_{tr}}}  && \\text{\\textcolor{grey}{{(weights)}}} \\notag \\\\\ns_j &= \\frac{1}{n_{tr}-C}\\sum_{c=1}^C\\sum_{i=1}^{n_{tr}} \\left(x_{ij} - \\bar{x}_{jc}\\right)^2 I(y_i = c)  && \\text{\\textcolor{grey}{(pooled standard deviation for predictor $j$)}}\\notag\n\\end{align}\\]\nwhere \\(n_{tr}^c\\) is the number of training set points for class \\(c\\) and \\(I(x)\\) is a function that returns a value of one when \\(x\\) is true.\nTo shrink this difference towards zero, a tuning parameter \\(\\lambda\\) is used to create a modified version of each predictor’s contribution to the difference:\n\\[\\begin{equation}\n\\delta^*_{jc} = sign(\\delta_{jc})\\,h(|\\delta_{jc}| - \\lambda)\n\\end{equation}\\]\nwhere \\(h(x) = x\\) when \\(x &gt; 0\\) and zero otherwise. If the difference between the class-specific and global centroid is small (relative to \\(\\lambda\\)), \\(\\delta^*_{jc} = 0\\) and predictor \\(j\\) does not functionally affect the calculations for class \\(c\\). The class-specific shrunken centroid is then\n\\[\\begin{equation}\n\\bar{x}^*_{jc}= \\bar{x}_j + w_c\\, s_j\\, \\delta^*_{jc}\n\\end{equation}\\]\nNew features are added to the model based on the distance between \\(\\boldsymbol{x}_0\\) and \\(\\boldsymbol{\\bar{x}}^*_{c}\\). The amount of shrinkage is best optimized using the tuning methods described in later chapters. There are several variations of this specific procedure. Wang and Zhu (2007) describe several different approaches and Efron (2009) demonstrates the connection to Bayesian methods.\n\n\n\n\n\n\n\n\nFigure 7.10: An example of shrunken, class-specific centroids. Panel (a) shows data where there are three classes. Panel (b) demonstrates how, with increasing regularization, the centorids converge towards the global centroid (the black open circle).\n\n\n\n\n\nFigure 7.10 shows the impact of regularization for a simple data set with two predictors and three classes. The data are shown in Panel (a), while Panel (b) displays the raw, class-specific centroids. As regularization is added, the lines indicate the path each takes toward the global centroid (in black). Notice that the centroid for the first class eventually does not use predictor \\(x_1\\); the path becomes completely vertical when that predictor is removed from the calculations. As a counter-example, the centroid for the third class moves in a straight line towards the center. This indicates that both predictors showed a strong signal, and neither was removed as regularization increased.\nLike distance-based methods, predictors based on data depth (R. Liu, Parelius, and Singh 1999; Ghosh and Chaudhuri 2005; Mozharovskyi, Mosler, and Lange 2015) can be helpful for separating classes. The depth of the data was initially defined by Tukey (1975), and it can be roughly understood as the inverse of the distance to a centroid. For example, Mahalanobis depth would be:\n\\[\nDepth_c(\\boldsymbol{x}_0) = \\left[1 + D_c(\\boldsymbol{x}_0)\\right]^{-1}\n\\]\nThere are more complex depth methods, and some are known for their robustness to outliers. Again, like distances, class-specific depth features can be used as features in a model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#other-methods",
    "href": "chapters/embeddings.html#other-methods",
    "title": "7  Embeddings",
    "section": "7.5 Other Methods",
    "text": "7.5 Other Methods\nThere are numerous other methods to create embeddings such as autoencoders (Borisov et al. 2022; Michelucci 2022). Boykis (2023) is an excellent survey and discussion of modern embedding methods, highlighting methods for text and deep learning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#chapter-references",
    "href": "chapters/embeddings.html#chapter-references",
    "title": "7  Embeddings",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAggarwal, C. 2020. Linear Algebra and Optimization for Machine Learning. Vol. 156. Springer.\n\n\nBanerjee, S, and A Roy. 2014. Linear Algebra and Matrix Analysis for Statistics. Vol. 181. CRC Press.\n\n\nBarker, M, and W Rayens. 2003. “Partial Least Squares for Discrimination.” Journal of Chemometrics: A Journal of the Chemometrics Society 17 (3): 166–73.\n\n\nBelkin, M, and P Niyogi. 2001. “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering.” Advances in Neural Information Processing Systems 14.\n\n\nBengio, Y, JF Paiement, P Vincent, O Delalleau, N Roux, and M Ouimet. 2003. “Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering.” In Advances in Neural Information Processing Systems, edited by S. Thrun, L. Saul, and B. Schölkopf. Vol. 16. MIT Press.\n\n\nBorisov, V, T Leemann, K Seßler, J Haug, M Pawelczyk, and G Kasneci. 2022. “Deep Neural Networks and Tabular Data: A Survey.” IEEE Transactions on Neural Networks and Learning Systems.\n\n\nBoykis, V. 2023. “What are embeddings?” https://doi.org/10.5281/zenodo.8015029.\n\n\nCroux, C, E Ollila, and H Oja. 2002. “Sign and Rank Covariance Matrices: Statistical Properties and Application to Principal Components Analysis.” In Statistical Data Analysis Based on the L1-Norm and Related Methods, 257–69. Springer.\n\n\nEfron, B. 2009. “Empirical Bayes Estimates for Large-Scale Prediction Problems.” Journal of the American Statistical Association 104 (487): 1015–28.\n\n\nEsposito Vinzi, V, and G Russolillo. 2013. “Partial Least Squares Algorithms and Methods.” Wiley Interdisciplinary Reviews: Computational Statistics 5 (1): 1–19.\n\n\nFernández P, J. A., A. Laborde, L. Lakhal, M. Lesnoff, M. Martin, Y. Roggo, and P. Dardenne. 2020. “The Applicability of Vibrational Spectroscopy and Multivariate Analysis for the Characterization of Animal Feed Where the Reference Values Do Not Follow a Normal Distribution: A New Chemometric Challenge Posed at the ’Chimiométrie 2019’ Congress.” Chemometrics and Intelligent Laboratory Systems 202: 104026.\n\n\nGeladi, P., and B Kowalski. 1986. “Partial Least-Squares Regression: A Tutorial.” Analytica Chimica Acta 185: 1–17.\n\n\nGhojogh, B, M Crowley, F Karray, and A Ghodsi. 2023. “Elements of Dimensionality Reduction and Manifold Learning.” In, 185–205. Springer International Publishing.\n\n\nGhosh, A K, and P Chaudhuri. 2005. “On Data Depth and Distribution-Free Discriminant Analysis Using Separating Surfaces.” Bernoulli 11 (1): 1–27.\n\n\nHinton, G, and S Roweis. 2002. “Stochastic Neighbor Embedding.” Advances in Neural Information Processing Systems 15.\n\n\nHyvärinen, Aapo, and Erkki Oja. 2000. “Independent Component Analysis: Algorithms and Applications.” Neural Networks 13 (4-5): 411–30.\n\n\nJohnson, K, and M Kuhn. 2024. “What They Forgot to Tell You about Machine Learning with an Application to Pharmaceutical Manufacturing.” Pharmaceutical Statistics n/a (n/a).\n\n\nJolliffe, I, and J Cadima. 2016. “Principal Component Analysis: A Review and Recent Developments.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 374 (2065): 20150202.\n\n\nKruskal, J. 1964a. “Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.” Psychometrika 29 (1): 1–27.\n\n\nKruskal, J. 1964b. “Nonmetric Multidimensional Scaling: A Numerical Method.” Psychometrika 29 (2): 115–29.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nLarsen, J, and L Clemmensen. 2019. “Deep Learning for Chemometric and Non-Translational Data.” https://arxiv.org/abs/1910.00391.\n\n\nLê Cao, KA, D Rossouw, C Robert-Granié, and P Besse. 2008. “A Sparse PLS for Variable Selection When Integrating Omics Data.” Statistical Applications in Genetics and Molecular Biology 7 (1).\n\n\nLee, D, and S Seung. 1999. “Learning the Parts of Objects by Non-Negative Matrix Factorization.” Nature 401 (6755): 788–91.\n\n\nLee, D, and S Seung. 2000. “Algorithms for Non-Negative Matrix Factorization.” Advances in Neural Information Processing Systems 13.\n\n\nLiu, R, J Parelius, and K Singh. 1999. “Multivariate Analysis by Data Depth: Descriptive Statistics, Graphics and Inference.” The Annals of Statistics 27 (3): 783–858.\n\n\nLiu, Y, and W Rayens. 2007. “PLS and Dimension Reduction for Classification.” Computational Statistics, 189–208.\n\n\nMichelucci, I. 2022. “An Introduction to Autoencoders.” arXiv.\n\n\nMozharovskyi, P, K Mosler, and T Lange. 2015. “Classifying Real-World Data with the DD\\(\\alpha\\)-Procedure.” Advances in Data Analysis and Classification 9 (3): 287–314.\n\n\nNing, B, and N Ning. 2021. “Spike and Slab Bayesian Sparse Principal Component Analysis.” arXiv.\n\n\nNordhausen, K, and H Oja. 2018. “Independent Component Analysis: A Statistical Perspective.” Wiley Interdisciplinary Reviews: Computational Statistics 10 (5): e1440.\n\n\nSainburg, T, L McInnes, and TQ Gentner. 2020. “Parametric UMAP: Learning Embeddings with Deep Neural Networks for Representation and Semi-Supervised Learning.” arXiv.\n\n\nSammon, J. 1969. “A Nonlinear Mapping for Data Structure Analysis.” IEEE Transactions on Computers 100 (5): 401–9.\n\n\nSchein, A, L Saul, and L Ungar. 2003. “A Generalized Linear Model for Principal Component Analysis of Binary Data.” In Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics, edited by C Bishop and B Frey, R4:240–47. Proceedings of Machine Learning Research.\n\n\nSchölkopf, B, A Smola, and KR Müller. 1998. “Nonlinear Component Analysis as a Kernel Eigenvalue Problem.” Neural Computation 10 (5): 1299–319.\n\n\nShen, H, and JZ Huang. 2008. “Sparse Principal Component Analysis via Regularized Low Rank Matrix Approximation.” Journal of Multivariate Analysis 99 (6): 1015–34.\n\n\nSinnott, R. 1984. “Virtues of the Haversine.” Sky and Telescope 68 (2): 158.\n\n\nStone, M, and R Brooks. 1990. “Continuum Regression: Cross-Validated Sequentially Constructed Prediction Embracing Ordinary Least Squares, Partial Least Squares and Principal Components Regression.” Journal of the Royal Statistical Society: Series B (Methodological) 52 (2): 237–58.\n\n\nTenenbaum, J, V Silva, and J Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290 (5500): 2319–23.\n\n\nTibshirani, R, T Hastie, B Narasimhan, and G Chu. 2003. “Class Prediction by Nearest Shrunken Centroids, with Applications to DNA Microarrays.” Statistical Science, 104–17.\n\n\nTipping, M, and C Bishop. 1999. “Probabilistic Principal Component Analysis.” Journal of the Royal Statistical Society Series B: Statistical Methodology 61 (3): 611–22.\n\n\nTorgerson, W. 1952. “Multidimensional Scaling: I. Theory and Method.” Psychometrika 17 (4): 401–19.\n\n\nTukey, J. 1975. “Mathematics and the Picturing of Data.” In Proceedings of the International Congress of Mathematicians, 2:523–31.\n\n\nVan der Maaten, L, and G Hinton. 2008. “Visualizing Data Using t-SNE.” Journal of Machine Learning Research 9 (11).\n\n\nVisuri, S, V Koivunen, and H Oja. 2000. “Sign and Rank Covariance Matrices.” Journal of Statistical Planning and Inference 91 (2): 557–75.\n\n\nWang, S, and J Zhu. 2007. “Improved Centroids Estimation for the Nearest Shrunken Centroid Classifier.” Bioinformatics 23 (8): 972–79.\n\n\nWold, S, H Martens, and H Wold. 1983. “The Multivariate Calibration Problem in Chemistry Solved by the PLS Method.” In Proceedings from the Conference on Matrix Pencils. Heidelberg: Springer-Verlag.\n\n\nWu, W, DL Massart, and S De Jong. 1997. “The Kernel PCA Algorithms for Wide Data. Part I: Theory and Algorithms.” Chemometrics and Intelligent Laboratory Systems 36 (2): 165–72.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#footnotes",
    "href": "chapters/embeddings.html#footnotes",
    "title": "7  Embeddings",
    "section": "",
    "text": "Retreived from https://chemom2019.sciencesconf.org/resource/page/id/13.html↩︎\nAs is, this is far too general. We could maximize the variance by making every value of \\(Q\\) equal to \\(\\infty\\). PCA, and other methods, impose an implicit constraint to limit the elements of \\(Q\\). In this case, PCA constrains the loading vectors to have unit length.↩︎\nAssuming that the \\(p\\) columns are linearly independent.↩︎\nThese wavelengths correspond to the 1st and 50th predictors.↩︎\nWe will see similar methods in ?sec-svm-cls.↩︎\nA note about some notation… We commonly think of the norm notation as \\(\\|\\boldsymbol{x}\\|_p = \\left(|x_1|^p + |x_2|^p + \\ldots + |x_n|^p\\right)^{1/p}\\). So what does the lack of a subscript in \\(||\\boldsymbol{x}||^2\\) mean? The convention is the sum of squares: \\(||\\boldsymbol{x}||^2 = x_1^2 + x_2^2 + \\ldots + x_n^2\\).↩︎\nSpecifically gradient descent with a user-defined learning rate.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html",
    "href": "chapters/interactions-nonlinear.html",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "8.1 Interactions\nWe saw in previous chapters that preprocessing is necessary because models can be sensitive to specific predictor characteristics. For instance, some models:\nPreprocessing methods address aspects of the predictors that place them in a form so that models can be built. This is what the model needs to function.\nWhile preprocessing techniques allow models to be built, they do not necessarily transform them in ways that help the model to identify predictive relationships with the outcome. This is the fundamental concept of feature engineering which addresses the question: what can we do to make it easier for the model to understand and predict the outcome.\nIn this chapter we will discuss techniques that address this question. For example, if the relationship between a predictor is nonlinear, how can we represent that predictor so that the relationship can be modeled? Or if two predictors work in conjunction with each other, then how can this information be engineered for a model? While feature engineering is not necessary for models to be built, it contains a crucial set of tools for improving model performance.\nLet’s look at a simple example. Occasionally, predictor pairs work better in unison rather than as main effects. For example, consider the data in Figure 8.1(a), where two predictors are:\nIn the predictors’ original form, there is a significant overlap between the two classes of samples. However, when the ratio of the predictors is used, the newly derived predictor better discriminates between classes (shown in Figure 8.1(b)). While not a general rule, the three data characteristics above suggest that the modeler attempts to form ratios from two or more predictors 1.\nIt is essential to understand that these data require transformation. We can put the original predictors into a model as-is. The model won’t produce an error but won’t have good predictive performance. We can induce a separation of the classes when a joint feature is used.\nThis aspect of feature engineering depends on the data set so it is difficult to enumerate all possible techniques. In this chapter, we’ll describe a few commonly used methods: splines, interactions, and discretization (a.k.a. binning).\nWhen building models for prediction, the majority of variation in the outcome is generally explained by the cumulative effect of the important individual predictors. For many problems, additional variation in the response can be explained by the effect of two or more predictors working in conjunction with each other.\nThe healthcare industry has long understood the concept of interactions among drugs for treating specific diseases Altorki et al. (2021). As an example of an interaction, consider treatment for the disease non-small cell lung cancer (NSCLC). In a recent study, patients with an advanced stage of NSCLC with an EGFR mutation were given either osimertinib alone or osimertinib in combination with traditional chemotherapy (Planchard et al. 2023). Patients taking the combination treatment had a significantly longer progression-free survival time than patients taking osmertinib alone. Hence, the interaction of the treatments is more effective than the single treatment. To summarize, two (or more) predictors interact if their combined impact is different (less or greater) than what would be expected from the added impact of each predictor alone.\nWe’ve already encountered an example of an interaction in Section 2.3 where the relationship between delivery time and the time of order differed across days of the week. This trend is reproduced in Figure 8.2(a). The telltale sign of the interaction is that the trendlines are not parallel with one another; they have different rates of increase and cross.\nHow would this plot change if there was no interaction between the order time and day? To illustrate, we estimated trendlines in a way that coerced the nonlinear trend for order time to be the same. Figure 8.2(b) shows the results: parallel lines for each day of the week.\nFigure 8.2: Delivery times versus the time of the order, colored by the day of the week. (a) A duplicate of a panel from Figure 2.2. (b) The same data with trendlines where the underlying model did not allow an interaction between the delivery day and hour.\nThis example demonstrates an interaction between a numeric predictor (hour of order) and a categorical predictor (day of the week). Interactions can also occur between two (or more) numeric or categorical predictors. Using the delivery data, let’s examine potential interactions solely between categorical predictor columns.\nFor regression problems, one visualization technique is to compute the means (or medians) of the outcome for all combinations of variables and then plot these means in a manner similar to the previous figure. Let’s look at the 27 predictors columns for whether a specific item was included in the order. The original column constains counts, but the data are mostly zero or one. We’ll look at two variables at a time and plot the four combinations of whether the item was ordered at all (versus not at all). Figure 8.3 shows two potential sets of interactions. The x-axis indicates whether Item 1 was in the order or not. The y-axis is the mean delivery time with 90% confidence intervals2.\nThe left panel shows the joint effect of items 1 and 9. The lines connecting the means are parallel. This indicates that each of these two predictors affects the outcome independently of one another. Specifically, the incremental change in delivery time is the same when item 9 is or is not included with item 1. The right panel has means for items 1 and 10. The mean delivery times are very similar when neither is contained in the order. Also, when only one of the two items is in the order, the average time is similarly small. However, when both are included, the delivery time becomes much larger. This means that you cannot consider the effect or either item 1 or 10 alone; their effect on the outcome occurs jointly.\nFigure 8.3: Interaction examples with two categorical predictors. Items 1 and 9 (left panel) do not interaction, while items 1 and 10 appear to have a strong interaction (right panel).\nAs a third example, interactions may occur between continuous predictors. It can be difficult to discover the interaction via visualization for two numeric predictors without converting one of the predictors to categorical bins. To illustrate the concept of an interaction between two numeric predictors, \\(x_1\\) and \\(x_2\\), let’s use a simple linear equation:\n\\[\ny = \\beta_0 + \\beta_{1}x_1 + \\beta_{2}x_2 + \\beta_{3}x_{1}x_{2} + \\epsilon\n\\tag{8.1}\\]\nThe \\(\\beta\\) coefficients represent the overall average response (\\(\\beta_0\\)), the average rate of change for each individual predictor (\\(\\beta_1\\) and \\(\\beta_2\\)), and the incremental rate of change due to the combined effect of \\(x_1\\) and \\(x_2\\) (\\(\\beta_3\\)) that goes beyond what \\(x_1\\) and \\(x_2\\) can explain alone. The parameters for this equation can be estimated using a technique such as ordinary least squares (REF). The sign and magnitude of \\(\\beta_3\\) indicate how and the extent to which the two predictors interact:\nTo understand this better, Figure 8.4 shows a contour plot of a predicted linear regression model with various combinations of the model slope parameters. The two predictors are centered at zero with values ranging within \\(x_j \\pm 4.0\\)). The default setting shows a moderate synergistic interaction effect since all of the \\(\\beta_j = 1.0\\)). In the plot, darker values indicate smaller predicted values.\nInteraction effects between predictors are sometimes confused with correlation between predictors. They are not the same thing. Interactions are defined by their relationship to the outcome while between-predictors correlations are unrelated to it. An interaction could occur independent of the amount of correlation between predictors.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-interactions",
    "href": "chapters/interactions-nonlinear.html#sec-interactions",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "When \\(\\beta_3\\) is positive, the interaction is synergistic since the response increases beyond the effect of either predictor alone.\n\nAlternatively, when \\(\\beta_3\\) is negative, the interaction is antagonistic since the response decreases beyond the effect of either predictor alone.\n\nA third scenario is when \\(\\beta_3\\) is essentially zero. In this case, there is no interaction between the predictors and the relationship between the predictors is additive.\nFinally, for some data sets, we may find that neither predictor is important individually (i.e., \\(\\beta_1\\) and \\(\\beta_2\\) are zero). However, the coefficient on the interaction term is not zero. Because this case occurs very infrequently, it is called atypical.\n\n\n\n\n\n\n#| label: fig-interaction-contours\n#| viewerHeight: 600\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-interaction-contours.R\")\n\napp\n\n\n\nFigure 8.4: Prediction contours for a linear regression model with Equation 8.1 with \\(\\beta_0 = 0\\). Darker values indicate smaller predictions.\n\n\n\n\n\n8.1.1 How Likely Are Interactions?\nWithin the context of statistical experimental design, Hamada and Wu (1992) discuss some probabilistic aspects of predictor importance. The effect sparsity principle is that there are often few predictors that are relevant for predicting the outcome. Similarly, the effect hierarchy principle states that “main effects” (i.e. a feature involving only predictor) are more likely to occur than interactions. Also, as more predictors are involved in the interaction, the less likely they become. Finally, the heredity principle conjectures that if an interaction is important, it is very likely that the corresponding main effects are likely too. Chipman (1996) further expands this principle.\nThe original context of these principles was envisioned for screening large numbers of predictors in experimental designs; they are still very relevant for the analysis of tabular data.\n\n\n8.1.2 Detecting Interactions\nAn ideal scenario would be that we would know which predictors interact before modeling the data. If this would be the case, then these terms could be included in a model. This would be ideal because the model could more easily find the relationship with the response, thus leading to better predictive performance. Unfortunately, knowledge of which predictors interact is usually not available prior to initiating the modeling process.\nIf meaningful interactions are unknown before modeling, can models still discover and utilize these potentially important features? Recent studies using various advanced modeling techniques have shown that some methods can inherently detect interactions. For example, tree-based models (Elith, Leathwick, and Hastie 2008), random forests (García-Magariños et al. 2009), boosted trees (Lampa et al. 2014), and support vector machines (Chen et al. 2008) are effective at uncovering them.\nIf modern modeling techniques can naturally find and utilize interactions, then why is it necessary to spend any time uncovering these relationships? The first reason is to improve interpretability. Recall the trade-off between prediction and interpretation that was discussed in Section TODO. More complex models are less interpretable and generally more predictive, while simpler models are more interpretable and less predictive. Therefore, if we know which interaction(s) are important, we can include these in a simpler model to enable a better interpretation. A second reason to spend time uncovering interactions is to help improve the predictive performance of models.\nWhat should we do if we have a candidate set of interactions? If we are using a more formal statistical model, such as linear or logistic regression, the traditional tool for evaluating whether additional model terms in a set of nested models are worth including is the conventional analysis of variance (ANOVA). This uses the statistical likelihood value as the objective function, which is equivalent to comparing the RMSE values between models for linear regression 3. The error reduction and how many additional terms are responsible for the improvement are computed. Using these results and some probability assumptions about the data, we can formally test the null hypothesis that the additional parameters all have coefficient values of zero.\nLooking at the delivery data, our model in Section 2.4 included a single set of interactions (e.g., hour-by-day). What if we included one more interaction: item 1 \\(\\times\\) item 10? Table 8.1 shows the ANOVA results. The RMSE computed on the training set is listed in the first and second columns. The reduction by including this additional model term is 0.18 (decimal minutes). Assuming normality of the model residuals, the p-value for this test is exceedingly small. This indicates that there is no evidence that this parameter is truly zero. In other words, there is strong evidence that the inclusion of the interaction helps explain the response. This statistical difference may not make much of a practical difference. However, machine learning models often behave like “a game of inches” where every small improvement adds up to an overall improvement that matters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Set RMSE\n\n\n\nValidation Set\n\n\n\nInteractions\nDecimal\nTime\nDeg. Free.\np-Value\nMAE\nRMSE\n\n\n\n\nHour x Day\n2.30\n(2m, 17s)\n5,890\n\n1.61\n2.29\n\n\nAdd One Item Interaction\n2.12\n(2m, 6s)\n5,889\n10-219.8\n1.56\n2.10\n\n\nAdd All Item Interactions\n2.06\n(2m, 3s)\n5,539\n10-18.4\n1.60\n2.13\n\n\n\n\n\n\n\n\n\n\nTable 8.1: ANOVA table and validation set statistics for three models with different interaction sets.\n\n\n\n\n\n\n\nWhat about the other two-way interactions between the other item predictors? Since the training set is fairly large (compared to the current number of model parameters), it is feasible to include the remaining 350 pairwise interactions and use the ANOVA method to validate this choice. These results are contained in the third row of Table 8.1 where the RMSE dropped further by 0.05 minutes. The p-value is also very small, indicating that there is no evidence that all of the 350 parameter estimates are zero. More extensive testing would be required to determine which actually are zero. This can be tedious and a potentially dangerous “fishing expedition” that could result in serious bias creeping into the modeling process.\nOne problem with the ANOVA method is that it calculates the model error using the training set, which we know may not indicate what would occur with previously unseen data. In fact, it is well known that, for linear regression via ordinary least squares estimation, it is impossible for the training set error to ever increase when adding new model terms. Therefore, was the drop in RMSE when all interactions were included due to this fact? Or was it due to other important interactions that were included? To understand this we can turn to the validation set to provide confirmation. You can see this in Table 8.1 where the validation set RMSE and MAE values are included. Note that, when the large set of interactions were added, these metrics both increase in the validation set, a result contrary to what the ANOVA results tell us.\nThe observed and predicted visualizations for each model in Table 8.1 are shown in Figure 8.5. Adding the additional interaction yielded a slight numerical improvement. However, the visualization in the middle panel shows fewer very large residuals. The figure also shows the model results that include the full set of 351 two-way interactions; this panel shows no significant reduction in large residuals, further casting doubt on using the entire set.\n\n\n\n\n\n\n\n\nFigure 8.5: Validation set predicted vs. observed delivery times for linear regression models with and without additional interactions.\n\n\n\n\n\nSo far, we have used visualizations to unearth potential interactions. This can be an effective strategy when the number of potential interactions is large, but the visual nature of this process is subjective. There are some specialized quantitative tools for identifying interactions. For example, Lim and Hastie (2015) used regularized generalized linear models (sections TODO) to estimate all possible two-way interactions and use a penalization method to determine which should be retained. Miller (1984) and Kuhn and Johnson (2019) (Section 7.4.3) describe the feasible solution algorithm, an iterative search method for interaction discovery. Another, which we will now describe in more detail, is Friedman’s H-statistic (Friedman and Popescu 2008).\nWe can estimate the joint effect of a set of predictors on the outcome as well as the effect of individual predictors. If we thought that only main effects and two-factor interactions were possible, we could factor out the individual effects from the joint effect. The leftover predictive ability would then be due to interactions. Consider the linear model in Equation 8.1. The joint effect would include all possible model terms associated with a predictor. We can also create main effects too:\n\\[\\begin{align}\nf(x_1, x_2) &= \\beta_{1}x_1 + \\beta_{2}x_2 + \\beta_{3}x_{1}x_{2} \\notag \\\\\nf(x_1) &= \\beta_{1}x_1  \\notag \\\\\nf(x_2) &= \\beta_{2}x_2 \\notag\n\\end{align}\\]\nTo isolate the potential interaction effect:\n\\[\nf(x_1\\times x_2) = f(x_1, x_2) - f(x_1) - f(x_2) = \\beta_{3}x_{1}x_{2}\n\\tag{8.2}\\]\nThis shows that, for this situation, we can isolate the effect of an interaction by removing any other systematic effects in the data4. Equation 8.2 is based on a simple parametric linear model. For models with more complex prediction equations, we can’t analytically pick out which model parameters should be used to investigate potential interactions.\nHowever, for any model, the joint and marginal effects can be quantified using partial dependence profiles (PDP) (Molnar (2020), Section 8.1). First, we determine a sequence of values covering the observed range of the predictor(s) of interest. Then we randomly sample a data point, perhaps from the training set, and over-ride the value of the predictor of interest with values from the grid. This produces a prediction profile over the grid. We can repeat this process many times to approximate \\(f(\\cdot)\\) by averaging the multiple prediction values for each grid point.\nFigure 8.6 visualizes the PDP data derived from using the emsembles of regression trees for \\(f(hour)\\), \\(f(day)\\), and \\(f(hour, day)\\). The first panel shows the random realizations of the relationship between delivery time and order hour from 1,000 randomly sampled rows of the training set. The results are unsurprising; we can see a similarity between these results and the initial visualizations in Figure 2.2. The single trend line in panel (b) is the average of these profiles for each value of the predictor (delivery hour). There appears to be, on average, a nonlinear effect of the delivery hour. The day of the week is an informative predictor and the joint effect profile in panel (d) shows that its effect induces different patterns in the delivery hour. If this were not the case, the patterns in panels (b) and (c) would, when combined, approximate the pattern in panel (d).\n\n\n\n\n\n\n\n\nFigure 8.6: Partial dependence profiles for the food delivery hour and day, derived for the tree-based ensemble. Panel (a) shows the individual grid predictions from 1,000 randomly sampled points in the training set. The other panels show the average trends.\n\n\n\n\n\nFriedman’s H-statistic can quantify the effect of interaction terms using partial dependence profiles. When investigating two-factor interactions between predictors \\(j\\) and \\(j'\\), the statistic is\n\\[\nH^2_{jj'}=\\frac{\\sum_\\limits{i=1}^B\\left[\\hat{f}(x_{ij},x_{ij'})-\\hat{f}(x_{ij})-\\hat{f}(x_{ij'})\\right]}{ \\sum_\\limits{i=1}^B\\hat{f}^2(x_{ij}, x_{ij'})}\n\\]\nwhere the \\(\\hat{f}(x_{ij})\\) term represents the partial dependence profile for predictor \\(j\\) for sample point \\(i\\) along the grid for that predictor and \\(B\\) is the number of training set samples. The denominator captures the total joint effect of predictors \\(j\\) and \\(j'\\) so that \\(H^2\\) can be interpreted as the fraction of the joint effect explained by the potential interaction.\nFor a set of \\(p\\) predictors, we could compute all \\(p(p-1)/2\\) pairwise interactions and rank potential interactions by their statistic values. This can become computationally intractable at some point. One potential shortcut suggested by the heredity principle is to quantify the importance of the \\(p\\) predictors and look at all pairwise combinations of the \\(p^*\\) most important values. For \\(p^* = 10\\), Figure 8.7 shows the top five interactions detected by this procedure. We’ve already visually identified the hour \\(\\times\\) day interaction, so seeing this in the rankings is comforting. However, another large interaction effect corresponds to the variables for items #1 and #10. Discovering this led us to visually confirm the effect back in Figure 8.3.\n\n\n\n\n\n\n\n\nFigure 8.7: \\(H^2\\) statistics for the top ten pairwise interaction terms.\n\n\n\n\n\nNote that the overall importance of the features are being quantified. From this analysis alone, we are not informed that the relationships between the hour and distance predictors and the outcome are nonlinear. It also doesn’t give any sense of the direction of the interaction (e.g., synergistic or antagonistic). We suggest using this tool early in the exploratory data analysis process to help focus visualizations on specific variables to understand how they relate to the outcome and other predictors.\nThere is a version of the statistic that can compute how much a specific predictor is interacting with any other predictor. It is also possible to compute a statistical test to understand if the H statistic is different than zero. In our opinion, it is more helpful to use these values as diagnostics rather than the significant/insignificant thinking that often accompanies formal statistical hypothesis testing results.\nInglis, Parnell, and Hurley (2022) discuss weak spots in the usage of the H-statistic, notably that correlated predictors can cause abnormally large values. This is an issue inherited from the use of partial dependence profiles (Apley and Zhu 2020).\nAlternatively, Greenwell, Boehmke, and McCarthy (2018) measures the potential for variables to interact by assessing the “flatness” over regions of the partial dependence profile. The importance of a predictor, or a pair of predictors, is determined by computing the standard deviation of the flatness scores over regions. For example, Figure 8.3 shows a flat profile for the item 1 \\(\\times\\) item 9 interaction while the item 1 \\(\\times\\) item 10 interaction has different ranges of means across values of item 1. Figure 8.8 shows ten interactions with the largest flatness importance statistics. As with the \\(H^2\\) results, the hour-by-day interaction. The two top ten lists have 6 other interactions in common.\n\n\n\n\n\n\n\n\nFigure 8.8: Importance statistics for the top ten pairwise interaction terms.\n\n\n\n\n\nOther alternatives can be found in Hooker (2004), Herbinger, Bischl, and Casalicchio (2022), and Oh (2022).\nThe two PDP approaches we have described apply to any model capable of estimating interactions. There are also model-specific procedures for describing which joint effects are driving the model (if any). One method useful for tree-based models is understanding if two predictors are used in consecutive splits in the data. For example, Figure 2.5 showed a shallow regression tree for the delivery time data. The terminal nodes had splits with orderings \\(\\text{hour}\\rightarrow\\text{hour}\\) and \\(\\text{hour}\\rightarrow\\text{day} \\rightarrow\\text{distance}\\) (twice). This implies that these three predictors have a higher potential to be involved in interactions with one another.\nKapelner and Bleich (2013) describe an algorithm that counts how many times pairs of variables are used in consecutive splits in a tree. They demonstrate this process with Bayesian Adaptive Regression Trees (BART, Section TODO) which creates an ensemble of fairly shallow trees. Figure 8.9 shows the top ten pairwise interactions produced using this technique. Note that the BART implementation split categorical predictors via single categories. For the delivery data, this means that the tree would split on a specific day of the week (.i.e., Friday or not) instead of splitting all of the categories at once. This makes a comparison between the other two methods difficult. However, it can be seen that the hour \\(\\times\\) day interaction has shown to be very important in all three methods.\n\n\n\n\n\n\n\n\nFigure 8.9: BART model statistics identifying interactions by how often they occur in the tree rules.\n\n\n\n\n\nCaution should be exercised with this method. First, some tree ensembles randomly sample a subset of predictors for each split (commonly known as \\(m_{try}\\)). For example, if \\(m_{try} = 4\\), a random set of four predictors are the only ones considered for that split point. This deliberately coerces non-informative splits; we are not using the best possible predictors in a split. This can dilute the effectiveness of counting predictors in successive splits. Second, as discussed in ?sec-classification-trees, many tree-based splitting procedures disadvantage predictors with fewer unique values. For example, in the delivery data, the distance and hour predictors are more likely to be chosen for splitting than the item predictors even if they are equally informative. There are splitting procedures that correct for this bias, but users should be aware of the potential impartiality.\nThe H-statistic and its alternatives can be an incredibly valuable tool for learning about the data early on as well as suggesting new features that should be included in the model. They are, however, computationally expensive.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-polynomials",
    "href": "chapters/interactions-nonlinear.html#sec-polynomials",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.2 Polynomial Basis Expansions",
    "text": "8.2 Polynomial Basis Expansions\nIn the food delivery data, we have thoroughly demonstrated that there are quantitative predictors that have nonlinear relationships with the outcome. In Section 2.4, two of the three models for these data could naturally estimate nonlinear trends. Linear regression, however, could not. To fix this, the hour predictor spawned multiple features called spline terms and adding these to the model enables it to fit nonlinear patterns. This approach is called a basis expansion. In this section, we’ll consider (global) polynomial basis expansions.\nThe most traditional basis function is the polynomial expansion. If we start with a simple linear regression model with a single predictor:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\epsilon_i\n\\]\nthe expansion would add additional terms with values of the predictor exponentiated to the \\(p^{th}\\) power5. For a cubic polynomial model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_{i}^3 + \\epsilon_i\n\\tag{8.3}\\]\nThis is a linear model in the sense that it is linear in the statistical parameters (the \\(\\beta\\) values), so we can use standard parameter estimation procedures (e.g., least squares).\nFigure 8.10 shows data from Bralower et al. (1997) which was collected to understand the relationship between the age of a fossil and the ratio of two radioactive isotopes. The data set contains 106 data points for these two variables. The relationship is highly nonlinear, with several regions where the data have significant convex or concave patterns.\nFor simplicity, a fit from a cubic model Equation 8.3 is shown as the default. While the pattern is nonlinear, the line does not conform well to the data. The lines above the plot show the three basis functions that, for \\(p=3\\), correspond to linear (\\(x\\)), quadratic (\\(x^2\\)), and cubic (\\(x^3\\)) terms. The effects of these terms are blended using their estimated slopes (\\(\\hat{\\beta}_j\\)) to produce the fitted line. Notice that the cubic fit is very similar to the cubic basis shown at the top of the figure. The estimated coefficient for the cubic term is much larger than the linear or quadratic coefficients\n\n\n\n\n#| label: fig-global-polynomial\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(splines2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(aspline)\n\ndata(fossil)\n\nui &lt;- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-3, 6, -3)),\n    sliderInput(\n      \"global_deg\",\n      label = \"Polynomial Degree\",\n      min = 1L, max = 20L, step = 1L, value = 3L,\n      ticks = TRUE\n    ) # sliderInput\n  ), # layout_columns\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 10, -1)),\n    as_fill_carrier(plotOutput('global'))\n  )\n)\n\n\nserver &lt;- function(input, output, session) {\n  \n  light_bg &lt;- \"#fcfefe\" # from aml4td.scss\n  grid_theme &lt;- bs_theme(\n    bg = \"#fcfefe\", fg = \"#595959\"\n  )\n  \n  # ------------------------------------------------------------------------------\n  \n  theme_light_bl&lt;- function(...) {\n    \n    ret &lt;- ggplot2::theme_bw(...)\n    \n    col_rect &lt;- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n    ret$panel.background  &lt;- col_rect\n    ret$plot.background   &lt;- col_rect\n    ret$legend.background &lt;- col_rect\n    ret$legend.key        &lt;- col_rect\n    \n    larger_x_text &lt;- ggplot2::element_text(size = rel(1.25))\n    larger_y_text &lt;- ggplot2::element_text(size = rel(1.25), angle = 90)\n    ret$axis.text.x &lt;- larger_x_text\n    ret$axis.text.y &lt;- larger_y_text\n    ret$axis.title.x &lt;- larger_x_text\n    ret$axis.title.y &lt;- larger_y_text  \n    \n    ret$legend.position &lt;- \"top\"\n    \n    ret\n  }\n  \n  col_rect &lt;- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n  \n  maybe_lm &lt;- function(x) {\n    try(lm(y ~ poly(x, input$piecewise_deg), data = x), silent = TRUE)\n  }\n  \n  expansion_to_tibble &lt;- function(x, original, prefix = \"term \") {\n    cls &lt;- class(x)[1]\n    nms &lt;- recipes::names0(ncol(x), prefix)\n    colnames(x) &lt;- nms\n    x &lt;- as_tibble(x)\n    x$variable &lt;- original\n    res &lt;- tidyr::pivot_longer(x, cols = c(-variable))\n    if (cls != \"poly\") {\n      res &lt;- res[res$value &gt; .Machine$double.eps,]\n    }\n    res\n  } \n  \n  mult_poly &lt;- function(dat, degree = 4) {\n    rng &lt;- extendrange(dat$x, f = .025)\n    grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n    grid_df &lt;- tibble(x = grid)\n    feat &lt;- poly(grid_df$x, degree)\n    res &lt;- expansion_to_tibble(feat, grid_df$x)\n    \n    # make some random names so that we can plot the features with distinct colors\n    rand_names &lt;- lapply(1:degree, function(x) paste0(sample(letters)[1:10], collapse = \"\"))\n    rand_names&lt;- unlist(rand_names)\n    rand_names &lt;- tibble(name = unique(res$name), name2 = rand_names)\n    res &lt;- \n      dplyr::inner_join(res, rand_names, by = dplyr::join_by(name)) %&gt;% \n      dplyr::select(-name) %&gt;% \n      dplyr::rename(name = name2)\n    res\n  }\n  \n  # ------------------------------------------------------------------------------\n  \n  spline_example &lt;- tibble(x = fossil$age, y = fossil$strontium.ratio)\n  rng &lt;- extendrange(fossil$age, f = .025)\n  grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n  grid_df &lt;- tibble(x = grid)\n  alphas &lt;- 1 / 4\n  line_wd &lt;- 1.0\n  \n  base_p &lt;-\n    spline_example %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point(alpha = 3 / 4, pch = 1, cex = 3) +\n    labs(x = \"Age\", y = \"Isotope Ratio\") +\n    lims(x = rng) +\n    theme_light_bl()\n  \n  output$global &lt;- renderPlot({\n    \n    poly_fit &lt;- lm(y ~ poly(x, input$global_deg), data = spline_example)\n    poly_pred &lt;- \n      predict(poly_fit, grid_df, interval = \"confidence\", level = .90) %&gt;% \n      bind_cols(grid_df)\n    \n    global_p &lt;- base_p\n    \n    if (input$global_deg &gt; 0) {\n      global_p &lt;-\n        global_p +\n        geom_ribbon(\n          data = poly_pred,\n          aes(y = NULL, ymin = lwr, ymax = upr),\n          alpha = 1 / 15) +\n        geom_line(\n          data = poly_pred,\n          aes(y = fit),\n          col = \"black\",\n          linewidth = line_wd) +\n        theme(\n          plot.margin = margin(t = -20, r = 0, b = 0, l = 0),\n          panel.background = col_rect,\n          plot.background = col_rect,\n          legend.background = col_rect,\n          legend.key = col_rect\n        )\n      \n      feature_p &lt;-\n        poly(grid_df$x,  input$global_deg) %&gt;% \n        expansion_to_tibble(grid_df$x) %&gt;% \n        ggplot(aes(variable, y = value, group = name, col = name)) +\n        geom_line(show.legend = FALSE) + # , linewidth = 1, alpha = 1 / 2\n        theme_void() +\n        theme(\n          plot.margin = margin(t = 0, r = 0, b = -20, l = 0),\n          panel.background = col_rect,\n          plot.background = col_rect,\n          legend.background = col_rect,\n          legend.key = col_rect\n        ) +\n        scale_color_viridis(discrete = TRUE, option = \"turbo\")\n      \n      p &lt;- (feature_p / global_p) + plot_layout(heights = c(1.5, 4))\n    }\n    \n    print(p)\n    \n  })\n  \n}\n\napp &lt;- shinyApp(ui = ui, server = server)\n\n\n\nFigure 8.10: A global polynomial approach to modeling data from Bralower et al. (1997) along with a representation of the features (above the data plot). The black dashed line is the true function and the shaded region is the 90% confidence intervals around the mean.\n\n\n\nWe might improve the fit by increasing the model complexity, i.e., adding additional polynomial terms. An eight-degree polynomial seems to fit better, especially in the middle of the data where the pattern is most dynamic. However, there are two issues.\nFirst, we can see from the 90% confidence bands around the line that the variance can be very large, especially at the ends. The fitted line and confidence bands go slightly beyond the range of the data (i.e., extrapolation). Polynomial functions become erratic when they are not close to observed data points. This becomes increasingly pathological as the polynomial degree increases. For example, in Figure 8.10, choosing degrees greater than 14 will show extreme uncertainty in the variance of the mean fit (the solid line). This is a good example of the variance-bias tradeoff discussed in Section 8.4 below.\nDecreasing the polynomial degree will result in less complex patterns that tend to underfit the data. Increasing the degree will result in the fitted line being closer to the data, but, as evidenced by the confidence interval in the shaded region, the uncertainty in the model explodes (especially near or outside of the range range). This is due to severe overfitting. Eventually, you can increase the degree until the curve passes through every data point. However, the fit for any other values will be wildly inaccurate and unstable.\nSecond, the trend is concave for age values greater than 121 when it probably should be flat. The issue here is that a global polynomial pattern is stipulated. The effect of each model term is the same across the entire data range6. We increased the polynomial degree to eight to improve the fit via additional nonlinearity. Unfortunately, this means that some parts of the data range will be too nonlinear than required, resulting in poor fit.\nA global polynomial is often insufficient because the nonlinear relationships are different in different data sections. There may be steep increases in one area and gradual increases in others. Rather than using a global polynomial, what if we used different basis expansions in regions with more consistent trends rather than a global polynomial?\nFigure 8.11 shows a crude approach where three different regions have different polynomial fits with the same degree. The pre-set degree and data ranges are probably as good as can be (after much manual fiddling) but the approach isn’t terribly effective. The curves do not connect. This discontinuity is visually discordant and most likely inconsistent with the true, underlying pattern we are attempting to estimate. Also, there are large jumps in uncertainty when predicting values at the extremes of each region’s range.\n\n\n\n\n#| label: fig-piecewise-polynomials\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(splines2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(aspline)\n\ndata(fossil)\n\nui &lt;- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 5, 5, -1)),\n    sliderInput(\n      \"piecewise_deg\",\n      label = \"Polynomial Degree\",\n      min = 0L, max = 6L, step = 1L, value = 4L\n    ), # sliderInput\n    sliderInput(\n      \"cuts\",\n      label = \"Cutpoints\",\n      min = 93L, max = 122L, step = 1, value = c(101, 118)\n    ) # sliderInput\n  ), # layout_columns\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 10, -1)),\n    as_fill_carrier(plotOutput('pieces'))\n  )      \n)\n\nserver &lt;- function(input, output, session) {\n  \n  light_bg &lt;- \"#fcfefe\" # from aml4td.scss\n  grid_theme &lt;- bs_theme(\n    bg = \"#fcfefe\", fg = \"#595959\"\n  )\n  \n  # ------------------------------------------------------------------------------\n  \n  theme_light_bl&lt;- function(...) {\n    \n    ret &lt;- ggplot2::theme_bw(...)\n    \n    col_rect &lt;- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n    ret$panel.background  &lt;- col_rect\n    ret$plot.background   &lt;- col_rect\n    ret$legend.background &lt;- col_rect\n    ret$legend.key        &lt;- col_rect\n    \n    larger_x_text &lt;- ggplot2::element_text(size = rel(1.25))\n    larger_y_text &lt;- ggplot2::element_text(size = rel(1.25), angle = 90)\n    ret$axis.text.x &lt;- larger_x_text\n    ret$axis.text.y &lt;- larger_y_text\n    ret$axis.title.x &lt;- larger_x_text\n    ret$axis.title.y &lt;- larger_y_text  \n    \n    ret$legend.position &lt;- \"top\"\n    \n    ret\n  }\n  \n  col_rect &lt;- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n  \n  maybe_lm &lt;- function(x) {\n    try(lm(y ~ poly(x, input$piecewise_deg), data = x), silent = TRUE)\n  }\n  \n  expansion_to_tibble &lt;- function(x, original, prefix = \"term \") {\n    cls &lt;- class(x)[1]\n    nms &lt;- recipes::names0(ncol(x), prefix)\n    colnames(x) &lt;- nms\n    x &lt;- as_tibble(x)\n    x$variable &lt;- original\n    res &lt;- tidyr::pivot_longer(x, cols = c(-variable))\n    if (cls != \"poly\") {\n      res &lt;- res[res$value &gt; .Machine$double.eps,]\n    }\n    res\n  } \n  \n  mult_poly &lt;- function(dat, degree = 4) {\n    rng &lt;- extendrange(dat$x, f = .025)\n    grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n    grid_df &lt;- tibble(x = grid)\n    feat &lt;- poly(grid_df$x, degree)\n    res &lt;- expansion_to_tibble(feat, grid_df$x)\n    \n    # make some random names so that we can plot the features with distinct colors\n    rand_names &lt;- lapply(1:degree, function(x) paste0(sample(letters)[1:10], collapse = \"\"))\n    rand_names&lt;- unlist(rand_names)\n    rand_names &lt;- tibble(name = unique(res$name), name2 = rand_names)\n    res &lt;- \n      dplyr::inner_join(res, rand_names, by = dplyr::join_by(name)) %&gt;% \n      dplyr::select(-name) %&gt;% \n      dplyr::rename(name = name2)\n    res\n  }\n  \n  # ------------------------------------------------------------------------------\n  \n  spline_example &lt;- tibble(x = fossil$age, y = fossil$strontium.ratio)\n  rng &lt;- extendrange(fossil$age, f = .025)\n  grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n  grid_df &lt;- tibble(x = grid)\n  alphas &lt;- 1 / 4\n  line_wd &lt;- 1.0\n  \n  base_p &lt;-\n    spline_example %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point(alpha = 3 / 4, pch = 1, cex = 3) +\n    labs(x = \"Age\", y = \"Isotope Ratio\") +\n    theme_light_bl()\n  \n  output$pieces &lt;- renderPlot({\n    \n    cuts &lt;- c(0, sort(input$cuts), 60)\n    piece_cols &lt;- c(\"#1B9E77\", \"#D95F02\", \"#7570B3\")\n    piece_p &lt;- base_p\n    \n    if (input$piecewise_deg &gt; 0) {\n      data_splt &lt;-\n        spline_example %&gt;%\n        dplyr::mutate(x_cut = cut(x, breaks = cuts, include.lowest = TRUE)) %&gt;%\n        tidyr::nest(.by = x_cut) %&gt;%\n        mutate(\n          fit = lapply(data, maybe_lm),\n          features = lapply(data, mult_poly, degree = input$piecewise_deg)\n        )\n      grid_splt &lt;-\n        dplyr::tibble(x = grid) %&gt;%\n        dplyr::mutate(x_cut = cut(x, breaks = cuts, include.lowest = TRUE))  %&gt;%\n        tidyr::nest(.by = x_cut)\n      \n      for (i in 1:3) {\n        sub_pred &lt;- grid_splt$data[[i]]\n        if (!inherits(data_splt$fit[[i]], \"try-error\")) {\n          sub_pred &lt;-\n            sub_pred %&gt;%\n            dplyr::bind_cols(predict(data_splt$fit[[i]], sub_pred, \n                                     interval = \"confidence\", level = .90))\n          \n          piece_p &lt;-\n            piece_p +\n            geom_ribbon(\n              data = sub_pred,\n              aes(y = NULL, ymin = lwr, ymax = upr),\n              alpha = 1 / 15\n            ) +\n            geom_line(\n              data = sub_pred,\n              aes(y = fit),\n              linewidth = line_wd\n            )\n        }\n      }\n      \n      set.seed(383) # to control colors\n      feature_p &lt;- \n        data_splt %&gt;% \n        dplyr::select(features) %&gt;% \n        tidyr::unnest(features) %&gt;% \n        ggplot(aes(x = variable, y = value, col = name)) + \n        geom_line(show.legend = FALSE) +\n        theme_void() +\n        theme(\n          plot.margin = margin(t = 0, r = 0, b = -20, l = 0),\n          panel.background = col_rect,\n          plot.background = col_rect,\n          legend.background = col_rect,\n          legend.key = col_rect\n        ) +\n        scale_color_viridis(discrete = TRUE, option = \"turbo\")\n      \n      p &lt;- (feature_p / piece_p) + plot_layout(heights = c(1, 4))\n      \n    }\n    \n    print(p)\n    \n  })\n}\n\napp &lt;- shinyApp(ui = ui, server = server)\n\n\n\nFigure 8.11: A piecewise polynomial approach to model the data from Figure 8.10.\n\n\n\nIt turns out that we can make sure that the different fitted curves can connect at the ends and that they do that smoothly. This is done via a technique called splines which yield better results and are more mathematically elegant.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-splines",
    "href": "chapters/interactions-nonlinear.html#sec-splines",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.3 Spline Functions",
    "text": "8.3 Spline Functions\nLet’s assume we use piecewise cubic polynomials in the regional curve fits. To make the curves connect, we can add additional features that isolate local areas of \\(x\\) via a new feature column such as \\(h(x - \\xi)^3\\) where\n\\[\nh(u)  =\n\\begin{cases} x & \\text{if $u &gt; 0$,}\n\\\\\n0 & \\text{if $u \\le 0$.}\n\\end{cases}\n\\]\nand \\(\\xi\\) defines one of the boundary points.\nThis zeros out parts of the predictor’s range that are greater than \\(\\xi\\). A smooth, continuous model would include a global polynomial expansion (i.e., \\(x\\), \\(x^2\\), and \\(x^3\\)) and these partial features for each value that defines the regions (these breakpoints, denoted with \\(\\xi\\), are called knots7). For example, if a model has three regions, the basis expansion has 3 + 2 = 5 feature columns. For the fossil data, a three region model with knots at \\(\\xi_1 = 107\\) and \\(\\xi_2 = 114\\) would have model terms\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x^2 + \\beta_3x^3 + \\beta_4h(x_i - 107)^3 + \\beta_5h(x_i - 114)^3 + \\epsilon_i\n\\tag{8.4}\\]\nThe model fit, with the default interior knots, shown in Figure 8.12 looks acceptable. As with global polynomials, there is a significant increase in the confidence interval width as the model begins to extrapolate slightly beyond the data used to fit the model.\n\n\n\n\n#| label: fig-simple-spline\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(aspline)\n\ndata(fossil)\n\nui &lt;- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-2, 8, -2)),\n    sliderInput(\n      \"cuts\",\n      label = \"Cutpoints\",\n      min = 93L, max = 122L, step = 1, value = c(107, 114)\n    ) # sliderInput\n  ), # layout_columns\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 10, -1)),\n    as_fill_carrier(plotOutput('splime_spline'))\n  )      \n)\n\nserver &lt;- function(input, output, session) {\n  \n  light_bg &lt;- \"#fcfefe\" # from aml4td.scss\n  grid_theme &lt;- bs_theme(\n    bg = \"#fcfefe\", fg = \"#595959\"\n  )\n  \n  # ------------------------------------------------------------------------------\n  \n  theme_light_bl&lt;- function(...) {\n    \n    ret &lt;- ggplot2::theme_bw(...)\n    \n    col_rect &lt;- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n    ret$panel.background  &lt;- col_rect\n    ret$plot.background   &lt;- col_rect\n    ret$legend.background &lt;- col_rect\n    ret$legend.key        &lt;- col_rect\n    \n    larger_x_text &lt;- ggplot2::element_text(size = rel(1.25))\n    larger_y_text &lt;- ggplot2::element_text(size = rel(1.25), angle = 90)\n    ret$axis.text.x &lt;- larger_x_text\n    ret$axis.text.y &lt;- larger_y_text\n    ret$axis.title.x &lt;- larger_x_text\n    ret$axis.title.y &lt;- larger_y_text  \n    \n    ret$legend.position &lt;- \"top\"\n    \n    ret\n  }\n  \n  col_rect &lt;- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n  \n  # ------------------------------------------------------------------------------\n  \n  spline_example &lt;- tibble(x = fossil$age, y = fossil$strontium.ratio)\n  rng &lt;- extendrange(fossil$age, f = .025)\n  grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n  grid_df &lt;- tibble(x = grid)\n  alphas &lt;- 1 / 4\n  line_wd &lt;- 1.0\n  \n  base_p &lt;-\n    spline_example %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point(alpha = 3 / 4, pch = 1, cex = 3) +\n    labs(x = \"Age\", y = \"Isotope Ratio\") +\n    theme_light_bl()\n  \n  output$splime_spline &lt;- renderPlot({\n    \n    spline_p &lt;- base_p\n    \n    h &lt;- function(x) {\n      ifelse(x &gt; 0, x, 0)\n    }\n    \n    mod_dat &lt;- \n      spline_example %&gt;% \n      mutate(\n        x_2 = x^2, \n        x_3 = x^3,\n        x_k_1 = pmax(h(x-min(input$cuts))^3, 0),\n        x_k_2 = pmax(h(x-max(input$cuts))^3, 0)\n      )\n    \n    grid_spln &lt;- \n      grid_df %&gt;% \n      mutate(\n        x_2 = x^2, \n        x_3 = x^3,\n        x_k_1 = pmax(h(x-min(input$cuts))^3, 0),\n        x_k_2 = pmax(h(x-max(input$cuts))^3, 0)\n      )\n    \n    features &lt;- \n      rbind(\n        tibble::tibble(x = grid_spln$x, value = grid_spln$x_k_1, term = \"4\") %&gt;% \n          filter(value != 0),\n        tibble::tibble(x = grid_spln$x, value = grid_spln$x_k_2, term = \"5\") %&gt;% \n          filter(value != 0)\n      )\n    \n    fit_1 &lt;- lm(y ~ ., data = mod_dat)\n    spline_pred &lt;- \n      predict(fit_1, grid_spln, interval = \"confidence\", level = .90) %&gt;% \n      bind_cols(grid_df)\n    \n    spline_p &lt;-\n      spline_p +\n      geom_ribbon(\n        data = spline_pred,\n        aes(y = NULL, ymin = lwr, ymax = upr),\n        alpha = 1 / 15\n      ) +\n      geom_line(\n        data = spline_pred,\n        aes(y = fit),\n        linewidth = line_wd\n      ) +\n      geom_vline(xintercept = min(input$cuts), col = \"#A6CEE3\", lty = 2, linewidth = 1) +\n      geom_vline(xintercept = max(input$cuts), col = \"#1F78B4\", lty = 2, linewidth = 1)\n    \n    term_p &lt;- \n      features %&gt;% \n      ggplot(aes(x, value, col = term)) +\n      geom_line(show.legend = FALSE, linewidth = 1) +\n      lims(x = rng) +\n      theme_void() +\n      theme(\n        plot.margin = margin(t = 0, r = 0, b = -20, l = 0),\n        panel.background = col_rect,\n        plot.background = col_rect,\n        legend.background = col_rect,\n        legend.key = col_rect\n      ) +\n      scale_color_brewer(palette = \"Paired\")\n    \n    p &lt;- (term_p / spline_p) + plot_layout(heights = c(1, 4))\n    \n    print(p)\n    \n  })\n}\n\napp &lt;- shinyApp(ui = ui, server = server)\n\n\n\nFigure 8.12: A simple cubic spline function with three regions. By default, the regions are partitions at age values of 107 and 114. The fitted curve has the same form as Equation 8.4. The two curves above the data plot indicate the range and values of the partial featured truncated using the function \\(h(\\cdot)\\).\n\n\n\nLet’s consider how each model parameter affects the predictor’s space by labeling the regions A, B, and C (from left to right). For region A, the model is a standard cubic function (using \\(\\beta_0\\) though \\(\\beta_3\\)). Region B adds the effect of the first partial function by including \\(\\beta_4\\). Finally, region C is a function of all five model parameters. This representation of a spline is called a truncated power series and is mostly used to demonstrate how spline functions can flexibly model local trends.\n\nThe word “spline” is used in many different contexts. It’s confusing, but we’ll try to clear it up.\nWe’ll define a “spline” or “spline function” as a collection of features that represent continuous and smooth piece-wise polynomial features of a single-number predictor (with some specific properties).\n“Regression splines” usually describe the use of splines of one or more predictors in a model fit such as ordinary least squares (Wood 2006; Arnold, Kane, and Lewis 2019).\nThe term “smoothing splines” refers to a procedure that encapsulates both a spline function as well as an estimation method to fit it to the data (usually via regularization). There is typically a knot for each unique data point in \\(x\\) (Wang 2011).\nAlso, there are many types of spline functions; we’ll focus on one but describe a few others at the end of this section.\n\nA different method for representing spline basis expansions is the B-spline (De Boor 2003). The terms in the model are parameterized in a completely different manner. Instead of the regions using increasingly more model parameters to define the regional fit, B-spline terms have more localized features such that only a few are “active” (i.e., non-zero) at a particular point \\(x_0\\). This can be seen in Figure 8.13 which contains the basis function values for the truncated power series and B-spines\n\n\n\n\n\n\n\n\nFigure 8.13: Two representations of a cubic spline for the fossil data with knots at 107 and 114. The top panel illustrates the model terms defined by Equation 8.4. The bottom panel shows the B-spline representation. All lines have been scaled to the same y-axis range for easy visualization.\n\n\n\n\n\nB-spline parameterizations are used primarily for their numerical properties and, for this reason, we’ll use them to visualize different types of splines.\nBefore considering another type of spline function, let’s look at one practical aspect of splines: choosing the knots.\n\n8.3.1 Defining the Knots\nHow do we choose the knots? We can select them manually, and some authors advocate for this point of view. As stated by Breiman (1988):\n\nMy impression, after much experimentation, is the same - that few knots suffice providing that they are in the right place.\n\nA good example is seen in Figure 8.12 with the fossil data. Using the knot positioning widget at the top, we can find good and bad choices for \\(\\xi_1\\) and \\(\\xi_2\\); knot placement can matter. However, hand-curating each feature becomes practically infeasible as the number of knots/features increases. Otherwise, they can be set algorithmically.\nThere are two main choices for automatic knot selection. The first uses a sequence of equally-spaced values encompassing the predictor space. The second is to estimate percentiles of the predictor data so that regions have about the same number of values they capture. For example, a fourth degree of freedom spline would have three split points within the data arranged at the 25th, 50th, and 75th percentiles (with the minimum and maximum values bracketing the outer regions).\nIt would be unwise to place knots as evenly spaced values between the minimum and maximum values. Without taking the distribution of the predictor into account, it is possible that the region between some knots might not contain any training data. Since there are usually several overlapping spline features for a given predictor value, we can still estimate the model despite empty regions (but it should be avoided).\nMany, but not all, splines specify how complex the fit should be in the area between the knots. However, cubic splines are a common choice because they allow for greater flexibility than linear or quadratic fits, but are not overly flexible, which could lead to over-fitting. Increasing the polynomial degree beyond three has more disadvantages than advantages. Recall that, in Figure 8.10, variance exploded at the ends of the predictor distribution when the polynomial degree was very large.\n\n\n8.3.2 Natural cubic splines\nThe type of spline that we suggest using by default is the natural cubic spline8 (Stone and Koo 1985; Arnold, Kane, and Lewis 2019; Gauthier, Wu, and Gooley 2020). It uses cubic fits in the interior regions and linear fits in regions on either end of the predictor distribution. For a simple two-knot model similar to Equation 8.4, there are only three slope parameters (the global quadratic and cubic terms are not used):\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2h(x_i - \\xi_1)^3 + \\beta_3h(x_i - \\xi_2)^3 + \\epsilon_i\n\\tag{8.5}\\]\nFor the fossil data, Figure 8.14 shows how this model performs with different numbers of knots chosen using quantiles. The 90% confidence bands show an increase in uncertainty when extrapolating, but the increase is orders of magnitude smaller than a global polynomial with the same number of model terms.\n\n\n\n\n#| label: fig-natural-cubic-spline\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(splines2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(aspline)\n\ndata(fossil)\n\nui &lt;- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-3, 6, -3)),\n    sliderInput(\n      \"spline_df\",\n      label = \"# Spline Terms\",\n      min = 3L, max = 20L, step = 1L, value = 9L\n    ), # sliderInput\n  ), # layout_columns\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 10, -1)),\n    as_fill_carrier(plotOutput('spline'))\n  )         \n)\n\nserver &lt;- function(input, output, session) {\n  \n  light_bg &lt;- \"#fcfefe\" # from aml4td.scss\n  grid_theme &lt;- bs_theme(\n    bg = \"#fcfefe\", fg = \"#595959\"\n  )\n  \n  # ------------------------------------------------------------------------------\n  \n  theme_light_bl&lt;- function(...) {\n    \n    ret &lt;- ggplot2::theme_bw(...)\n    \n    col_rect &lt;- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n    ret$panel.background  &lt;- col_rect\n    ret$plot.background   &lt;- col_rect\n    ret$legend.background &lt;- col_rect\n    ret$legend.key        &lt;- col_rect\n    \n    larger_x_text &lt;- ggplot2::element_text(size = rel(1.25))\n    larger_y_text &lt;- ggplot2::element_text(size = rel(1.25), angle = 90)\n    ret$axis.text.x &lt;- larger_x_text\n    ret$axis.text.y &lt;- larger_y_text\n    ret$axis.title.x &lt;- larger_x_text\n    ret$axis.title.y &lt;- larger_y_text  \n    \n    ret$legend.position &lt;- \"top\"\n    \n    ret\n  }\n  \n  col_rect &lt;- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n  \n  maybe_lm &lt;- function(x) {\n    try(lm(y ~ poly(x, input$piecewise_deg), data = x), silent = TRUE)\n  }\n  \n  expansion_to_tibble &lt;- function(x, original, prefix = \"term \") {\n    cls &lt;- class(x)[1]\n    nms &lt;- recipes::names0(ncol(x), prefix)\n    colnames(x) &lt;- nms\n    x &lt;- as_tibble(x)\n    x$variable &lt;- original\n    res &lt;- tidyr::pivot_longer(x, cols = c(-variable))\n    if (cls != \"poly\") {\n      res &lt;- res[res$value &gt; .Machine$double.eps,]\n    }\n    res\n  } \n  \n  mult_poly &lt;- function(dat, degree = 4) {\n    rng &lt;- extendrange(dat$x, f = .025)\n    grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n    grid_df &lt;- tibble(x = grid)\n    feat &lt;- poly(grid_df$x, degree)\n    res &lt;- expansion_to_tibble(feat, grid_df$x)\n    \n    # make some random names so that we can plot the features with distinct colors\n    rand_names &lt;- lapply(1:degree, function(x) paste0(sample(letters)[1:10], collapse = \"\"))\n    rand_names&lt;- unlist(rand_names)\n    rand_names &lt;- tibble(name = unique(res$name), name2 = rand_names)\n    res &lt;- \n      dplyr::inner_join(res, rand_names, by = dplyr::join_by(name)) %&gt;% \n      dplyr::select(-name) %&gt;% \n      dplyr::rename(name = name2)\n    res\n  }\n  \n  # ------------------------------------------------------------------------------\n  \n  spline_example &lt;- tibble(x = fossil$age, y = fossil$strontium.ratio)\n  rng &lt;- extendrange(fossil$age, f = .025)\n  grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n  grid_df &lt;- tibble(x = grid)\n  alphas &lt;- 1 / 4\n  line_wd &lt;- 1.0\n  \n  base_p &lt;-\n    spline_example %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point(alpha = 3 / 4, pch = 1, cex = 3) +\n    labs(x = \"Age\", y = \"Isotope Ratio\") +\n    theme_light_bl()\n  \n  output$spline &lt;- renderPlot({\n    \n    spline_fit &lt;- lm(y ~ naturalSpline(x, df = input$spline_df), data = spline_example)\n    spline_pred &lt;- \n      predict(spline_fit, grid_df, interval = \"confidence\", level = .90) %&gt;% \n      bind_cols(grid_df)\n    \n    spline_p &lt;- base_p +\n      geom_ribbon(\n        data = spline_pred,\n        aes(y = NULL, ymin = lwr, ymax = upr),\n        alpha = 1 / 15) +\n      geom_line(\n        data = spline_pred,\n        aes(y = fit),\n        col = \"black\",\n        linewidth = line_wd)\n    \n    feature_p &lt;-\n      naturalSpline(grid_df$x, df = input$spline_df) %&gt;% \n      expansion_to_tibble(grid_df$x) %&gt;% \n      ggplot(aes(variable, y = value, group = name, col = name)) +\n      geom_line(show.legend = FALSE) + \n      lims(x = rng) +\n      theme_void() +\n      theme(\n        plot.margin = margin(t = 0, r = 0, b = -20, l = 0),\n        panel.background = col_rect,\n        plot.background = col_rect,\n        legend.background = col_rect,\n        legend.key = col_rect\n      ) +\n      scale_color_viridis(discrete = TRUE, option = \"turbo\")\n    \n    p &lt;- (feature_p / spline_p) + plot_layout(heights = c(1, 4))\n    \n    print(p)\n    \n  })\n  \n}\n\napp &lt;- shinyApp(ui = ui, server = server)\n\n\n\nFigure 8.14: A natural cubic spline fit to the data previously shown in Figure 8.12. Good fits occur when approximately nine degrees of freedom are used. The top curves are the B-spline representation of the model terms.\n\n\n\n\nGiven the simplicity of this spline function, the fixed polynomial degree, and its stability in the tails of the predictor’s distribution, we suggest choosing natural cubic splines for most problems.\n\nSplines can be configured in different ways. Two details to specify are how many regions should be separately modeled and how they are apportioned. The number of regions is related to how many features are used in the basis expansion. As the number of regions increases, so does the ability of the spline to adapt to whatever trend might be in the data. However, the risk of overfitting the model to individual data points increases as the flexibility increases.\nFor this reason, we often tune the amount of complexity that the spline will accommodate. Generally, since the number of regions is related to the number of features, we’ll refer to the complexity of the basis function via the number of degrees of freedom afforded by the spline. We don’t necessarily know how many degrees of freedom to use. There are two ways to determine this. First, we can treat the complexity of the spline as a tuning parameter and optimize it with the tuning methods mentioned in Chapters ?sec-grid and ?sec-iterative. Another option is to over-specify the number of degrees of freedom and let specialized training methods solve the potential issue of over-fitting. From Wood (2006):\n\nAn alternative to controlling smoothness by altering the basis dimension, is to keep the basis dimension fixed, at a size a little larger than it is believed it could reasonably be necessary, but to control the model’s smoothness by adding a “wiggliness” penalty to the least squares fitting objective.\n\nThis approach is advantageous when there are separate basis expansions for multiple predictors. The overall smoothness can be estimated along with the parameter estimates in the model. The process could be used with general linear models (Chapters ?sec-ols and ?sec-ordinary-logistic-regression) or other parametric linear models.\nSplines are extremely useful and are especially handy when we want to encourage a simple model (such as linear regression) to approximate the predictive performance of a much more complex black-box model (e.g., a neural network or tree ensemble). We’ll also see splines and spline-like features used within different modeling techniques, such as generalized additive models (Sections ?sec-reg-gam and ?sec-cls-gam), multivariate adaptive regression splines (?sec-mars), and a few others.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-variance-bias",
    "href": "chapters/interactions-nonlinear.html#sec-variance-bias",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.4 Sidebar: The Variance-Bias Tradeoff",
    "text": "8.4 Sidebar: The Variance-Bias Tradeoff\nOur discussion of basis expansions presents an excellent time for a segue to discuss an essential idea in statistics and modeling: the variance-bias tradeoff. This idea will be relevant in upcoming sections for resampling and specific models. At the end of this section, we’ll also connect it to Section 6.4.3.\nWhat does variance mean in terms of a machine learning model? In this context, it would quantify how much the fitted model changes if we slightly change the data. For example, would the curve change much if we repeat the data collection that produces the values in Section 8.2 and fit the sample model (for a fixed sample size and polynomial degree)? We can also look at the variance of prediction: for a specific new data point \\(x_0\\), how much intrinsic uncertainty is there? Similar to the discussion regarding extrapolation of global polynomials, we might want to compare how much the uncertainty changes as we move outside the training set’s range.\nBias is the difference between some estimate, like an average or a model prediction, and its true value. The true value is not the same as the data we collect; it is the unknowable theoretical value. For this reason, bias is often difficult to compute directly. For machine learning, the most pertinent idea of bias relates to how well a model can conform to the patterns we see in the data (hoping that the observed data are a good representation of the true values). Back in Figure 8.10, we saw that a linear model didn’t fit the data well. This is due to simple linear regression being a high-bias model because, without additional feature engineering, it cannot replicate nonlinear trends. Adding polynomial or spline terms decreased the model’s bias since it was more flexible.\n\nWe can think of bias in terms of the question: “How close are we aiming to the center of the target?” and variance as: “How much do our results vary when shooting at the target?”\n\nThe concepts of variance and bias are paired because they are often at odds with one another. For models that predict, we’d like a low variance, low bias model. In many cases, that can be exceedingly difficult to achieve. We can often lower bias by adding model complexity. However, increased complexity usually comes at the expense of stability (i.e., high variance). We will often be in the position of trying to find an acceptable compromise between the two. This was discussed with global polynomials; linear models were bad for our data, but adding too many polynomial terms adds unnecessary complexity and an explosion of variance (reflected in the confidence bands).\nThis leads us to the mean squared error (MSE). We’ve seen the root mean squared error already where we used it as a measure of accuracy for regression models. More generally, we can write it in terms of some unknown parameter \\(\\theta\\) and some estimate \\(\\hat{\\theta}\\) based on statistical estimation from data: \\[MSE = E\\left[(\\theta - \\hat{\\theta})^2\\right]\\]\nIt turns out that the MSE is a combination of model variance and (squared) bias (\\(\\theta - \\hat{\\theta}\\)):\n\\[MSE = E\\left[(\\theta - \\hat{\\theta})^2\\right] = Var[\\hat{\\theta}] + (\\theta - \\hat{\\theta})^2 + \\sigma^2\\]\nwhere \\(\\sigma^2\\) represents some unknown amount of “irreducible noise.” Because of this, MSE offers a statistic to minimize that accounts for both properties. This can offer a compromise between the two.\nTo illustrate this, we simulated a simple nonlinear model:\n\\[y_i = x_i^3 + 2\\exp\\left[-6(x_i - 0.3)^2\\right] + \\epsilon_i\\]\nwhere the error terms are \\(\\epsilon_i \\sim N(0, 0.1)\\) and the predictor values were uniformly spaced across [-1.2, 1.2]. Since we know the true values, we can compute the model bias.\nTo create a simulated data set, 31 samples were generated. Of these, 30 were used for the training set and one was reserved for the estimating the variance and bias. The training set values were roughly equally spaced across the range of the predictor. One simulated data set is shown in Figure 8.15, along with the true underlying pattern.\n\n\n\n\n\n\n\n\nFigure 8.15: A simulated data set with a nonlinear function. The test set point is the location where our simulation estimates the bias and variance.\n\n\n\n\n\nA linear model was estimated using ordinary least squares and a polynomial expansion for each simulated data set. The test set values were predicted, and the bias, variance, and root mean squared error statistics were computed. This was repeated 1,000 times for polynomial degrees ranging from one to twenty.\nFigure 8.16 shows the average statistic values across model complexity (i.e., polynomial degree). A linear model performs poorly for the bias due to underfitting (as expected). In panel (a), adding more nonlinearity results in a substantial decrease in bias because the model fit is closer to the true equation. This improvement plateaus at a sixth-degree polynomial and stays low (nearly zero). The variance begins low; even though the linear model is ineffective, it is stable. Once additional terms are added, the variance of the model steadily increases then explodes around a 20th degree polynomial9. This shows the tradeoff; as the model becomes more complex, it fits the data better but eventually becomes unstable.\n\n\n\n\n\n\n\n\nFigure 8.16: Simulation results showing variances, squared biases, and mean squared errors.\n\n\n\n\n\nThe mean squared error in panel (b) shows the combined effect of variance and bias. It shows a steep decline as we add more terms, then plateaus between seven and fifteenth-degree models. After that, the rapidly escalating variance dominates the MSE as it rapidly increases.\nThe variance-bias tradeoff is the idea that we can exploit one for the other. Let’s go back to the basic sample mean statistic. If the data being averaged are normally distributed, the simple average is an unbiased estimator: it is always “aiming” for the true theoretical value. That’s a great property to have. The issue is that many unbiased estimators can have very high variance. In some cases, a slight increase in bias might result in a drastic decrease in variability. The variance-bias tradeoff helps us when one of those two quantities is more important.\nFigure 8.17 shows a simple example. Suppose we have some alternative method for estimating the mean of a group that adds some bias while reducing variance. That tradeoff might be worthwhile if some bias will greatly reduce the variance. In our figure, suppose the true population mean is zero. The green curve represents the sample mean for a fixed sample size. It is centered at zero but has significant uncertainty. The other curve might be an alternative estimator that produces a slightly pessimistic estimate (its mean is slightly smaller than zero) but has 3-fold smaller variation. When estimating the location of the mean of the population, the biased estimate will have a smaller confidence interval for a given sample size than the unbiased estimate. While the biased estimate may slightly miss the target, the window of the location will be much smaller than the unbiased estimate. This may be a good idea depending on how the estimate will be used 10.\n\n\n\n\n\n\n\n\nFigure 8.17: A simple example of the variance-bias tradeoff.\n\n\n\n\n\nThe categorical encoding approach shown in Equation 6.3 from Section 6.4.3 is another example. Recall that \\(\\bar{y}_j\\) was the average daily rate for agent \\(j\\). That is an unbiased estimator but has high variance, especially for agents with few bookings. The more complex estimator \\(\\hat{y}_j\\) in Equation 6.3 is better because it is more reliable. That reliability is bought by biasing the estimator towards the overall mean (\\(\\mu_0\\)).\nAs mentioned, we’ll return to this topic several times in upcoming sections.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#discretization",
    "href": "chapters/interactions-nonlinear.html#discretization",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.5 Discretization",
    "text": "8.5 Discretization\nDiscretization11 is the process of converting quantitative data into a set of qualitative groups (a.k.a “bins”). The model uses these values instead of the original predictor column (perhaps requiring an additional step to convert them into binary indicator columns). Sadly, Figure 8.18 illustrates numerous analyses that we have witnessed. This example uses the food delivery data and breaks the order hour and distance predictors into six and three groups, respectively. It also converts the delivery time outcome into three groups. This visualization, colloquially known as the “Wall of Pie,” tries to explain how the two predictors affect the outcome categories, often with substantial subjectivity.\n\n\n\n\n\n\n\n\nFigure 8.18: An unfortunate visualization of two predictors and the outcome in the food delivery data. The colors of the pie chart reflect the binned delivery times at cut points of 20 and 30 minutes with lighter blue indicating earlier times.\n\n\n\n\n\nOur general advice, described in more detail in Kuhn and Johnson (2019), is that the first inclination should never be to engineer continuous predictors via discretization12. Other tools, such as splines, are both theoretically and practically superior to converting quantitative data to qualitative data. In addition, if a predictor can be split into regions that are predictive of the response, then methods that use recursive partitioning will be less arbitrary and more effective.\nThe literature supporting this is extensive, such as: Cohen (1983), Altman (1991), Maxwell and Delaney (1993), Altman et al. (1994), Buettner, Garbe, and Guggenmoos-Holzmann (1997), Altman (1998), Taylor and Yu (2002), MacCallum et al. (2002), Irwin and McClelland (2003), Owen and Froman (2005), Altman and Royston (2006), Royston, Altman, and Sauerbrei (2006), van Walraven and Hart (2008), Fedorov, Mannino, and Zhang (2009), Naggara et al. (2011), Bennette and Vickers (2012), Kuss (2013), Kenny and Montanari (2013), Barnwell-Menard, Li, and Cohen (2015), Fernandes et al. (2019), as well as the references shown in Harrell (2015). These articles identify the main problems of discretization as follows:\n\nArbitrary (non-methodological) choice of breaks for binning can lead to significant bias.\nThe predictor suffers a significant loss of information, making it less effective. Moreover, there is reduced statistical power to detect differences between groups when they exist.\nThe number of features are increased, thus exacerbating the challenge of feature selection.\nCorrelations between predictors are inflated due to the unrealistic reduction in the variance of predictors.\n\nPettersson et al. (2016) shows differences in analyses with and without discretization. Their Fig. 1 shows a common binning analysis: a continuous outcome and one or more predictors are converted to qualitative formats and a grid of pie charts is created. Inferences are made from this visualization. One main problem is related to uncertainty. The noise in the continuous data is squashed so that any visual signal that is seen appears more factual than it is in reality13. Also, the pie charts do not show measures of uncertainty; how do we know when two pie charts are “significantly different”?\nAlternatively, Figs. 4 and 5 of their paper shows the results of a logistic regression model where all predictors were left as-is and splines were used to model the probability of the outcome. This has a much simpler interpretation and confidence bands give the reader a sense that the differences are real.\nWhile it is not advisable to discretize most predictors, there are some cases when discretization can be helpful. As a counter-example, one type of measurement that is often appropriate to discretize is date. For example, Kuhn and Johnson (2019) show a data set where daily ridership data was collected for the Chicago elevated train system. The primary trend in the data was whether or not the day was a weekday. Ridership was significantly higher when people commute to work. A simple indicator for Saturday/Sunday (as well as major holiday indicators) was the driving force behind many regression models on those data. In this case, making qualitative versions of the date was rational, non-arbitrary, and driven by data analysis.\nNote that several models, such as classification/regression trees and multivariate adaptive regression splines, estimate cut points in the model-building process. The difference between these methodologies and manual binning is that the models use all the predictors to derive bins based on a single objective (such as maximizing accuracy). They evaluate many variables simultaneously and are usually based on statistically sound methodologies.\nIf it is the last resort, how should one go about discretizing predictors? First, topic specific expertise of the problem can be used to create appropriate categories when categories are truly merited as in the example of creating an indicator for weekend day in the Chicago ridership data. Second, and most important, any methodology should be well validated using data that were not used to build the model (or choose the cut points for binning). To convert data to a qualitative format, there are both supervised and unsupervised methods.\nThe most reliable unsupervised approach is to choose the number of new features and use an appropriate number of percentiles to bin the data. For example, if four new features are required, the 0, 25%, 50%, 75%, and 100% quantiles would be used. This ensures that each resulting bin contains about the same number of samples from the training set.\nIf you noticed that this is basically the same approach suggested for choosing spline knots in the discussion earlier in this chapter, you are correct. This process is very similar to using a zero-order polynomial spline, the minor difference being the placement of the knots. A zero-order model is a simple constant value, usually estimated by the mean. This is theoretically interesting but also enables users to contrast discretization directly with traditional spline basis expansions. For example, if a B-spline was used, the modeler could tune over the number of model terms (i.e., the number of knots) and the spline polynomial degree. If binning is the superior approach, the tuning process would select that approach as optimal. In other words, we can let the data decide if discretization is a good idea14.\nA supervised approach would, given a specific number of new features to create, determine the breakpoints by optimizing a performance measure (e.g., RMSE, classification accuracy, etc.). A good example is a tree-based model (very similar to the process shown in Figure 6.3). After fitting a single tree or, better yet, an ensemble of trees, the split values in the trees can be used as the breakpoints.\nLet’s again use the fossil data from Bralower et al. (1997) illustrated in previous section on basis functions. We can fit a linear regression with qualitative terms for the age derived using:\n\nAn unsupervised approach using percentiles at cut-points.\nA supervised approach where a regression tree model is used to set the breakpoints for the bins.\n\nIn each case, the number of new features requires tuning. Using the basic grid search tools described in ?sec-grid, the number of required terms was set for each method (ranging from 2 to 10 terms) by minimizing the RMSE from a simple linear regression model. The results are that both approaches required the same number of new features and produced about the same level of performance; the unsupervised approach required 6 breaks to achieve an RMSE of 0.0000355 and the supervised model has an RMSE of 0.0000302 with 6 cut points. Figure 8.19 shows the fitted model using the unsupervised terms.\n\n\n\n\n\n\n\n\nFigure 8.19: The estimated relationship between fossil age and the isotope ratio previously shown in Figure 8.11, now using discretization methods.\n\n\n\n\n\nThe results are remarkably similar to one another. The blocky nature of the fitted trend reflects that, within each bin, a simple mean is used to estimate the sale price.\nAgain, we want to emphasize that arbitrary or subjective discretization is almost always suboptimal.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#chapter-references",
    "href": "chapters/interactions-nonlinear.html#chapter-references",
    "title": "8  Interactions and Nonlinear Features",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAltman, D G. 1991. “Categorising Continuous Variables.” British Journal of Cancer 64 (5): 975.\n\n\nAltman, D G. 1998. “Suboptimal Analysis Using ’Optimal’ Cutpoints.” British Journal of Cancer 78 (4): 556–57.\n\n\nAltman, D G, B Lausen, W Sauerbrei, and M Schumacher. 1994. “Dangers of Using \"Optimal\" Cutpoints in the Evaluation of Prognostic Factors.” Journal of the National Cancer Institute 86 (11): 829.\n\n\nAltman, D G, and P Royston. 2006. “The Cost of Dichotomising Continuous Variables.” BMJ 332 (7549): 1080.\n\n\nAltorki, Nasser K, Timothy E McGraw, Alain C Borczuk, Ashish Saxena, Jeffrey L Port, Brendon M Stiles, Benjamin E Lee, et al. 2021. “Neoadjuvant Durvalumab with or Without Stereotactic Body Radiotherapy in Patients with Early-Stage Non-Small-Cell Lung Cancer: A Single-Centre, Randomised Phase 2 Trial.” The Lancet Oncology 22 (6): 824–35.\n\n\nApley, D, and J Zhu. 2020. “Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.” Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (4): 1059–86.\n\n\nArnold, T, M Kane, and B Lewis. 2019. A Computational Approach to Statistical Learning. Chapman; Hall/CRC.\n\n\nBarnwell-Menard, JL, Q Li, and A Cohen. 2015. “Effects of Categorization Method, Regression Type, and Variable Distribution on the Inflation of Type-I Error Rate When Categorizing a Confounding Variable.” Statistics in Medicine 34 (6): 936–49.\n\n\nBennette, C, and A Vickers. 2012. “Against Quantiles: Categorization of Continuous Variables in Epidemiologic Research, and Its Discontents.” BMC Medical Research Methodology 12: 21.\n\n\nBralower, T, P Fullagar, C Paull, G Dwyer, and R Leckie. 1997. “Mid-Cretaceous Strontium-Isotope Stratigraphy of Deep-Sea Sections.” Geological Society of America Bulletin 109 (11): 1421–42.\n\n\nBreiman, L. 1988. “Monotone Regression Splines in Action (Comment).” Statistical Science 3 (4): 442–45.\n\n\nBuettner, P, C Garbe, and I Guggenmoos-Holzmann. 1997. “Problems in Defining Cutoff Points of Continuous Prognostic Factors: Example of Tumor Thickness in Primary Cutaneous Melanoma.” Journal of Clinical Epidemiology 50 (11): 1201–10.\n\n\nChen, SH, J Sun, L Dimitrov, A Turner, T Adams, D Meyers, BL Chang, et al. 2008. “A Support Vector Machine Approach for Detecting Gene-Gene Interaction.” Genetic Epidemiology 32 (2): 152–67.\n\n\nChipman, H. 1996. “Bayesian Variable Selection with Related Predictors.” Canadian Journal of Statistics 24 (1): 17–36.\n\n\nCohen, J. 1983. “The Cost of Dichotomization.” Applied Psychological Measurement 7 (3): 249–53.\n\n\nDe Boor, C. 2003. A Practical Guide to Splines. Springer-Verlag.\n\n\nElith, J, J Leathwick, and T Hastie. 2008. “A Working Guide to Boosted Regression Trees.” Journal of Animal Ecology 77 (4): 802–13.\n\n\nFedorov, V, F Mannino, and R Zhang. 2009. “Consequences of Dichotomization.” Pharmaceutical Statistics 8 (1): 50–61.\n\n\nFernandes, A, C Malaquias, D Figueiredo, E da Rocha, and R Lins. 2019. “Why Quantitative Variables Should Not Be Recoded as Categorical.” Journal of Applied Mathematics and Physics 7 (7): 1519–30.\n\n\nFriedman, J, and B Popescu. 2008. “Predictive Learning via Rule Ensembles.” The Annals of Applied Statistics 2 (3): 916–54.\n\n\nGarcía-Magariños, M, I López-de-Ullibarri, R Cao, and A Salas. 2009. “Evaluating the Ability of Tree-Based Methods and Logistic Regression for the Detection of SNP-SNP Interaction.” Annals of Human Genetics 73 (3): 360–69.\n\n\nGauthier, J, QV Wu, and TA Gooley. 2020. “Cubic Splines to Model Relationships Between Continuous Variables and Outcomes: A Guide for Clinicians.” Bone Marrow Transplantation 55 (4): 675–80.\n\n\nGreenwell, B, B Boehmke, and A J McCarthy. 2018. “A Simple and Effective Model-Based Variable Importance Measure.” arXiv.\n\n\nHamada, M, and CF Wu. 1992. “Analysis of Designed Experiments with Complex Aliasing.” Journal of Quality Technology 24 (3): 130–37.\n\n\nHarrel, F, K Lee, and B Pollock. 1988. “Regression Models in Clinical Studies: Determining Relationships Between Predictors and Response.” JNCI: Journal of the National Cancer Institute 80 (15): 1198–1202.\n\n\nHarrell, F. 2015. Regression Modeling Strategies. Springer.\n\n\nHerbinger, J, B Bischl, and G Casalicchio. 2022. “REPID: Regional Effect Plots with Implicit Interaction Detection.” In International Conference on Artificial Intelligence and Statistics, 10209–33.\n\n\nHooker, G. 2004. “Discovering Additive Structure in Black Box Functions.” In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 575–80.\n\n\nInglis, A, A Parnell, and C Hurley. 2022. “Visualizing Variable Importance and Variable Interaction Effects in Machine Learning Models.” Journal of Computational and Graphical Statistics 31 (3): 766–78.\n\n\nIrwin, J R, and G H McClelland. 2003. “Negative Consequences of Dichotomizing Continuous Predictor Variables.” Journal of Marketing Research 40 (3): 366–71.\n\n\nKapelner, A, and J Bleich. 2013. “bartMachine: Machine Learning with Bayesian Additive Regression Trees.” arXiv.\n\n\nKenny, P W, and C A Montanari. 2013. “Inflation of Correlation in the Pursuit of Drug-Likeness.” Journal of Computer-Aided Molecular Design 27 (1): 1–13.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nKuss, O. 2013. “The Danger of Dichotomizing Continuous Variables: A Visualization.” Teaching Statistics 35 (2): 78–79.\n\n\nLampa, E, L Lind, P Lind, and A Bornefalk-Hermansson. 2014. “The Identification of Complex Interactions in Epidemiology and Toxicology: A Simulation Study of Boosted Regression Trees.” Environmental Health 13 (1): 57.\n\n\nLim, M, and T Hastie. 2015. “Learning Interactions via Hierarchical Group-Lasso Regularization.” Journal of Computational and Graphical Statistics 24 (3): 627–54.\n\n\nMacCallum, R C, S Zhang, K J Preacher, and D Rucker. 2002. “On the Practice of Dichotomization of Quantitative Variables.” Psychological Methods 7 (1): 19–40.\n\n\nMaxwell, S, and H Delaney. 1993. “Bivariate Median Splits and Spurious Statistical Significance.” Psychological Bulletin 113 (1): 181–90.\n\n\nMiller, A. 1984. “Selection of Subsets of Regression Variables.” Journal of the Royal Statistical Society. Series A (General), 389–425.\n\n\nMokhtari, Reza Bayat, Tina S Homayouni, Narges Baluch, Evgeniya Morgatskaya, Sushil Kumar, Bikul Das, and Herman Yeger. 2017. “Combination Therapy in Combating Cancer.” Oncotarget 8 (23): 38022–43.\n\n\nMolnar, C. 2020. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. Lulu.com.\n\n\nNaggara, O, J Raymond, F Guilbert, D Roy, A Weill, and D G Altman. 2011. “Analysis by Categorizing or Dichotomizing Continuous Variables Is Inadvisable: An Example from the Natural History of Unruptured Aneurysms.” AJNR. American Journal of Neuroradiology 32 (3): 437–40.\n\n\nOh, S. 2022. “Predictive Case-Based Feature Importance and Interaction.” Information Sciences 593: 155–76.\n\n\nOwen, S, and R Froman. 2005. “Why Carve up Your Continuous Data?” Research in Nursing and Health 28 (6): 496–503.\n\n\nPettersson, M, XiXnjun Hou, M Kuhn, T T Wager, G W Kauffman, and P R Verhoest. 2016. “Quantitative Assessment of the Impact of Fluorine Substitution on P-Glycoprotein (P-gp) Mediated Efflux, Permeability, Lipophilicity, and Metabolic Stability.” Journal of Medicinal Chemistry 59 (11): 5284–96.\n\n\nPlanchard, D, P Jänne, Y Cheng, J Yang, N Yanagitani, SW Kim, S Sugawara, et al. 2023. “Osimertinib with or Without Chemotherapy in EGFR-Mutated Advanced NSCLC.” New England Journal of Medicine 389 (21): 1935–48.\n\n\nRoyston, P, D G Altman, and W Sauerbrei. 2006. “Dichotomizing Continuous Predictors in Multiple Regression: A Bad Idea.” Statistics in Medicine 25 (1): 127–41.\n\n\nSingh, Nina, and Pamela J Yeh. 2017. “Suppressive Drug Combinations and Their Potential to Combat Antibiotic Resistance.” The Journal of Antibiotics 70 (11): 1033–42.\n\n\nStone, C, and CY Koo. 1985. “Additive Splines in Statistics.” Proceedings of the American Statistical Association 45: 48.\n\n\nTaylor, J M G, and M Yu. 2002. “Bias and Efficiency Loss Due to Categorizing an Explanatory Variable.” Journal of Multivariate Analysis 83 (1): 248–63.\n\n\nvan Walraven, C, and R Hart. 2008. “Leave ’Em Alone - Why Continuous Variables Should Be Analyzed as Such.” Neuroepidemiology 30 (3): 138–39.\n\n\nWang, Y. 2011. Smoothing Splines: Methods and Applications. CRC Press.\n\n\nWood, S. 2006. Generalized Additive Models: An Introduction with R. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#footnotes",
    "href": "chapters/interactions-nonlinear.html#footnotes",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "Alternatively, PCA after skewness-correcting transformations may be another good option.↩︎\nThese were computed using the bootstrap, as in Section 2.3.↩︎\nIt so happens that the Gaussian likelihood function is equivalent to the sums of squared errors (SSE). That, in turn, is equivalent to the RMSE. That simplification does not automatically occur for other probability distributions.↩︎\nThis approach works because the model in Equation 8.1 is capable of estimating the interaction. There are many models that do not have the ability to measure interaction effects, and, for this case, it would be impossible to isolate the interaction term(s). However, tree-based ensembles are good at estimating interactions, as are other complex black-box models such as neural networks and support vector machines. The tools described below only work with “interaction capable” models.↩︎\nIn actuality, a more computationally preferred method is to use orthogonal polynomials which rescale the variables before we exponentiate them. We’ll denote these modified versions of the predictor in equation Equation 8.3 as just \\(x\\) to reduce mathematical clutter. We’ve seen these before in Figure 6.4↩︎\nThis can be seen in the basis functions above the plot: each line covers the entire range of the data.↩︎\nActually, they are the interior knots. The full set of knots includes the minimum and maximum values of \\(x\\) in the training set.↩︎\nAlso called a restricted cubic spline in Harrel, Lee, and Pollock (1988).↩︎\nIn this particular case, the variance becomes very large since the number of parameters is nearly the same as the number of training set points (30). This makes the underlying mathematical operation (matrix inversion) numerically unstable. Even so, it is the result of excessive model complexity.↩︎\nThis specific example will be referenced again when discussing resampling in Chapter 10.↩︎\nAlso known as binning or dichotomization.↩︎\nIn the immortal words12 of Lucy D’Agostino McGowan and Nick Strayer: “Live free or dichotomize.”↩︎\nKenny and Montanari (2013) does an excellent job illustrating this issue.↩︎\nBut is most likely not a good idea.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html",
    "href": "chapters/overfitting.html",
    "title": "9  Overfitting",
    "section": "",
    "text": "9.1 Model Complexity and Overfitting\nThis chapter describes the most crucial concept in predictive modeling: overfitting. When a model fails, it is almost always due to overfitting of some sort. The problem is that you may only realize that there is an issue once you finish the development phase of modeling and are exposed to completely new data.\nTo get started, we’ll discuss model complexity and how it can be both good and bad.\nMany predictive models have parameters. For example, the linear regression model initially shown in Equation 8.3 contains slopes and intercepts (the \\(\\beta\\) values) estimated during model training using the predictor and outcome data. In other words, it is possible to write the equations to solve to estimate these values.\nHowever, in that same equation, a series of polynomial terms were used. The number of additional features is also a parameter but, unlike the \\(\\beta\\) coefficients, it cannot be directly estimated from the data. Equation 8.3 arbitrarily used three terms for the predictor \\(x\\).\nTwo other models were discussed in Chapter 2. First, the tree-based model shown in Figure 2.5 had five splits of the data. This led to a model fit that resulted in six regions of the \\(x\\) space, each with its specific predicted value. We could have continued to split the training set into finer and finer partitions so that the fitted curve more effectively modeled the observed data. As such, the depth of the tree was a parameter that needed to be set before the model fit could be estimated.\nSimilarly, the neural network required the number of hidden units to be defined (among other values). We weren’t sure what that should be, so a set of candidate values were assessed, and the one with the best results was used for the final model fit (see Figure 2.7). We’ve also seen specific tuning parameters in Section 6.4.2, Section 6.4.4, and also for the embedding methods in Chapter 7.\nIn each of these cases, there was a tuning parameter (a.k.a., hyperparameter) that defined the structure of the model and could not be directly estimated from the data. Increasing the parameter’s value for each model made the model’s potential structure more elaborate. The effect of this increase in the model’s complexity is additional capability to adapt to any pattern seen in the training data. The straight line fit associated with a linear regression model without polynomial terms would have done an abysmal job on such a nonlinear data set. Adding additional complexity allowed it to better conform to the training set data.\nThe problem is that added complexity can also add risk. Being more capable of adapting to subtle trends in the training data is only a good thing when such trends generalize to other data. Recall in Section 6.4.3 where we were estimating the effect of an agent on the price of a hotel room. One agent had a single booking. The naive estimate of the ADR wouldn’t generalize well. As they accrue more and more bookings, their mean would undoubtedly change to a more realistic value. Using the raw ADR value would “overfit to the training set data.”\nFor many sophisticated models, the complexity can be modulated via one or more tuning parameters that detect inconsequential patterns in the training data. The problem is that they may go too far and over-optimize the model fit.\nAs an example, Figure 9.1 shows a scenario where there are two predictors (A and B) and each point belongs to one of two classes (vermilion circles or blue triangles). These data were simulated and panel (a) shows a thick grey line representing the true class boundary where there is a 50% chance that a data point belongs to either class. Points to the left of the grey curve are more likely to be circles than points to the right. The curve itself has a smooth, parabolic shape.\nWe can fit a model to these data and estimate the class boundary. The model we used1 has a tuning parameter called the cost value. For now, you can think of this parameter as one that discourages the model from making an incorrect prediction (i.e., placing it on the wrong side of the estimated boundary). In other words, as the cost parameter increases, the model is more and more incentivized to be accurate on the training data.\nWhen the cost is low, the results are often boundaries with low complexity. Figure 9.1(b) shows the result where the fitted boundary is in black; points on the left of the boundary would be classified as circles. There is some curvature and, while correctly classifying the points in the middle of the training set, it could do a better job of emulating the true boundary. This model is slightly underfit since more complexity would make it better.\nFigure 9.1: Examples of how a model can overfit the training data across different levels of complexity.\nWhen the cost is increased, the complexity also increases (Figure 9.1(c)). The boundary is more parabolic, and a few more training set points are on the correct side of the estimated line.\nFigure 9.1(d) shows what occurs when the model is instructed that it is incredibly bad when incorrectly predicting a data point in the training set. Its boundary contorts in a manner that attempts to gain a few more points of accuracy. Two specific points, identified by black arrows, appear to drive much of the additional structure in the black curve. Since we know the actual boundary, it’s easy to see that these contortions will not reproduce with new data. For example, it is implausible that a small island of vermilion circles exists in the mainstream of blue triangles.\nAnother sign that a model probably has too much complexity is the additional component of the decision boundary at the bottom of Figure 9.1(d) (red arrow). Points inside this part of the boundary would be classified as triangles even though no triangles are near it. This is a sort of “blowback” of an overly complex model where choices the model makes in one part of the predictor space add complexity in a completely different part of the space. This blowback is often seen in areas where there are no data. It’s unnecessary and points to the idea that the model is trying to do too much.\nThis boundary demonstrates how added complexity can be a bad thing. A test set of data was simulated and is shown in Figure 9.2. The performance for this overly complex model is far from optimal since the extra areas induced by the high cost value do not contain the right class of points.\nFigure 9.2: The high complexity fit from Figure 9.1(d) superimposed over a test set of data.\nOur goal is to find tuning parameter values that are just right: complex enough to accurately represent the data but not complex enough to over-interpret it.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-how-to-overfit",
    "href": "chapters/overfitting.html#sec-how-to-overfit",
    "title": "9  Overfitting",
    "section": "9.2 The Ways Models Overfit",
    "text": "9.2 The Ways Models Overfit\nThis leads us to a formal definition of overfitting:\n\nOverfitting is the situation where a model over-interprets trends and patterns in the training set that do not generalize to new data.\nThese irreproducible data trends result in models that do very well on the training set but greatly underperform on the validation set.\n\nWe can overinterpret the training set data in a few different ways. For example, ?sec-imbalances will discuss class imbalance problems that occur when one or more outcome classes have a very low probability of occurrence. As models are allowed more complexity, they will often maximize accuracy (or some proxy for it) by simply declaring that all data points belong to the majority class. If there is a 1% event rate, 99% accuracy is easily achievable by overfitting to the majority class.\nAnother example is overfitting the predictor set. The “low \\(N\\), high \\(P\\)” problem (Johnstone and Titterington 2009) occurs when data points are expensive to obtain, but each comes with abundant information. For example, in high-dimensional biology data, it is common to have dozens of data points and hundreds of thousands of predictors. Many models cannot be estimated with this imbalance of dimensions, and often, the first step is to filter out uninformative predictors.\nSuppose some statistical inferential method, such as a t-test, is used to determine if a predictor can differentiate between two classes. Small sample sizes make the chance of uninformative predictors falsely passing the filter large. If the same data are then used to build and evaluate the model, there is a significant probability that an ineffective model will appear to be very predictive (Ambroise and McLachlan 2002; Kuhn and Johnson 2019). Again, using a data set external to the model training and development process shows you when this occurs.\nFinally, it is possible to overfit a model via sample filtering. For example, in some machine learning competitions, participants might sub-sample the training set to be as similar to the unlabelled test set as possible. This will most likely improve their test set performance, but it generally handicaps a model when predicting a new yet-to-be-seen data set.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-external-validation",
    "href": "chapters/overfitting.html#sec-external-validation",
    "title": "9  Overfitting",
    "section": "9.3 External Data to Measure Effectiveness",
    "text": "9.3 External Data to Measure Effectiveness\nBy now, it should seem clear that the best way of avoiding overfitting is to quantify the model performance using a separate data set. How do we do that? We’ve said that most modeling projects create an initial partition of the data into training and testing sets. Can the testing set detect overfitting?\nWe’ll again use the classification data from Figure 9.1 to demonstrate, this time with a different model. A boosted tree2 is an ensemble model. The model creates a sequence of decision trees, each depending on the previous, to create a large ensemble of individual trees. For a new sample, each tree is used to predict the outcome, and a weighted average is used to create the predicted class probabilities.\nThe model has tuning parameters related to the tree (e.g., tree depth) and ones related to the process of building the ensemble (e.g., the number of trees in the ensemble). We’ll use 100 trees to create our ensemble. However, we don’t know how deep the trees should be. One method of controlling the number of splits in a tree is to control how much data should be in a partition to continue splitting. For example, in Figure 2.5, the left-most terminal node in the tree contained \\(n_{tr}\\) = 971 data points after all splits are applied. If we keep splitting with no constraint, at some point, the amount of data in the node is too small to continue.\nThis tuning parameter, we’ll call it \\(n_{min}\\), is used to moderate the tree-growing process. Small values increase the risk of overfitting; it’s possible to have a single data point left in the terminal node. Large values prevent the tree from accurately modeling the data, and underfitting occurs. What does the trade-off between complexity and performance look like for these data?\nWe took the training data and fit models using values \\(n_{min}\\) = 3, 4, …, 80. To understand performance, the Brier score (Brier 1950; Steyerberg 2009) is used. It is an error metric for classification models (similar to RMSE) and measures how close the predicted probabilities are to their binary (i.e., 0/1) values. A Brier score of zero is a perfect model, and ineffectual models tend to have values \\(\\ge\\) 0.25.\nFigure 9.3 shows the results. The yellow curve illustrates the Brier score when the training set is re-predicted. There is a sharp increase in error as the tuning parameter values increase. This line suggests that we use very small values. The smallest Brier score is associated with \\(n_{min}\\) = 3 (producing an effectively zero Brier score).\n\n\n\n\n\n\n\n\nFigure 9.3: An example to show how complexity and model error, measured two ways, interact. The tuning parameter on the x-axis is \\(n_{min}\\) used for moderating tree growth.\n\n\n\n\n\nThe purple line shows the Brier score trend on the test set. This tells a different story where small \\(n_{min}\\) values overfit, and the model error improves as larger values are investigated. The Brier score is lowest around \\(n_{min}\\) = 39 with a corresponding Brier score estimate of 0.0928. From here, increasing the tuning parameter results in shallow trees that underfit. As a result the Brier score slowly increases.\nContrasting these two curves, data that are external to the training set gives a much more accurate picture of performance. However:\n\nDo not use the test set to develop your model.\n\nWe’ve only used it here to show that an external data set is needed to effectively optimize model complexity. If we only have training and testing data, what should we do?\nThere are two ways to solve this problem:\n\nUse a validation set.\nResample the training set Chapter 10.\n\nBoth of these options are described in detail in the next chapter.\nAs previously discussed, validation sets are a third split of the data, created at the same time as the training and test sets. They can be used as an external data set during model development (as in Chapter 2). This approach is most applicable when the project starts with abundant data.\nResampling methods take the training set and make multiple variations of it, usually by subsampling the rows of the data. For example, 10-fold cross-validation would make ten versions of the training set, each with a different 90% of the training data. We would say that, for this method, there are ten “resamples” or “folds.” To measure performance, we would fit ten models on the majority of the data in each fold, then predict the separate 10% that was held out. This would generate ten different Brier scores, which are averaged to produce the final resampling estimate of the model. If we are trying to optimize complexity, we will apply this process for every candidate tuning parameter value.\nFigure 9.4 shows the results when 10-fold cross-validation is used to pick \\(n_{min}\\). The points on the additional curve are the averages of 10 Brier scores from each fold for each value of \\(n_{min}\\). The resampling curve doesn’t precisely mimic the test set results but does have the same general pattern: there is a real improvement in model error as \\(n_{min}\\) is increased but it begins to worsen due to underfitting. In this case, resampling finds that \\(n_{min}\\) = 56 is numerically best and the resampling estimate of the Brier score was 0.104, slightly more pessimistic when compared to the test set result.\n\n\n\n\n\n\n\n\nFigure 9.4: Model complexity versus model error with external validation via resampling.\n\n\n\n\n\nWe suggest using either of these two approaches (validation set or resampling) to measure performance during model development. Before going to the next chapter, let’s give a preview of Chapters 11 and ?sec-iterative-search.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-optimizing-complexity",
    "href": "chapters/overfitting.html#sec-optimizing-complexity",
    "title": "9  Overfitting",
    "section": "9.4 How Should We Optimize Complexity?",
    "text": "9.4 How Should We Optimize Complexity?\nNow that we know how to appropriately estimate metrics for evaluating effectiveness, how do we choose tuning parameter values? Do we pick them at random?\nOperationally, there are two main classes of search routines for optimizing model complexity via tuning parameters. The oldest is grid search. For each tuning parameter in the model, we define a reasonable set of specific candidate values to investigate. When there is more than one tuning parameter, there are various ways to create a multidimensional grid (these are discussed in Chapter 11). Each candidate in the grid is evaluated and the “best” combination of parameter values is chosen as the one to use (if we think the model works well enough).\nOne tuning parameter used in tree-based ensembles and neural networks is the learning rate. This parameter typically governs how quickly a model adapts during iterations of model fitting (i.e., training). In gradient descent, the learning rate specifies how far we proceed in the optimal direction. For machine learning models, the parameter must be greater than zero and usually has a maximum value around 0.1 to 1.0. When evaluating different learning rates, it is common to think of them in logarithmic units (typically, base 10). This is because the effect of the parameter on model performance is often nonlinear; a per-unit change of 0.05 has different effects on where in the total range that it occurs.\nSuppose that the performance goal is to minimize some notion of error (e.g., RMSE, the Brier score, etc.). Figure 9.5 shows an example of how the learning rate can affect the model error3. At low rates, the error is high. Increasing the rate results in a drop in error that reaches a trough of optimal performance. However, increasing the learning rate at some point causes a different type of underfitting4, increasing the error.\nThe solid points illustrate a very simple five-point grid of errors whose y-axis value is the resampled error. While this grid does not pick the absolute best value, it does result in a model with good performance (relatively speaking) very quickly. If a larger grid were used, we would have placed a grid point much closer the optimal value.\n\n\n\n\n\n\n\n\nFigure 9.5: An example of grid search containing five candidates for a learning rate parameter.\n\n\n\n\n\nGrid search pre-defines the grid points and all of them are tested before the results can be analyzed. There are good and bad aspects of this method, and it has been (unjustly) criticized for being inefficient. For most models, it can be very efficient and there are additional tools and tricks to make it even faster. See Chapter 11 for more details.\nThe other type of search method is iterative. These tools start with one or more initial candidate points and conduct analyses that predict which tuning parameter value(s) should be evaluated next. The most widely used iterative tuning method is Bayesian optimization (Močkus 1975; Gramacy 2020; Garnett 2023). After each candidate is evaluated, a Bayesian model is used to suggest the next value and this process repeats until a pre-defined number of iterations is reached. Any search method could be used, such as simulated annealing, genetic algorithms, and others.\nFigure 9.6 has an animation to demonstrate. First, three initial points were sampled (shown as open circles). A Bayesian model is fit to these data points and is used to predict the probability distribution of the metric (e.g. RMSE, Brier score, .etc). A tool called an acquisition function is utilized to choose which learning rate value to evaluate on the next search iteration based on the mean and variance of the predicted distribution. In Figure 9.6, this process repeats for a total of 9 iterations.\n\n\n\n\n\n\n\n\nFigure 9.6: An example of iterative optimization. The open circles represent the three initial points and the solid circles show the progress of the Bayesian optimization of the learning rate.\n\n\n\n\n\nThe animation shows that the search evaluated very disparate values, including the lower and upper limits that were defined. However, after a few iterations, the search focuses on points near the optimal value. Again, this method discussed in more detail in ?sec-interative-search.\nAll of our optimization methods assume that a reasonable range of tuning parameters is known. The range is naturally bounded in some cases, such as the number of PCA components. Otherwise, you (and/or the ML community) will develop a sense of appropriate ranges of these parameters. We will suggest basic ranges here. For example, for the learning rate, we know it has to be greater than zero but the fuzzy upper bound of about 0.1 is a convention learned by trial and error.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#chapter-references",
    "href": "chapters/overfitting.html#chapter-references",
    "title": "9  Overfitting",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBrier, G. 1950. “Verification of Forecasts Expressed in Terms of Probability.” Monthly Weather Review 78 (1): 1–3.\n\n\nGarnett, Roman. 2023. Bayesian Optimization. Cambridge University Press.\n\n\nGramacy, R. 2020. Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.\n\n\nJohnstone, I, and M Titterington. 2009. “Statistical Challenges of High-Dimensional Data.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 367 (1906): 4237–53.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nMočkus, J. 1975. “On Bayesian Methods for Seeking the Extremum.” In Optimization Techniques IFIP Technical Conference Novosibirsk, edited by G Marchuk, 400–404. Springer Berlin Heidelberg.\n\n\nSteyerberg, E. 2009. Clinical Prediction Models. Springer.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#footnotes",
    "href": "chapters/overfitting.html#footnotes",
    "title": "9  Overfitting",
    "section": "",
    "text": "A support vector machine. See ?sec-cls-svm.↩︎\nThese are described in more detail in ?sec-cls-boosting↩︎\nThis was taken from a real example.↩︎\nA very high learning rate for tree-based ensembles can result in models that produce near-constant predicted values (as opposed to many unique predicted values that are inaccurate).↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html",
    "href": "chapters/resampling.html",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "10.1 What is Resampling Trying to Do?\nIn Section 9.3, it was shown that using the same data to estimate and evaluate our model can produce inaccurate estimates of how well it functions to predict new samples. It also described using separate partitions of the data to fit and evaluate the model.\nThere are two general approaches for using external data for model evaluation. The first is a validation set, which we’ve seen before in Section 3.7. This is a good idea if you have a lot of data on hand. The second approach is to resample the training set. Resampling is an iterative approach that reuses the training data multiple times based on sound statistical methodology. We’ll discuss both validation sets and resampling in this chapter.\nFigure 10.1 shows a standard data usage scheme that incorporates resampling. After an initial partition, resampling creates multiple versions of the training set. We’ll use special terminology1 for the data partitions within each resample which serve the same function as the training and test sets:\nLike the training and test sets, the analysis and assessment sets are mutually exclusive data partitions and are different for each of the B iterations of resampling.\nThe resampling process fits a model to an analysis set and predicts the corresponding assessment set. One or more performance statistics are calculated from these held-out predictions and saved. This process continues for B iterations, and, in the end, there is a collection of B statistics of efficacy. These are averaged to produce the overall resampling estimate of performance. To formalize this, we’ll consider a mapping function \\(M(\\mathfrak{D}^{tr}, B)\\) that takes the training set as input and can output \\(B\\) sets of analysis and assessment sets.\nAlgorithm 10.1 describes this process.\nThere are many different resampling methods, such as cross-validation and the bootstrap. They differ in how the analysis and assessment sets are created in line 7 of Algorithm 10.1. Subsequent sections below discuss a number of mapping functions.\nTwo primary aspects of resampling make it an effective tool. First, the separation of data used to create and appraise the model avoids the data reuse problem described in Section 9.3.\nSecond, using multiple iterations (B &gt; 1) means that you are evaluating your model under slightly different conditions (because the analysis and assessment sets are different). This is a bit like a “multiversal” science fiction story: what would have happened if the situation were slightly different before I fit the model? This lets us directly observe the model variance; how stable is it when the inputs are slightly modified? An unstable model (or one that overfits) will have a high variance.\nIn this chapter, we’ll describe some conceptual aspects of resampling. Then, we’ll define and discuss various methods. Finally, the last section lists frequently asked questions we often hear when teaching these tools.\nUp until Section 10.6, we will assume that each row of the data are independent. We’ll see examples of non-independent in the later sections.\nIt’s important to understand some of the philosophical aspects of resampling. It is a statistical procedure that tries to estimate some true, unknowable performance value (let’s call it \\(Q\\)) associated with the same population of data represented by the training set. The quantity \\(Q\\) could be RMSE, the Brier score, or any other performance statistic.\nThere have been ongoing efforts to understand what resampling methods do on a theoretical level. Bates, Hastie, and Tibshirani (2023) is an interesting work focused on linear regression models with ordinary least squares. Their conclusion regarding resampling estimates is that they\nIt is a good idea to think that this will also be true for more complex machine learning models. The important idea is that resampling is measuring performance on training sets similar to ours (and the same size).\nTo understand different methods, let’s examine some theoretical properties of resampling estimates that matter to applied data analysis.\nLet’s start with analogy; consider the usual sample mean estimator used in basic statistics (\\(\\bar{x}\\)). Based on assumptions about the data, we can derive an equation that optimally estimates the true mean value based on some criterion. Often, the statistical theory focuses on our estimators’ theoretical mean and variance. For example, if the data are independent and follow the same Gaussian distribution, the sample mean attempts to estimate the actual population mean (i.e., it is an unbiased estimator). Under the same assumptions we can also derive what the estimator’s theoretical variance. These properties, or some combination of them, such as mean squared error2, help guide us to the best estimator.\nThe same is true for resampling methods. We can understand how their bias and variance change under different circumstances and choose an appropriate technique. To demonstrate resampling methods, we’ll focus on bias and variance as the main properties of interest.\nFirst is bias: how accurately does a resampling technique estimate the true population value \\(Q\\)? We’d like it to be unbiased, but to our knowledge, that is nearly impossible. However, some things we do know. Figure 10.1 shows that the training set (of size \\(n_{tr}\\)) is split into B resampling partitions which are then split into analysis and assessment sets of size \\(n_{fit}\\) and \\(n_{pred}\\), respectively. It turns out that, as \\(n_{pred}\\) becomes larger, the bias increases in a pessimistic direction3. In other words, if our training set has 1,000 samples, a resampling method where the assessment set has \\(n_{pred} = 100\\) data points has a smaller bias than one using \\(n_{pred} = 500\\). Another thing that we know is that increasing the number of resamples (B) can’t significantly reduce the bias. Therefore, we must think carefully about the way the data is partitioned in the resampling process to reduce potential bias.\nThe variance (often called precision) of resampling is also important. We want to get a performance estimate that gives us consistent results if we repeat it. The resampling precision is driven mainly by B and \\(n_{tr}\\). With some resampling techniques, if your results are too noisy, you can resample more and stabilize or reduce the estimated variance (at the cost of increased computational time). We’ll examine the bias and precision of different methods as we discuss each method.\nLet’s examine a few specific resampling methods, starting with the most simple: a single validation set.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#what-is-resampling-trying-to-do",
    "href": "chapters/resampling.html#what-is-resampling-trying-to-do",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "… cannot be viewed as estimates of the prediction error of the final model fit on the whole data. Rather, the estimate of prediction error is an estimate of the average prediction error of the final model across other hypothetical data sets from the same distribution.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-validation",
    "href": "chapters/resampling.html#sec-validation",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.2 Validation Sets",
    "text": "10.2 Validation Sets\nWe’ve already described a validation set; it is typically created via a three-way initial split4 of the data (as shown in Figure 10.2). For time series data, it is common to use the most recent data for the test set. Similarly, the validation set would include the most recent data (once the test set partition is created). The training set would include everything else.\n\n\n\n\n\n\n\n\nFigure 10.2: An initial data splitting scheme that incorporates a validation set.\n\n\n\n\n\nWhile not precisely the same, a validation set is extremely similar to an approach described below called Monte Carlo cross-validation (MCCV). If we used a single MCCV resample, the results would be effectively the same as those of a validation set; the difference not substantive.\nOne aspect of using the validation set is what to do with it after you’ve made your final decision about the model pipeline. Ordinarily, the entire training set is used for the final model fit.\nData can be scarce and, in some cases, an argument can be made that the final model fit could include the training and validation set. Doing this does add some risk; your validation set statistics are no longer completely valid since they measured how well the model works with similar training sets of size \\(n_{tr}\\). If you have an abundance of data, the risk is low, but, at the same time, the model fit won’t change much by adding \\(n_{val}\\) data points. However, if your training data set is not large, adding more data could have a profound impact, and you risk using the wrong model for your data5.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-cv-mc",
    "href": "chapters/resampling.html#sec-cv-mc",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.3 Monte Carlo Cross-Validation",
    "text": "10.3 Monte Carlo Cross-Validation\nMonte Carlo cross-validation emulates the initial train/test partition. For each one of B resamples, it takes a random sample of the training set (say about 75%) to use as the analysis set6. The remainder is used for model assessment. Each of the B resamples is created independently of the others, so some of the training set points are included in multiple assessment sets. Figure 10.3 shows the results of \\(M(\\mathfrak{D}^{tr}, 3)\\) for MCCV with a data set with \\(n_{tr} = 30\\) data points and 80% of the data are allocated to the analysis set. Note that samples 16 and 17 are in multiple assessment sets.\n\n\n\n\n\n\n\n\nFigure 10.3: A schematic of three Monte Carlo cross-validation resamples created from an initial pool of 30 data points.\n\n\n\n\n\nHow many resamples should we use, and what proportion of the data should be used for the analysis set? These choices are partly driven by computing power and training set size. Clearly, using more resamples means better precision.\nA thought experiment can be useful for any resampling method. Based on the proportion of data going into the assessment set, how confident would you feel in a performance metric being computed for this much data? Suppose that we are computing the RMSE for a regression model. If our assessment set contained \\(n_{pred} = 10\\) (on average), we might think our metric’s mean is excessively noisy. In that case, we could either increase the proportion held out (better precision, worse bias) or resample more (better precision, same bias, long computational time).\nEach resampling method has different trade-offs between bias and variance. Let’s look at some simulation results to help understand the trade-off for MCCV.\n\nAnother Simulation Study\nSection 9.1 described a simulated data set that included 200 training set points. In that section, Figure 9.1 illustrates the effect of overfitting using a particular model (KNN). This chapter will use the same training set but with a simple logistic regression model where a four-degree of freedom spline was used with each of the two predictors.\nSince these are simulated data, an additional, very large data set was simulated and used to approximate the model’s true performance, once again evaluated using a Brier score. Our logistic model has a Brier score value of \\(Q \\approx\\) 0.0898, which demonstrates a good fit. Using this estimate, the model bias can be computed by subtracting the resampling estimate from 0.0898.\nThe simulation created 500 realizations of this 200 sample training set7. We resampled the logistic model for each and then computed the corresponding Brier score estimate from MCCV. This process was repeated using the different analysis set proportions and values of B shown in Figure 10.4. The left panel shows that the bias8 decreases as the proportion of data in the analysis set becomes closer to the amount in the training set. It is also apparent that the bias is unaffected by the number of resamples (B). The panel on the right shows the precision, estimated using the standard error, of the resampled estimates. This decreases nonlinearly as B increases; the cost-benefit ratio of adding more resamples shows eventual diminishing returns. The amount retained for the analysis set shows that values close to one have higher precision than the others. Regarding computational costs, the time to resample a model increases with both parameters (amount retained and B).\n\n\n\n\n\n\n\n\nFigure 10.4: Variance and bias statistics for simulated data using different configurations for Monte Carlo cross-validation. The range of the y-axes are common across similar plots below for different resampling techniques.\n\n\n\n\n\nThe results of this simulation indicate that, in terms of precision, there is little benefit in including more than 70% of the training set in the assessment set. Also, while more resamples always help, there is incremental benefit in using more than 50 or 60 resamples (for this size training set). Regarding bias, the decrease somewhat slows down near 80% of the training set retained. Holdout proportions of around 75% - 80% might be a reasonable rule of thumb (these are, of course, subjective and based on this simulation).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-cv",
    "href": "chapters/resampling.html#sec-cv",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.4 V-Fold Cross-Validation",
    "text": "10.4 V-Fold Cross-Validation\nV-fold cross-validation (sometimes called K-fold cross-validation) (Stone 1974) is the most well-known resampling method9. It randomly allocates the training data to one of V groups of about equal size, called a “fold” (a stratified allocation can also be used). There are B = V iterations where the analysis set comprises V -1 of the folds, and the remaining fold defines the assessment set. For example, for 10-fold cross-validation, the ten analysis sets consist of 90% of the data, and the ten assessment sets contain the remaining 10% of the data and are used to quantify performance. As a visual illustration, Figure 10.5 shows the process for V = 3 (for brevity), for a data set with 30 samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.5: An example mapping with \\(V = 3\\)-fold cross-validation.\n\n\n\nArlot and Celisse (2010) is a comprehensive survey of cross-validation. Let’s look at two special cases.\n\nLeave-One-Out Cross-Validation\nLeave-one-out cross-validation (LOOCV, also called the jackknife) sets \\(V = n_{tr}\\). A single sample is withheld and, over \\(n_{tr}\\) iterations, a set of models is created, each with an analysis set of \\(n_{tr} - 1\\) samples. The resampling estimate is created by applying the metric function to the resulting \\(n_{tr}\\) predicted values. There are no replicate performance values; only a single \\(\\hat{Q}\\). As one might expect, the bias is nearly zero for this method. Although it would be difficult to quantify, the standard error of the estimator should be very large.\nThis method is extremely computationally expensive10 and is not often used in practice unless computational shortcuts are available.\n\n\nRepeated Cross-Validation\nOne way improve precision of this method is to use repeated V-fold cross-validation. In this case, V-fold CV is repeated multiple times using different random number seeds. For example, two repeats of 10-fold cross-validation11 create two sets of V folds, which are treated as a collection of twenty resamples (e.g., B = V \\(\\times\\) R for R repeats). Again, increasing the number of resamples does not generally change the bias.\n\n\nVariance and Bias for V-Fold Cross-Validation\nThe discussions of LOO and repeated cross-validation beg the question: what value of V should we use? Sometimes the choice of V is driven by computational costs; smaller values of V require less computation time. A second consideration for choice of V is metric performance in terms of bias and variance. As we will see later in this chapter, the degree of bias is determined by how much data are retained in the assessment sets (as defined by V). As V increases, bias decreases; V=5 has substantially more bias than V=10. Fushiki (2011) describes post hoc methods for reducing the bias for this resampling method.\nCompared to other resampling methods, V-fold cross-validation generally has a smaller bias, but relatively poor precision (that would be improved via repeats).\nTo understand the trade-off, the same simulation approach from the previous section is used. Figure 10.6 shows bias and variance results, where the x-axis is the total number of resamples \\(V\\times R\\). As expected, the bias decreases with V and is constant over the number of resamples. Five-fold cross-validation stands out as particularly bad with a percent bias of roughly 3.9%. Using ten folds decreases this to about 2.2%. The precision values show that smaller values of V have better precision; 10-fold CV needs substantially more resamples to have the same standard error than V = 5.\n\n\n\n\n\n\n\n\nFigure 10.6: Variance and bias statistics for simulated data using different configurations for \\(V\\)-fold cross-validation.\n\n\n\n\n\nIs there any advantage to using V &gt; 10? Not much. The decrease in bias is small and many more replicates are required to reach the same precision as V = 10. For example, twice as many resamples are required for 20-fold CV to match the variance of 10-fold CV.\nFor this simulation, the properties are about the same when MCCV and 10-fold CV are matched in terms of number of resamples and the amount allocated to the assessment set.\nOur recommendation is to almost always use V = 10. The bias and variance improvements are both good with 10 folds. Reducing bias in 5-fold cross-validation is difficult but the precision for 10-fold can be improved by increased replication.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-bootstrap",
    "href": "chapters/resampling.html#sec-bootstrap",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.5 The Bootstrap",
    "text": "10.5 The Bootstrap\nThe bootstrap (Efron 1979, 2003; Davison and Hinkley 1997; Efron and Hastie 2016) is a resampling methodology originally created to compute the sampling distribution of statistics using minimal probabilistic assumptions12. In this chapter, we’ll define a bootstrap sample and show how it can be used to measure fit quality for predictive models.\nFor a training set with \\(n_{tr}\\) data points, a bootstrap resample takes a random sample of the training set that is also size \\(n_{tr}\\). It does this by sampling with replacement; when each of the \\(n_{tr}\\) samples is drawn, it has no memory of the prior selections. This means that each row can be randomly selected again. For example, Figure 10.7 shows another schematic for three bootstrap samples. In this figure, the thirteenth training set sample was selected to go into the first analysis set three times. For this reason, a bootstrap resample will contain multiple replicates of some training set points, while others will not be selected at all. The data that were never selected are used to create the assessment set. This means that while the training set will have the same number of data points, the assessment sets will have varying numbers of data points.\nFor the bootstrap, we’ll refer to the mean estimate (line 12 in Algorithm 10.1) as the “ordinary” estimator (others will follow shortly).\n\n\n\n\n\n\n\n\nFigure 10.7: A schematic of three bootstraps resamples created from an initial pool of 30 data points.\n\n\n\n\n\nThe probability that a data point will be picked is \\(1 / n_{tr}\\) and, from this, the probability that a training set point is not selected at all is\n\\[\\prod_{i=1}^{n_{tr}} \\left(1 - \\frac{1}{n_{tr}}\\right) = \\left(1 - \\frac{1}{n_{tr}}\\right)^{n_{tr}}\\approx e^{-1} = 0.368\\]\nSince the bootstrap sample contains the selected data, each training point is selected with probability \\(1 - e^{-1} \\approx 0.632\\). The implication is that, on average, the analysis set, contains about 63.2% unique training set points and the assessment set includes, on average, 36.8% of the training data. When comparing the number of unique training set points excluded by the bootstrap method to those excluded by V-fold cross-validation, the bootstrap method is roughly equivalent to using \\(V=3\\) in cross-validation.\nFigure 10.8 shows bias and variance for the simulated data. As one might expect, there is considerable bias in the bootstrap estimate of performance. In comparison to the corresponding plot for MCCV, the bootstrap bias is worse than the MCCV curve where 60% were held out. The curve is also flat; the bias does’t go away by increasing B.\nHowever, the precision is extremely good, even with very few resamples.\n\n\n\n\n\n\n\n\nFigure 10.8: Variance and bias statistics for simulated data using different configurations for the bootstrap.\n\n\n\n\n\n\nCorrecting for Bias\nThere have been some attempts to de-bias the ordinary bootstrap estimate. The “632 estimator” (Efron 1983) uses the ordinary bootstrap estimate (\\(\\hat{Q}_{bt}\\)) and the resubstitution estimate (\\(\\hat{Q}_{rsub}\\)) together:\n\\[\\hat{Q}_{632} = e^{-1}\\, \\hat{Q}_{rsub} + (1 - e^{-1})\\, \\hat{Q}_{bt} = 0.368\\,\\hat{Q}_{rsub} + 0.632\\,\\hat{Q}_{bt}\\] Figure 10.8 shows that there is a significant drop in the bias when using this correction. The 632 estimator combines two different statistical estimates, and we only know the standard error of \\(\\hat{Q}_{bt}\\). Therefore, the right-hand panel does not show a standard error curve for the 632 estimator.\nLet’s look at an average example from the simulated data sets with B = 100. Table 10.1 has the estimator values and their intermediate values. Let’s first focus on the values for the column labeld “Logistic.” The ordinary bootstrap estimate was \\(\\hat{Q}_{bt}\\) = 0.101 and repredicting the training set produced \\(\\hat{Q}_{rsub}\\) = 0.0767. The 632 estimate shifts the Brier score downward to \\(\\hat{Q}_{632}\\) = 0.0919. This reduces the pessimistic bias; our large sample estimate of the true Brier score is 0.0898 so we are closer to that value.\nAnother technique, the 632+ estimator (Efron and Tibshirani 1997), uses the same blending strategy but uses dynamic weights based on how much the model overfits (if at all). It factors in a model’s “no-information rate”: the metric value if the predicted and true outcome values were independent. The author gives a formula for this, but it can also be estimated using a permutation approach where we repeatedly shuffle the outcome values and compute the metric. We will denote this value as \\(\\hat{Q}_{nir}\\). For our simulated data set \\(\\hat{Q}_{nir}\\) = 0.427; we believe that this is the worst case value for our metric.\nWe then compute the relative overfitting rate (ROR) as\n\\[ ROR = \\frac{\\hat{Q}_{bt}-\\hat{Q}_{rsub}}{\\hat{Q}_{nir} -\\hat{Q}_{rsub}} \\]\nThe denominator measures the range of the metric (from most optimistic to most pessimistic). The numerator measures the optimism of our ordinary estimator. A value of zero implies that the model does not overfit, and values of one indicate the opposite. For our logistic model, the ratio is 0.024 / 0.35 = 0.0686, indicating that the logistic model is not overinterpeting the training set.\nThe final 632+ estimator uses a different weighting system to combine estimates:\n\\[\n\\begin{align}\n\\hat{Q}_{632+} &= (1 - \\hat{w})\\, \\hat{Q}_{rsub} + \\hat{w}\\, \\hat{Q}_{bt} \\quad \\text{with}\\notag \\\\\n\\hat{w} &= \\frac{0.632}{1 - 0.368ROR} \\notag\n\\end{align}\n\\] Plugging in our values, the final weight on the ordinary estimator (\\(w\\) = 0.648) is very close to what is used by the regular 632 estimator (\\(w\\) = 0.632). The final estimate is also similar: \\(\\hat{Q}_{632+}\\) = 0.0923.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrier Score\n\n\n\n\nLogistic\n1 NN\n\n\n\n\nEstimates\n\n\n(truth)\n0.090\n0.170\n\n\nresubstitution\n0.077\n0.000\n\n\nsimple mean\n0.101\n0.174\n\n\nIntermediates\n\n\nno information rate\n0.427\n0.496\n\n\nrelative overfitting rate\n0.069\n0.351\n\n\nweights\n0.648\n0.726\n\n\nFinal Estimates\n\n\n632\n0.092\n0.110\n\n\n632+\n0.092\n0.126\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 10.1: Bias correction values for two models on a simulated data set.\n\n\n\nThe simulations studies shown here in Figure 10.8 replicated what Molinaro (2005) found; the 632+ estimator has bias properties about the same as 10-fold cross-validation.\nHow do these bias corrected estimators work when there is extreme overfitting? One way to tell is to consider a 1-nearest neighbor model. In this case, re-predicting the training set predicts every data point with its outcome value (i.e., a “perfect” model). For the Brier score, this means \\(\\hat{Q}_{rsub}\\) is zero. Table 10.1 shows the rest of the computations. For this model \\(Q\\approx\\) 0.17 and the ordinary estimate comes close: \\(\\hat{Q}_{bt}\\) = 0.174.\nThe 632 estimate is \\(\\hat{Q}_{632} = 0.632\\,\\hat{Q}_{bt} =\\) 0.11; this is too much bias reduction as it overshoots the true value by a large margin. The 632+ estimator is slightly better. The ROR value is higher (0.351) leading to a higher weight on the ordinary estimator to produce \\(\\hat{Q}_{632+}\\) = 0.126.\nShould we use the bootstrap to compare models? Its small variance is enticing, but the bias remains an issue. The bias is likely to change as a function of the magnitude of the metric. For example, the bootstrap’s bias is probably different for models with 60% and 95% accuracy. If we think our models will have about the same performance, the bootstrap (with a bias correction) may be a good choice. Note that Molinaro (2005) found that the 632+ estimator may not perform well when the training set is small (and especially of the number of predictors is large).\nLet’s take a look at a few specialized resampling methods.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-time-series-resampling",
    "href": "chapters/resampling.html#sec-time-series-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.6 Time Series Data",
    "text": "10.6 Time Series Data\nTime series data (Hyndman and Athanasopoulos 2024) are sequential observations ordered over time. The most common example is daily stock prices. These data are a special case due to autocorrelation: rows of the data set are not independent. This can occur for many different reasons (e.g. underlying seasonal trends, etc.), and this dependency complicates the resampling process in two primary ways. We need to think more carefully about how analysis and assessment sets are created.\n\nRandomly allocating samples to these sets would deteriorate the relationships among the samples due to autocorrelation. If the autocorrelation is important information related to the response, then random sampling will negatively impact a model’s ability to uncover the predictive relationship with the response.\nPlacing newer data in the analysis set and older data in the assessment set would not make sense because of the time trends in the data.\n\nRolling forecasting origin splitting (Tashman 2000) is a resampling scheme that mimics the train/test splitting pattern from Section 3.4 where the most recent data are used in the assessment set. We would choose how much data (or what range of time) to use for the analysis and assessment sets, then slide this pattern over the whole data set. Figure 10.9 illustrates this process where each block could represent some unit of time. For example, we might build the model on six months of data and then predict the next month. The next resample iteration could bump that forward by a month or some other period (so that the assessment sets can be distinct or overlapping).\n\n\n\n\n\n\n\n\nFigure 10.9: A schematic of rolling origin resampling for time series data. Each box corresponds to a unit of time (e.g., day, month, year, etc.).\n\n\n\n\n\nWe also have the choice to cumulatively expand the analysis data from the start of the training set to the end of the current slice. For example, in Resample 2 of Figure 10.9, the model would be fit using samples 1-9; in Resample 3, the model would be fit using samples 1-10; and so on. This approach may be useful when the number of periods of time are smaller and we would like to include as much of the historical data as possible in each resample.\nWe’ll use rolling forecasting origin resampling in one of the upcoming case studies using the hotel rate data previously discussed in Section 6.1.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-spatial-resampling",
    "href": "chapters/resampling.html#sec-spatial-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.7 Spatial Data",
    "text": "10.7 Spatial Data\nSection 3.9 described the problem of spatial correlation between locations and a method for creating an initial split for the forestation data.\nLike time-series data, the resampling scheme for this type of data emulates the initial splitting process. We could create groupings for the training set using the following methods:\n\nClustering methods: These methods create partitions of the data where points with spatial coordinates are closer to each other within the same cluster than to points in other clusters.\nGrid-based methods: A grid of equally sized blocks, either rectangular or hexagonal, is created to encompass the training set points.\n\nFrom these groupings, we can apply procedures similar to previous cross-validation variations:\n\nLeave-one-group-out resampling: This method creates as many resamples as there are groups. In each resample, data points from one group are used as the assessment set, while the remaining groups are used to train the model. This process repeats for each group.\nV-fold cross-validation: This method assigns each group to one of V meta-groups (e.g., folds). In each of the V iterations, one fold is left out as the assessment set, while the remaining folds are used to train the model.\nRepeated V-fold cross-validation: This method involves restarting the grouping process with a different random number seed, allowing for multiple rounds of V-fold cross-validation.\n\nEach of these approaches can be augmented using a buffer to exclude very close neighbors from being used.\nGoing back to our forestry data, we previously used hexagonal blocks to split the data (see Figure 3.5). The same blocking/buffering scheme was re-applied to the training set in order to create a 10-fold cross-validation. Recall that a 25 x 25 grid was used. For the training set, some of these hexagons are missing all of their data (since they were previously added to the test set). Of the remaining grid points, we systematically assign them to the ten folds. Figure 10.10 shows this process for the first fold. The figure shows the blocks of data, along with the corresponding buffer for this iteration of cross-validation. The purple blocks are, for the most part, not adjacent to one another. These are combined into the first assessment set. The other locations that are not in the buffers are pooled into the first analysis set.\n\n\n\n\n\n\n\n\nFigure 10.10: An example of one split produced by block cross-validation on the forestation data. The analysis (magenta) and assessment (purple) sets are shown with locations in the buffer in black. The empty spaces represent the data reserved for the test set (see Figure 3.5).\n\n\n\n\nOur training set contains 4,834 locations. Across each of the ten folds, every location appears once in the analysis and assessment sets.\nJust as with standard V-fold cross-validation, the number of folds can be chosen based on bias and variance concerns. Unlike standard resampling, any investigation into this topic would depend on a specific spatial autocorrelation structure. As such, we have decided to follow the recommendations for non-spatial data and use V=10. If we find that the variation in our statistics estimated from ten resamples is too large, we can employ the same process of repeating the block resampling process using different random number seeds. For our data, the average number of locations in the assessment sets is 483 and this appears large enough to produce sufficiently precise performance statistics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-multilevel-resampling",
    "href": "chapters/resampling.html#sec-multilevel-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.8 Grouped or Multi-Level Data",
    "text": "10.8 Grouped or Multi-Level Data\nWhen rows of data are correlated for other reasons, such as those discussed in Section 3.8, we can extend the techniques described above. Consider the previously mentioned example of having the purchase histories of many users from a retail database. If we had a training set of 10,000 instances that included 100 customers, we could conduct V-fold cross-validation to determine which customers would go into the analysis and assessment sets. As with the initial split, any rows associated with specific customers would be placed into the same partition (i.e., analysis or assessment).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampling-faq",
    "href": "chapters/resampling.html#sec-resampling-faq",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.9 Frequently Asked Questions",
    "text": "10.9 Frequently Asked Questions\nThe general concept of resampling seems clear and straightforward. However, we regularly encounter confusion and commonly asked questions about this process. In this section we will address these common questions and misconceptions.\n\nIs it bad to get different results from each resample?\nThe point of resampling is to see how the model pipeline changes when the data changes. There is a good chance that some important estimates differ from resample to resample. For example, when algorithmically filtering predictors, you might get B different predictor lists, and that’s okay. It does give you a sense of how much noise is in the part of your model pipeline.\nIt can be good to examine the distribution of resampling performance. Occasionally, we may find a resample that has unusual performance with respect to the rest of the resamples. This may merit further investigation as to why the resample is unusual.\n\n\nCan/should I use the individual resampling results to change the model pipeline?\nWe often hear questions about how to use the cross-validated results to define the model. For example, using the top X% of predictors selected during resampling as the final predictor set in a new model pipeline seems intuitive.\nOne important thing to consider is that you now have a different model pipeline than what was just resampled. To understand the performance of your newly informed model, you need to resample a model pipeline that algorithmically selects the X% of predictors and resample it.\nNote that resampling the specific set of top predictors determined from the resamples is not appropriate. We have seen this done many times before in the hopes of improving predictive performance. Performance often improves for the assessment sets. But the performance cannot be generalized to the test set or for other new samples. The rule to determine the top set should be resampled, not the specific output of that rule.\nFor the original pipeline, the final predictor set is determined when you fit that pipeline on the training set. The resampling results tell you about what could have happened; the training set results show what did happen.\n\n\nWhat happens to the B trained models? Which one should I keep?\nThe B models fit within resampling are only used to estimate how well the model fits the data. You don’t need them after that. You can retain them for diagnostic purposes though. As mentioned above, using the replicate models to look into how parts of the model change can be helpful.\nAlso, if you are not using a single validation set, recall that none of these models are trained with the unaltered training set, so none would be the final model.\n\n\nIs this some sort of ensemble?\nNo. We’ll talk model about ensembles in ?sec-ensembles and how the assessment set predictions can be used to create an ensemble.\n\n\nCan I accidentally resample incorrectly?\nUnfortunately, yes. As previously mentioned, improper data usage is one of the most frequent ways that machine learning models silently fail; that is, you won’t know that there is a problem until the next set of labeled data. A good example discussed in ?sec-removing-predictors is Ambroise and McLachlan (2002).\nIf you are\n\ntraining every part of your model pipeline after the initial data split and using only the training set and,\nevaluating performance with external data\n\nthe risk of error is low.\n\n\nWhat is nested resampling?\nThis is a version of where an additional layer of resampling occurs. For example, suppose you are using 10-fold cross-validation. Within each of the 10 iterations, you might add 20 bootstrapping iterations (that resamples the analysis set). That ends up training 200 distinct models13.\nFor a single model, that’s not very useful. However, as discussed in Section 11.4, there are situations where the analysis set is being used for too many purposes. For example, during model tuning, you might use resampling to find the best model and measure its performance. In certain cases, that can lead to bad results for the latter task.\nAnother example is recursive feature selection, where we are trying to rank predictors, sequentially remove them, and determine how many to remove. In this case, it’s a good idea to use an outer resampling loop to determine where to stop and an inner loop for the other tasks.\nWe will see nested resampling in the next chapter.\n\n\nIs resampling related to permutation tests?\nNot really. Permutation methods are similar to resampling only in that they perform multiple calculations on different versions of the training set.\nWe used permutation methods in the bootstrapping section to estimate the no-information rate. Otherwise, they won’t be used to evaluate model performance.\n\n\nAre sub-sampling or down-sampling the same as resampling?\nNo. These are techniques to modify your training set (or analysis set) to rebalance the data when a classification problem has rare events. They’ll be discussed in ?sec-imbalance-sampling.\n\n\nWhy do I mostly hear about validation sets?\nAs discussed in Section 1.6, stereotypical deep learning models are trained on very large sets of data. There is very little reason to use multiple resamples in these instances, and deep learning takes up a lot of space in the media and social media.\nGenerally, every data set that is not massive could benefit from multiple resamples.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#chapter-references",
    "href": "chapters/resampling.html#chapter-references",
    "title": "10  Measuring Performance with Resampling",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nArlot, S, and A Celisse. 2010. “A Survey of Cross-Validation Procedures for Model Selection.” Statistics Surveys 4: 40–79.\n\n\nBates, S, T Hastie, and R Tibshirani. 2023. “Cross-Validation: What Does It Estimate and How Well Does It Do It?” Journal of the American Statistical Association, 1–12.\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nEfron, B. 1983. “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.” Journal of the American Statistical Association, 316–31.\n\n\nEfron, B. 2003. “Second Thoughts on the Bootstrap.” Statistical Science, 135–40.\n\n\nEfron, B, and T Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.\n\n\nEfron, B, and R Tibshirani. 1997. “Improvements on Cross-Validation: The 632+ Bootstrap Method.” Journal of the American Statistical Association 92 (438): 548–60.\n\n\nFushiki, T. 2011. “Estimation of Prediction Error by Using k-Fold Cross-Validation.” Statistics and Computing 21: 137–46.\n\n\nHyndman, RJ, and G Athanasopoulos. 2024. Forecasting: Principles and Practice, 3rd Edition. Otexts.\n\n\nMolinaro, A. 2005. “Prediction Error Estimation: A Comparison of Resampling Methods.” Bioinformatics 21 (15): 3301–7.\n\n\nStone, M. 1974. “Cross-Validatory Choice and Assessment of Statistical Predictions.” Journal of the Royal Statistical Society: Series B (Methodological) 36 (2): 111–33.\n\n\nTashman, L. 2000. “Out-of-Sample Tests of Forecasting Accuracy: An Analysis and Review.” International Journal of Forecasting 16 (4): 437–50.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#footnotes",
    "href": "chapters/resampling.html#footnotes",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "These names are not universally used. We only invent new terminology to avoid confusion; people often refer to the data in our analysis set as the “training set” because it has the same purpose.↩︎\nThis discussion is very similar to the variance-bias tradeoff discussed in Section 8.4. The context here differs, but the themes are the same.↩︎\nMeaning that performance looks worse than it should. For example, smaller \\(R^2\\) or inflated Brier score.↩︎\nThe split can be made using the same tools already discussed, such as completely random selection, stratified random sampling, etc.↩︎\nAlso, if \\(n_{tr}\\) is not “large,” you shouldn’t use a validation set anyway.↩︎\nAgain, this could be accomplished using any of the splitting techniques described in Chapter 3.↩︎\nThe simulation details, sources, and results can be found at https://github.com/topepo/resampling_sim.↩︎\nThe percent bias is calculated as \\(100(Q_{true} - \\hat{Q})/Q_{true}\\) for metrics where smaller is better.↩︎\nArlot and Celisse (2010) provides a comprehensive survey of cross-validation↩︎\nHowever, for linear regression, the LOOCV predictions can be quickly computed without refitting a large number of models.↩︎\nThis isn’t the same as using V = 20. That scheme has very different bias properties than two repetitions of V = 10.↩︎\nThis application will be discussed in ?sec-boot-intervals.↩︎\nWe will refer to the first resample (10-fold CV) as the outer resample and the bootstrap as the inner resample.↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html",
    "href": "chapters/grid-search.html",
    "title": "11  Grid Search",
    "section": "",
    "text": "11.1 Regular Grids\nGrid search is a method to optimize a model pipeline’s tuning parameters. It creates a pre-defined set of candidate values and computes performance for each. From there, the numerically best candidate could be chosen, or the relationship between the tuning parameter(s) and model performance can be inspected to see if the model might benefit from additional optimization.\nSuppose there is a single tuning parameter, as in the \\(n_{min}\\) example of Section 9.3; each candidate takes a scalar value (quantitative or qualitative). For other models, there are multiple tuning parameters. For example, support vector machine models can have two or three tuning parameters, the Elastic Net model has two tuning parameters, and the boosted tree model has multiple tuning parameters. We’ll look at two of the boosted tree parameters in detail below.\nThe previous chapter demonstrated that using external data to evaluate the model is crucial (be it resampling or a validation set). Grid search has no free lunch: we cannot simply fit the model to each candidate set and evaluate them by simply re-predicting the same data.\nAlgorithm 11.1 formally describes the grid search process. For a model with \\(m\\) tuning parameters, we let \\(\\Theta\\) represent the collection of \\(s\\) candidate values. For each specific combination of parameters (\\(\\theta_j\\)), we resample the model to produce some measure of efficacy (e.g., \\(R^2\\), accuracy, etc.)1. From there, the best value is chosen, or additional work is carried out to find a suitable candidate.\nTo demonstrate, this chapter will initially focus on grids for a boosted tree model with two tuning parameters. Recall that a boosted tree is a collection of individual decision trees created sequentially. One parameter related to the process of creating the ensemble is the learning rate: the next tree uses information from the last tree to improve it. An important parameter is how much, or how fast it learns. It could be that some data sets need a large number of trees that evolve slowly (i.e., low learning rate) to find an optimal value of the performance metric. Alternatively, other data sets require fewer trees that change rapidly for optimal predictive performance. The learning rate parameter must be greater than zero, and is typically in the range of 10-5 to 10-1. Because the range of this parameter is very large, it is best conceptualized in log units.\nWe’ll illustrate different strategies of grid search using this model setup with varying configurations of the learning rate and the number of trees in the ensemble.\nThere are two main classes of grids: regular and irregular. We can view generating a collection of candidate models as a statistical design of experiments (DOE) problem (Box, Hunter, and Hunter 1978). There is a long history of DOE, and we’ll invoke relevant methods for each grid type. Santner, Williams, and Notz (2018) and Gramacy (2020) have excellent overviews of the DOE methods discussed in this chapter.\nThe following two sections describe different methods for creating the candidate set \\(\\Theta\\). Subsequent sections describe strategies for making grid search efficient using tools such as parallel processing and model racing.\nA regular grid starts with a sequence or set of candidate values for each tuning parameter and then creates all combinations. In statistics, this is referred to as a factorial design. The number of values per tuning parameter does not have to be the same.\nTo illustrate a regular grid, we’ll use five values of the number of trees (1, 500, 1000, 1500, and 2000) and three values of the learning rate (10-3, 10-2, and 10-1). This grid of 15 candidates is shown in Figure 11.1. The grid covers the entire space with significant lacuna in between.\nFigure 11.1: Three types of grids (in columns) are illustrated with tuning parameters from a boosting model. Each grid contains 15 candidates. The space-filling design was created using the Audze-Eglais method described in Section 11.2.\nWe’ll use the simulated training set in Section 9.1 with the same 10-fold cross-validation scheme described there. Once again, Brier scores were used to measure how well each of the 15 configurations of boosted tree model predicted the data. Figure 11.2 shows the results: a single tree is a poor choice (due to underfitting), and there are several learning rate values that work well. Even though this is not a diverse set of candidate values, we can probably pick out a reasonable candidate with a small Brier score, such as 500 trees and and a learning rate of 10-2 (although there are a few other candidates that would be good choices).\nFigure 11.2: The boosted tree tuning results for the regular grid shown in Figure 11.1.\nThe pattern shown in this visualization is interesting: the trajectory for the number of trees is different for different values of the learning rate. The two larger learning rates are similar, showing that the optimal number of trees is in the low- to mid-range of our grid. The results for the smallest learning rate indicate that better performance might be found using more trees than were evaluated. This indicates that there is an interaction effect in our tuning parameters (just as we saw for predictors in Section 8.1). This might not affect how we select the best candidate value for the grid, but it does help build some intuition for this particular model that might come in handy when tuning future models.\nGaining intuition is much easier for regular grids than other designs since we have a full set of combinations. As we’ll see shortly, this interaction is very hard to see for a space-filling design.\nThe primary downside to regular grids is that, as the number of tuning parameters increases, the number of points required to fill the space becomes extremely large (due to the curse of dimensionality). However, for some models and pre-processing methods, regular grids can be very efficient despite the number of tuning parameters (see the following section that describes the “submodel trick”).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-irregular-grid",
    "href": "chapters/grid-search.html#sec-irregular-grid",
    "title": "11  Grid Search",
    "section": "11.2 Irregular Grids",
    "text": "11.2 Irregular Grids\nIrregular grids are not factorial in nature. A simple example is a random grid where points are randomly placed using a uniform distribution on an appropriate range for each tuning parameter. A design of size 15 is shown in Figure 11.1. There are some gaps and clustering of the points, but the space for each individual dimension is covered well. The tuning results are shown in the top panels of Figure 11.3. Because it is an irregular design, we can’t use the same visualization as in Figure 11.2. Instead, a “marginal” plot is shown, where each numeric tuning parameter is plotted against performance in a separate panel.\nThe top left panel suggests that a very small learning rate is bad, but otherwise, there is not a strong trend. For the number of trees (in the top right panel), a small number of trees should be avoided, and perhaps the maximum number tested would be a good choice. From this plot, it is impossible to discover the interaction effect seen with the regular grid. However, this marginal plot is the main technique for visualizing the results when there are a moderate to large number of tuning parameters. The numerically best candidate was not too dissimilar from the regular grid: 1,898 trees and and a learning rate of 10-2.7\n\n\n\n\n\n\n\n\nFigure 11.3: The tuning results for the irregular grids shown in Figure 11.1. The lines are spline smooths.\n\n\n\n\n\nAnother type of irregular grid is a space-filling design (Joseph 2016), where the goal is to make sure that the tuning parameter space is covered and that there is minimal redundancy in the candidate values. There are a variety of methods for achieving this goal.\nFor example, Figure 11.1 shows a 15 point Audze-Eglais space-filling design. The space is covered more compactly than the regular design of the same size and is much more uniform than the 15 point random design. The lower panels of Figure 11.3 show the tuning results. The results are somewhat cleaner than the random grid results but show similar trends. Here, the numerically best candidate was: 286 trees and and a learning rate of 10-2.\nThere are many types of space-filling designs, and it is worth taking the time to take a quick tour of some of them.\nThe Latin hypercube design (LHD) is the most popular method for constructing space-filling designs (Husslage et al. 2011; Viana 2016). These designs have a simple definition. Suppose our hyperparameter space is rectangular and partitioned into smaller (hyper)cubes. From this, a LHD is a set of distinct points where no one dimension has multiple values in any bins2. We desire candidates that fill the space of each parameter and are not close to one another.\nThe most basic approach to creating a Latin hypercube design is random sampling (Mckay, Beckman, and Conover 2000). If there are \\(m\\) parameters and we request \\(s\\) candidate values, the parameter space is initially divided into \\(s^m\\) hypercubes of equal size. For each tuning parameter, \\(s\\) regions in its dimension are selected at random, and a value is placed in this box (also at random). This process repeats for each dimension. Suppose there are ten bins for a parameter that ranges between zero and one. If the first design point selects bin two, a random uniform value is created in the range [0.1 0.2). Figure 11.4 shows three such designs, each generated with different random numbers.\n\n\n\n\n\n\n\n\nFigure 11.4: Three replicate Latin hypercube sampling designs of size ten for two parameters. Each design uses different random numbers. The grid lines illustrate the 100 hypercubes used to generate the values.\n\n\n\n\n\nTechnically, these designs cover the space of each predictor uniformly. However, large multivariate regions can be empty. For example, one design only samples combinations along the diagonal. Additional constraints can make the design more consistent with our desires.\nFor example, we could choose points to maximize the minimum pairwise distances between the candidates. These designs are usually referred to as MaxiMin designs (Pronzato 2017). Comparing the two irregular designs in Figure 11.1, the random grid has a maximum minimum distance between candidates of 0.09. In contrast, the corresponding space-filling design’s value (0.26) is 2.9-fold larger3. The latter design was optimized for coverage, and we can see far less redundancy in this design.\nA similar method, initially proposed by Audze and Eglais (1977), maximizes a function of the inverse distances between \\(s\\) candidate points:\n\\[\ncriterion = \\sum_{i=1}^s \\sum_{j=1,\\;i\\ne j}^s\\frac{1}{dist(\\theta_i, \\theta_j)^2}\n\\]\nBates, Sienz, and Toropov (2004) devised search methods to find optimal designs for this criterion.\nSome other space-filling designs of note:\n\nMaximum entropy sampling selects points based on assumptions related to the distributions of the tuning parameters and their covariance matrix (Shewry and Wynn 1987; Joseph, Gul, and Ba 2015).\nUniform designs (Fang et al. 2000; Wang, Sun, and Xu 2022) optimally allocate points so that they are uniformly distributed in the space.\n\nWhile these methods can be generally constructed by sampling random points and using a search method to optimize a specific criterion, there has been scholarship that has pre-optimized designs for some combination of the number of tuning parameters and the requested grid size.\nThe advantage of space-filling designs over random designs is that, for smaller designs, the candidates do a better job covering the space and have a low probability of producing redundant points. Also, it is possible to create a space-filling design so that the candidates for each numerical parameter are nearly equally spaced (as was done in Figure 11.1).\nMany designs assume that all the tuning parameter values are quantitative. That may not always be the case. For example, K-nearest neighbors can adjust its predictions by considering how far the cost are from a new point; more distant points should not have the same influence as close cost. A weighting function can be used for this purpose. For example, weights based on the inverse of the distance between neighbors may produce better results. Equal weighting is often called a “rectangular weighting” function. The type of algebraic function used for weighting is a qualitative tuning parameter.\nA simple workaround for creating a space-filling design is to repeat the unique parameter values as if they were \\(s\\) distinct values when making the design. This allows them to be used with a Latin hypercube design and any space-filling design that uses this approach is not technically optimal. Practically speaking, this is an effective approach for tuning models. However, there are sliced LHD that can accomplish the same goal (Qian 2012; Ba, Myers, and Brenneman 2015).\nWe recommend space-filling designs since they are more efficient than regular designs. Regular designs have a lot of benefits when using an unfamiliar modeling methodology since you will learn a lot more about the nuances of how the tuning parameters affect one another.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-efficient-grid",
    "href": "chapters/grid-search.html#sec-efficient-grid",
    "title": "11  Grid Search",
    "section": "11.3 Efficient Computations for Conventional Grid Search",
    "text": "11.3 Efficient Computations for Conventional Grid Search\nThe computational cost of grid search can become large, depending on the resampling strategy and the number of candidates under consideration. In our example, a total of 150 boosted tree models are evaluated (5 values of the number of trees, 3 values of the learning rate, and 10-fold cross-validation) before determining which candidates are most favorable to our data.\nWe’ll look at three approaches to making grid search more efficient. One method (submodels) happens automatically for some types of models, another approach (parallel processing) uses software engineering tools, and the last tool (racing) is a statistical solution.\nTo illustrate these more efficient search approaches, a 100 grid point space-filling design will be used with a large parameter range (1 to 3000 trees) and learning rates between 10-5 and 10-1.\n\n11.3.1 Submodels\nLet’s begin by understanding the submodel trick. For some models a single training model can be used to predict many tuning parameter candidates. For example, the boosted tree model that we have been using creates a sequential ensemble of decision trees, each depending on the previous. Suppose that a boosting model with 3,000 trees is created. Most implementations of boosted trees can use this model to predict the outcomes of any smaller ensemble size. Therefore, we only need to build the boosted tree model using the largest number of trees for each specific combination of the remaining tuning parameters. For a regular grid, this can effectively drop a dimension of the computations. Depending on the model and grid, the speed-up for using this approach can be well into double digits.\nSome different models and pre-processors can have this quality, including the glmnet model (?sec-penalized-logistic-regression), partial least squares (Section 7.2.3), principal component feature extraction (Section 7.2.1), and others.\nUnfortunately, irregular designs cannot exploit the submodel trick since they are not factorial in nature. A hybrid design could be used where a dense sequence of \\(s_1\\) parameter values is created for the tuning parameter associated with submodels and a separate space-filling design of size \\(s_2\\) for the other parameters. These two grids can be crossed so that the space-filling design is replicated for each value of the submodel parameter. This produces \\(s_1\\times s_2\\) grid points but, effectively, only \\(s_2\\) models are fit.\nFor our larger grid, we created a similar regular grid of 100 points with the same expanded ranges of parameters. There were 10 unique, evenly spaced values for both parameters. To evaluate this grid, we only need to train 10 models. Compared to the analogous space-filling design, the regular grid was 3.8 times faster to evaluate.\n\n\n11.3.2 Parallel Processing\nFortunately, none of the 1,000 models in our large grid depend on one another and can be computed separately. Since almost all modern computers have GPUs and multiple CPUs, we can break the computations into different “chunks” of work and execute them simultaneously on distinct processors (or separate computers entirely). The parallel processing of models can significantly reduce the time it takes to tune using grid search.\nFor example, consider the space-filling designs with 15 candidates evaluated across 10 resamples. We spread the 150 model fits across 10 worker processes (on the same computer). Despite the meager computation costs of training each boosted tree, there was still a 6.3-fold speedup (25.8s versus 4.1s) when run in parallel.\nHowever, there are some important nuances that we should consider before initiating parallel processing.\nFirst, it is an excellent idea to parallelize the “longest loop” (literally or figuratively). Looking back at Algorithm 11.1, there is a loop across the \\(s\\) candidates in line 6. Line 7 contains the resampling loop across \\(B\\) resamples. If the data set is large, it may be optimal to invert the loop where we parallel process across the \\(B\\) resamples and execute the \\(s\\) models within each. Keeping each resample on a single worker means less input/output traffic across the workers. See Kuhn and Silge (2022) Section 13.5.2. Alternatively, if the data are not large, it could be best to “flatten” the two loops into a single loop with \\(s\\times B\\) iterations (with as many workers as possible).\nAdditionally, if expensive preprocessing is used, a naive approach where each pipeline is processed in parallel might be counterproductive because we unnecessarily repeat the same preprocessing. For example, suppose we are tuning a supervised model along with UMAP preprocessing. By sending each of the \\(s\\) candidates to a worker, the same UMAP training occurs for each set of candidates that correspond to the supervised model. Instead, a conditional process can be used where we loop across the UMAP tuning parameter combinations and, for each, run another loop across the parameters associated with the supervised model.\nAnother consideration is memory. If the data used to train and evaluate the model are copied for each worker, the number of workers can be restricted to fit within system memory. A well thought out process can avoid this unnecessary restriction.\nWhen we understand the computational aspects of parallel processing, we can almost always use it to greatly reduce the time required for model tuning.\n\n\n11.3.3 Racing\nA third way we can improve the model tuning process is through Racing (Maron and Moore 1997). This technique adaptively resamples the data during grid search. The goal is to cull tuning parameter combinations that have no real hope of being optimal before they are completely resampled. For example, in our previous tuning of the boosting model using a regular grid, it is clear that a single tree performs very poorly. Unfortunately, we will not know this until all the computations are finished.\nRacing tries to circumvent this issue by doing an interim statistical analysis of the tuning results. From this, we can compute a probability (or score) that measures how likely each candidate will be the best (for some single metric). If a candidate is exceedingly poor, we usually can tell that after just a few resamples4.\nThere are a variety of ways to do this. See Kuhn (2014), Krueger, Panknin, and Braun (2015), and Bergman, Purucker, and Hutter (2024). The general process is shown in Algorithm 11.2. The resampling process starts normally for the first \\(B_{min}\\) resamples. Lines 7-12 can be conducted in parallel for additional computation speed. After the initial resampling for all candidates, the interim analysis at iteration \\(b\\) results in a potentially smaller set of \\(s_b\\) candidates. Note that parallel processing can be incorporated into this process for each loop and the speed-up from using submodels also occurs automatically.\nAt each resampling estimate beyond the first \\(B_{min}\\) iterations, the current candidate set is evaluated for a single resample, and the interim analysis potentially prunes tuning parameter combinations.\n\n\n\n\n\n\n\n\n\n\\begin{algorithm} \\begin{algorithmic} \\State $\\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$ \\State $B$: number of resamples \\State Initial number of resamples $1 \\lt B_{min} \\lt B$ executed prior to analysis \\State $M(\\mathfrak{D}^{tr}, B)$: a mapping function to split $\\mathfrak{D}^{tr}$ for each of $B$ iterations. \\State $f()$: model pipeline \\State $\\Theta$: Parameter set ($s \\times m$) with candidates $\\theta_j$ \\For{$j=1$ \\To $s$} \\For{$b=1$ \\To $B_{min}$} \\State Generate $\\hat{Q}_{jb} =$ \\Call{Resample}{$\\mathfrak{D}^{tr}, f(\\cdot;\\theta_j), M_b(\\mathfrak{D}^{tr}, B)$} \\EndFor \\State Compute $\\hat{Q}_{j} = 1/B_{min}\\sum_b \\hat{Q}_{jb}$. \\EndFor \\State Eliminate candidates to produce $\\Theta^b$ ($s_b \\times m$) \\For{$b = B_{min} + 1$ \\To $B$} \\For{$j=1$ \\To $s$} \\State Generate $\\hat{Q}_{jb} =$ \\Call{Resample}{$\\mathfrak{D}^{tr}, f(\\cdot;\\theta_j), M_b(\\mathfrak{D}^{tr}, B)$} \\State Update candidate subset $\\Theta^b$ by applying the filtering analysis \\Endfor \\Endfor \\State Determine $\\hat{\\theta}_{opt}$ that optimizes $\\hat{Q}_j^k$. \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nAlgorithm 11.2: Using racing for grid search tuning parameter optimization.\n\n\n\nThe details are in the analysis used to discard tuning parameter combinations. For simplicity, we’ll focus on a basic method that uses a form of analysis of variance (ANOVA), a standard statistical tool for determining whether there are differences between conditions (Kutner, Nachtsheim, and Neter 2004). In our context, the conditions are the tuning parameter candidates; we ignore their values and treat them as qualitative samples from the distribution of all possible tuning parameter combinations. The ANOVA model uses the candidate conditions as the predictor and the metric of choice as the numeric outcome.\nThere are two statistical considerations that we need to understand when using an ANOVA model for evaluating Racing results.\nFirst, the ANOVA model is used for statistical inference. Specifically, we’ll use it to compute confidence intervals on differences in performance. This means that the probabilistic assumptions about our data matter. The ANOVA method requires that the errors follow a Gaussian distribution. That distribution can be strongly influenced by the distribution of the outcome, and, in our case, this is a performance metric. Previously, we used the Brier score, which has non-negative values and might be prone to follow a right-skewed statistical distribution. However, each resampled Brier score is the average of differences, and the Central Limit Theorem suggests that as the number of data points used to compute the score increases, the sampling distribution will become more Gaussian. If we use this approach to assess parameter candidates, then we should check the normality assumptions of the errors.\nThe second statistical complication is related to the resamples. Basic ANOVA methods require the data to be independent of one another, which is definitely not the case for resampling results. A “within-resample” correlation occurs since some resamples are “easier” to predict than others. This means that the metrics associated with each resample are more similar to one another than to the metrics from other resamples.\nThis extra correlation means that a simple ANOVA model cannot be used. Instead, our interim analysis should instead use a hierarchical random effects model. This is the same methodology used in Section 6.4.3 for effect encodings. We’ll treat our set of resamples as a random sample of possible resampling indices. The ANOVA model itself is:\n\\[\nQ_{ij} =(\\beta_0 + \\beta_{0i}) + \\beta_1x_{i1} + \\ldots +  \\beta_{s-1}x_{i(s-1)}+ \\epsilon_{ij}\n\\tag{11.1}\\]\nfor \\(i=1,\\ldots, B\\) resamples and \\(j=1, \\ldots, s\\) candidates. This random intercept model assumes that the ranking of candidates is the same across resamples and only the magnitude of the pattern changes from resample to resample.\nThis model’s form is the reference cell parameterization discussed in Section 6.2. For each interim analysis, the reference cell will be set to the current best candidate. This means that the \\(\\hat{\\beta}_j\\) parameter estimates represent the loss of performance relative to the current best. A one-sided confidence interval is constructed to determine if a candidate should be removed. We can stop considering candidates whose interval does not contain zero.\nTo demonstrate the racing procedure, we re-evaluated our larger a space-filling design. The order of the folds was randomized, and after 3 resamples, the ANOVA method was used to analyze the results. Figure 11.5 shows the one-sided 95% confidence intervals for the loss of Brier score relative to the current best configuration (2030 trees and a learning rate of 10-2.78). Sixty-Six were eliminated at this round.\n\n\n\n\n\n\n\n\nFigure 11.5: The first interim analysis in racing using a larger space-filling design for the boosted tree model.\n\n\n\n\n\nIn the end, the racing process eliminated all but 7 candidates. The process eliminated candidates at iterations three (66 configurations), four (14 configurations), five (7 configurations), six (1 configuration), seven (2 configurations), eight (2 configurations), and nine (1 configuration). In all, 404 models were fit out of the possible 1,000 (40.4%). The numerically best candidate set for racing and basic grid search were the same: 576 trees and and a learning rate of 10-2.3.\nThe model in Equation 11.1 allows us to estimate the within-resample correlation coefficient. This estimates how similar the performance metric values are to one another relative to values between resamples. In our example, the estimate of within resample correlation is 0.6, and is a good example of why we should use Equation 11.1 when culling parameter candidates.\nIn summary, racing can be an effective tool for screening a large number of models while using comprehensive grids.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-nested-resampling",
    "href": "chapters/grid-search.html#sec-nested-resampling",
    "title": "11  Grid Search",
    "section": "11.4 Optimization Bias and Nested Resampling",
    "text": "11.4 Optimization Bias and Nested Resampling\nIn the last section, we found that the numerically best candidate was 576 trees and and a learning rate of 10-2.3. The corresponding resampling estimate of the Brier score was 0.0783 with a standard error of 0.0053. If someone were to ask us how well the optimal boosted tree performs, we would probably give them these performance estimates.\nHowever, there is an issue in doing so that may affect our decision for selecting the optimal tuning parameters. We are using grid search and resampling to find the best combination of parameters and to estimate the corresponding performance. The problem is that we don’t know how accurate or precise the estimate of the optimal candidate may be. In some situations, this dual use of the model tuning process can introduce optimization bias where, to some degree, our performance statistics are optimistic.\nThis issue has been studied extensively in the high-dimensional biology literature, where there could be dozens of samples but thousands of predictors. In this case, feature selection is of paramount importance. However, the process of selecting important predictors in this situation can be very difficult and often leads to unstable models or preprocessing methods that overfit the predictor set to the data. Discussions of these issues can be found in Varma and Simon (2006), Boulesteix and Strobl (2009), Bischl et al. (2012), and Bernau, Augustin, and Boulesteix (2013).\nIn this section, we’ll introduce a few methods to quantify and correct for optimization bias. These approaches are not specific to traditional grid search; they can be used with racing, iterative search, or any algorithm that we use to optimize a model. Some of the concepts can be difficult and, for this reason, we’ll use a simplified grid search scenario. Let’s optimize our boosted tree by fixing the number of boosting iterations to 500 and only optimize the learning rate. But the tools that we are about to discuss are not limited to a single tuning parameter. Simplifying the task in this way allos us to visualize and explain the process5.\nUsing the same 10-fold cross-validation scheme, Figure 11.6 shows the results of a conventional grid search over the learning rate where 100 values of that parameter were evaluated. As usual, the line indicates the average of the Brier scores produced by the 10 assessment sets. The results of this process indicates that the smallest Brier score is achieved with a learning rate of 10-2.23. The Brier score that corresponds to this tuning parameter value is estimated to be 0.0782 with corresponding standard error of 0.00536.\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\nFigure 11.6: Basic grid search for learning rate with 100 grid points. The dotted vertical line shows the optimal learning rate using the overall resampling estimates. The colored rug values show the learning rates that were optimal within each resample. There were 2 learning rates selected more than once: 10-2.27 and 10-1.91.\n\n\n\nHow can we estimate the potential bias in the process that led to this performance metric? Tibshirani and Tibshirani (2009) describes a simple analytical estimator applicable when multiple resamples are used to estimate performance. First, we obtain the performance estimates corresponding to the optimal tuning parameter candidate within each fold. Let’s call these \\(\\hat{Q}_j^*\\) (where \\(j=1\\ldots B\\)). They are represented in Figure 11.6 as the colored “rug” lines at the bottom of the figure. We know that our conventional analysis of this grid search finds that a learning rate of 10-2.23 to be optimal. We can find the metric values associated with the global optimum for each resample (denoted as \\(\\hat{Q}_j^{opt}\\)). For each resample, we now have matched performance estimates for the local optima as well as the global optimum. The difference between these values is an estimate of the bias in optimization. If different resamples have very different optimal performance metrics compared to the optimal performance determined using the averages of the resamples (as shown in Figure 11.6), bias can increase. The estimate of the bias is then:\n\\[\\widehat{bias} = \\frac{1}{B}\\sum_{j=1}^B\\left[\\hat{Q}_j^{opt} - \\hat{Q}_j^*\\right]\\] For this particular example, the bias is fairly small (0.000931 with standard error 0.000304). To correct the conventional estimator, we add the bias; the Brier score is adjusted from 0.0782 to 0.0791. A more complex analytical method can be found in Tibshirani and Rosset (2019).\nWe can also estimate optimization bias with more complex resampling schemes. Recall that the issue is that we are overextending the conventional resampling scheme by doing too much with the same data (i.e., estimating overall performance assessment and the optimal performance value). Nested resampling (Varma and Simon 2006; Boulesteix and Strobl 2009) prevents this overuse by using two layers of resampling: the “inner resamples” are used to estimate the optimal candidate values, and the “outer resamples” estimate performance at those values.\nFor example, our current resampling scheme is 10-fold cross-validation. Since the training set has 2,000 samples, each fold uses 1,800 to fit the model and a separate 200 for assessment. Nested resampling would create 10 more independent resampling schemes within each of the 1,800-point analysis sets. If we once again used 10-fold cross-validation for the inner resamples, each would contain 10 analysis sets of size 1,620 with corresponding assessment sets of 180 data points.\nThe process starts with the inner resamples. The same model tuning procedure is used (basic grid search in our example), and each of the inner resamples estimates its own optimal candidate \\(\\hat{\\theta}_k\\). The outer resample takes this candidate value and estimates its performance using the assessment sets from the outer resampling loop. These outer estimates, whose data were never used to tune the model, are averaged to produce the nested resampling estimate of performance.\nConsider Figure 11.7. The colored lines in the left-hand panel show the results of the 10 inner resamples. Each line is made up of the averages of the inner 10 assessment sets of size 180. The filled circles along these lines indicate the optimal learning rate for that inner resample. Each outer resample takes its corresponding learning rate, trains that model using its analysis set of 1,800 points, and computes performance using its 200 assessment samples. These 10 statistics are averaged to get the final performance estimate, which should be free of any optimization bias.\n\n\n\n\n\n\n\n\nFigure 11.7: Each line corresponds to a complete resampled grid search that uses the inner resamples. The points indicate the estimates of the optimal learning rate as determined by the inner results. The panel on the right contrasts the Brier score for the inner assessment set with the outer assessment sets.\n\n\n\n\n\nWe can see that the resample-specific optimal learning rates vary but are within a consistent region. There are a few resamples that found the same optimal value. There is some variation in the y-axis too; different assessment sets produce different values. The standard error of these inner statistics, 0.000537, is much smaller than the value of the conventional estimate (0.00536).\nThe panel on the left shows boxplots of the Brier scores for the inner resamples and the corresponding outer resample estimates. The standard error of the outer resamples is very similar to the the level of noise in the conventional estimate.\nIn the end, the nested resampling estimate of the Brier score was estimated as 0.0784; a value very close to the single 10-fold cross-validation result shown in Figure 11.6.\n\nIt is important to emphasize that nested resampling is for verification, not optimization. It provides a better estimate of our model optimization process; it does not replace it.\n\nWe might best communicate the results of nested resampling like this:\n\nWe tuned the learning rate of our boosted classification tree using grid search and found that a rate of 10-2.23 to be best. We think that this model has a corresponding Brier score, measured without optimization bias, of 0.0784.\n\nWe can look at the candidates produced by the inner resamples to understand the stability of the optimization process and potentially diagnose other issues. We would not choose “the best” inner resampling result and move forward with its candidate value6.\nIt is a good idea to use nested resampling when the training set is small, the predictor set is very large, or both. The lack of significant bias in the analysis above does not discount the problem of optimization bias. It exists but can sometimes be within the experimental noise.\nIt is a good idea to choose an inner resampling scheme that is as close as possible to the single resampling scheme that you used for optimization. The outer scheme can vary depending on your needs. Care should be used when the outer resampling method is the bootstrap. Since it replicates training set points in its analysis sets, the inner resamples need to use the unique rows of the original training set. Otherwise the same data point might end up in both the inner analysis and assessment sets.\nThe primary downside to nested resampling is the computational costs. Two layers of resampling have a quadratic cost, and only the inner resamples can be executed in parallel. Using 10 workers, the analysis in Figure 11.7 took 6.6-fold longer to compute than the basic grid search that produced Figure 11.6.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#setting-parameter-ranges",
    "href": "chapters/grid-search.html#setting-parameter-ranges",
    "title": "11  Grid Search",
    "section": "11.5 Setting Parameter Ranges",
    "text": "11.5 Setting Parameter Ranges\nSpecifying the range of parameters used to define the grid may involve some guesswork. For some parameter types, there is a well understood and defined range. For others, such as learning rate, there is a lower bound (zero) and a loosely defined upper bound mostly based on convention and prior experience. For grids, we want to avoid configurations with few unique values and a wide range. This might not sample the section of the parameter space that includes nonlinearity and the region of optimal results. Figure 9.6 shows an example of this for the learning rate. The initial grid included three points that missed the regions of interest. Figure 9.5 did a better job with more grid points. In high dimensional parameter space, the likelihood of a poor grid increases, especially for relatively “small” grid sizes.\nWe often envision the relationship between a predictor and model performance as being a sharp peak. If we cannot find the pinnacle, the model optimization would fail. Luckily, as seen in Figure 11.7, there are often substantial regions of parameter space with good performance. Model optimization may not be superficially easy but it is often not impossible. We only need to sample the optimal region once to be successful.\nIf we worry that our tuning grid did not produce good results, another strategy is to increase the parameter ranges and use an iterative approach that can naturally explore the parameter space and let the current set of results guide the exploration about the space. These methods are discussed in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#chapter-references",
    "href": "chapters/grid-search.html#chapter-references",
    "title": "11  Grid Search",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAudze, P, and V Eglais. 1977. “New Approach to Planning Out of Experiments.” Problems of Dynamics and Strengths 35: 104–7.\n\n\nBa, S, R Myers, and W Brenneman. 2015. “Optimal Sliced Latin Hypercube Designs.” Technometrics 57 (4): 479–87.\n\n\nBates, S, J Sienz, and V Toropov. 2004. “Formulation of the Optimal Latin Hypercube Design of Experiments Using a Permutation Genetic Algorithm.” In 45th AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics & Materials Conference, 2011.\n\n\nBergman, E, L Purucker, and F Hutter. 2024. “Don’t Waste Your Time: Early Stopping Cross-Validation.” arXiv.\n\n\nBernau, C, T Augustin, and AJ Boulesteix. 2013. “Correcting the Optimal Resampling-Based Error Rate by Estimating the Error Rate of Wrapper Algorithms.” Biometrics 69 (3): 693–702.\n\n\nBischl, B, O Mersmann, H Trautmann, and C Weihs. 2012. “Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation.” Evolutionary Computation 20 (2): 249–75.\n\n\nBoulesteix, AL, and C Strobl. 2009. “Optimal Classifier Selection and Negative Bias in Error Rate Estimation: An Empirical Study on High-Dimensional Prediction.” BMC Medical Research Methodology 9: 1–14.\n\n\nBox, GEP, W Hunter, and J. Hunter. 1978. Statistics for Experimenters. New York: Wiley.\n\n\nFang, KT, D Lin, P Winker, and Y Zhang. 2000. “Uniform Design: Theory and Application.” Technometrics 42 (3): 237–48.\n\n\nGramacy, R. 2020. Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.\n\n\nHusslage, B, G Rennen, E Van Dam, and D Den Hertog. 2011. “Space-Filling Latin Hypercube Designs for Computer Experiments.” Optimization and Engineering 12: 611–30.\n\n\nJoseph, V. 2016. “Space-Filling Designs for Computer Experiments: A Review.” Quality Engineering 28 (1): 28–35.\n\n\nJoseph, V, E Gul, and S Ba. 2015. “Maximum Projection Designs for Computer Experiments.” Biometrika 102 (2): 371–80.\n\n\nKrueger, T, D Panknin, and ML Braun. 2015. “Fast Cross-Validation via Sequential Testing.” Journal of Machine Learning Research 16 (1): 1103–55.\n\n\nKuhn, M. 2014. “Futility Analysis in the Cross-Validation of Machine Learning Models.” arXiv.\n\n\nKuhn, M, and J Silge. 2022. Tidy Modeling with R. O’Reilly Media, Inc.\n\n\nKutner, M, C Nachtsheim, and J Neter. 2004. Applied Linear Regression Models. McGraw-Hill/Irwin.\n\n\nMaron, O, and AW Moore. 1997. “The Racing Algorithm: Model Selection for Lazy Learners.” Artificial Intelligence Review 11: 193–225.\n\n\nMckay, M, R Beckman, and W Conover. 2000. “A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code.” Technometrics 42 (1): 55–61.\n\n\nPronzato, L. 2017. “Minimax and Maximin Space-Filling Designs: Some Properties and Methods for Construction.” Journal de La Société Française de Statistique 158 (1): 7–36.\n\n\nQian, P. 2012. “Sliced Latin Hypercube Designs.” Journal of the American Statistical Association 107 (497): 393–99.\n\n\nSantner, T, B Williams, and W Notz. 2018. The Design and Analysis of Computer Experiments. Springer.\n\n\nShewry, M, and H Wynn. 1987. “Maximum Entropy Sampling.” Journal of Applied Statistics 14 (2): 165–70.\n\n\nTibshirani, RJ, and S Rosset. 2019. “Excess Optimism: How Biased Is the Apparent Error of an Estimator Tuned by SURE?” Journal of the American Statistical Association 114 (526): 697–712.\n\n\nTibshirani, RJ, and R Tibshirani. 2009. “A Bias Correction for the Minimum Error Rate in Cross-Validation.” The Annals of Applied Statistics 3 (2): 822–29.\n\n\nVarma, S, and R Simon. 2006. “Bias in Error Estimation When Using Cross-Validation for Model Selection.” BMC Bioinformatics 7: 1–8.\n\n\nViana, F. 2016. “A Tutorial on Latin Hypercube Design of Experiments.” Quality and Reliability Engineering International 32 (5): 1975–85.\n\n\nWang, Y, F Sun, and H Xu. 2022. “On Design Orthogonality, Maximin Distance, and Projection Uniformity for Computer Experiments.” Journal of the American Statistical Association 117 (537): 375–85.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#footnotes",
    "href": "chapters/grid-search.html#footnotes",
    "title": "11  Grid Search",
    "section": "",
    "text": "This process was illustrated in Algorithm 10.1.↩︎\nOther applications may want the points to have properties such as orthogonality, symmetry, etc.↩︎\nUsing Euclidean distance.↩︎\nNote that racing requires multiple assessment sets. A single validation set could not be used with racing.↩︎\nUnfortunately, this is an example that is not prone to optimization bias. We’ll see examples later on this website where we can more effectively use these tools.↩︎\nTo reiterate, the nested resampling process is used to characterize our optimization process. If it becomes our optimization process, we would need to nest it inside another nested resample.↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  }
]