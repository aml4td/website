[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "",
    "text": "Preface\nWelcome! This is a work in progress. We want to create a practical guide to developing quality predictive models from tabular data. We’ll publish materials here as we create them and welcome community contributions in the form of discussions, suggestions, and edits.\nWe also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).\nOur intention is to write these materials and, when we feel we’re done, pick a publishing partner to produce a print version.\nThe book takes a holistic view of the predictive modeling process and focuses on a few areas that are usually left out of similar works. For example, the effectiveness of the model can be driven by how the predictors are represented. Because of this, we tightly couple feature engineering methods with machine learning models. Also, quite a lot of work happens after we have determined our best model and created the final fit. These post-modeling activities are an important part of the model development process and will be described in detail.\nWe deliberately avoid using the term “artificial intelligence.” Eugen Rochko’s (@Gargron@mastodon.social) comment on Mastodon does a good job of summarizing our reservations regarding the term:\nTo cite this website, we suggest:\n@online{aml4td,\n  author = {Kuhn, M and Johnson, K},\n  title = {{Applied Machine Learning for Tabular Data}},\n  year = {2023},\n  url = { https://aml4td.org},\n  urldate = {2026-01-20}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "License",
    "text": "License\n\nThis work is licensed under CC BY-NC-SA 4.0\n\nThis license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, for noncommercial purposes only. If others modify or adapt the material, they must license the modified material under identical terms.\n\nBY: Credit must be given to you, the creator.\nNC: Only noncommercial use of your work is permitted. Noncommercial means not primarily intended for or directed towards commercial advantage or monetary compensation.\nSA: Adaptations must be shared under the same terms.\n\nOur goal is to have an open book where people can reuse and reference the materials but can’t just put their names on them and resell them (without our permission).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Intended Audience",
    "text": "Intended Audience\nOur intended audience includes data analysts of many types: statisticians, data scientists, professors and instructors of machine learning courses, laboratory scientists, and anyone else who desires to understand how to create a model for prediction. We don’t expect readers to be experts in these methods or the math behind them. Instead, our approach throughout this work is applied. That is, we want readers to use this material to build intuition about the predictive modeling process. What are good and bad ideas for the modeling process? What pitfalls should we look out for? How can we be confident that the model will be predictive for new samples? What are advantages and disadvantages of different types of models? These are just some of the questions that this work will address.\nSome background in modeling and statistics will be extremely useful. Having seen or used basic regression models is good, and an understanding of basic statistical concepts such as variance, correlation, populations, samples, etc., is needed. There will also be some mathematical notation, so you’ll need to be able to grasp these abstractions. But we will keep this to those parts where it is absolutely necessary. There are a few more statistically sophisticated sections for some of the more advanced topics.\nIf you would like a more theoretical treatment of machine learning models, then we recommend Hastie, Tibshirani, and Friedman (2017). Other books for gaining a more in-depth understanding of machine learning are Bishop and Nasrabadi (2006), Arnold, Kane, and Lewis (2019) and, for more of a deep learning focus, Goodfellow, Bengio, and Courville (2016) and/or Prince (2023).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#is-there-code",
    "href": "index.html#is-there-code",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Is There Code?",
    "text": "Is There Code?\nWe definitely want to decouple the content of this work from specific software. One of our other books on modeling had computing sections. Many people found these sections to be a useful resource at the time of the book’s publication. However, code can quickly become outdated in today’s computational environment. In addition, this information takes up a lot of page space that would be better used for other topics.\nWe will create computing supplements to go along with the materials. Since we use R’s tidymodels framework for calculations, the supplement currently in-progress is:\n\ntidymodels.aml4td.org\n\nIf you are interested in working on a python/scikit-learn supplement, please file an issue\nWhen there is code that corresponds to a particular section, there will be one or more icons at the ends of the section to link to the computing supplements, such as this:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-exercises",
    "href": "index.html#sec-exercises",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Are There Exercises?",
    "text": "Are There Exercises?\nMany readers found the Exercise sections of Applied Predictive Modeling to be helpful for solidifying the concepts presented in each chapter. The current set can be found at exercises.aml4td.org",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-help",
    "href": "index.html#sec-help",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "How can I Ask Questions?",
    "text": "How can I Ask Questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like cross-validated. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what I call Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.\nIf you think there is a bug, please file an issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Can I Contribute?",
    "text": "Can I Contribute?\nThere is a contributing page with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list. It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.\nAlso note that the aml4td project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto was used to compile and render the materials\n\nQuarto 1.8.26\n[✓] Checking environment information...\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.6.3: OK\n      Dart Sass version 1.87.0: OK\n      Deno version 2.3.1: OK\n      Typst version 0.13.0: OK\n[✓] Checking versions of quarto dependencies......OK\n[✓] Checking Quarto installation......OK\n      Version: 1.8.26\n[✓] Checking tools....................OK\n      TinyTeX: (external install)\n      Chromium: (not installed)\n[✓] Checking LaTeX....................OK\n      Using: TinyTex\n      Version: 2025\n[✓] Checking Chrome Headless....................OK\n      Using: Chrome found on system\n      Source: MacOS known location\n[✓] Checking basic markdown render....OK\n[✓] Checking Python 3 installation....OK\n      Version: 3.9.6\n      Jupyter: (None)\n      Jupyter is not available in this Python installation.\n[✓] Checking R installation...........OK\n      Version: 4.5.2\n      LibPaths:\n      knitr: 1.51\n      rmarkdown: 2.30\n[✓] Checking Knitr engine render......OK\n\nR version 4.5.2 (2025-10-31) was used for the majority of the computations. torch 2.7.1 was also used. The versions of the primary R modeling and visualization packages used here are:\n\n\n\n\n\n\n\n\n\naorsf (0.1.6)\naplore3 (0.9)\napplicable (0.1.1)\n\n\naspline (0.2.0)\nbaguette (1.1.0)\nbestNormalize (1.9.2)\n\n\nbibtex (0.5.1)\nbonsai (0.4.0)\nbroom (1.0.11)\n\n\nbroom.mixed (0.2.9.6)\nbrulee (0.6.0)\nbslib (0.9.0)\n\n\nC50 (0.2.0)\ncar (3.1-3)\nCubist (0.5.1)\n\n\nDALEXtra (2.3.1)\ndbarts (0.9-32)\nddalpha (1.3.16)\n\n\ndesirability2 (0.2.0)\ndials (1.4.2)\ndimRed (0.2.7)\n\n\ndiscrim (1.1.0)\ndplyr (1.1.4)\ne1071 (1.7-17)\n\n\nearth (5.3.5)\nembed (1.2.1)\nemmeans (2.0.1)\n\n\nfastICA (1.2-7)\nfiltro (0.2.0)\nfinetune (1.2.1)\n\n\nforested (0.2.0)\nGA (3.2.5)\ngganimate (1.0.11)\n\n\nggforce (0.5.0)\nggiraph (0.9.2)\nggplot2 (4.0.1)\n\n\nglmnet (4.1-10)\ngt (1.2.0)\ngtExtras (0.6.1)\n\n\nhardhat (1.4.2)\nheatmaply (1.6.0)\nhstats (1.2.2)\n\n\nimportant (0.2.1)\nipred (0.9-15)\nirlba (2.3.5.1)\n\n\njanitor (2.2.1)\nkernlab (0.9-33)\nklaR (1.7-3)\n\n\nleaflet (2.2.3)\nlightgbm (4.6.0)\nlme4 (1.1-38)\n\n\nMatrix (1.7-4)\nmda (0.5-5)\nmeasure (0.0.1.9002)\n\n\nmgcv (1.9-4)\nmirai (2.5.3)\nmixdir (0.3.0)\n\n\nmixOmics (6.32.0)\nmodeldata (1.5.1)\nmodeldatatoo (0.3.0)\n\n\nnaniar (1.1.0)\nnlme (3.1-168)\npamr (1.57)\n\n\nparsnip (1.4.1)\npartykit (1.2-24)\npatchwork (1.3.2)\n\n\nplsmod (1.0.0)\nprobably (1.2.0)\npROC (1.19.0.1)\n\n\npurrr (1.2.1)\nragg (1.5.0)\nranger (0.17.0)\n\n\nrecipes (1.3.1)\nrpart (4.1.24)\nrsample (1.3.1)\n\n\nRSpectra (0.16-2)\nrstudioapi (0.17.1)\nrules (1.0.2)\n\n\nsf (1.0-24)\nsfd (0.1.0)\nshinylive (0.3.0.9000)\n\n\nsparsediscrim (0.3.0)\nsparseLDA (0.1-9)\nsparsevctrs (0.3.5)\n\n\nspatialsample (0.6.1)\nsplines2 (0.5.4)\nstacks (1.1.1)\n\n\nstopwords (2.3)\ntextrecipes (1.1.0)\nthemis (1.0.3)\n\n\ntidymodels (1.4.1)\ntidyposterior (1.0.2)\ntidyr (1.3.2)\n\n\ntidysdm (1.0.4)\ntorch (0.16.3)\ntune (2.0.1)\n\n\nusethis (3.2.1)\nuwot (0.2.4)\nVBsparsePCA (0.1.0)\n\n\nviridis (0.6.5)\nworkflows (1.3.0)\nworkflowsets (1.1.1)\n\n\nxgboost (3.1.3.1)\nxrf (0.3.1)\nyardstick (1.3.2)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#chapter-references",
    "href": "index.html#chapter-references",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nArnold, T, M Kane, and B Lewis. 2019. A Computational Approach to Statistical Learning. Chapman; Hall/CRC.\n\n\nBishop, C M, and N M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nHastie, T, R Tibshirani, and J Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/news.html",
    "href": "chapters/news.html",
    "title": "News",
    "section": "",
    "text": "2026-01-23\nNew sections on",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-1",
    "href": "chapters/news.html#section-1",
    "title": "News",
    "section": "2025-07-25",
    "text": "2025-07-25\nPart 4 has begun with\n\n15  Characterizing Classification Models\n16  Generalized Linear and Additive Classifiers (partial)\n\nWe’re also keeping placeholders for the remaining classification chapters.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-2",
    "href": "chapters/news.html#section-2",
    "title": "News",
    "section": "2025-06-24",
    "text": "2025-06-24\nA new chapter, 14  Comparing Models, was added, concluding Part 3 of the book.\nAlso, a small section on permutation statistics was added to 9  Overfitting.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-3",
    "href": "chapters/news.html#section-3",
    "title": "News",
    "section": "2025-05-21",
    "text": "2025-05-21\nNew chapters on iterative optimization and feature selection.\nWhenever a section has a corresponding computing supplement, an icon (e.g.,  ) is shown that directly links to that section.\nNon-user facing changes:\n\nSome prep work for future light/dark mode.\nMove to mirai for parallel processing.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-4",
    "href": "chapters/news.html#section-4",
    "title": "News",
    "section": "2024-10-01",
    "text": "2024-10-01\nEnabled google analytics.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-5",
    "href": "chapters/news.html#section-5",
    "title": "News",
    "section": "2024-09-23",
    "text": "2024-09-23\nChapter 4 is now on missing data, bumping subsequent chapter numbers.\nNew chapters on resampling and grid search are now online.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-6",
    "href": "chapters/news.html#section-6",
    "title": "News",
    "section": "2024-06-17",
    "text": "2024-06-17\nSeveral new chapters on embeddings, splines/interactions/discretization, and overfitting. Also, shinylive is used for interactive visualizations of concepts.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-7",
    "href": "chapters/news.html#section-7",
    "title": "News",
    "section": "2024-01-02",
    "text": "2024-01-02\nFixed various typos.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-8",
    "href": "chapters/news.html#section-8",
    "title": "News",
    "section": "2023-12-21",
    "text": "2023-12-21\nTypo fix on main page.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-9",
    "href": "chapters/news.html#section-9",
    "title": "News",
    "section": "2023-12-19",
    "text": "2023-12-19\nSmall updates.\nUpdated snapshot.\nThe background color of premade outputs has been changed to the pages background color (#16).",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-10",
    "href": "chapters/news.html#section-10",
    "title": "News",
    "section": "2023-12-11",
    "text": "2023-12-11\nNo new content.\nUpdated snapshot.\nIncludes new Google Scholar links for articles (#8)",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-11",
    "href": "chapters/news.html#section-11",
    "title": "News",
    "section": "2023-11-17",
    "text": "2023-11-17\nUpdated renv snapshot.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/contributing.html",
    "href": "chapters/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Software\nPlease note that the aml4td project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nIf you plan to do anything beyond fixing a typo, the best thing you can do is to open an issue and discuss changes before you spend a lot of time doing them.\nIf you don’t have a lot of experience with git or GitHub, take a look at the wonderful Happy Git and GitHub for the useR.\nIf you want to contribute, some general advice is:\nA merged PR will make you appear in the contributor list (see below). It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.\nYou will mostly work with the *.qmd files in the chapters directory.\nHere is a list of the elements in the repo:\nRegarding R packages, the repository has a DESCRIPTION file as if it were an R package. This lets us specify precisely what packages and their versions should be installed. The packages listed in the imports field contain packages for modeling/analysis and packages used to make the website/book. Some basic system requirements are likely needed to install packages: Fortran, gdal, and others.\nThe main requirements are as follows.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#software",
    "href": "chapters/contributing.html#software",
    "title": "Contributing",
    "section": "",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing system. Quarto version 1.8.26 is used to compile the website.\nWe also use a few Quarto extensions. These should be installed from the project’s root directory via:\nquarto add quarto-ext/fontawesome\nquarto add quarto-ext/shinylive\nquarto add quarto-ext/fancy-text\nquarto add leovan/quarto-pseudocode\n\n\nR and renv\nR version 4.5.2 (2025-10-31) is what we are currently using. We suggest using rig to manage R versions. There are several IDEs that you can use. We’ve used RStudio (&gt;= 2023.6.1.524).\nThe current strategy is to use the renv (&gt;= version 1.1.5) package to make this project more isolated, portable and reproducible.\nTo get package dependencies installed…\n\nWhen you open the website.Rproj file, the renv package should be automatically installed/updated (if neded). For example:\n# Bootstrapping renv 1.0.3 ---------------------------------------------------\n- Downloading renv ... OK\n- Installing renv  ... OK\n\nThe following package(s) will be installed:\n- BiocManager [1.30.22]\nThese packages will be installed into \"~/content/aml4td/renv/library/R-4.3/x86_64-apple-darwin20\".\nif you try to compile the book, you probably get and error:\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nYou can get more information using renv::status() but you can get them installed by first running renv::activate(). As an example:\n&gt; renv::activate()\n\nRestarting R session...\n\n- Project '~/content/aml4td' loaded. [renv 1.0.3]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nSince we have package versions recorded in the lockfile, we can installed them using renv::restore(). Here is an example of that output:\n&gt; renv::restore() \nThe following package(s) will be updated:\n\n# Bioconductor ---------------------------------------------------------------\n- mixOmics         [* -&gt; mixOmicsTeam/mixOmics]\n\n# CRAN -----------------------------------------------------------------------\n- BiocManager      [1.30.22 -&gt; 1.30.21.1]\n- lattice          [0.21-9 -&gt; 0.21-8]\n- Matrix           [1.6-1.1 -&gt; 1.6-0]\n- nlme             [3.1-163 -&gt; 3.1-162]\n- rpart            [4.1.21 -&gt; 4.1.19]\n- survival         [3.5-7 -&gt; 3.5-5]\n- abind            [* -&gt; 1.4-5]\n\n&lt;snip&gt;\n\n- zip              [* -&gt; 2.2.0]\n- zoo              [* -&gt; 1.8-12]\n\n# GitHub ---------------------------------------------------------------------\n- BiocParallel     [* -&gt; Bioconductor/BiocParallel@devel]\n- BiocVersion      [* -&gt; Bioconductor/BiocVersion@devel]\n- modeldatatoo     [* -&gt; tidymodels/modeldatatoo@HEAD]\n- parsnip          [* -&gt; tidymodels/parsnip@HEAD]\n- usethis          [* -&gt; r-lib/usethis@HEAD]\n\n# RSPM -----------------------------------------------------------------------\n- bslib            [* -&gt; 0.5.1]\n- fansi            [* -&gt; 1.0.5]\n- fontawesome      [* -&gt; 0.5.2]\n- ggplot2          [* -&gt; 3.4.4]\n- htmltools        [* -&gt; 0.5.6.1]\n- withr            [* -&gt; 2.5.1]\n\nDo you want to proceed? [Y/n]: y\n\n# Downloading packages -------------------------------------------------------\n- Downloading BiocManager from CRAN ...         OK [569 Kb in 0.19s]\n- Downloading nlme from CRAN ...                OK [828.7 Kb in 0.19s]\n- Downloading BH from CRAN ...                  OK [12.7 Mb in 0.4s]\n- Downloading BiocVersion from GitHub ...       OK [826 bytes in 0.37s]\n\n&lt;snip&gt;\n\nDepending on whether you have to install packages from source, you may need to install some system dependencies and try again (I had to install libgit2 the last time I did this). It is also recommended that you use the install of gfortran suggested by CRAN (esp avoiding the homebrew version on macOS). See the gfortran-for-macOS repository.\nOnce you have everything installed, we recommend installing the underlying torch computational libraries. You can do this by loading the torch package A download will automatically begin if you need one.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#contributor-list",
    "href": "chapters/contributing.html#contributor-list",
    "title": "Contributing",
    "section": "Contributor List",
    "text": "Contributor List\nWe would like to thank those who have made a contribution to the project: @amy-palmer, @bmreiniger, @coatless, @krz, @syclik, and @tomsing1.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Non-Tabular Data\nMachine learning (ML) models are mathematical equations that take inputs, called predictors, and try to estimate some future output value. The output, often called an outcome or target, can be numbers, categories, or other types of values.\nFor example, in the next chapter, we try to predict how long it takes to deliver food ordered from a restaurant. The outcome is the time from the initial order (in minutes). There are multiple predictors, including: the distance from the restaurant to the delivery location, the date/time of the order, and which items were included in the order. These data are tabular; they can be arranged in a table-like way (such as a spreadsheet or database table) where variables are arranged in columns and individual data points (i.e., instances of food orders) in rows, as shown in Table 1.11.\nNote that the predictor values are almost always known. For future data, the outcome is not; it is a machine learning model’s job to predict unknown outcome values.\nHow does it do that? A specific machine learning model has a defined mathematical prediction equation defining exactly how the predictors relate to the outcome. We’ll see two very different prediction equations shortly.\nThe prediction equation includes some unknown parameters. These are placeholders for values that help us best predict the outcomes. The process of model training (also called “fitting” or “estimation”) takes our existing predictor and outcome data and uses them to find the “optimal” values of these unknown parameters. Once we estimate these unknowns, we can use them and specific predictor values to estimate future outcome values.\nHere is a simple example of a prediction equation with a single predictor (the distance) and two unknown parameters that have been estimated:\n\\[\ndelivery\\:time = \\exp\\left[2.944 + 0.096 \\: distance\\right]\n\\tag{1.1}\\]\nWe could use this equation for new orders:\nand so on.\nThis is an incredibly reductive approach; it assumes that the relationship between the distance and time are log-linearly related and is not influenced by any other characteristics of the process. On the bright side, the exponential function ensures that there are no negative time predictions, and it is straightforward to explain.\nHow did we arrive at values of 2.944 and 0.096? They were estimated from data once we made a few assumptions about our model. We started off by proposing a format for the prediction equation: an additive, log-linear function2 that includes unknown parameters \\(\\beta_0\\) and \\(\\beta_1\\):\n\\[\n{ \\color{darkseagreen} \\underbrace{\\color{black} log(time)}_{\\text{the outcome}} } =\n{ \\color{darkseagreen} \\overbrace{\\color{black} \\beta_0+\\beta_1\\, distance}^{\\text{an additive function of the predictors}} } +  \n{ \\color{darkseagreen} \\underbrace{\\color{black} \\epsilon}_{\\text{the errors}} }\n\\]\nTo estimate the parameters, we chose some type of criterion to quantify what it means to be “good” values (called an objective function or performance metric). Here we will pick values of the \\(\\beta\\) parameters that minimize the squared error, with the error (\\(\\epsilon\\)) defined as the difference in the observed and predicted times (on the log scale).\nBased on this criterion, we can define intermediate equations that, if we solve them, will result in the best possible parameter estimates given the data and our choices for the prediction equation and objective function.\nSadly, Equation 1.1, while better than a random guess, is not very effective. It does the best it can with a the single predictor. It just isn’t very accurate.\nTo illustrate the diversity of different ML models, Equation 1.2 shows a completely different kind of prediction equation. This was estimated from a model called a regression tree. The function \\(I(\\cdot)\\) is one if the logical statement is true and zero otherwise. Two predictors (distance and day of the week) were used in this case.\n\\[\n\\begin{align}\ntime = \\:&23.0\\, I\\left(distance &lt;  4.465   \\text{ miles and } day \\in \\{Mon, Tue, Wed, Sun\\}\\right)  + \\notag \\\\\n       \\:&27.0\\, I\\left(distance &lt;  4.465   \\text{ miles and } day \\in \\{Thu, Fri, Sat\\}\\right)  + \\notag \\\\\n       \\:&29.8\\, I\\left(distance \\ge  4.465 \\text{ miles and } day \\in \\{Mon, Tue, Wed, Sun\\}\\right)  + \\notag \\\\\n       \\:&36.5\\, I\\left(distance \\ge  4.465 \\text{ miles and } day \\in \\{Thu, Fri, Sat\\}\\right)\\notag\n\\end{align}\n\\tag{1.2}\\]\nA regression tree determines the best predictors(s) to “split” the data, making smaller and smaller subsets of the data. The goal is to make the subsets within each split have about the same outcome value. The coefficients for each logical group are the average of the delivery times for each subset defined by the statement with the \\(I(\\cdot)\\) terms. Note that these logical statements, called rules, are mutually exclusive; only one of the four subsets will be true for any data point being predicted.\nLike the linear regression model that produced Equation 1.1, the performance of Equation 1.2 is quite poor. There are only four possible predicted delivery times. This particular model is not known for its predictive ability, but it can be highly predictive when combined with other regression trees into an ensemble model.\nThis book is focused on the practice of applying various ML models to data to make accurate predictions. Before proceeding, we’ll discuss different types of the data used to estimate models and then explore some aspects of ML models, particularly neural networks.\nThe columns shown in Table 1.1 are defined within a small scope. For example, the delivery hour column encapsulates everything about that order characteristic:\nNon-tabular data does not have the same properties. Consider images, such as the one shown Figure 1.1 that depicts four cells from a biological experiment (Yu et al. 2007). These cells that have been stained with pigments that fluoresce at different wavelengths to “paint” different parts of the cells. In this image, blue reflects the part of the cell called the cytoskeleton, which encompasses the entire cell body. The red color corresponds to a stain that only attaches itself to the contents of the cell nucleus, specifically DNA. This image3 is 371 pixels by 341 pixels with quantification of two colors.\nFigure 1.1: An example image of several cells using two colors. Image from Yu et al. (2007).\nFrom a data perspective, this image is represented as a three-dimensional array (371 rows, 341 columns, and 2 colors). Probably the most important attribute of this data is the spatial relationships between pixels. It is critical to know where each value in the 3D array resides, but knowing what the nearby values are is at least as important.\nWe could “flatten” the array into 371 \\(\\times\\) 341 \\(\\times\\) 2 = 253,022 columns to use in a table4. However, this would break the spatial link between the data points; each column is no longer as self-contained as the columns from the delivery data. The result is that we have to consider the 3D array as the data instead of each row/column/color combination.\nAdditionally, images often have different dimensions which results in different array sizes. From a tabular data perspective, a flattened version of such data would have a different number of columns for different-sized images.\nVideos are an extension of image data if we were to consider it a four-dimensional array (with time as the additional dimension). The temporal aspects of such data are critical to understanding and predicting values.\nText data are often thought of as non-tabular data. As an example, consider text from Sanderson (2017):\nWith text data, it is common to define the “token”: the unit of text that should be analyzed. This could be a paragraph, sentence, word, etc. Suppose that we used sentences as tokens. In this case, the table for this quote would have four rows. Where do we go from here? The sequence of words (or characters) is likely to be important, and this makes a case for keeping the sentences as strings. However, as will be seen shortly, we might be able to convert these four tokens to numeric columns in a way that preserves the information required for prediction.\nPerhaps a better text example relates to the structure of chemicals. A chemical is a three-dimensional molecule but is often described using a SMILES string (Engel and Gasteiger 2018, chap. 3). This is a textual description of molecule that lists its elements and how they relate to one another. For example, the SMILES string CC(C)CC1=CC=C(C=C1)C(C)C(=O)O defines the anti-inflammatory drug Ibuprofen. The letters describe the elements (C is carbon, O is oxygen, etc) and the other characters defines the bonds between the elements. The character sequence within this string is critical to understanding the data.\nOur motives for categorizing data as tabular and non-tabular are related to how we choose an appropriate model. Our opinion is that most modeling projects involve tabular data (or data that can be effectively represented as tabular). Models for non-tabular data are very specialized and, while these ML approaches are the most discussed in the social media, they tend not to be the best approach for tabular data.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular",
    "href": "chapters/introduction.html#sec-nontabular",
    "title": "1  Introduction",
    "section": "",
    "text": "You don’t need any other columns to define it.\nA single number is all that is required to describe that information.\n\n\n\n\n\n\n\n\n\n“The most important step a man can take. It’s not the first one, is it? It’s the next one. Always the next step, Dalinar.”",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular-convert",
    "href": "chapters/introduction.html#sec-nontabular-convert",
    "title": "1  Introduction",
    "section": "1.2 Converting Non-Tabular Data to Tabular",
    "text": "1.2 Converting Non-Tabular Data to Tabular\nIn some situations, non-tabular data can be effectively converted to a tabular format, depending on the specific problem and data.\nFor some modeling projects using images, we might not be interested in every pixel. For Figure 1.1, we really care about the cells in the image. It is important to understand within-cell characteristics (e.g. shape or size) and some between-cell information (e.g., the distance between cells). The vast majority of the pixels don’t need to be analyzed.\nWe can pre-analyze the 3D arrays to determine which pixels in an image correspond to specific cells (or nuclei within a cell). Segmentation (Holmes and Huber 2018) is the process of estimating regions of an image that define some object(s). Figure 1.2 shows the same cells with lines generated from a simple segmentation.\n\n\n\n\n\n\n\n\nFigure 1.2: The cell segmentation results for the four neuroblastomas shown in Figure 1.1.\n\n\n\n\n\nOnce our cells have been segmented, we can compute various statistics of interest based on size, shape, or color. If a cell is our unit of data, we can create a tabular format where rows are cells and columns are cell characteristics (Table 1.2). The result is a data table that has more rows than the original non-tabular structure since there are multiple cells in an image. There are far fewer columns but these columns are designed to be more informative than the raw pixel data since the researchers define the cell characteristics that they desire.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\n\nEccentricity\n\n\n\nArea\n\n\n\nIntensity\n\n\n\nNucleus\nCell\nNucleus\nCell\nNucleus\nCell\n\n\n\n\n17\n0.494\n0.836\n\n3,352\n11,699\n\n0.274\n0.155\n\n\n18\n0.708\n0.550\n\n1,777\n4,980\n\n0.278\n0.210\n\n\n21\n0.495\n0.802\n\n1,274\n3,081\n\n0.326\n0.218\n\n\n22\n0.809\n0.975\n\n1,169\n3,933\n\n0.583\n0.229\n\n\n\n\n\n\n\n\nTable 1.2: Cells, such as those found in Figure 1.1, translated into a tabular format using three features for the nuclear and non-nuclear regions of the segmented cells.\n\n\n\n\nThis conversion to tabular data isn’t appropriate for all image analysis problems. If you want to know if an image contains a cat or not it probably isn’t a good idea.\nLet’s also consider the text examples. For the book quote, Table 1.3 shows a few simple predictors that are tabular in nature:\n\nPresence/absence columns for each word in the entire document. Table 1.3 shows columns corresponding to the words “always”, “can”, and “take”. These are represented as counts.\nSentence summaries, such as the number of commas, words, characters, first-person speech, etc.\n\nThere are many more ways to represent these data. More complex numeric embeddings that are built on separate large databases can reflect different parts of speech, text sequences (called n-grams), and others. Hvitfeldt and Silge (2021) and Boykis (2023) describe these and other tools for analyzing text data (in either format).\n\n\n\n\n\n\n\n\n\n\n\nSentence\n\"always\"\n\"can\"\n...\n\"take\"\nCharacters\nCommas\n\n\n\n\nThe most important step a man can take.\n0\n1\n...\n1\n32\n0\n\n\nIt's not the first one, is it?\n0\n0\n...\n0\n24\n1\n\n\nIt's the next one.\n0\n0\n...\n0\n15\n0\n\n\nAlways the next step, Dalinar.\n1\n0\n...\n0\n26\n1\n\n\n\n\n\n\n\n\nTable 1.3: An example of converting non-numeric data to a numeric, tabular format.\n\n\n\n\nFor the chemical structure of Ibuprofen, there is a rich field of molecular descriptors (Leach and Gillet 2003, chap. 3; Engel and Gasteiger 2018) that can be used to produce thousands of informative predictor columns. For example, we might be interested in the size of the molecule (perhaps measured by surface area), its electrical charge, or whether it contains specific sub-structures.\nThe process of determining the best data representations is called feature engineering. This is a critical task in machine learning and is often overlooked. Part 2 of this book spends some time looking at feature engineering methods (once the data are in a tabular format).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-types",
    "href": "chapters/introduction.html#sec-model-types",
    "title": "1  Introduction",
    "section": "1.3 Models and Machine Learning",
    "text": "1.3 Models and Machine Learning\nSince we have mentioned linear regression and regression trees, it makes sense to talk about possible machine learning scenarios. There are many, but we will focus on two types.\nRegression models have a numeric outcome (e.g., delivery time) and our goal is to predict that value. For some data sets we are mostly interested in ranking new data (as opposed to accurately predicting the value).\nThe other scenario that we discuss is classification models. In this case, the outcome is a qualitative categorical value (e.g., “cat” or “no cat” in an image). One interesting aspect of these models is that there are two main types of predictions:\n\nhard predictions correspond to the original outcome value.\nsoft predictions return the probability of each possible outcome (e.g., there is a 27% chance that a cat is in the image).\n\nThe number of categories is important; some models are specific to two classes, while others can accommodate any number of outcome categories. Most classification outcomes have mutually exclusive classes (one of all possible class values). There are also ordered class values where, while categorical, there is some type of ordering. For example, tumors are often classified in stages, with stage I being a localized malignancy and stage IV corresponding to one that has spread throughout the body.\nMulti-label outcomes can have multiple classes per row. For example, if we were trying to predict which languages that someone can speak, multilingual people would have multiple outcome values.\nWe are focused on prediction problems. This implies that our data will contain an outcome. Such scenarios falls under the broader class of supervised models. An unsupervised model is one where the is no true outcome value. Tools such as clustering, principal component analysis (PCA), and others look to quantify patterns in the data without some prediction goal. We’ll encounter unsupervised methods in our pre-model activities such as feature engineering described in Part 2.\nSeveral synonyms for machine learning include statistical learning, predictive modeling, pattern recognition, and others. While many specific models are designed for machine learning, we suggest that users think about machine learning as the problem and not the solution. Any model that can adequately predict the outcome values deserves the title. A commonly used argument for this position is that two models that are conventionally thought of as tools to make inferences (e.g., p-values) are linear regression and logistic regression. Equation 1.1 is a simplistic linear regression model. As will be seen in future chapters, linear and logistic regression models can produce highly accurate predictions for complex problems.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-data-types",
    "href": "chapters/introduction.html#sec-data-types",
    "title": "1  Introduction",
    "section": "1.4 Data Characteristics",
    "text": "1.4 Data Characteristics\nIt’s helpful to discuss different nomenclature and types of data. With tabular data, there are a number of ways to refer to a row. The terms sample, instance, observation, and/or data point are commonly used. The first, “sample”, can be used in a few different contexts; we will also use it to describe a subset of a greater data collection (as in “we took a random sample of the data”.). The notation for the number of rows in a data set is \\(n\\) with subscripts for specificity (e.g., \\(n_{tr}\\)).\nFor predictors, there are also many synonyms. In statistical nomenclature, independent variable or covariate are used. More generic terms are attribute and descriptor. The term feature is generally equated to predictors, and we’ll use both. An additional adjective may also be assigned to “feature”. Consider the date and time of the orders shown in Table 1.1. The “original feature” is the data/time column in the data. The table includes two “derived features” in the form of the decimal hour and the day of the week. This distinction will be important when discussing feature engineering, importance scores, and explaining models. The number of overall predictors in a data set is symbolized with \\(p\\).\nOutcomes/target columns can also be called the dependent variable or response.\nFor numeric columns, we will make some distinctions. A real number is numeric with potentially fractional values (e.g., time to delivery). Integers are whole numbers, often reflecting counts. Binary data take two possible values that are almost always represented with zero and one.\nDense data describes a collection of numeric values with many possible values, also reflected by the delivery time and distances in Table 1.1. Sparse data have fewer possible values or when only a few values are contained in the observed data. The product counts in the delivery data are good examples. They are integer counts consisting mostly of zeros. The frequency of non-zero values decreases with the magnitiude of the counts.\nWhile numeric data are quantitative, qualitative data cannot be represented by a numeric scale5. The day of the week data are categories with seven possible values. Binary categorical data have two categories, such as alive/dead. The symbol \\(C\\) is used to describe the number of categories in a column.\nThe item columns in Table 1.1 are interesting too. If the order could only contain one item, we might configure that data with a qualitative single column. However, these data are multiple-choice and, as such, are shown in multiple integer columns with zero, reflecting that it was not in the order.\nvalue types, skewness, colinearity, distributions, ordinal",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-pipeline",
    "href": "chapters/introduction.html#sec-model-pipeline",
    "title": "1  Introduction",
    "section": "1.5 What Defines the Model?",
    "text": "1.5 What Defines the Model?\nWhen discussing the modeling process, it is traditional to think the parameter estimation steps occur only when the model is fit. However, as more complex techniques are used to prepare predictor values for the model, it is crucial to understand that important quantities are also being estimated before the model. Here are a few examples:\n\nImputation methods, discussed in Chapter 4, create sub-models that estimate missing predictor values. These are applied to the data before modeling to complete the data set.\nFeature selection tools (Kuhn and Johnson 2019) often compute measures of predictor importance to remove uninformative predictors before passing the data to the model.\nEffect encoding tools, discussed in Section 6.4.3, use the effect of the predictor on the outcome as a predictor.\nPost-model adjustments to predictions, such as model calibration (Section 2.6 and 15.8), are examples of postprocessing operations6.\n\n\nThese operations can profoundly affect the overall results, yet they are not usually considered part of “the model.” We’ll use the more expansive term “modeling pipeline” to describe any estimation for the model or operations before or after the model:\n\nmodel pipeline = preprocessing + supervised model + postprocessing\n\n\nThis is important to understand: a common pitfall in ML is only validating the model fitting step instead of the whole process. This can lead to performance statistics that can be extraordinarily optimistic and might deceive a practitioner into believing that their model is much better than it truly is.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-deep-learning",
    "href": "chapters/introduction.html#sec-deep-learning",
    "title": "1  Introduction",
    "section": "1.6 Deep Learning",
    "text": "1.6 Deep Learning\nWhile many models can be used for machine learning, neural networks are the elephant in the room. These complex, nonlinear models are frequently used in machine learning to the point where, in some domains, these models are synonymous with ML. One class of neural network models is deep learning (DL) models (Goodfellow, Bengio, and Courville 2016; Prince 2023). Here, “deep” means that the models have many structural layers, resulting in extraordinary amounts of complexity.\nFor example, Figure 1.3 shows a diagram of a type of deep learning model called a convolutional neural network that is often used for making predictions on images. It consists of several layers that manipulate the data. This particular model, dubbed VGG16, was proposed by Simonyan and Zisserman (2014).\nThe color image on the left is a data point. The first block has three layers. The two larger yellow boxes are convolutional layers. They move a small multidimensional array over the larger data array to compute localized features. For example, for an image that is 150 x 150 pixels, a convolution that uses a 3x3 filter will move this small rectangle across the array. At the (2,2) position, it takes the average of all adjacent pixel locations.\nAfter the convolutional operations, a smaller pooling layer is shown in orange. Pooling, usually by finding the maximum feature value across different image regions, compresses/downsamples the larger feature set so a smaller size without diminishing features with large signals.\nThe next three blocks consist of similar convolutional and pooling layers but of different dimensions. The idea behind these blocks is to reduce the image data to a smaller dimension width/height dimension in a way that extracts potentially important hierarchical features from the image.\nAfter block 5, the data are flattened into a single array, similar to the columns in a tabular data set. Up to this point, the network has been focused on preprocessing the image data. If our initial image were 150 pixels by 150 pixels, the flattening process would result in over 8 thousand predictors that would be used in a supervised ML model. The network would have over 14 million model parameters to estimate from the data to get to this point.\nAfter flattening, the final two layers correspond to the classic supervised neural network model (Bishop 1995). The original paper used four large supervised neural network layers; for simplicity, Figure 1.3 shows a single dense layer. This can add dozens of millions of additional parameters for a modest-sized dense layer.\n\n\n\n\n\n\n\n\nFigure 1.3: A simplified version of the convolutional neural network deep learning model proposed by Simonyan and Zisserman (2014). This version only includes a single dense layer (instead of four).\n\n\n\n\n\nThere are many types of deep learning models. Another example is recurrent neural networks, which create layers that are proficient at modeling data that occur in sequences such as time series data or sentences (i.e., a sequence of words).\nVery sophisticated deep learning models have generated, by far, the best predictive ability for images and similar data. It would seem natural to think these models would be just as effective for tabular data. However, this has not been the case. Machine learning competitions, which are very different from day-to-day machine learning work, have shown that many other models can consistently do as well or likely better on tabular data sets. This has generated an explosion of literature7 and social media posts that try to explain why this is the case. For example, see Borisov et al. (2022) and McElfresh et al. (2023).\nWe believe that exceptionally complex DL models are handicapped in several ways when taken out of the environments where they have been successful.\nFirst, the complexity of these models requires extreme amounts of data, and the constraints of such large data sets drive how the models are developed.\nFor example, keeping very large data sets in a computer’s memory is almost impossible. As discussed earlier, we use the data to find optimal values of our parameters for some objective functions (such as classification accuracy). Using traditional optimization procedures, this is very difficult if the data cannot be accessed simultaneously. Deep learning models use more modern optimization methods, such as stochastic gradient descent (SGD) methods, for estimating parameters. It does not simultaneously hold all of the data in memory and fits the model on small incremental batches of data. This is a very effective technique, but it is not an approach one would use with data set sizes that are more common (e.g., less than a million data points). SGD is less effective for smaller data sizes than traditional in-memory optimizers. Another consequence of huge data requirements is that model training can take an excruciatingly long time. As a result, efficient model development can require specialized hardware. It can also drive how models are optimized and compared. DL models are often optimized by sequentially adjusting the model. Other ML models can be optimized with a more methodical simultaneous design that is often more efficient, systematic, and effective than sequential optimizers used with deep learning.\nAnother issue with deep learning models is that their size and mathematical structure are associated with a very difficult optimization problem; their objective function is non-convex. We’d like to have an optimization problem that has some guarantees that a global optimal value exists (and can be found). That is often not the case for these models. For example, if we want to estimate the VGG16 model parameters with the goal of maximizing the classification accuracy for finding dogs in an image, it can be difficult to get a reliable estimate and, if we do, we don’t know if it is really the correct answer.\nBecause the optimization problem is so difficult, much of the user’s time is spent optimizing the model parameters or tuning the network structure to find the best results. When combined with how long it takes for a single model fit, deep learning models are very high maintenance compared to other models when used on tabular data. They can probably produce a model that is marginally better than the others, but doing so requires an inordinate amount of time, effort, and constraints. DL models are superior for certain hard problems with large data sets. For other situations, other ML models can go probably go further and faster8.\nFinally, there are some data-centric reasons that the successfulness of deep learning models doesn’t automatically translate to tabular data. There are characteristics of tabular data that don’t often occur in stereotypical DL applications. For example:\n\nIn some cases, a group of predictors might be highly correlated with one another. This can compromise some of the mathematical operations used to estimate parameters in neural networks (and many other models).\nUnless we have extensive prior experience with our data, we don’t know which are informative and which are irrelevant for predicting an outcome. As the number of non-informative predictors increases, the performance of neural networks decreases. See Section 19.1 of Kuhn and Johnson (2013).\nPredictors with “irregular” distributions, such as skewness or heavy tails, can harm performance (McElfresh et al. 2023).\nMissing predictor data are not naturally handled in basic neural networks.\n“Small” sample sizes or data dimensions with at least as many predictors as rows.\n\nas well as others. These issues can be overcome by adding layers to a network designed to counter specific challenges. However, in the end, this ends up adding more complexity. We propose that there are a multitude of other models, and many of them are resistant or robust to these types of data-specific characteristics. It is better to have different tools in our toolbox; making a more complex hammer may not help us put a screw in a wall.\nThe deep learning literature has resulted in many collateral benefits for the broader machine learning field. We’ll describe a few techniques that have improved machine learning in subsequent chapters,",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-aspects",
    "href": "chapters/introduction.html#sec-aspects",
    "title": "1  Introduction",
    "section": "1.7 Modeling Philosophies",
    "text": "1.7 Modeling Philosophies\ntalk about subject-matter knowledge/intuition vs “let the machine figure it out”. biased versus unbiased model development.\nassistive vs automated usage",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-outline",
    "href": "chapters/introduction.html#sec-outline",
    "title": "1  Introduction",
    "section": "1.8 Outline of These Materials",
    "text": "1.8 Outline of These Materials\nThis work is organized into parts:\n\nIntroduction: The next chapter shows an abbreviated example to illustrate important concepts used later.\nPreparation: These chapters discuss topics such as data splitting and feature engineering. These activities occur before the actual training of the machine learning model but are critically important.\nOptimization: To find the model that has the best performance, we often need to tune them so that they are effective but do not overfit. These chapters describe overfitting, methods to measure performance, and two different classes of optimization methods.\nClassification: Various models for classification are described in this part.\nRegression: These chapter describe models for numeric outcomes.\nCharacterization: Once you have a model, how do you describe it or its predictions? How do you know when you should question the results of your model? We’ll address these questions in this part.\nFinalization: Post-modeling activities such as monitoring performance are discussed.\n\nBefore diving into the process of creating a good model, let’s have a short case study. The next chapter is designed to give you a sense of the overall process and illustrate important ideas and concepts that will appear later in the materials.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#chapter-references",
    "href": "chapters/introduction.html#chapter-references",
    "title": "1  Introduction",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nBishop, C. 1995. Neural Networks for Pattern Recognition. Oxford: Oxford University Press.\n\n\nBorisov, V, T Leemann, K Seßler, J Haug, M Pawelczyk, and G Kasneci. 2022. “Deep Neural Networks and Tabular Data: A Survey.” IEEE Transactions on Neural Networks and Learning Systems.\n\n\nBoykis, V. 2023. “What are embeddings?” https://doi.org/10.5281/zenodo.8015028.\n\n\nEngel, T, and J Gasteiger. 2018. Chemoinformatics : A Textbook . Weinheim: Wiley-VCH.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nHolmes, S, and W Huber. 2018. Modern Statistics for Modern Biology. Cambridge University Press.\n\n\nHvitfeldt, E, and J Silge. 2021. Supervised Machine Learning for Text Analysis in R. CRC Press.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nLeach, A, and V Gillet. 2003. An Introduction to Chemoinformatics. Springer.\n\n\nMcElfresh, D, S Khandagale, J Valverde, V Prasad, G Ramakrishnan, M Goldblum, and C White. 2023. “When Do Neural Nets Outperform Boosted Trees on Tabular Data?” arXiv.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.\n\n\nSanderson, B. 2017. Oathbringer. Tor Books.\n\n\nSimonyan, K, and A Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv.\n\n\nYu, W, HK Lee, S Hariharan, WY Bu, and S Ahmed. 2007. “CCDB:6843, Mus Musculus, Neuroblastoma.”",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Non-tabular data are later in this chapter.↩︎\nThis is a linear regression model.↩︎\nThis is a smaller version of a larger image that is 1392 pixels by 1040 pixels.↩︎\nThe original image would produce 2,895,360 flattened features.↩︎\nAlso known as “discrete” or “nominal” data.↩︎\nA note to readers: these ?sec-* references are to sections or chapters that are planned by not (entirely) written yet. They will resolve to actual references as we add more content.↩︎\nTo be honest, the literature in this area has been fairly poor (as of this writing).↩︎\nThat said, simple neural network models can be very effective. While still a bit more high maintenance than other models, it’s a good idea to give them a try (as we will see in the following chapter).↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html",
    "href": "chapters/whole-game.html",
    "title": "2  The Whole Game",
    "section": "",
    "text": "2.1 Predicting Delivery Time\nMany resources for teaching machine learning gloss over some crucial details that might appear to be tangential to the model. As a result, they omit parts that are critical to the overall project.\nThe notion of “the whole game” is to illustrate machine learning in a way that reflects the complexity of the entire process. Perkins (2010) uses this phrase in the context of learning baseball. Instead of focusing players on one or two key aspects of the game, it is better to be exposed (at some level) to everything. In his words, a smaller focus “was kind of like batting practice without knowing the whole game.”\nThis chapter is a self-contained case study.\nOur goal is to provide a somewhat abbreviated, high-level overview to help readers understand the overall strategy before we get into the specific set of tactics that we might use. This chapter previews what is to come and sets the stage to discuss some important themes that may not be obvious at first glance.\nWe’ll avoid many details in this chapter by copiously referencing upcoming chapters. Also, in this an upcoming chapters, there are icons (e.g., ) that link to seperate pages that illustrate how to perforrm the calculations.\nThe illustrative example focuses on predicting the food delivery time (i.e., the time from the initial order to receiving the food). There is a collection of data containing 10,012 orders from a specific restaurant. The predictors, previously shown in Table 1.1, include:\nThe outcome is the time1 (in minutes).\nThe next section describes the process of splitting the overall data pool into different subsets and signifies the start of the modeling process. However, how we make that split depends on some aspects of the data. Specifically, it is a good idea to examine the distribution of our outcome data (i.e., delivery time) to determine how to randomly split the data2.\nFigure 2.1 (panel (a)) uses a histogram to examine the distribution of the outcome data. It is right-skewed and there is a subset of long delivery times that stand out from the mainstream of the data. The most likely delivery time is about 27 minutes, but there is also a hint of a second mode a few minutes earlier.\nFigure 2.1: Histrograms of the training set outcome data with and without a transformation.\nThis skewness might affect how we split the data and is discussed in the next section. Another important question is: should we model a transformed version of the delivery times? If we have large outlying values, the model might be inappropriately pulled to these values, often at the expense of the overall quality of fit. One approach is to model the logarithm of the times (as shown in Equation 1.1). Figure 2.1 (panel (b)) shows the distribution of the log (base 2) deliveries. It is more symmetric, and the potential second mode is more pronounced. This isn’t a bad idea, and the primary consequence is how we interpret some of the metrics that are used to quantify how well the model fits the data3. In the end, we decided to model the data in the original units but did try the analysis both ways (and did not find huge differences in the results).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-delivery-times",
    "href": "chapters/whole-game.html#sec-delivery-times",
    "title": "2  The Whole Game",
    "section": "",
    "text": "The time, in decimal hours, of the order.\nThe day of the week for the order.\nThe approximate distance between the restaurant and the delivery location.\nA set of 27 predictors that count the number of distinct menu items in the order.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-data-spending-whole-game",
    "href": "chapters/whole-game.html#sec-data-spending-whole-game",
    "title": "2  The Whole Game",
    "section": "2.2 Data Spending",
    "text": "2.2 Data Spending\nAn important aspect of machine learning is to use the right data at the right time. We don’t start modeling with the entire pool of data. As will be seen in Chapter 10, we need to reserve some data to evaluate the model and make sure that those data do not overlap with those used to fit the model.\nOften, our data spending strategy is to split the data into at least two subsets:\n\nThe training set is used to develop the model pipeline. It is used for investigating, fitting, and comparing them.\nThe test set is a smaller fraction of the data that is only used at the very end to independently validate how well we did.\n\nThe training set is used for a lot of our activities. We do need to evaluate multiple model pipelines along the way. We reserve the test set for the final assessment and should not use it for development. There are two main strategies for characterizing model efficacy during development.\nThe first is to resample the training set. This is an iterative computational tool to find good statistical estimates of model performance using the training set.\nThe other option is to initially split the data into three subsets: the training set, the testing set, and the validation set. Validation sets are small subset used for evaluating the model repeatedly during development.\nGenerally, we suggest using resampling unless the initial data pool is “sufficiently large.” For these data, 10,012 food orders is probably sufficient to choose a validation set over the resampling approach.\nTo split the data, we’ll allocate 60% for the training set and then use 20% for the validation set, and the remaining 20% for testing. The splitting will be conducted by stratification. To ensure that our delivery time distributions are approximately the same, it is temporarily “binned” into four quartiles. The three-way split is executed within each quartiles and the overall training/validation/testing sets are assembled by aggregating the corresponding data from the quartiles. As a result, the respective sample sizes are 6004/2004/2004 deliveries.\nAs previously stated, the majority of our time is spent with the training set samples. The first step in any model-building process is to understand the data, which can most easily be done through visualizations of the training set.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-eda-whole-game",
    "href": "chapters/whole-game.html#sec-eda-whole-game",
    "title": "2  The Whole Game",
    "section": "2.3 Exploratory Data Analysis",
    "text": "2.3 Exploratory Data Analysis\nOur goal now is to determine if there are aspects of predictors that would affect how we provide them to the model. We should look at individual variables as well as combinations multiple variables.\nTo start, for the dense numeric predictors, how would we characterize their relationship to the outcome? Is it linear, nonlinear, or random? Let’s start with the distance predictor. Figure 2.2(a) shows the values with the “scatter plot smoother” fit. The line is a flexible nonlinear approach that can adapt to any apparent pattern in the plot4. First, the plot shows that the distance distribution is also right-skewed (although a separate histogram would better illustrate this). The smoother shows a slightly nonlinear trend, although the nonlinearity is mainly in the right tail of the distances and might be an artifact of the small sample size in that region. Regardless, it appears to be an informative predictor.\n\n\n\n\n\n\n\n\nFigure 2.2: Visualizations of relationship between the outcome and several predictors using the training set.\n\n\n\n\n\nPanel (b) has a similar visualization for the order time. In this case, there is another nonlinear trend where the order increases with the hour, then peaks around 17:30, then begins to decrease. There is also an increase in delivery time variation as the order time approaches the peak.\nThe day of the week is visualized via box plots in panel (c). There is an increasing trend in both mean and variation as we move from Monday to Friday/Saturday, then a decrease on Sunday. Again, this appears to be an important predictor.\nThese three panels show relationships between individual predictors and delivery times. It is possible that multiple predictors can act in concert; their effect on the outcome occurs jointly. Panel (d) of Figure 2.2 shows that there is an interaction effect between the day and hour of the order. Mondays and Tuesdays have shorter delivery times that don’t change much over hours. Conversely, on Fridays and Saturdays, the delivery times are generally longer, with much higher peaks. This might explain the increase in delivery time variation that was seen in Panel (b). Now that we know of the existence of this interaction, we might add additional model terms to help the model understand and exploit this pattern in the training set.\nFor the 27 itemized frequency counts, we might want to know if there was a change in delivery time when a specific item was in the order (relative to those where it is not purchased). To do this, mean delivery times were computed for orders with and without the item. The ratio of these times was computed and then converted to the percent increase in time. This is used to quantify the effect of item on delivery time. We’d also like to know what the variation is on this statistic and a technique called the bootstrap (Davison and Hinkley 1997) was used to create 90% confidence intervals for the ratio. Figure 2.3 shows the results.\n\n\n\n\n\n\n\n\nFigure 2.3: Ranking the items by the increase in delivery time when ordered at least once.\n\n\n\n\n\nThere is a subset of items that increase the delivery time (relative to the experimental noise), many that seem irrelevant, and one that decreases the time.\nMore investigations into the data would be pursued to better understand the data and help us with feature engineering. For brevity, we’ll stop here and introduce three ML models.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-development-whole-game",
    "href": "chapters/whole-game.html#sec-model-development-whole-game",
    "title": "2  The Whole Game",
    "section": "2.4 Model Development",
    "text": "2.4 Model Development\nWhich model to use? There is a multitude of tools that can be used to represent and predict these data. We’ll illustrate three in this section and the essential concepts to consider along the way.\nBefore proceeding, we need to pick a performance metric. This is used to quantify how well the model fits the data. For any model fit, the residual (usually denoted as \\(\\epsilon\\), also called the error) is the difference between the model prediction and the observed outcome. We’d like our model to produce residuals near zero. One metric5 is the mean absolute error (MAE). For a data set with \\(n\\) data points, the equation is\n\\[\nMAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| = \\frac{1}{n}\\sum_{i=1}^n |\\epsilon_i|\n\\tag{2.1}\\]\nwhere \\(\\hat{y}\\) is the predicted numeric outcome value and \\(\\epsilon\\) is the model error. The MAE units are the same as the outcome which, in this case, is minutes. We desire to minimize this statistic.\nTo get started, we’ll use a basic linear model for these data.\n\n\nLinear Regression\nLinear regression is probably the most well-known statistical model in history. It can predict a numeric outcome using one or more predictors using the equation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\ldots + \\beta_px_{ip} + \\epsilon_i\n\\tag{2.2}\\]\nwhere \\(y_i\\) is the outcome for the \\(i^{th}\\) data point (out of \\(n_{tr}\\)), and \\(x_{ij}\\) is value for the \\(j^{th}\\) predictor (out of \\(p\\)).\nFor Equation 2.2, there are many ways to estimate the model parameters (\\(\\beta\\)’s). Ordinary least squares is used most often. It finds the values of the \\(\\beta_j\\) that that minimize sum of squared errors (SSE). ?sec-reg-linear discusses several other ways to estimate the model parameters that use other criteria (variations of the SSE).\nThe main assumptions about the data for ordinary least squares are that they are independent and identically distributed. Given what we know about the problem, both assumptions are at least mildly incorrect. Orders are being processed simultaneously (affecting independence), and our visualizations have indicated that there is an increase in variation with the mean (i.e., not identically distributed). However, we are most interested in prediction here, so we can often overlook slight to moderate violations of these assumptions6\nWhat are our \\(x_{ij}\\) values in Equation 2.2? We can’t give the model equations values of \"Monday\" or \"Friday\"; OLS requires numbers in its equations. A simple workaround is to create binary indicator variables for six of the seven days of the week. For example, the indicator for Friday would have a value of one for orders placed on a Friday and zero otherwise7.\nAlso, for Equation 2.2, what do we do about the numeric predictors that appear to have nonlinear trends? We can augment the predictor by describing it with multiple columns. Equation 2.2 shows that linear regression is linear in the parameters (not the predictors). We can add terms such as \\(x_{ij} = distance_i^2\\) to allow the fit to be nonlinear. Alternatively, an effective tool for enabling nonlinear trends is spline functions8. These are special polynomial terms. For example, we could represent that original predictor with 10 spline terms (i.e., columns in the data) for the order hour. As we add more terms, the model fit has increased capacity to be nonlinear. We’ll use 10 separate splines for both distance and the order hour (in lieu of the original data columns.)\nHow do we encode the interaction seen in Figure 2.2(d)? We have six indicator columns for the order day and ten spline columns for the order hour. We can facilitate having different nonlinear hour trends for each day by making 60 cross-product terms. For example, the interactions for Fridays would multiply each of the ten spline terms by the binary indicator associated with Fridays.\nThese feature engineering operations result in a model with \\(p = 114\\) \\(\\beta\\) parameters. We fit this model to the training set to estimate the model parameters. To evaluate it, we use the trained model to predict the validation set, and these predictions are the substrate to compute the MAE. For this model, the validation set MAE is 1.609 (i.e., 1 minute and 36 seconds). If we think of MAE as an average error, this doesn’t seem too bad. However, what does this look like? Let’s plot the observed times against predicted values to get a better sense of how well this model did. Figure 2.4 has the results which also feature a scatter plot smoother.\n\n\n\n\n\n\n\n\nFigure 2.4: A scatter plot of the observed and predicted delivery times, via a linear regression model, for the validation set. The red line is a smooth trend estimate.\n\n\n\n\n\nThe best case would be that the points arrange themselves tightly around the diagonal line. The model slightly under-predicts for very rapid deliveries but substantially under-predicts for times greater than 40 minutes. However, overall, the model works well for the majority of the deliveries.\nFrom here, our next step would be to investigate poorly predicted samples and determine if there is some commonality to them. For example, are they short distances that are ordered late on a Friday, etc? If there appears to be a pattern, we would add additional model terms to compensate for the flaw and see if the validation set MAE decreases. We would iterate between exploratory analysis of the residuals, adding or subtracting features, then refitting the model.\nFor illustration, we stop here9 and take a look at very different model.\n\n\n\nRule-Based Ensemble\nCubist is a rule-based ensemble method. It creates a rule set by first generating a regression tree that is composed of a hierarchical set of if/then statements10. Figure 2.5 shows a simple tree for our data. A set of splits on different predictors results in six terminal nodes (at the bottom of the tree). A rule is a distinct path through the tree to a terminal node. For example, Node 11 in the figure has the corresponding rule: hour &gt;= 14.77 & day in {Fri, Sat} & distance &gt;= 4.045. For each rule, Cubist fits a linear regression to the data that the rule covers (\\(n = 261\\) for Node 11). It builds new rule sets sequentially and uses many rules/regression equations to predict new samples.\n\n\n\n\n\n\n\n\nFigure 2.5: An example of a regression tree used to create a set of six rules.\n\n\n\n\n\nThe method isolates different parts of the predictor space and, with these subsets, models the data using linear functions. The creation of multiple rules enables the model to emulate nonlinear functions easily. When a rule consists of multiple predictors, this constitutes an interaction effect. Additionally, the data preprocessing requirements are minimal: it can internally account for any missing predictor values, and there is no need to convert categorical predictors into binary indicator columns, and so on.\nWhen fit to the training set, the Cubist model created an ensemble with 2,007 rules. On average, the rules consisted of 2.7 variables and the the regression models contained 12.5 predictors. Two example rules, and their regression models, were:\n\nif\n   hour &lt;= 14.252\n   day in {Fri, Sat}\nthen\n   outcome = -23.039 + 2.85 hour + 1.25 distance + 0.4 item_24\n             + 0.4 item_08 + 0.6 item_01 + 0.6 item_10 + 0.5 item_21\n             + 0.3 item_09 + 0.4 item_23 + 0.2 item_03 + 0.2 item_06\n\nand\n\nif\n   hour &gt; 15.828\n   hour &lt;= 18.395\n   distance &lt;= 4.35\n   item_10 &gt; 0\n   day = Thu\nthen\n   outcome = 11.29956 + 4.24 distance + 14.3 item_01 + 4.3 item_10\n             + 2.1 item_02 + 0.03 hour\n\nThese two examples are reasonable and understandable. However, Cubist blends many rules together to the points where is becomes a black-box model. For example, the first data point in the validation set is affected by 115 rules (about 6% of the total ensemble).\nDoes this complexity help? The validation set estimated MAE at 1 minute and 24 seconds, which is an improvement over the linear regression model. Figure 2.6 visualizes the observed and predicted plot. The model under-predicts fewer points with long delivery times than the previous model but does contain a few very large residuals.\n\n\n\n\n\n\n\n\nFigure 2.6: A scatter plot of the observed and predicted delivery times from the Cubist model, for the validation set.\n\n\n\n\n\nWould this model have benefited from using the feature set from the linear regression analysis? Not really. Refitting with those predictors had an MAE of 1 minute and 26 seconds. In this particular case, the original Cubist was able to estimate nonlinear trends and interactions without additional feature engineering.\nThere is more that we can do with the Cubist model but, in this initial model screening phase, we should focus our efforts on evaluating a disparate set of models and features to understand what works, what doesn’t work, and the difficulty level of the problem. Let’s try one additional model.\n\n\n\nNeural Network\nA neural network is a highly nonlinear modeling technique. They relate the predictors to the outcome through intermediate substructures, called layers, that contain hidden units. The hidden units allow the predictors to affect the outcomes differently11 allowing the model to isolate essential patterns in the data. As the number of hidden units (or layers of hidden units) increases, so does the complexity of the model. We’ll only consider a single layer network with multiple hidden units for these data.\nNeural networks, like linear regression, require numbers as inputs. We’ll convert the day of the week predictor to indicators. Additionally, the model requires that all of the predictors be in the same units. There are a few ways to accomplish this. First, we will compute the training set mean and standard deviation of each feature. To standardize them we subtract the mean and divide the result by the standard deviation. As a result, all features have a zero mean and a unit standard deviation. A common scaling of the predictor columns is a necessary precondition for fitting neural networks\nOne question about this model is: how many hidden units should we use12? This decision will determine the complexity of the model. As models become model complex, they have more capacity to find complicated relationships between the predictors and response. However, added complexity increases the risk of overfitting. This situation, discussed in Section 9.1, occurs when a model fits exceptionally well to the training data but fails for new data when it finds patterns in the training data that are artifacts (i.e., do not generalize for other data sets). How should we choose the number of hidden units?\nUnfortunately, no equation can directly compute an estimate the number of units from the data. Values such as these are referred to as tuning parameters or hyperparameters. However, one solution is to extend the validation process previously shown for the linear regression and Cubist models. We will consider a candidate set of tuning parameter values ranging from 3 to 100 hidden units. We’ll fit a model to the training set for each of these 98 values and use the validation set MAE values to select an appropriate value. The results of this process is shown in Figure 2.7.\n\n\n\n\n\n\n\n\nFigure 2.7: The relationship between the number of hidden units and the mean absolute deviation from the validation set data.\n\n\n\n\n\nA small number of hidden units performs poorly due to underfitting (i.e., insufficient complexity). Adding more improves the situation and, while the numerically best result corresponds to a value of 33, there is appreciable noise in the MAE values13 . Eventually, adding too much complexity will either result in the MAE values becoming larger due to overfitting or will drastically increase the time to fit each model. The MAE pattern in Figure 2.7 shows a plateau of values with a similar MAE once enought hidden units are added. Given the noise in the system, we’ll select a value of 33 hidden units, by determining the least complex model that is within 1% of the numerically best MAE. The MAE associated with the selected candidate value14 was 1 minute and 34 seconds (but is likely to be closer to 1 minute and 38 seconds, given the noise in Figure 2.7).\nFigure 2.8 shows the familiar plot of the observed and predicted delivery times. There are fewer large residuals than the Cubist model but the underfitting for longer delivery times appears more pronounced.\n\n\n\n\n\n\n\n\nFigure 2.8: A scatter plot of the validation set observed and predicted delivery times from a neural network with 33 hidden units.\n\n\n\n\n\nThese results suggest that the Cubist and neural network models have roughly equivocal performance15. Like Cubist, the finalized model is very complex; it is a highly nonlinear regression with 26,884 model parameters (i.e., slopes and intercepts).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-selection-whole-game",
    "href": "chapters/whole-game.html#sec-model-selection-whole-game",
    "title": "2  The Whole Game",
    "section": "2.5 Choosing Between Model",
    "text": "2.5 Choosing Between Model\nThere are a variety of criteria used to rank and compare model fits. Obviously, performance is important. Also, it is good practice to choose a model that is as simplistic as possible (subject to having acceptable performance). There are other practical factors too. If a model is to be deployed, the size of the files(s) required to automate predictions can be a constraint. There may also be data constraints; which predictors are easy or inexpensive to obtain? Does the number of predictors affect how quickly predictions can be made?\nWhile prediction is the focus for ML models, we often have to explain the model and/or predicted values to consumers. While we can develop explainers for any model, some types are easier to explain than others.\nFor this chapter, we have three models whose MAE values are within about 12 seconds of one another. Each has the same deficiency: they all significantly underpredict long delivery times. In fact, a few poor predictions may be a limiting factor for any regression metric; it might be impossible for one model’s MAE to really stand out from the others.\nWe’ll select the linear regression model as our final model due to its relative simplicity and linear structure.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-calibration-whole-game",
    "href": "chapters/whole-game.html#sec-calibration-whole-game",
    "title": "2  The Whole Game",
    "section": "2.6 Calibration",
    "text": "2.6 Calibration\nThere is one option for remedying the systematic underprediction of long delivery times. ?sec-reg-calibration describes calibration methods that might be able to correct such issues. To do so, we use the held out predictions to estimate the average unwanted trend and remove it from new predictions. For the linear regression results, the estimated average trend is almost identical to the smooth line shown in Figure 2.4.\nFor our test set predictions, we’ll remove this trend and, hopefully, this will recenter the long predictions to be closer to the diagonal line.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-test-results-whole-game",
    "href": "chapters/whole-game.html#sec-test-results-whole-game",
    "title": "2  The Whole Game",
    "section": "2.7 Test Set Results",
    "text": "2.7 Test Set Results\nWe can predict the test set using the linear regression fit from the training set. Recall that the validation set MAE was 1 minute and 36 seconds. Coincidentally, without the additional calibration, the test set value was also 1 minute and 36 seconds. The similarity of these results gives us confidence that the performance statistics we estimated during development are consistent with the test set. The test set plot of the predictions is shown in Figure 2.9(a).\nThe calibration results are shown in panel (b). There does appear to be an improvement in long delivery times; these are closer to the diagonal line. The MAE value is slightly improved: 1 minute and 32 seconds. The small difference in these results and the similar MAE values across models indicates that the few large residuals are probably the gating factor for these data.\n\n\n\n\n\n\n\n\nFigure 2.9: Observed versus predicted test set delivery times for the final model. Results are shown for pre-calibrated and post-calibration.\n\n\n\n\n\nAt this point, we should spend a fair amount of time documenting the model and our development process. It might be helpful to employ inferential analyses to explain to interested parties which model components were most important (e.g., how much the interaction or nonlinear terms improve the model, etc.). It is very important to document where the model does not work well, its intended use, and the boundaries of the training set. Finally, if the model is deployed, we should also monitor its effectiveness over time and also understand if the sample population is changing over time too.\nThe next section accentuates important aspects of this case study.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-themes-whole-game",
    "href": "chapters/whole-game.html#sec-themes-whole-game",
    "title": "2  The Whole Game",
    "section": "2.8 Themes",
    "text": "2.8 Themes\nAt a high level, let’s review the process that we just used to create a model; Figure 2.10 highlights several themes16. The overall process involves initial data splitting, model development, then model selection, and validation. We’ll look at each of these in turn to discuss important points.\n\n\n\n\n\n\n\n\nFigure 2.10: A general high level view of the process of creating a predictive model. The diagram reflects which steps use the test set, validation set, or the entire training set. Figure 2.11 describes the operations labeled “model development”.\n\n\n\n\n\n\nData Spending\nAlthough discussed in Chapter 3, how we allocate data to specific tasks (e.g., model building, evaluating performance, etc.) is an essential aspect of modeling. In our analysis, a minority of the data were explicitly allocated to measure how well the model worked. The test set is meant to be unbiased since it was never used to develop the model. This philosophy of empirical validation is a crucial feature for judging how models function.\nThe majority of the data ends up in the training set. The core activities of creating, optimizing, and selecting are accomplished with these data.\nFigure 2.10 highlights how we use the right data at the right time. A lack of a data usage methodology (or an inappropriate one) can lead to having a false sense of the quality of the model.\nFor example, when the linear regression model was calibrated, we chose to use the held-out samples as the substrate for estimating the unwanted trend. We could have used the re-predicted training set data but, as will be discussed in Section 9.1, these values can be unrealistically optimistic. Also, we chose not to compute the MAE on the held-out predictions after they were used to estimate the calibration. This dual use of the same data would result in an MAE value that is smaller than it should be. There are other ways of estimating the utility of the calibration prior to the test set; these are discussed in ?sec-reg-calibration.\n\n\nInvestigate Many Options\nFor the delivery data, three different models were evaluated. It is our experience that some modeling practitioners have a favorite model that is relied on indiscriminately (boosted trees immediately come to mind). The “No Free Lunch” Theorem (Wolpert 1996) argues that, without substantive information about the modeling problem and data, no single model will always do better than any other model. Because of this, a strong case can be made to try various techniques and then determine which to focus on. In our example, simple data visualizations elucidated the relationships between the outcome and the predictors. Given this knowledge, we might exclude some models from consideration, but there is still a wide variety of techniques to evaluate.\nFor individual models, Figure 2.11 summarizes the cyclic process of speculation, configuration, and validation of model parameters and features. For these data, we discovered most of the important trends prior to our first model fit. As previously mentioned, we would cycle through a few rounds where we try to make improvments for each of the models.\n\n\n\n\n\n\n\n\nFigure 2.11: The cycle of developing a specific type of model. EDA standard for exploratory data analysis.\n\n\n\n\n\nFor novel data sets, it is a challenge to know a priori what good choices are for tuning parameters, feature engineering methods, and predictors. The modeler must iterate using choices informed by their experience and knowledge of the problem.\n\n\nPredictor Representations\nThis example revolves around a small set of predictors. In most situations, there will be numerous predictors of different types (e.g., quantitative, qualitative, etc.). It is unclear which predictors should be included in the model for new modeling projects and how they should be represented.\nWhen considering how to approach a modeling project, the stereotype is that the user focuses on which new, fancy model to apply to the data. The reality is that many of the most effective decisions that users must confront are which predictors to use and in what format. Thoughtfully adding the right feature representations often obviates the need (and overhead and risks) for complex models. That is one of the lessons from this particular data set.\nWe’ll put the feature engineering process on equal footing with model training For this reason, Figure 2.11 shows that the choice of model features and parameters are both important.\nWhile Cubist and the neural network were able to internally determine the more elaborate patterns in the data, our discovery of these same motifs has a higher value. We learned something about the nature of the data and can relate them to the model (via feature engineering) and the end-users17. Later, in Section 8.1, we’ll see how a black-box model can be used to suggests additional interactions to add to our linear regression model. This has the effect of eliminating some of the large residuals seen in Figure 2.4.\n\n\nModel Selection\nAt some point in the process, a specific model pipeline must be chosen. This chapter demonstrated two types of selection. First, we chose some models over others: the linear regression model did as well as more complex models and was the final selection. In this case, we decided between model pipelines. There was also a second type of model selection shown. The number of hidden units for the neural network was determined by trying different values and assessing the results. This was also model selection, where we decided on the configuration of a neural network (as defined by the number of hidden units). In this case, we selected within different neural networks.\nIn either case, we relied on data separate from the training set to produce quantitative efficacy assessments to help choose. This book aims to help the user gain intuition regarding the strengths and weaknesses of different models to make informed decisions.\n\n\nMeasuring Performance\nBefore using the test set, two techniques were used to determine the model’s effectiveness. First, quantitative assessments of statistics (i.e., MAE) from the validation set help the user understand how each technique would perform on new data. The other tool was to create simple visualizations of a model, such as plotting the observed and predicted values, to discover areas of the data where the model does particularly well or poorly. This qualitative information is lost when the model is gauged only using summary statistics and is critical for improving models. There are many more visualization methods for displaying the outcomes, their predictions, and other data. The figures shown here are pretty simplistic.\nNote that almost all of our approaches for model evaluation are data-driven in a way where we determine if the predicted values are “close” to the actual values. This focus on empirical validation is critical in proving that a model pipeline is fit for purpose. Our general mantra is\n\nAlways have a separate piece of data that can contradict what you believe.\n\nFor models that require specific probabilistic assumptions, it is a good idea to check these too, primarily if a model will also be used for inference. However, before considering questions regarding theoretical assumptions, empirical validation should be used to ensure that the model can explain the data well enough to be worth our inferences. For example, if our validation set MAE was around 15 minutes, the model predictions would not show much fidelity to the actual times. Any inferences generated from such a model might be suspect.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-summary-whole-game",
    "href": "chapters/whole-game.html#sec-summary-whole-game",
    "title": "2  The Whole Game",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nAt face value, model building appears straightforward: pick a modeling technique, plug in the data, and generate a prediction. While this approach will yield a predictive model, it unlikely to generate a reliable, trustworthy model. To achieve this we must first understand the data and the objective of the modeling. We finally proceed to building, evaluating, optimizing, and selecting models after these steps.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#chapter-references",
    "href": "chapters/whole-game.html#chapter-references",
    "title": "2  The Whole Game",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nPerkins, D. 2010. Making Learning Whole. Wiley.\n\n\nWolpert, D. 1996. “The Lack of a Priori Distinctions Between Learning Algorithms.” Neural Computation 8 (7): 1341–90.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#footnotes",
    "href": "chapters/whole-game.html#footnotes",
    "title": "2  The Whole Game",
    "section": "",
    "text": "For event time data, it is possible to get incomplete data due to censoring. Suppose that, after 10 minutes you don’t have your order yet. You don’t know the exact delivery time but you do know that it is at least 10 minutes. There are specialized tools for analyzing and modeling censored data. Thankfully, all of our data are complete since we know the exact time of delivery.↩︎\nOverall, we advise looking at the other columns in the data after the splitting.↩︎\nThese are described more in ?sec-outcome-transformation.↩︎\nThe two strategies for creating flexible, smooth fits are splines and generalized additive models. These are described in detail in Sections 8.3 and 16.4, respectively. If you have used the ggplot2 function geom_smooth() function, you’ve used a smoother. ↩︎\nDiscussed in ?sec-reg-metrics↩︎\nIf we want to make inferences on the parameters, such as p-values and confidence intervals, we we’ll need to make more specific assumptions (such as normality of the errors).↩︎\nDifferent ways to convert categorical predictors to numeric predictors are detailed in Chapter 6.↩︎\nSection 8.3↩︎\nWe will re-examine the linear regression model for these data in Section 8.1.2, where we discover (spoilers!) an important term left out of this analysis.↩︎\nThe details for regression trees and Cubist are in ?sec-reg-trees↩︎\nSee Sections 17.4 and ?sec-reg-nnet for discussions and details.↩︎\nThis model description is deliberately vague (so as not to confuse readers who are new to this model). Other details: each model could be trained for up to 5,000 epochs. However, a small amount of the training set was randomly set aside to monitor the sums of squared errors at each epoch. Early stopping was triggered when this out-of-sample loss degraded for 10 consecutive epochs. A rectified linear unit activation function was used between the predictor and hidden layers. A cyclic learning rate scheduler was used with a maximum rate of 0.1 and a weight decay value of 1e-06 was applied. The ADAMw optimizer was used with a batch size of 64 data points. In a more complete analysis, all of these values would also be tuned.↩︎\nThe noisy profile suggests that our validation set might not be large enough to differentiate between subtle differences within or between models. If the problem required more precision in the performance statistics, a more thorough resampling method would be appropriate, such as cross-validation. These are described in Chapter Chapter 10.↩︎\nTo some degree, this is an artificially optimistic estimate since we are using the same data to pick the best result and estimate the model’s overall performance. This is called optimization bias and it is discussed more in Section 11.4. For the characteristics of these data, the optimism is likely to be small, especially compared to the noise in the MAE results.↩︎\nWe will revisit this statement in ?sec-comparing-holdout, where methods for estimating confidence intervals for validation and test results are presented.↩︎\nWhen resampling is used in place of a validation set, Chapter 10 has a similar diagram that reflects the subtle differences in data usage.↩︎\nIn fact, the day-to-day users of the data are the best resources for feature engineering.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html",
    "href": "chapters/initial-data-splitting.html",
    "title": "3  Initial Data Splitting",
    "section": "",
    "text": "3.1 The Ames Housing Data\nIn the previous chapter, Figures 2.10 and 2.11 described various operations for the development and evaluation of ML models. We’ve also emphasized that “the right data should be used at the right time.” If the same samples were used for many different purposes, we run the risk of overfitting. Illustrated in Chapter 9, this occurs when the model over-interprets irreproducible patterns in the modeling data that don’t happen in any other data set. As a result, the model performance statistics are likely to be very optimistic and give us a false sense of how well the model works. If the model were evaluated on a separate set of data (that does not have abnormal patterns), performance would look considerably worse. Because of potential overfitting, the modeler must decide how to best utilize their data across different operations.\nThis chapter will examine how we can appropriately utilize our data. Except in Section 3.8, we’ll assume that each data set row is statistically independent of the others. Before proceeding further, we’ll introduce an example data set used in multiple chapters.\nThese data, originally published by De Cock (2011), are an excellent teaching example. Data were collected for 2,930 houses in Ames, Iowa, via the local assessor’s office. A variety of different characteristics of the houses were measured. Chapter 4 of Kuhn and Silge (2022) contains a detailed examination of these data. For illustration, we will focus on a smaller set of predictors, summarized in Tables 3.1 and 3.2. The geographic locations of the properties are shown in Figure 3.2.\nColumn\n      Min\n      Median\n      Max\n      Std. Dev.\n      Skewness\n      Distribution\n    \n  \n  \n    Baths\n     0.0\n      2.0\n      5.0\n     0.64\n 0.3\n\n    Gross Living Area\n   334.0\n  1,442.0\n  5,642.0\n   505.51\n 1.3\n\n    Latitude\n    42.0\n     42.0\n     42.1\n     0.02\n-0.5\n\n    Longitude\n   -93.7\n    -93.6\n    -93.6\n     0.03\n-0.3\n\n    Lot Area\n 1,300.0\n  9,436.5\n215,245.0\n 7,880.02\n12.8\n\n    Sale Price\n12,789.0\n160,000.0\n755,000.0\n79,886.69\n 1.7\n\n    Year Built\n 1,872.0\n  1,973.0\n  2,010.0\n    30.25\n-0.6\n\n    Year Sold\n 2,006.0\n  2,008.0\n  2,010.0\n     1.32\n 0.1\n\n  \n  \n\n\n\n\n\nTable 3.1: A summary of numeric predictors in the Ames housing data.\nColumn\n      # Values\n      Most Frequent (n)\n      Least Frequent (n)\n      Distribution\n    \n  \n  \n    Building Type\n5\nSingle-Family Detached (2425)\nTwo-Family Conversion (62)\n\n    Central Air\n2\nYes (2734)\nNo (196)\n\n    Neighborhood\n28\nNorth Ames (443)\nLandmark (1)\n\n  \n  \n\n\n\n\n\nTable 3.2: A summary of categorical predictors in the Ames housing data.\nAs shown in Table 3.1, the sale price distribution is fairly right-skewed. For this reason, and because we do not want to be able to predict negative prices, the outcome is analyzed on the log (base-10) scale.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-train-test",
    "href": "chapters/initial-data-splitting.html#sec-train-test",
    "title": "3  Initial Data Splitting",
    "section": "3.2 Training and Testing Sets",
    "text": "3.2 Training and Testing Sets\nOne of the first decisions is to decide which samples will be used to evaluate performance. We should evaluate the model with samples that were not used to build or fine-tune it. An “external sample” will help us obtain an unbiased sense of model effectiveness. A selection of samples can be set aside to evaluate the final model. The training data set is the general term for the samples used to create the model. The remaining samples, or a subset of them, are placed in the testing data set. The testing data set is exclusively used to quantify how well the model works on an independent set of data. It should only be accessed once to validate the final model candidate.\nHow much data should be allocated to the training and testing sets? This depends on several characteristics, such as the total number of samples, the distribution of the response, and the type of model to be built. For example, suppose the outcome is binary and one class has far fewer samples than the other. In that case, the number of samples selected for training will depend on the number of samples in the minority class. Finally, the more tuning parameters required for a model, the larger the training set sample size will need to be. In general, a decent rule of thumb is that 75% could be used from training.\nWhen the initial data pool is small, a strong case can be made that a test set should be avoided because every sample may be needed for model building. Additionally, the size of the test set may not have sufficient power or precision to make reasonable judgments. Several researchers (J. Martin and Hirschberg 1996; Hawkins, Basak, and Mills 2003; Molinaro 2005) show that validation using a single test set can be a poor choice. Hawkins, Basak, and Mills (2003) concisely summarizes this point:\n\n“hold-out samples of tolerable size […] do not match the cross-validation itself for reliability in assessing model fit and are hard to motivate”.\n\nResampling methods (Chapter 10), such as cross-validation, are an effective tool that indicates if overfitting is occurring. Although resampling techniques can be misapplied, such as the example shown in Ambroise and McLachlan (2002), they often produce performance estimates superior to a single test set because they evaluate many alternate versions of the data.\n\nOverfitting is the greatest danger in predictive modeling. It can occur subtly and silently. You cannot be too paranoid about overfitting.\n\nFor this reason, it is crucial to have a systematic plan for using the data during modeling and ensure that everyone sticks to the program. This can be particularly important in cases where the modeling efforts are collaborations between multiple people or institutions. We have had experiences where a well-meaning person included the test set during model training and showed stakeholders artificially good results. For these situations, it might be a good idea to have a third party split the data and blind the outcomes of the test set. In this way, we minimize the possibility of accidentally using the test set (or people peeking at the test set results).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-leakage",
    "href": "chapters/initial-data-splitting.html#sec-leakage",
    "title": "3  Initial Data Splitting",
    "section": "3.3 Information Leakage",
    "text": "3.3 Information Leakage\nInformation leakage (a.k.a data leakage) is another aspect of data handling to consider at the onset of a modeling project. This occurs when the model has access to data that it should not. For example,\n\nUsing the distribution of the predictor data in the test set (or other future data) to inform the model.\nIncluding identical or statistically related data in training and test sets.\nExploiting inadvertent features that are situationally confounded with the outcome.\n\nAn example of the last item we experienced may be familiar to some readers. A laboratory was producing experimental results to evaluate the difference between two treatments for a particular disorder. The laboratory was under time constraints due to an impending move to another building. They prioritized samples corresponding to the new treatment since these were more interesting. Once finished, they moved to their new home and processed the samples from the standard treatment.\nOnce the data were examined, there was an enormous difference between the two treatment sets. Fortuitously, one sample was processed twice: before and after they moved. The two replicate data points for this biological sample also showed a large difference. This means that the signal seen in the data was potentially driven by the changes incurred by the laboratory move and not due to the treatment type.\nThis type of issue can frequently occur. See, for example, Baggerly, Morris, and Coombes (2004), Kaufman et al. (2012), or Kapoor and Narayanan (2023).\nAnother example occurs in the Ames housing data set. These data were produced by the local assessor’s office, whose job is to appraise the house and estimate the property’s value. The data set contains several quality fields for things like the heating system, kitchen, fireplace, garage, and so on. These are subjective results based on the assessor’s experience. These variables are in a qualitative, ordinal format: “poor”, “fair”, “good”, etc. While these variables correlate well with the sale price, they are actually outcomes and not predictors. For this reason, it is inappropriate to use them as independent variables.\nFinally, the test set must emulate the data that will be seen “in the wild”, i.e., in future samples. We have had experiences where the person in charge of the initial data split had a strong interest in putting the “most difficult” samples in the test set. The prevalence of such samples should be consistent with their prevalence in the population that the model is predicting.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "href": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.4 Simple Data Splitting",
    "text": "3.4 Simple Data Splitting\nWhen splitting the data, it is vital to think about the model’s purpose and how the predictions will be used. The most important issue is whether the model will predict the same population found in the current data collection. For example, for the Ames data, the purpose is to predict new houses in the town. This definition implies a measure of interpolation since we are primarily concerned with what is happening in Ames. The existing data capture the types of properties that might be seen in the future.\nAs a counter-example, Chapter 4 of Kuhn and Johnson (2019) highlights a prediction problem in which a model is used to predict the future ridership of commuters on the Chicago elevated trains. This data set has daily records of how many commuters ride the train, and temporal factors highly affect the patterns. In this case, the population we will predict is future ridership. Given the heavy influence of time on the outcome, this implies that we will be extrapolating outside the range of existing data.\nIn cases of temporal extrapolation, the most common approach to creating the training and testing set is to keep the most recent data in the test set. In general, it is crucial to have the data used to evaluate the model be as close to the population to be predicted. For times series data, a deterministic split is best for partitioning the data.\nWhen interpolation is the focus, the simplest way to split the data into a training and test set is to take a simple random sample. If we desire the test set to contain 25% of the data, we randomly generate an appropriately sized selection of row numbers to allocate sales to the test set. The remainder is placed in the training set.\nWhat is the appropriate percentage? Like many other problems, this depends on the characteristics of the data (e.g., size) and the modeling context. Our general rule of thumb is that one-fourth of the data can go into testing. The criticality of this choice is driven by how much data are available. The split size is not terribly important if a massive amount of data is available. When data are limited, deciding how much data to withhold from training can be challenging.\nT. Martin et al. (2012) compares different methods of splitting data, including random sampling, dissimilarity sampling, and other methods.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "href": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "title": "3  Initial Data Splitting",
    "section": "3.5 Using the Outcome",
    "text": "3.5 Using the Outcome\nSimple random sampling does not control for any data attributes, such as the percentage of data in the classes. When one class has a disproportionately small frequency compared to the others (discussed in ?sec-cls-imbalance), the distribution of the outcomes may be substantially different between the training and test sets.\nWhen splitting the data, stratified random sampling (Kohavi 1995) applies random sampling within sub-groups (such as the classes) to account for the outcome. In this way, there is a higher likelihood that the outcome distributions will match. When an outcome is a number, we use a similar strategy; the numeric values are broken into similar groups (e.g., low, medium, and high) and execute the randomization within these groups.\nLet’s use the Ames data to demonstrate stratification. The outcome is the sale price of a house. Figure 3.1(a) shows the distribution of the outcomes with vertical lines that separate 20% partitions of the data. Panel (b) shows that the outcome distributions are nearly identical after partitioning into training and testing sets.\n\n\n\n\n\n\n\n\nFigure 3.1: (a) A density plot of the sale price of houses in Ames with vertical lines that indicate regions that cover 20% of the data. The ‘rug’ on the axis shows the individual data points. (b) Density plots of the training set outcomes (solid red) and test set outcomes (dashed blue) for the Ames data.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "href": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "title": "3  Initial Data Splitting",
    "section": "3.6 Using the Predictors",
    "text": "3.6 Using the Predictors\nAlternatively, we can split the data based on the predictor values. Willett (1999) and Clark (1997) proposed data splitting based on maximum dissimilarity sampling. The dissimilarity between two samples can be measured in several ways. The simplest method uses the distance between the predictor values for two samples. If the distance is small, the points are nearby. Larger distances between points are indicative of dissimilarity. To use dissimilarity as a tool for data splitting, we should initialize the training set with a single sample. We calculate the dissimilarity between this initial sample and the unallocated samples. The unallocated sample that is most dissimilar is added to the training set. A method is needed to allocate more instances to the training set to determine the dissimilarities between groups of points (i.e., the two in the training set and the unallocated points). One approach is to use the average or minimum of the dissimilarities. For example, to measure the dissimilarities between the two samples in the training set and a single unallocated point, we can determine the two dissimilarities and average them. The third point added to the training is chosen as having the maximum average dissimilarity to the existing set. This process continues until we achieve the targeted training set size.\nFigure 3.2 illustrates this process for the Ames housing data. Starting with a data point near the middle of the town, dissimilarity sampling selected 25 data points using scaled longitude and latitude as predictors. As the sampling proceeds, the algorithm initially chooses samples near the outskirts of the data, especially if they are outliers. Overall, the selected data points cover the space with no redundancy.\n\n\n\n\n\n\n\n\nFigure 3.2: Maximum dissimilarity sampling of 25 points in the Ames data. The small black circles are individual properties. Larger, lighter colors indidicate earlier selection.\n\n\n\n\n\nFor this example, the two predictors used for splitting were numeric. In this case, we typically use simple distance functions to define dissimilarity. Many other functions are possible. The Gower distance (Gower 1971) is a good alternative when a data set has non-numeric predictors. Section 17.2 discusses this metric in more detail.\n\nWhile this analysis nicely illustrates the dissimilarity sampling process, it is flawed since it ignores the issue of spatial autocorrelation. This is the idea that things close to one another act more similarly than objects farther away and will be discussed more in Section 3.9 below.\n\nThere are various other methods to split the data using the predictor set. For example, Kennard and Stone (1969) describes an algorithm that attempts to sequentially select points to be uniformly distributed in the space defined by the splitting variables. Similarly, Vakayil and Joseph (2022) proposed a data splitting method called twinning, where a split of the data is sought that minimizes an aggregate distance between points in the training and testing set. Twinning uses the energy distance of Székely and Rizzo (2013), which measures the equality of distributions, to make the two data sets similar. Any variables can be used in the distance calculations.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-three-way-split",
    "href": "chapters/initial-data-splitting.html#sec-three-way-split",
    "title": "3  Initial Data Splitting",
    "section": "3.7 Validation Sets",
    "text": "3.7 Validation Sets\nAs previously discussed, validation sets are a separate partition of the data that function as a precursor for the testing set. It allows us to obtain performance estimates on our model(s) during the development cycle. These are commonly used in deep learning and other domains where the initial data sizes range from very large to massive. This additional partition is often created simultaneously with the training and testing sets.\nValidation sets serve the same purpose as resampling methods described in Section 10.3 and we can consider them single resamples of the training data. Methods like bootstrapping or cross-validation use many alternative versions of the training set to compute performance statistics. When our data are extensive, multiple resamples are computationally expensive without significantly improving the precision of our estimates.\nWithout loss of generalization, we will treat the validation set as a particular case of resampling where there is a single resample of the training set. This difference is not substantive and allows us to have a common framework for measuring model efficacy (before the testing set).\nWe’ll see validation sets discussed in Section 10.3 and used in Sections TODO and TODO.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "href": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.8 Multi-Level Data",
    "text": "3.8 Multi-Level Data\nThere are cases where the rows of a data set may not be statistically independent. This often occurs when multiple data points are collected on individual people, such as\n\nPatients in medical studies may have data collected over time.\nPurchase histories of individual customers in a retail database.\n\nIn these and other situations, the data within a person tend to be correlated. This means that the data from a specific person have a higher correlation than data between people. There are many names for this type of data: multi-level data, hierarchical data, longitudinal data, random effect data, profile data, functional data, and so on. In some cases, there are multiple layers of data hierarchies.\nNote that the variable that indicates the person is generally not a predictor; we would not be making predictions about individual people. People, in this example, are sampled from the broader population. In this case, we are more concerned with the population rather than the individuals sampled from that population.\nThis aspect of the data differentiates it from the neighborhood predictor in the Ames data. The houses within each neighborhood may be more similar to one another than houses between neighborhoods. However, the difference is that we want to make predictions using information from these specific neighborhoods. Therefore, we will include neighborhood as a predictor since the individual neighborhoods are not a selected subset of those in the town; instead, the data contain all of the neighborhoods currently in the city.1\nChapter 9 of Kuhn and Johnson (2019) has a broad discussion on this topic with an illustrative example.\nWhen splitting multi-level data into a training and test set, the data are split at the subject level (as opposed to the row level). Each subject would have multiple rows in the data, and all of the subject’s rows must be allocated to either the training or the test set. In essence, we conduct random sampling on the subject identifiers to partition the data, and all of their data are added to either the training or test set.\nIf stratification is required, the process becomes more complicated. Often, the outcome data can vary within a subject. To stratify to balance the outcome distribution, we need a way to quantify the outcome per subject. For regression models, the mean of each subject’s outcome might be an excellent choice to summarize them. Analogously, the mode of categorical outcomes may suffice as an input into the stratification procedure.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-spatial-splitting",
    "href": "chapters/initial-data-splitting.html#sec-spatial-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.9 Splitting Spatial Data",
    "text": "3.9 Splitting Spatial Data\nSpatial data reflect information that corresponds to specific geographic locations, such as longitude and latitude values on the earth2. Spatial data can have autocorrelation, with objects being more similar to closer objects than to objects further away. Tobler’s First Law of Geography (Bjorholm et al. 2008) is:\n\n“Everything is related to everything else, but near things are more related than distant things.”\n\nThink of an overhead color satellite image of a forest. Different types of foliage—or lack thereof—have different colors. If we focus on the color green, the distribution of “greeness” is not a random pattern of the x/y axes (random distribution would resemble static). In other words, pixels cannot be assumed to be independent of one another.\nSpatial autocorrelation will generally affect how we partition the data; we want to avoid having very similar data points in both the training and testing sets.\nThe example data that we’ll use is from the US Forest Service, who send experts to specific locations to make a determination about whether an area is forested. Couch, White, and Frick (2024) state:\n\nThe U.S. Department of Agriculture, Forest Service, Forest Inventory and Analysis (FIA) Program provides all sorts of estimates of forest attributes for uses in research, legislation, and land management. The FIA uses a set of criteria to classify a plot of land as “forested” or “non-forested,” and that classification is a central data point in many decision-making contexts. A small subset of plots in Washington State are sampled and assessed “on-the-ground” as forested or non-forested, but the FIA has access to remotely sensed data for all land in the state.\n\nSee Frescino et al. (2023) and White et al. (2024) for similar data sets and background on the original problem. The Washington State data contains 7,107 locations. The data are shown in Figure 3.3 where the points are colored by the outcome class (green representing forestation). This figure shows that locations across the state are not spread uniformly. Bechtold and Patterson (2015) describes the sampling protocols and related issues. Finley, Banerjee, and McRoberts (2008) and May, Finley, and Dubayah (2024) show examples of how models have been applied to US forestry data in other geographic regions.\nThe rate for forestation in these data is nearly even at 54.8%.\n\n\n\n\n\n\n\n\nFigure 3.3: Area locations in Washington State sampled for determining forestation. Green points reflect locations that were determined to be forested.\n\n\n\n\nHow should we split these data? A simple random sample is inadequate; we need to ensure that adjacent/nearby locations are not allocated to the training and testing sets. There are a few ways to compensate for this issue.\nFirst, we can group the data into local areas and iterate by removing one or more group at a time. If there is no rational, objective process for creating groups, we can use clustering methods or create a regular grid across the data range (Roberts et al. 2017). For example, Figure 3.4(a) shows a collection of data grouped by a grid of hexagons. We could randomly sample some groups, often referred to as blocks, to allocate to the testing set.\nHowever, notice that some points, while in different hexagonal blocks, are still close to one another. Buffering (Brenning 2005; Le Rest et al. 2014) is a method that excludes data around a specific data point or group. Panel (b) shows a buffer for one block. If this region was selected for the testing set, we would discard the points that are within the buffer (but are in different blocks) from being allocated to the training set.\n\n\n\n\n\n\n\n\nFigure 3.4: A plot of 65 spatial data points where (a) shows how they can be blocked into one of seven hexagonal regions. Panel (b) shows the same data with a circular buffer around the center of a single block.\n\n\n\n\n\nJ Pohjankukka T Pahikkala and Heikkonen (2017), Lyons et al. (2018), Meyer et al. (2019) Karasiak et al. (2022), and Mahoney et al. (2023) have good overviews of the importance of countering spatial autocorrelation and appropriate sampling methods.\nFor our example, we used blocking and buffering to allocate roughly 20% of the forestry data to the testing set. First, the locations were allocated to a 25 x 25 grid of hexagons across the state. After this, a buffer was created around the hexagon such that outside points within the buffer would be excluded from being used in the training or testing sets. Finally, roughly 20% of the locations were selected for the test set by moving through the space and selecting hexagons sequentially. Figure 3.5 shows the results of this process.\nIn the end, a training set of 4,832 locations and a test set of 1,371 locations (roughly a 22.1% holdout) were assembled. The buffer contained 893 locations which was about 12.6% of the original pool of data points. The event rates were similar between the data sets: 56.2% for the training set and 53% for the testing set.\n\n\n\n\n\n\n\n\nFigure 3.5: A split of the forestation data into training (magenta) and testing (purple) sets. Some locations, in black, correspond to the buffer and are not allocated to either set. Clicking on individual points will show their allocation results.\n\n\n\n\nFrom this map, we can see that the hexagons selected for the testing set cover the geographic area well. While the training set dominates some spaces, this should be sufficient to reduce the effects of spatial autocorrelation on how we measure model performance.\nWe will revisit these data in Section 10.8 to demonstrate how to resample them appropriately.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#chapter-references",
    "href": "chapters/initial-data-splitting.html#chapter-references",
    "title": "3  Initial Data Splitting",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBaggerly, K, J Morris, and K Coombes. 2004. “Reproducibility of SELDI-TOF Protein Patterns in Serum: Comparing Datasets from Different Experiments.” Bioinformatics 20 (5): 777–85.\n\n\nBechtold, W, and P Patterson. 2015. The Enhanced Forest Inventory and Analysis Program - National Sampling Design and Estimation Procedures. U.S. Department of Agriculture, Forest Service, Southern Research Station.\n\n\nBjorholm, S, JC Svenning, F Skov, and H Balslev. 2008. “To What Extent Does Tobler’s 1 St Law of Geography Apply to Macroecology? A Case Study Using American Palms (Arecaceae).” BMC Ecology 8: 1–10.\n\n\nBrenning, A. 2005. “Spatial Prediction Models for Landslide Hazards: Review, Comparison and Evaluation.” Natural Hazards and Earth System Sciences 5 (6): 853–62.\n\n\nClark, R. 1997. “OptiSim: An Extended Dissimilarity Delection Method for Finding Diverse Representative Subsets.” Journal of Chemical Information and Computer Sciences 37 (6): 1181–88.\n\n\nCouch, S, G White, and H Frick. 2024. Forested: Forest Attributes in Washington State. https://github.com/simonpcouch/forested.\n\n\nDe Cock, D. 2011. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19 (3).\n\n\nFinley, A, S Banerjee, and R McRoberts. 2008. “A Bayesian Approach to Multi-Source Forest Area Estimation.” Environmental and Ecological Statistics 15: 241–58.\n\n\nFrescino, T, G. Moisen, P Patterson, C Toney, and G White. 2023. “‘FIESTA’: A Forest Inventory Estimation and Analysis R Package’.” Ecography 2023 (7).\n\n\nGower, J. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71.\n\n\nHawkins, D, S Basak, and D Mills. 2003. “Assessing Model Fit by Cross-Validation.” Journal of Chemical Information and Computer Sciences 43 (2): 579–86.\n\n\nJ Pohjankukka T Pahikkala, P Nevalainen, and J Heikkonen. 2017. “Estimating the Prediction Performance of Spatial Models via Spatial k-Fold Cross Validation.” International Journal of Geographical Information Science 31 (10): 2001–19.\n\n\nKapoor, S, and A Narayanan. 2023. “Leakage and the Reproducibility Crisis in Machine-Learning-Based Science.” Patterns 4 (9).\n\n\nKarasiak, N, JF Dejoux, C Monteil, and D Sheeren. 2022. “Spatial Dependence Between Training and Test Sets: Another Pitfall of Classification Accuracy Assessment in Remote Sensing.” Machine Learning 111 (7): 2715–40.\n\n\nKaufman, S, S Rosset, C Perlich, and O Stitelman. 2012. “Leakage in Data Mining: Formulation, Detection, and Avoidance.” ACM Transactions on Knowledge Discovery from Data 6 (4): 1–21.\n\n\nKennard, R W, and L A Stone. 1969. “Computer Aided Design of Experiments.” Technometrics 11 (1): 137–48.\n\n\nKohavi, R. 1995. “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.” International Joint Conference on Artificial Intelligence 14: 1137–45.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nKuhn, M, and J Silge. 2022. Tidy Modeling with R. O’Reilly Media, Inc.\n\n\nLe Rest, K, D Pinaud, P Monestiez, J Chadoeuf, and V Bretagnolle. 2014. “Spatial Leave-One-Out Cross-Validation for Variable Selection in the Presence of Spatial Autocorrelation.” Global Ecology and Biogeography 23 (7): 811–20.\n\n\nLyons, M, D Keith, S Phinn, T Mason, and J Elith. 2018. “A Comparison of Resampling Methods for Remote Sensing Classification and Accuracy Assessment.” Remote Sensing of Environment 208: 145–53.\n\n\nMahoney, MJ, LK Johnson, J Silge, H Frick, M Kuhn, and CM Beier. 2023. “Assessing the Performance of Spatial Cross-Validation Approaches for Models of Spatially Structured Data.” arXiv.\n\n\nMartin, J, and D Hirschberg. 1996. “Small Sample Statistics for Classification Error Rates I: Error Rate Measurements.” Department of Informatics and Computer Science Technical Report.\n\n\nMartin, T, P Harten, D Young, E Muratov, A Golbraikh, H Zhu, and A Tropsha. 2012. “Does Rational Selection of Training and Test Sets Improve the Outcome of QSAR Modeling?” Journal of Chemical Information and Modeling 52 (10): 2570–78.\n\n\nMay, P, A Finley, and R Dubayah. 2024. “A Spatial Mixture Model for Spaceborne Lidar Observations over Mixed Forest and Non-Forest Land Types.” Journal of Agricultural, Biological and Environmental Statistics, 1–24.\n\n\nMeyer, H, C Reudenbach, S Wöllauer, and T Nauss. 2019. “Importance of Spatial Predictor Variable Selection in Machine Learning Applications–Moving from Data Reproduction to Spatial Prediction.” Ecological Modelling 411: 108815.\n\n\nMolinaro, A. 2005. “Prediction Error Estimation: A Comparison of Resampling Methods.” Bioinformatics 21 (15): 3301–7.\n\n\nRoberts, DR, V Bahn, S Ciuti, MS Boyce, J Elith, G Guillera-Arroita, S Hauenstein, et al. 2017. “Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.” Ecography 40 (8): 913–29.\n\n\nSzékely, G J, and M L Rizzo. 2013. “Energy Statistics: A Class of Statistics Based on Distances.” Journal of Statistical Planning and Inference 143 (8): 1249–72.\n\n\nVakayil, A, and V R Joseph. 2022. “Data Twinning.” Statistical Analysis and Data Mining: The ASA Data Science Journal 15 (5): 598–610.\n\n\nWhite, G, J Yamamoto, D Elsyad, J Schmitt, N Korsgaard, J Hu, G Gaines III, T Frescino, and K McConville. 2024. “Small Area Estimation of Forest Biomass via a Two-Stage Model for Continuous Zero-Inflated Data.” arXiv.\n\n\nWillett, P. 1999. “Dissimilarity-Based Algorithms for Selecting Structurally Diverse Sets of Compounds.” Journal of Computational Biology 6 (3): 447–57.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#footnotes",
    "href": "chapters/initial-data-splitting.html#footnotes",
    "title": "3  Initial Data Splitting",
    "section": "",
    "text": "If you are familiar with non-Bayesian approaches to multi-level data, such as mixed effects models, this is the same as the difference between random and fixed effects. ↩︎\nChapters 11 and 12 will also look at spatial effects unassociated with real geographies.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html",
    "href": "chapters/missing-data.html",
    "title": "4  Missing Data",
    "section": "",
    "text": "4.1 Root Causes\nIt is not uncommon for some predictor values to be unknown. There can be a multitude of reasons. To illustrate, let’s consider a laboratory test for a respiratory disease. One or more results may be missing due to a failed control or an inappropriate database join. It is also possible that the test itself failed to produce a result. For example, the test might fail for diagnostics that use throat swabs because an interfering substance, such as food coloring from a lozenge, is on the swab.\nOne of our illustrative data, the Ames housing data, has missing values in 22 predictors. Figure 4.1 illustrates the occurrence of missingness for the these data. This figure presents the full rectangular data matrix where each property (sorted alphabetically by neighborhood) is on the x-axis, and each predictor is on the y-axis. Red indicates that the value was missing in the data.\nThe figure highlights several characteristics. The most noteworthy: missing data affects every property (i.e., row) in the data. Pool quality is missing for nearly every property, while the columns for types of alleys, basements, and fences as well as the quality of fireplaces are missing for the vast majority of properties. Second, there are specific patterns of missing information that are visually apparent. For example, when missing data occurs in one garage variable, it likely occurs across other garage variables (condition, finish, quality, type, and year built). The same is true for basement variables (condition, exposure, quality, type 1, and type 2). Other variables, like fence, fireplace quality, and lot frontage do not appear to have any visual structure to their missingness.\nAn “upset plot” is a method for visualizing high-dimensional Venn diagrams (Lex et al. 2014). We can use this to explore potential patterns of missingness across predictors. For example, Figure 4.2 shows that the majority of the properties in Ames are missing values for ally, fence, or pool.\nThe problem with missing data is that many models are not naturally equipped to deal with this lack of information. As a simple example, consider the multiple linear regression model. Deriving the regression coefficients depends on operations on the predictor values (e.g., computing the covariance matrix). These calculations cannot be performed if the predictors contain any missing values; this is also true for the majority of models. Consequently, it is crucial to address the presence of missing data to build any of these predictive methods1. In addition to addressing missing data, the pattern and nature of missingness can sometimes serve as a significant predictor of the response. In this chapter, we will explore the root causes of missing data, examine approaches for resolving the problem, and understand when it should be addressed in the modeling process.\nAllison (2002) has an excellent and succinct summary of relevant statistical concepts and methods. Emmanuel et al. (2021) and Hasan et al. (2021) provide literature surveys on methods for missing data, while Nijman et al. (2022) describes a survey on just how badly these topics are described/documented in specific studies in the literature.\nWhen encountering missing data, the primary question is, “Why are these values missing?” Knowing is a good idea, and the answer can substantially affect how we compensate for the problem. In some instances, the answer might be apparent or can be inferred from the data. In the example of the laboratory test, suppose a random defect in the test kit was to blame for missing measurements. In this instance, the value was missing completely at random (MCAR). MCAR means that the mechanisms that affect missingness in the predictor are known and unrelated to the columns in the data set. When this is the case, our complete case sample (achieved by using rows with no missing values) corresponds to sampling from the same distribution as the entire data set (i.e., includes the missing rows).\nWith MCAR, we have significant latitude in terms of how to handle the situation. A complete case analysis will not induce any systematic bias in our results but, due to the smaller sample size, the model will have increased variability.\nAs a counter-example, suppose that patients with more severe respiratory illnesses were more likely to deal with their symptoms by using a lozenge, possibly leading to missing values. If the likelihood of a missing predictor value is a function of some other variable(s) contained in the data, this is called missing at random (MAR). You can think of this as conditional MCAR: each each level of a column describing lozenge usage (yes/no), the data are MCAR. Bhaskaran and Smeeth (2014) contains practical examples that help distinguish MAR and MCAR.\nAnother example data set will be introduced in Section 6.1, where we try to predict the cost of a hotel room (known as the “average daily rate” or ADR). The data set includes data on the method of booking the room, such as the name of the travel agent, the travel company, and the customer’s country of origin. The agent variable in the hotel rate data was missing for 22.7% of the data. We don’t always know if the agent is missing because no agent was used or because the value was not recorded. Treating the missingness of the agent as a binary outcome, we can use a simple recursive partitioning model (?sec-cart-cls) to understand potential relationships with other predictors. This might suggest the mechanism(s) influencing the occurrence of missing data. Figure 4.3 illustrates the predictors that partition the missing agent data into increasing homogeneous groups. For example, non-missing company values are highly associated with missing values. This might mean that the company making the reservation used automated systems instead of a specific person in the company being associated with the reservation. Lead time also appears to be related to missingness, with shorter lead times occurring more frequently with missing agent values. We don’t know whether lead time influences missingness or vice-versus. However, this analysis would provide a direction to begin an investigation.\nFigure 4.3: A classification tree to predict the missing category of agent in the hotel rate dataset. “TA” stands for travel agent and “TO” means tour operators.\nWith some simple exploration, as demonstrated with the Ames and hotel rate data sets, we can understand the causes or potential reasons for missing data. However, determining the cause of missing data may be more challenging for many other data sets. We need a framework to understand and manage missing data in such cases.\nOne helpful framework involves examining the mechanisms behind missing data, including structural deficiencies, random occurrences, and specific causes.\nFirst, there can be structural deficiencies. This type of missing data arises when necessary information about a predictor is omitted. Moreover, it is often the easiest to address once the missing information has been identified. The agent variable in the hotel rate data is one example of a structural deficiency. As a second example, we saw earlier in the Ames housing data that several of the garage predictors were simultaneously missing across 5% of homes. The missing information was because these homes did not have a garage2. Some or all of the variables of finish, quality, type, and year built for garages will likely be important for determining a home’s value. Therefore, we will need an approach to address these structural deficiencies.\nThere can also be random occurrences. Missing values often occur at random with no defined root cause. For example, a patient in a clinical trial may miss a scheduled visit due to a scheduling mishap. As another example, sporadically occurring severe weather can create missing data for collection devices that depend on continuous power. Generally, missing data due to random occurrences can happen a small percentage of the time and can be remedied. However, if randomly missing data occurs a large percentage of the time, the measurement system should be evaluated, and the collected variables should be scrutinized before being included in the modeling process. For a detailed understanding of this type of problem, see Little and Rubin (2019).\nWe might be able to identify specific causes for missingness. As we will see later in this chapter (Section 4.2.4), several basement variables in the original Ames data have missing values. This information was not missing because there was some failure in recording the data. The explanation was much simpler: these homes had no basements. This type of missing data is the most challenging to manage, and the appropriateness of techniques can vary. Hence, understanding the nature of the missing data is crucial before applying any methods.\nThe worst situation is missing not at random (MNAR)3, where we do not know the factors that influence the probability that values are missing. The best approach to MNAR data is not to have MNAR data; it is essential to determine why the data are missing and, hopefully, relate that to columns in the data. Otherwise, we can use imputations and other methods described below to solve the issues. However, there will likely be significant ambiguity about the analysis and how well it works. With models built in the MNAR assumption, it would behoove us to conduct extensive sensitivity analyses to assess the robustness of our approach to the missing data problem.\nAs an example, Chapters 12 - 15 of Kuhn and Johnson (2013) showed multiple classification models to predict the probability that a grant would be accepted. Two categorical predictors contained encoded values of “unknown” and had very high empirical associated with the outcome classes:\nThis is not a good place to be: we cannot explain the existence of two of our main predictors. It would be that the populations of grants that had missing data for these variables correspond to very successful results or that the process that assembled the data contains a systematic flaw. We do know that we can’t induce success in a grant application by just labeling the value as unknown. Clearly, we would want to investigate this situation to determine what is happening.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-root-causes",
    "href": "chapters/missing-data.html#sec-root-causes",
    "title": "4  Missing Data",
    "section": "",
    "text": "Informative missingness seemed to occur in these data; unknown values of the contract value band and sponsor code were heavily used in many models. This itself is a likely surrogate or signal for some other piece of information.\n\n\n\nUnderstanding these mechanisms will guide us in choosing appropriate techniques for handling missing data appropriately.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-resolve-missing-data",
    "href": "chapters/missing-data.html#sec-resolve-missing-data",
    "title": "4  Missing Data",
    "section": "4.2 Approaches for Resolving Missing Data",
    "text": "4.2 Approaches for Resolving Missing Data\nThere are three general ways to resolve missing values: removal, imputation, or encoding. Each approach has advantages and disadvantages, and which we choose should depend on the specific problem context and data set. We will review these approaches in the subsections below and provide guidance on when each would be appropriate.\n\n4.2.1 Models that Tolerate Missing Data\nInstead of directly addressing the missing values, we could sidestep it by using models that tolerate missingness. Let’s examine a few.\nThe CART decision tree model (Breiman et al. 1984) recursively identifies variables and split points that optimally partition the data into subsets whose outcome frequency distributions are more homogeneous. For each selected variable and split point, additional variables and split points are identified with the next best ability to partition the data into more pure subsets. These additional variables are called surrogate splits. When a sample has a missing value for a predictor in the tree, the surrogate predictors are then used to direct the sample toward the appropriate terminal node.\nWhile C5.0 (Quinlan 1993; Kuhn and Johnson 2013) is also a decision tree, it adopts a unique approach to addressing missing values. This method utilizes fractional counts in subsequent splits based on the frequency distribution of missing data for a predictor. This approach enables the model to estimate where the missing values might fall within the partitioning.\nBoosted trees, such as xgboost (Chen and Guestrin 2016), are also based on a recursive partitioning framework. However, xgboost’s approach to addressing missing data is more complex and is called sparsity-aware split finding. In the model-building process, the algorithm determines which direction would be more optimal for each node in the tree if a sample had a missing value for that predictor.\nRandom forest (Breiman 2001) has several approaches to handling missing values. The naive approach internally imputes using the median of non-missing values for continuous predictors or the most frequent value for categorical predictors. A more advanced approach identifies the nearest non-missing samples to the sample with the missing value and imputes the value based on a weighted distance score.\nThe primary benefit of these models is that we can use the original data as-is. They are eliminate the propagation of errors that can occur when imputation tools are used. The drawback is that the number of models that can be applied to the data is very limited. As we know from the No Free Lunch theorem, no one model will be optimal for all problems. Therefore, we must address the missing data problem head-on to adequately explore the predictive ability of a wide range of models.\n\nThese models only solve the logistical aspect of missing data. If the nature of missingness in your data causes a systematic bias, the models above will not correct this bias.\n\nFor example, naive Bayes (Webb, Keogh, and Miikkulainen 2010) builds models by analyzing each predictor independently of the others. When a predictor contains a missing value, this missing sample’s information is omitted from probability calculations for only the affected predictors. If a distribution is systematically affected by missingness, the probabilities computed during prediction will be biased and may result in poor results in new samples. If missing data informs our understanding of the response, omitting the missing samples will be detrimental to the model.\nFigure 4.4 shows a two-class classification example where one predictor is used. The top panel shows the probability of a missing predictor value is related to its own value (i.e., MCAR). The predictor value is less likely to be complete was its value increases.\nNaive Bayes would use the predictor data to compute conditional densities of the predictor for each outcome class. The middle plot shows what these densities should resemble if the dataset were complete. There is a small overlap between the predictor densities; decent model performance is achievable.\n\n\n\n\n\n\n\n\nFigure 4.4: An example of how missingness can cause bias in the training data and impair a model fit.\n\n\n\n\n\nThe bottom set of densities reflects the observed data. The density for class level B is more affected and would appear to have a tighter distribution. This induces more overlap in the densities, making the classification problem more difficult.\nThis demonstrates that a model being agnostic to missing values does not mean that the missing value problem does not go away.\n\n\n4.2.2 Removal\nThe simplest method for managing missing data is to eliminate the predictors or samples that contain them. However, the deletion of data requires careful consideration of several factors within the dataset. When deleting data, the order in which we assess the proportion of missing values (in columns or rows) is important to consider. In some scenarios, there are many more predictors than samples; samples are often difficult or expensive to collect. In this case, it would be wise to first identify predictors that should be removed due to excessive missing values, then proceed to identify and possibly remove samples that have excessive missing values. If, on the other hand, the data contain many more samples than predictors, then the removal procedure could be reversed. In any case, however, we need to remember that the samples are our currency for tuning models and assessing model performance. Therefore, we will often place a higher priority on preserving samples over predictors.\n\n\n\n\n\n\n\n\nFigure 4.5: The distribution of percent missing values across predictors for the Ames housing data set.\n\n\n\n\n\nLet’s return to the Ames data set. In this example, there are many more samples (\\(n\\)=2930) than predictors (\\(p\\)=73). While this is true, Figure 4.1 clearly illustrates that a few of the predictors were missing for the vast majority of the samples. The proportion of missing values across the predictors is shown in Figure 4.5. This figure reveals that 4 predictors have more than 20% missing sample values. Because the percentage of missing values is large, these predictors would be candidates for removal. In the hotel rate data, the agent and company predictors had missing values of 22.7% and 91%, respectively. In our experience, we have tended to remove predictors that have more than 20%-30% missing values. This percentage is simply a point of guidance and not a hard rule to be applied to every data set. After removing these predictors, the Ames data set now has 0.7% missing values and the hotel rate data set had 0.05% missing values.\nAfter removing predictors with excessive missing data, we can then consider removing samples with excessive missing data. No sample in the Ames data had more than 12.1% of missing predictors. The samples with the greatest percentage of missing predictors were those that had missing basement information. No sample in the hotel rate data had more than 3.6% of missing predictors. Neither of these percentages is large enough to merit the removal of these samples. Therefore, we will keep these samples and utilize an imputation or encoding procedure to fill in these gaps.\nThere are a couple of caveats to removing predictors or samples based on missing information. First, the missing information in a predictor may be informative for predicting the response. For the hotel rate data, the missing status of the agent variable is associated with the response. Therefore, it may be better to encode the missing status for this variable rather than eliminate the variable altogether. This approach will be discussed below. Second, as seen in Figure 4.4, removing samples may create a bias in the remaining data which would impact the predictive ability of the model on future samples.\n \n\n\n4.2.3 Imputation\nImputation is the process of using other existing information (i.e., the predictors) to estimate what each missing value might have been. In other words, we will build an imputation model to fill in the missing column(s) so that we can run the primary machine-learning model. A separate imputation model is required for each column that contains (or could contain) missing data.\nImputation is a well-researched statistical methodology. This technique has traditionally been used for inferential models, focusing on maintaining the validity of test statistics to support hypothesis testing. As mentioned in earlier chapters, there is an important distinction between the objectives of statistical inference and prediction accuracy. See Sperrin et al. (2020) for a discussion related to imputation.\nIn the statistical literature (D’Agostino McGowan, Lotspeich, and Hepler 2024), two terms can help us understand the distinction:\n\nDeterministic imputation creates a single model to estimate the missing predictor’s value using one or more predictors in the training set4. After imputation, the new values are treated as known (i.e., not random variables).\nStochastic imputation involves creating many imputation models with the goal of creating a distribution of possible values for the missing data. It is used primarily to conduct proper inferential analyses and includes traditional multiple imputation techniques.\n\nOne important concept heavily featured in subsequent chapters is resampling, where we’ll train our preprocessors and supervised models using slightly different data sets. The goal of resampling is to estimate model performance accurately. Although resampling will involve repeated re-imputation of missing data, it is not a stochastic imputation method.\nLet’s highlight a couple of key distinctions when considering imputation for the purpose of inference versus prediction. One is that inferential models generally make assumptions about the statistical distributions of the predictors. Conversely, many machine learning models, like support vector machines, tree-based models, and neural networks, do not make such assumptions. Therefore, stochastic imputation is less relevant for predictive models. A second difference is that multiple imputation methods focus on understanding relationships within the existing data, while predictive models aim for generalizable relationships to unseen samples. This difference has implications on when imputation should be applied to the data, which will be discussed later in Section 4.4. Finally, in some cases, including the outcome data in the imputation model might make sense. This is inappropriate for our goals here (i.e., deterministic imputation). While D’Agostino McGowan, Lotspeich, and Hepler (2024) explains the theoretical reasons why this is the case, our objections can be viewed as purely functional: if we require the outcome to make decisions, we cannot predict new samples where the outcome is unknown.\nWhat are the important characteristics of an imputation technique that will be used in a prediction model? There are a few that we will focus on:\n\nTolerate other missing data: as we saw in the Ames data, multiple variables within a sample may be missing. Therefore, an imputation technique should be feasible in the presence of other missing data.\nHandle different predictor types: many data sets have different variables, such as numeric, categorical, and ordinal. The method should be able to accommodate numeric and qualitative predictors seamlessly and without changing the nature of the data (e.g., categorical predictors should be be converted to indicator columns).\nProduce efficient prediction equations: each predictor with missing data will require an equation. Imputation for large data sets, when used with resampling approaches during model training, will increase computation time as well as the size of the imputation equation. Therefore, the more efficient the imputation approach, the less computation time will be needed to obtain the final model.\nEnsure robustness: the method should be stable and not overly affected by outliers.\nBe accurate: the results should be close to what the actual value would have been5.\n\nBy considering each of these characteristics, imputation can effectively alleviate missing data problems, enhancing models’ quality and predictive performance. There are several imputation methods that meet most or all of the above characteristics. These imputation techniques fall into two general categories: most likely value and model-based (i.e., “regression imputation”).\nTo illustrate how these methods work, we will use two simple, simulated two-predictor classification data sets. Figure 4.6 displays the simulated data. For these data sets, the optimal class separation is defined by the black lines. We will initially focus on the nonlinear data set. From these data set, 10% of the samples will be randomly selected. Of these samples, half will have the value of first predictor deleted, while the other half will have the second predictor deleted.\n\n\n\n\n\n\n\n\nFigure 4.6: Two complete two-class simulated datasets with 200 data points, one with a linear relationship between the predictors and the other nonlinear. The optimal partitions of the classes are represented by the black lines.\n\n\n\n\n\n\n\n\n4.2.4 Encoding Missing Data\nWhen a predictor takes categorical values, an alternative approach to imputation is encoding. Consider the basement exposure variable in the original Ames data set. Possible exposure values are: “good,” “average,” “minimum,” and “no exposure” and contains 83 missing values. The most likely value approach would impute the missing values with the “no exposure” category since this is the most frequent category. A model-based approach would utilize information across the rest of the variables to predict which of the 4 available categories would be best. Would imputing with one of the 4 available categories be a good approach for the missing samples? In this case, the answer is “no!”. With a little more investigation, we can see that the reason the exposure variable contains missing values for these houses is because these houses do not have basements.\nWhen a value is missing for a specific reason like we see with the basement exposure variable, encoding the missing information will be more informative to the models. In an encoding procedure, we simply acknowledge that the value is missing by creating a new category such as “unknown,” “unspecified,” or “not applicable.” In the case of basement exposure, a more appropriate categorization would be “no basement.”\nThe encoding procedure is mostly used for categorical variables. However, there are times when encoding can be applied to continuous variables. In many analytical procedures, an instrument cannot reliably provide measurements below a specified limit. Measurements of samples above the limit are returned as numeric values, but measurements below the limit are returned as either “BLQ” (below lower limit of quantitation) or “&lt; 3.2”, where 3.2 is the lower limit of quantitation. These values cannot be included with the continuous values. What can we do in this situation? One approach would be to impute the BLQ values with a reasonable numeric value less than lower limit of quantitation6. This information could also be encoded by creating a new variable that contains two values, “ALQ” and “BLQ”, which would identify samples that could and could not be measured. If this variable is related to the outcome, then the ability to measure the quantity may be predictively informative.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-imputation-methods",
    "href": "chapters/missing-data.html#sec-imputation-methods",
    "title": "4  Missing Data",
    "section": "4.3 Specific Imputation Methods",
    "text": "4.3 Specific Imputation Methods\nTechnically, almost any model that can make a prediction is a potential candidate for imputing data. However, some are better than others and we summarize a few of the most used below.\n\n4.3.1 Most Likely Value\nThe simplest approach to imputation is to replace a missing value with its most likely value based on that predictor’s non-missing values. For numeric data, the most likely value can be summarized by the mean or median. For categorical data, the most likely value is the mode (or most frequent value). Most likely value imputation meets many of the desirable characteristics for imputation. They can tolerate missing values from other predictors (because they operate one-predictor-at-a-time) and can handle different predictor types while also producing efficient prediction equations. Furthermore, achieving a robust imputation can be done using either the median or the trimmed mean (Barnett and Lewis 1994). The trimmed mean is the mean of the middle-most samples. For example, we could compute the mean of the samples with 5% of the the most extreme (smallest and largest) samples removed. Doing this would minimize the impact of any extreme sample values on the computation of the mean. However, imputing with the most likely value for a single predictor may not produce an imputed value that is close to the true value.\nTo demonstrate, we have imputed the missing values in the simulated data using the mean and median imputation techniques for both data sets seen in Figure 4.6. Figure 4.7 shows the values of the original samples that were selected to have missing values along with the imputed values based on mean and median imputation.\nFor both data sets, the imputed values are towards the center over the overall plot and away from the parabolic/linear regions where we know the actual data are. The median imputed values are less affected by extreme values and are closer to the region of actual data. Both techniques, however, are unable to place most of the missing data near their true values.\nFor many data sets, and especially for sets with a minimal number of missing values, most likely value imputation may be sufficiently good for what we need. The procedure is fast but is terribly inaccurate. But there may be occasions when when we need the imputations to be closer to the true (yet unknown) values.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-imputation-exploration\n#| out-width: \"80%\"\n#| viewerHeight: 425\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(bslib)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-imputation-exploration.R\")\n\napp\n\n\n\nFigure 4.7: A visualization of the four new features for different linear embedding methods. The data shown are the validation set results from the two simulations shown in Figure 4.6.\n\n\n\n\n\n4.3.2 Linear Methods\nWhen a data set has many predictors and the predictors have some correlation with each other, then multiple linear regression can be a more effective imputation technique than the most likely value methods. That is, multiple linear regression will utilize the information contained in other predictors to estimate an imputed value that will be closer to what the actual value would have been. Multiple linear regression also produces a straightforward, compact prediction equation for each predictor with missing values which is computationally efficient when applied to new samples. However, this technique cannot naturally handle missing values in the predictors used to develop the imputation model. In practice, therefore, the imputation approach utilizes a complete case analysis.\nReturning to the simulation illustration, the linear imputation method creates two simple linear regression models, where one model uses predictor 1 as the response and predictor 2 as the predictor, and the other model reverses the roles of the predictors.\nSelecting the nonlinear data set in Figure 4.7 illustrates the impact of this technique. Assuming that we have conducted some exploratory data analysis, we would be aware of the nonlinear relationship between the predictors. As such, the imputation model for the second predictor included an additional squared term. This greatly improves the imputation quality. Unfortunately, the same approach is not possible for imputing the first predictor (which is poorly predicted).\nHowever, if these predictors were linearly related, then this technique would generate values closer to the actual values. This can be seen by choosing the linear data set in Figure 4.7.\nSince this imputation method is basically linear regression, we can use it for slightly more sophisticated results. For example, if we want to impute a predictor’s values based on some other predictors categories, linear regression can achieve this. There is also the possibility of interactions, splines, and so on (as long as the “predictors” in the imputation model are not missing themselves).\nWhen the predictor that requires imputation is categorical, then logistic or multinomial regression can be used to generate an imputed category.\nFigure 4.8 provides another visual comparison of the imputation techniques. In this figure, each point represents the Euclidean distance from the imputed sample to its known location. The mean, median, and linear methods have similar distributions of distances.\n\n\n\n\n\n\n\n\nFigure 4.8: A comparison of the distribution of distances between the actual and imputed samples across imputation techniques for the nonlinear data shown in Figure 4.6. The mean, median, and linear techniques perform similarly, while K-NN and bagging generate imputed values that are modestly closer to the actual values.\n\n\n\n\n\n\n\n4.3.3 Nearest Neighbors\n\\(K\\)-nearest neighbor calculations will be discussed in Section 7.3, where it will be used in some multidimensional scaling methods, and in ?sec-knn-cls in the context of a supervised model. In short, when imputing a missing predictor value for a new sample, the \\(K\\) most similar samples from the training set are determined (using all complete predictor values). The \\(K\\) predictor values for the predictor of interest are summarized via the mean (for numeric outcomes) or the mode. This summary statistic is used to fill in the missing value. This is a localized version of the most likely value approach.\nFigure 4.7 shows that a nearest-neighbor approach generally increases the accuracy of the imputation, especially for the first predictor. For the linear case, both predictors have high-quality imputations.\n\n\n4.3.4 Trees\nAs mentioned earlier, tree-based models are a reasonable choice for imputation techniques because many types of trees do not require complete data themselves. They generally provide good accuracy and do not extrapolate values beyond the bounds of the training data. While a single tree can be used for imputation, it will likely have low bias but high variance. Ideally, we would like imputed values to have low bias as well as low variance.\nTree ensembles, such as bagging and random forests (Chapters ?sec-trees-cls and ?sec-trees-reg), help solve this issue since they blend the predictors from many individual tree models. However, these methods can be computationally taxing for moderate- to large-sized data sets. Specifically, random forests require many trees (hundreds to thousands) to achieve a stable and reliable imputation model. This comes at the cost of a large computational footprint, which may become a challenge as the number of predictors with missing data increases; a separate model must be trained and retained for each predictor. As discussed later, bagged tree models tend to be smaller and faster than their random forest counterparts. Typically, there is a marginal loss in accuracy from using bagging instead of random forests.\nLike nearest-neighbor imputation, the bagged tree places most of the imputed values close to the original data distribution, as seen in Figure 4.7. The distribution of distances (Figure 4.8) for bagged imputation is similar to that of nearest neighbors. Figure 4.7 provides a comparison of the imputation techniques discussed here with the nonlinear simulated data and a data set in which the two groups are optimally separated by a linear boundary. In this comparison, the percentage of missing data can be adjusted to demonstrate how each imputation method performs.\nWhich imputation technique we choose depends on the problem. If a few predictors have missing values, then using \\(K\\)-NN or bagged imputation may not add much computational burden to the modeling process. However, if many predictors have missing values and the data set is large, then the model-based imputation techniques may become cumbersome. In this case, beginning with a most likely value imputation approach may be prudent. If the predictive performance of the optimally tuned machine learning technique is high, then the imputation approach was sufficient. However, if predictive performance lags, we could implement a different imputation technique while training models.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-when-to-address-missing-data",
    "href": "chapters/missing-data.html#sec-when-to-address-missing-data",
    "title": "4  Missing Data",
    "section": "4.4 When to Address Missing Data",
    "text": "4.4 When to Address Missing Data\nWhen missing values are present, it may be tempting to immediately address this problem and create a complete data set prior to beginning the modeling or data splitting processes. We recommend that imputation be done as part of the model development process as discussed in Chapter 2. In the diagram presented in Figure 2.10, imputation would occur as part of the training process. Recall, the model based imputation techniques discussed earlier have parameters that can be tuned. Understanding how these parameters affect model performance would be done during the training process, and an optimal value can be selected.\nIt is also important to understand that imputation is fundamentally a preprocessing step–we must do this before commencing model building. Therefore, we must think about where imputation should be located in the order of preprocessing steps. Specifically, imputation should occur as the first step. It is advisable to impute qualitative predictors before creating indicator variables to maintain the binary nature of the resulting data. Moreover, imputation should precede parameter estimation steps. For example, if centering and scaling are done before imputation, the resulting means and standard deviations will reflect biases and issues from the missing data. This approach ensures the integrity and accuracy of subsequent data processing and analysis stages.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#chapter-references",
    "href": "chapters/missing-data.html#chapter-references",
    "title": "4  Missing Data",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAllison, P. 2002. Missing Data. SAGE Publications, Inc.\n\n\nBarnett, V, and T Lewis. 1994. Outliers in Statistical Data. Vol. 3. 1. Wiley New York.\n\n\nBhaskaran, K, and L Smeeth. 2014. “What Is the Difference Between Missing Completely at Random and Missing at Random?” International Journal of Epidemiology 43 (4): 1336–39.\n\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nBreiman, L, J Friedman, C Stone, and RA Olshen. 1984. Classification and Regression Trees. CRC Press.\n\n\nChen, T, and C Guestrin. 2016. “Xgboost: A Scalable Tree Boosting System.” In Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining, 785–94.\n\n\nD’Agostino McGowan, L, S Lotspeich, and S Hepler. 2024. “The \"Why\" Behind Including \"Y\" in Your Imputation Model.” Statistical Methods in Medical Research 33 (6): 996–1020.\n\n\nEmmanuel, T, T Maupong, D Mpoeleng, T Semong, B Mphago, and O Tabona. 2021. “A Survey on Missing Data in Machine Learning.” Journal of Big Data 8: 1–37.\n\n\nHasan, K, A Alam, S Roy, A Dutta, T Jawad, and S Das. 2021. “Missing Value Imputation Affects the Performance of Machine Learning: A Review and Analysis of the Literature (2010-2021).” Informatics in Medicine Unlocked 27: 100799.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\nLex, A, N Gehlenborg, H Strobelt, R Vuillemot, and H Pfister. 2014. “UpSet: Visualization of Intersecting Sets.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983–92.\n\n\nLittle, R, and D Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley & Sons.\n\n\nNijman, SWJ, AM Leeuwenberg, I Beekers, I Verkouter, JJL Jacobs, ML Bots, FW Asselbergs, KGM Moons, and TPA Debray. 2022. “Missing Data Is Poorly Handled and Reported in Prediction Model Studies Using Machine Learning: A Literature Review.” Journal of Clinical Epidemiology 142: 218–29.\n\n\nQuinlan, R. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers.\n\n\nSisk, R, M Sperrin, N Peek, M van Smeden, and G Martin. 2023. “Imputation and Missing Indicators for Handling Missing Data in the Development and Deployment of Clinical Prediction Models: A Simulation Study.” Statistical Methods in Medical Research 32 (8): 1461–77.\n\n\nSperrin, M, G Martin, R Sisk, and N Peek. 2020. “Missing Data Should Be Handled Differently for Prediction Than for Description or Causal Explanation.” Journal of Clinical Epidemiology 125: 183–87.\n\n\nWebb, G, E Keogh, and R Miikkulainen. 2010. “Naı̈ve Bayes.” Encyclopedia of Machine Learning 15 (1): 713–14.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#footnotes",
    "href": "chapters/missing-data.html#footnotes",
    "title": "4  Missing Data",
    "section": "",
    "text": "Exceptions are listed below.↩︎\nHowever, note that these properties have complete values for their size column (i.e., a zero ft2 garage). ↩︎\nAlso called not missing at random (NMAR), informative missingness, or nonignorable missing data.↩︎\nSisk et al. (2023) calls this approach, when using multiple predictors for imputation, as “regression imputation.” Their definition would exclude simple “most likely value” imputations.↩︎\nUnfortunately, we may not be able to judge the accuracy of the imputation for most data sets since the missing data will never be known.↩︎\nOne approach is to generate a random uniform predictor value between zero and the limit of quantitation).↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html",
    "href": "chapters/numeric-predictors.html",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "5.1 What are Problematic Characteristics, and When Should Transformations be Applied?\nData that are available for modeling are often collected passively without the specific purpose of being used for building a predictive model. As an example, the Ames Housing data contains a wealth of information on houses in Ames, Iowa. But this available data may not contain the most relevant measurements for predicting house price. This may be due to the fact that important predictors were not measured. Or, it may be because the predictors we have collected are not in the best form to allow models to uncover the relationship between the predictors and the response.\nAs mentioned previously, feature engineering is the process of representing your predictor data so that the model has to do the least amount of work to explain the outcome effectively. A tool of feature engineering is predictor transformations. Some models also need predictors to be transformed to meet the model’s mathematical requirements (i.e., pre-processing). In this chapter we will review transformations for quantitative predictors.\nWe will begin by describing transformations that are applied to one predictor at a time that yield a revised for of the predictor (one in, one out). After these, an example of a group transformation is described called the spatial sign. Later, Chapter 7 will describe different types of many-to-many transformations such as principal component analysis (PCA) and multidimensional scaling (MDS). Additionally, in Chapter 8, we will examine techniques for expanding a single numeric predictor to many predictors (one in, many out).\nLet’s begin by understanding some general data characteristics that need to be addressed via feature engineering and when transformations should be applied.\nCommon problematic characteristics that occur across individual predictors are:\nSome models, like those that are tree-based, are able to tolerate these characteristics. However, these characteristics can detrimentally affect most other models. Techniques used to address these problems generally involve transformation parameters. For example, to place the predictors on the same scale, we would subtract the mean of a predictor from a sample and then divide by the standard deviation. This is know as standardizing and will be discussed in the next section.\nWhat data should be used to estimate the mean and standard deviation? Recall, the training data set was used to estimate model parameters. Similarly, we will use the training data to estimate transformation parameters. When the test set or any future data set are standardized, the process will use the estimates from the training data set. Any model fit that uses these standardized predictors would want new samples being predicted to have the same reference distribution.\nSuppose that a predictor column had an underlying Gaussian distribution with a sample mean estimate of 5.0 and a sample standard deviation of 1.0. Suppose a new sample has a predictor value of 3.7. For the training set, this new value lands around the 10th percentile and would be standardized to a value of -1.3. The new value is relative to the training set distribution. Also note that, in this scenario, it would be impossible to standardize using a recomputed standard deviation for the new sample (which means we try to divide with a zero standard deviation).\nMany transformations that involve a single predictor change the data distribution. Most predictive models do not place specific parametric assumptions on the predictor variables (e.g., require normality), but some distributions might facilitate better predictive performance than others.\nTODO some based on convention or scientific knowledge. Others like the arc-sin (ref The arcsine is asinine: the analysis of proportions in ecology) or logit? See issue #10.\nThe next two sections will consider two classes of transformations for individual predictors: those that resolve distributional skewness and those that convert each predictor to a common distribution (or scale).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-general-transformation",
    "href": "chapters/numeric-predictors.html#sec-general-transformation",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "skewed or unusually shaped distributions,\nsample(s) that have extremely large or small values, and\nvastly disparate scales.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-skewness",
    "href": "chapters/numeric-predictors.html#sec-skewness",
    "title": "5  Transforming Numeric Predictors",
    "section": "5.2 Resolving Asymmetry and Skewness",
    "text": "5.2 Resolving Asymmetry and Skewness\nAn asymmetric statistical distribution is one in which the probability of a sample occurring is not symmetric around the center of the distribution (e.g., the mean). For example, Figure 5.1 (panel a) shows the training set distribution of the lot area of houses in Ames. There is a much higher likelihood of the lot area being lower than the mean (or median) lot size. There are fewer large lots than there are proportionally smaller lots. And, in a few cases, the lot sizes can be extremely large.\nThe skew of a distribution indicates the direction and magnitude of the asymmetry. It can be quantified using the skewness statistic:\n\\[\n\\begin{align}\n  skewness &= \\frac{1}{(n-1)v^{3/2}} \\sum_{i=1}^n (x_i-\\overline{x})^3 \\notag \\\\\n  \\text{where}\\quad  v &= \\frac{1}{(n-1)}\\sum_{i=1}^n (x_i-\\overline{x})^2 \\notag\n\\end{align}\n\\]\nwhere values near zero indicate a symmetric distribution, positive values correspond a right skew, and negative values left skew. The lot size data are significantly right-skewed (with a skewness value of 13.5). As previously mentioned, there are 2 samples in the training set that sit far beyond the mainstream of the data.\n\n\n\n\n\n\n\n\nFigure 5.1: Lot area for houses in Ames, IA. The raw data (a) are shown along with transformed versions using the Yeo-Johnson (b), percentile (c), and ordered quantile normalization (d) transformations.\n\n\n\n\n\nOne might infer that “samples far beyond the mainstream of the data” is synonymous with the term “outlier”; The Cambridge dictionary defines an outlier as\n\na person, thing, or fact that is very different from other people, things, or facts […]\n\nor\n\na place that is far from the main part of something\n\nThese statements imply that outliers belong to a different distribution than the bulk of the data. For example, a typographical error or an incorrect merging of data sources could be the cause.\nThe Croarkin et al. (2012) describes them as\n\nan observation that lies an abnormal distance from other values in a random sample from a population\n\nIn our experience, researchers are quick to label (and discard) extreme data points as outliers. Often, especially when the sample size is not large, these data points are not abnormal but belong to a highly skewed distribution. They are ordinary in a distributional sense. That is the most likely case here; some houses in Ames have very large lot areas, but they certainly fall under the definition of “houses in Ames, Iowa.” These values are genuine, just extreme.\nThis, by itself, is okay. However, suppose that this column is used in a calculation that involves squaring values, such as Euclidean distance or the sample variance. Extreme values in a skewed distribution can influence some predictive models and cause them to place more emphasis on these predictors1. When the predictor is left in its original form, the extreme samples can end up degrading a model’s predictive performance.\nOne way to resolve skewness is to apply a transformation that makes the data more symmetric. There are several methods to do this. The first is to use a standard transformation, such as logarithmic or the square root, the latter being a better choice when the skewness is not drastic, and the data contains zeros. A simple visualization of the data can be enough to make this choice. The problem is when there are many numeric predictors; it may be inefficient to visually inspect each predictor to make a subjective judgment on what if any, transformation function to apply.\nBox and Cox (1964) defined a power family of transformations that use a single parameter, \\(\\lambda\\), for different methods:\n\n\n\n\n\nno transformation via \\(\\lambda = 1.0\\)\nsquare (\\(x^2\\)) via \\(\\lambda = 2.0\\)\nsquare root (\\(\\sqrt{x}\\)) via \\(\\lambda = 0.5\\)\n\n\n\nlogarithmic (\\(\\log{x}\\)) via \\(\\lambda = 0.0\\)\ninverse square root (\\(1/\\sqrt{x}\\)) via \\(\\lambda = -0.5\\)\ninverse (\\(1/x\\)) via \\(\\lambda = -1.0\\)\n\n\n\n\n\nand others in between. The transformed version of the variable is:\n\\[\nx^* =\n\\begin{cases} \\lambda^{-1}(x^\\lambda-1) & \\text{if $\\lambda \\ne 0$,}\n\\\\[3pt]\nlog(x) &\\text{if $\\lambda = 0$.}\n\\end{cases}\n\\]\nTheir paper defines this as a supervised transformation of a non-negative outcome (\\(y\\)) in a linear regression model. They find a value of \\(\\lambda\\) that minimizes the residual sums of squared errors. In our case, we can co-opt this method to use for unsupervised transformations of non-negative predictors (in a similar manner as Asar, Ilk, and Dag (2017)). Yeo and Johnson (2000) extend this method by allowing the data to be negative via a slightly different transformation:\n\\[\nx^* =\n\\begin{cases}\n\\lambda^{-1}\\left[(x + 1)^\\lambda-1\\right] & \\text{if $\\lambda \\ne 0$ and $x \\ge 0$,} \\\\[3pt]\nlog(x + 1) &\\text{if $\\lambda = 0$ and $x \\ge 0$.} \\\\[3pt]\n-(2 - \\lambda)^{-1}\\left[(-x + 1)^{2 - \\lambda}-1\\right] & \\text{if $\\lambda \\ne 2$ and $x &lt; 0$,} \\\\[3pt]\n-log(-x + 1) &\\text{if $\\lambda = 2$ and $x &lt; 0$.}\n\\end{cases}\n\\]\nIn either case, maximum likelihood is also used to estimate the \\(\\lambda\\) parameter.\nIn practice, these two transformations might be limited to predictors with acceptable density. For example, the transformation may not be appropriate for a predictor with a few unique values. A threshold of five or so unique values might be a proper rule of thumb (see the discussion in Section 13.4). On occasion the maximum likelihood estimates of \\(\\lambda\\) diverge to huge values; it is also sensible to use values within a suitable range. Also, the estimate will never be absolute zero. Implementations usually apply a log transformation when the \\(\\hat{\\lambda}\\) is within some range of zero (say between \\(\\pm 0.01\\))2.\nFor the lot area predictor, the Box-Cox and Yeo-Johnson techniques both produce an estimate of \\(\\hat{\\lambda} = 0.15\\). The results are shown in Figure 5.1 (panel b). There is undoubtedly less right-skew, and the data are more symmetric with a new skewness value of 0.114 (much closer to zero). However, there are still outlying points.\nThere are numerous other transformations that attempt to make the distribution of a variable more Gaussian. Table 5.1 shows several more, most of which are indexed by a transformation parameter \\(\\lambda\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nEquation\nSource\n\n\n\n\nModulus\n\\[x^* = \\begin{cases} sign(x)\\lambda^{-1}\\left[(|x|+1)^\\lambda-1\\right] & \\text{if $\\lambda \\neq 0$,}\\\\[3pt]\nsign(x) \\log{(|x|+1)} &\\text{if $\\lambda = 0$}\n\\end{cases}\\]\nJohn and Draper (1980)\n\n\nBickel-Docksum\n\\[x^* = \\lambda^{-1}\\left[sign(x)|x| - 1\\right]\\quad\\text{if $\\lambda \\neq 0$}\\]\nBickel and Doksum (1981)\n\n\nGlog / Gpower\n\\[x^* = \\begin{cases} \\lambda^{-1}\\left[({x+ \\sqrt{x^2+1}})^\\lambda-1\\right]  & \\text{if $\\lambda \\neq 0$,}\\\\[3pt]\n\\log({x+ \\sqrt{x^2+1}}) &\\text{if $\\lambda = 0$}\n\\end{cases}\\]\nDurbin et al. (2002), Kelmansky, Martínez, and Leiva (2013)\n\n\nNeglog\n\\[x^* = sign(x) \\log{(|x|+1)}\\]\nWhittaker, Whitehead, and Somers (2005)\n\n\nDual\n\\[x^* = (2\\lambda)^{-1}\\left[x^\\lambda - x^{-\\lambda}\\right]\\quad\\text{if $\\lambda \\neq 0$}\\]\nYang (2006)\n\n\n\n\n\nTable 5.1: Examples of other families of transformations for dense numeric predictors.\n\n\n\n\n\n\n\nSkewness can also be resolved using techniques related to distributional percentiles. A percentile is a value with a specific proportion of data below it. For example, for the original lot area data, the 0.1 percentile is 4,726 square feet, which means that 10% of the training set has lot areas less than 4,726 square feet. The minimum, median, and maximum are the 0, 50th and 100th percentiles, respectively.\nNumeric predictors can be converted to their percentiles, and these data, inherently between zero and one, are used in their place. Probability theory tells us that the distribution of the percentiles should resemble a uniform distribution. This results from the transformed version of the lot area shown in Figure 5.1 (panel c). For new data, values beyond the range of the original predictor data can be truncated to values of zero or one, as appropriate.\nAdditionally, the original predictor data can be coerced to a specific probability distribution. Peterson and Cavanaugh (2020) define the Ordered Quantile (ORQ) normalization procedure. It estimates a transformation of the data to emulate the true normalizing function where “normalization” literally maps the data to a standard normal distribution. In other words, we can coerce the original distribution to a near exact replica of a standard normal. Figure 5.1 (panel d) illustrates the result for the lot area. In this instance, the resulting distribution is precisely what would be seen if the true distribution was Gaussian with zero mean and a standard deviation of one.\nIn Section 5.4 below, another tool for attenuating outliers in groups of predictors is discussed.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-common-scale",
    "href": "chapters/numeric-predictors.html#sec-common-scale",
    "title": "5  Transforming Numeric Predictors",
    "section": "5.3 Standardizing to a Common Scale",
    "text": "5.3 Standardizing to a Common Scale\nAnother goal for transforming individual predictors is to convert them to a common scale. This is a pre-processing requirement for some models. For example, a K-nearest neighbors model computes the distances between data points. Suppose Euclidean distance is used with the Ames data. One predictor, the year a house was built, has training set values ranging between 1872 and 2010. Another, the number of bathrooms, ranges from 0 to 5. If these raw data were used to compute the distance, the value would be inappropriately dominated by the year variable simply because its values were large. See TODO appendix for a summary of which models require a common scale.\nThe previous section discussed two transformations that automatically convert predictors to a common distribution. The percentile transformation generates values roughly uniformly distributed on the [0, 1] scale, and the ORQ transformation results in predictors with standard normal distributions. However, two other standardization methods are commonly used.\nFirst is centering and scaling (as previously mentioned). To convert to a common scale, the mean (\\(\\bar{x}\\)) and standard deviation (\\(\\hat{s}\\)) are computed from the training data and the standardized version of the data are \\(x^* = (x - \\bar{x}) / \\hat{s}\\). The shape of the original distribution is preserved; only the location and scale are modified to be zero and one, respectively.\nIn Section 6.2, methods are discussed to convert categorical predictors to a numeric format. The standard tool is to create a set of columns consisting of zeros and ones called indicator or dummy variables. When centering and scaling, what should we do with these binary features? These should be treated the same as the dense numeric predictors. The result is that a binary column will still have two unique values, one positive and one negative. The values will depend on the prevalence of the zeros and ones in the training data. While this seems awkward, it is required to ensure each predictor has the same mean and standard deviation. Note that if the predictor set is only scaled, Gelman (2008) suggests that the indicator variables be divided by two standard deviations instead of one.\nFigure 5.2(b) shows the results of centering and scaling the gross living area predictor from the Ames data. Note that the shape of the distribution does not change; only the magnitude of the values is different.\n\n\n\n\n\n\n\n\nFigure 5.2: The original gross living area data and two standardized versions.\n\n\n\n\n\nAnother common approach is range standardization. Based on the training set, a predictor’s minimum and maximum values are computed, and the data are transformed to a [0, 1] scale via\n\\[\nx^* = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\]\nWhen new data are outside the training set range, they can either be clipped to zero/one or allowed to go slightly beyond the intended range. The nice feature of this approach is that the range of the raw numeric predictors matches the range of any indicator variables created from previously categorical predictors. However, this does not imply that the distributional properties are the same (e.g., mean and variance) across predictors. Whether this is an issue depends on the model being used downstream. Figure 5.2(c) shows the result when the gross living predictor is range transformed. Notice that the shape of the distributions across panels (a), (b), and (c) are the same — only the scale of the x-axis changes.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-spatial-sign",
    "href": "chapters/numeric-predictors.html#sec-spatial-sign",
    "title": "5  Transforming Numeric Predictors",
    "section": "5.4 Spatial Sign",
    "text": "5.4 Spatial Sign\nSome transformations involve multiple predictors. An upcoming chapter describes a specific class of simultaneous feature extraction transformations. Here, we will focus on the spatial sign transformation (Serneels, Nolf, and Espen 2006). This method, which requires \\(p\\) standardized predictors as inputs, projects the data points onto a \\(p\\) dimensional unit hypersphere. This makes all of the data points equally distant from the center of the hypersphere, thereby eliminating all potential outliers. The equation is:\n\\[\nx^*_{ij}=\\frac{x_{ij}}{\\sum\\limits^{p}_{j=1} x_{ij}^2}\n\\]\nNotice that all of the predictors are simultaneously modified and that the calculations occur in a row-wise pattern. Because of this, the individual predictor columns become combinations of the other columns and now reflect more than the individual contribution of the original predictors. In other words, after this transformation is applied, if any individual predictor is considered important, its significance should be attributed to all of the predictors used in the transformation.\nFigure 5.3 shows predictors from the Ames data. In these data, we somewhat arbitrarily labeled 29 samples as being “far away” from most of the data in either lot area and/or gross living area. Each of these predictors may follow a right-skewed distribution, or there is some other characteristic that is associated with these samples. Regardless, we would like to transform these predictors simultaneously.\nThe second panel of the data shows the same predictors after an orderNorm transformation. Note that, after this operation, the outlying values appear less extreme.\n\n\n\n\n\n\n\n\nFigure 5.3: Lot area (x) versus gross living area (y) in raw format as well as with order-norm and spatial sign transformations.\n\n\n\n\n\nThe panel on the right shows the data after applying the spatial sign. The data now form a circle centered at (0, 0) where the previously flagged instances are no longer distributionally abnormal. The resulting bivariate distribution is quite jarring when compared to the original. However, these new versions of the predictors can still be important components in a machine-learning model.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#chapter-references",
    "href": "chapters/numeric-predictors.html#chapter-references",
    "title": "5  Transforming Numeric Predictors",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAsar, O, O Ilk, and O Dag. 2017. “Estimating Box-Cox Power Transformation Parameter via Goodness-of-Fit Tests.” Communications in Statistics-Simulation and Computation 46 (1): 91–105.\n\n\nBickel, P, and K Doksum. 1981. “An Analysis of Transformations Revisited.” Journal of the American Statistical Association 76 (374): 296–311.\n\n\nBox, GEP, and D Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society. Series B (Methodological), 211–52.\n\n\nCroarkin, C, P Tobias, J Filliben, B Hembree, W Guthrie, L Trutna, and J Prins, eds. 2012. NIST/SEMATECH e-Handbook of Statistical Methods. NIST/SEMATECH. http://www.itl.nist.gov/div898/handbook/.\n\n\nDurbin, B, J Hardin, D Hawkins, and D Rocke. 2002. “A Variance-Stabilizing Transformation for Gene-Expression Microarray Data.” Bioinformatics 18.\n\n\nGelman, A. 2008. “Scaling Regression Inputs by Dividing by Two Standard Deviations.” Statistics in Medicine 27 (15): 2865–73.\n\n\nJohn, J, and N Draper. 1980. “An Alternative Family of Transformations.” Journal of the Royal Statistical Society Series C: Applied Statistics 29 (2): 190–97.\n\n\nKelmansky, D, E Martínez, and V Leiva. 2013. “A New Variance Stabilizing Transformation for Gene Expression Data Analysis.” Statistical Applications in Genetics and Molecular Biology 12 (6): 653–66.\n\n\nPeterson, R, and J Cavanaugh. 2020. “Ordered Quantile Normalization: A Semiparametric Transformation Built for the Cross-Validation Era.” Journal of Applied Statistics 47 (13-15): 2312–27.\n\n\nSerneels, S, E De Nolf, and P Van Espen. 2006. “Spatial Sign Preprocessing: A Simple Way to Impart Moderate Robustness to Multivariate Estimators.” Journal of Chemical Information and Modeling 46 (3): 1402–9.\n\n\nWhittaker, J, C Whitehead, and M Somers. 2005. “The Neglog Transformation and Quantile Regression for the Analysis of a Large Credit Scoring Database.” Journal of the Royal Statistical Society Series C: Applied Statistics 54 (5): 863–78.\n\n\nYang, Z. 2006. “A Modified Family of Power Transformations.” Economics Letters 92 (1): 14–19.\n\n\nYeo, I, and R Johnson. 2000. “A New Family of Power Transformations to Improve Normality or Symmetry.” Biometrika 87 (4): 954–59.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#footnotes",
    "href": "chapters/numeric-predictors.html#footnotes",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "The field of robust techniques is predicated on making statistical calculations insensitive to these types of data points.↩︎\nIf you’ve never seen it, the “hat” notation (e.g. \\(\\hat{\\lambda}\\)) indicates an estimate of some unknown parameter.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html",
    "href": "chapters/categorical-predictors.html",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "6.1 Example Data: Daily Hotel Rates\nThis chapter will discuss tools for handling predictor variables that are not measured on a strict numeric scale. For example, the building type and neighborhood variables are qualitative in the Ames housing data. The possible values of qualitative predictors are often called categories or levels (which will be used interchangeably here).\nWhy would we need to modify this type of predictor? First, most mathematical models require information to be numbers. We’ll need a method to convert qualitative data effectively to some quantitative format. This is an encoding problem; we are finding another representation of our predictor data. There are various tools for converting qualitative data, ranging from simple indicator variables (a.k.a. dummy variables) to more statistically sophisticated tools. These are the focus of this chapter.\nSecond, there are other circumstances where simplifying or transforming categorical predictors might be advantageous. These are usually related to exceptional situations, such as when there are many possible values (i.e., high cardinality) or infrequently occurring values. Techniques to address these issues are also discussed.\nAs implied above, some modeling techniques can take the categorical predictors in their natural form. These include tree- or rule-based models, naive Bayes models, and others. In this book’s third and fourth parts, we’ll discuss how certain models naturally handle categorical variables in their respective sections.\nTo begin, the primary data set used in this chapter is introduced.\nAntonio, de Almeida, and Nunes (2019) describe a data set with booking records from two Portuguese hotels. There are numerous variables in the data related to the\nIn addition, there are several qualitative variables: customer type, market segment, room type, etc.\nWe’ll analyze these data in various ways (especially in ?sec-reg-linear to ?sec-reg-summary where they are used as a case study). Our goal is to predict the average daily rate (ADR, i.e., the average cost in euros) from one of the two hotels in the original data.\nThere were 15,402 bookings for the resort hotel. An initial two-way split was used:\nThis chapter will use these data to demonstrate how qualitative data, such as the booking agent’s name or company, can be converted into quantitative information that many models require.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-hotel-rates",
    "href": "chapters/categorical-predictors.html#sec-hotel-rates",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "Reservation: room type, date, meal plan, etc.,\nBooking method: agent, distribution channel, etc.,\nCustomer: type, number of children staying, country of origin, etc.\n\n\n\n\n\nThe training set contains 11,551 (75%) bookings whose arrivals are between 2016-07-02 and 2017-05-15.\nThe remaining 25% were used as the test set with 3,851 bookings (between 2017-05-15 to 2017-08-31).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-indicators",
    "href": "chapters/categorical-predictors.html#sec-indicators",
    "title": "6  Working with Categorical Predictors",
    "section": "6.2 Simple Indicator Variables",
    "text": "6.2 Simple Indicator Variables\nAs a simple example, consider the customer type predictor with categories: “contract”, “group”, “transient”, and “transient (party)”. What would we do if a model required these data to be numeric? There is no natural ordering to these values, so giving the models integer values (e.g., contract = 1.0, group = 2.0, etc.) would be erroneous since that implies a scale for these values. Instead, it is common to create multiple binary columns (numeric with zeros or ones) representing the occurrence of specific values. These are called indicator columns or sometimes dummy variables.\nTable 6.1 shows how this works for the customer type. The rows depict the possible values in the data, while the columns are the resulting features used in place of the original column. This table uses the most common indicator encoding method called reference cell parameterization (also called a treatment contrast). First, a category is chosen as the reference value. In Table 6.1, the first alpha-numeric value is used (\"contract\"), but this is an arbitrary choice. After this, we create separate columns for all possible values except for the reference value. Each of these columns has a value of one when the data matches the column for that value (and is zero otherwise).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer type\n\nIndicator Columns\n\n\n\ngroup\ntransient\ntransient party\n\n\n\n\ncontract\n0\n0\n0\n\n\ngroup\n1\n0\n0\n\n\ntransient\n0\n1\n0\n\n\ntransient party\n0\n0\n1\n\n\n\n\n\n\n\n\nTable 6.1: Indicator columns produced from a categorical column using a reference cell parameterization.\n\n\n\n\nThe rationale for excluding one column is that you can infer the reference value if you know the values of all of the existing indicator columns1. Including all possible indicator columns embeds a redundancy in the data. As we will see shortly, data that contain this type of redundancy pose problems for some models like linear regression.\nBefore moving on, let’s examine how indicator columns shown in Table 6.1 affect an example data analysis. Suppose that we are modeling a numeric outcome using a linear regression model (discussed in ?sec-reg-linear) of the form:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_4x_{i4} + \\epsilon_i\n\\tag{6.1}\\]\nwhere the \\(\\beta\\) values are the parameters to be estimated and, in this application, the \\(x_{ij}\\) are the indicator columns shown in Table 6.1. The intercept term (\\(\\beta_0\\)) corresponds to a column of all ones. It is almost always the case that we include the intercept term in the model.\nTable 6.2 shows the encoding in Table 6.1 and adds columns for a numeric outcome and the intercept term. The outcome column shows the average daily rate (in €). Using a standard estimation procedure (called ordinary least squares), the bottom row of Table 6.2 shows the 4 parameter estimates. Since all of the indicators for the contract customer row are zero, the intercept column estimates the mean value for that level (\\(\\widehat{\\beta}_0\\) = 73). The variable \\(x_{i1}\\) only has an indicator for the “group” customers. Hence, its estimate corresponds to the difference in the average group outcome values (63.8) minus the effect of the reference cell: 73 - 63.8. From this, the resulting estimate (\\(\\widehat{\\beta}_1\\) = -9.23) is the effect of the group customers above and beyond the impact of the contract customers. The parameter estimates for the other possible values follow analogous interpretations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADR\n\nModel Terms\n\n\n\n(Intercept)\ngroup\ntransient\ntransient_party\n\n\n\n\ncontract\n73.0\n1\n0\n0\n0\n\n\ngroup\n63.8\n1\n1\n0\n0\n\n\ntransient\n94.6\n1\n0\n1\n0\n\n\ntransient party\n76.4\n1\n0\n0\n1\n\n\nestimate (€):\n\n\n73.0\n-9.2\n21.6\n3.4\n\n\n\n\n\n\n\n\nTable 6.2: An example of linear regression parameter estimates corresponding to a reference cell parameterization.\n\n\n\n\nAnother popular method for making indicator variables is called one-hot encoding (also known as a cell means encoding). This technique, shown in Table 6.3, makes indicators for all possible levels of the predictor and does not show an intercept column (for reasons described shortly). In this model parameterization, indicators are specific to each value in the data, and the linear regression estimates are the average response values for each customer type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADR\n\nModel Terms\n\n\n\ncontract\ngroup\ntransient\ntransient_party\n\n\n\n\ncontract\n73.0\n1\n0\n0\n0\n\n\ngroup\n63.8\n0\n1\n0\n0\n\n\ntransient\n94.6\n0\n0\n1\n0\n\n\ntransient party\n76.4\n0\n0\n0\n1\n\n\nestimate (€):\n\n\n73.0\n63.8\n94.6\n76.4\n\n\n\n\n\n\n\n\nTable 6.3: One-hot encoded indicator variables from a categorical column of data.\n\n\n\n\nOne-hot encodings are often used in nonlinear models, especially in neural networks and tree-based models. Indicators are not generally required for the latter but can be used2.\nOne issue with creating indicator variables from categorical predictors is the potential for linear dependencies. This occurs when two or more sets of predictors have a mathematically linear relationship with one another. For example, suppose that each agent only worked with a single customer type. If we were to create indicator variables for the agent names and the customer type, there would be a redundancy: if we know the agent, we know the customer type. Mathematically, the row-wise sum of the agent name indicators would equal the row-wise sum of the position indicators. For some types of predictive models, including linear or logistic regression, the linear dependencies would cause the mathematical estimation of the model parameters to fail.\nAn example of this is the one-hot encoding method shown in Table 6.3. If we were to include an intercept term with a one-hot encoding, the row-wise sum of the indicator columns equals the intercept column. The implication is that one of the model terms is not “estimable.” For this reason, this encoding method is mainly used with models that do not use specific linear algebra operations (e.g., matrix inversion) to estimate parameters. Some tools can identify linear combinations (Elswick et al. 1991), and from this information, the smallest possible set of columns can be found to delete and thereby eliminate the linear dependencies.\nNote that linear dependencies are exact linear relationships between predictors. There are other circumstances where two or more predictors are highly correlated but not perfectly related. This situation, called multicollinearity, is discussed throughout the book but mostly in ?sec-colinearity. Multicollinearity can be a problem for many models and needs to be resolved prior to modeling.\nIn addition to reference cell and one-hot encodings, there are various tools for creating indicator columns. Some, called contrast methods, are associated with more niche strategies, and many produce fractional values instead of binary indicators. For example, Helmert contrasts (Ruberg 1989) are configured so that each category is compared to the average of the preceding categories. No encoding method is uniformly better than the others; the choice is often driven by how the user will interpret the parameter estimates.\nThe situation becomes slightly more complex when a model contains multiple categorical predictors. The reference value combines levels across predictors for reference value parameterizations. For example, if a model included the customer’s country of residence, the reference would include that factor. Suppose Canadians were the reference nationality. Then the overall reference cell would be Canadian contract customers.\nNote that these encoding techniques are basically look-up tables that map individual categories known in the training set to specific numeric values. As discussed in the next section, mapping categories can be problematic when novel values are seen in new data (such as the test set).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-novel-categories",
    "href": "chapters/categorical-predictors.html#sec-novel-categories",
    "title": "6  Working with Categorical Predictors",
    "section": "6.3 Novel Categories",
    "text": "6.3 Novel Categories\nWhen creating indicator variables, there is the assumption that the categories in the training data reflect all possible categories. This may not be the case. For example, the agents observed in our data are not the only agents that will be booking rooms in the future. After we create a model with a specific set of agents, what happens when we predict future data that involves new agents?\nOne approach we could take is to prospectively create a new value for the predictor (let’s give it a value of \"new\"). When a future sample has an agent that was not in the original data, we could assign this sample to the general category so that the model can anticipate this new level. In general, this won’t help since the model estimation process will never have any “new” values on hand when the model is fit. For some models, this is innocuous and has no real harm (but offers no benefit either).\nLet’s revisit linear regression results from the reference cell parameterization again. Suppose there is a new customer type of “influencer.” Our reference cell encoding method could add an indicator column for that customer type, but since no influencers currently exist in the training data, it will be a column filled with zeros. We can call this a zero-variance predictor since it has a single unique value. It is entirely non-informative. For linear regression, this column of zeros creates a linear dependency with the intercept column because all rows have the same numeric value. Most regression software would detect zero-variance columns and remove them before the model is fit.\nSince there would be no influencer column in the data, all of the indicator columns would be zero when an influencer was observed in new data being predicted. However, from Table 6.2 we know that such a pattern would map to the reference level (which is the \"contract\" customer in our data). The new levels would be mapped to the reference level for this parameterization. This would occur more by happenstance and would not be a deliberate strategy.\nTraditional indicator encoding methods should be avoided if new predictor categories are anticipated. Instead, the techniques used in the next section (effect encodings or feature hashing) will likely be more effective at dealing with this situation.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-high-cardinality",
    "href": "chapters/categorical-predictors.html#sec-high-cardinality",
    "title": "6  Working with Categorical Predictors",
    "section": "6.4 Predictors with Many Values",
    "text": "6.4 Predictors with Many Values\nSome predictors have high cardinality; they can take many possible values relative to the number of data points. For example, in the entirety of the hotel rate data, there are 122 unique agents (see Figure 6.1). Should we create so many indicator variables for this predictor? In the data, 49 agents had fewer than six bookings. This means there will be many sparse columns with almost all zero entries. Additionally, 71 of the agents have no data in the training set; they only occur in the test set. Their indicator columns would contain all zeros.\n\n\n\n\n\n\n\n\nFigure 6.1: The frequency distribution of agents in the training set.\n\n\n\n\n\nThis section will consider several methods for effectively representing high cardinality predictors. The first two techniques are unsupervised (i.e., they do not use the outcome), while the others are supervised. Cerda and Varoquaux (2022) surveys different methods for dealing with these types of predictors.\n\n6.4.1 “Othering”\nIt is possible to isolate infrequently occurring categories into a new “other” category. For example, there are 49 agents with fewer bookings than 0.05% of the training set. It may make sense to group these agents into their own categories as long as the predictive ability of the variable is maintained. There is no magic number for the training set frequency cutoff that decides which categories to pool. This pre-processing value requires optimization since each data set will have its own nuances with infrequent categories.\n\n\n\n6.4.2 Feature Hashing\nFeature hashing (Weinberger et al. 2009) is a technique where the user specifies how many indicator columns should be used to represent the data. Using cryptographic principles, an algorithm assigns each row to an indicator column. This assignment only uses the value of the category in the calculations and, unlike the traditional methods discussed in Section 6.2, does not require a dictionary that maps pre-defined categories to indicator columns.\nSuppose our agent column had a value of \"nobody owens\". The algorithm translates this value to a fixed-length string using a hash function. Hash functions are designed to be non-random and, effectively, irreversible. Once the original value is converted to the hash value, it is very difficult to reverse engineer. Also, the frequency distributions of the hash values are designed to appear uniform in distribution, as seen below.\nThe hashed value of the predictor’s level can be converted to an integer, and, using modular arithmetic, the integer is mapped to a feature column. Suppose that we create 256 hashing features and nobody owens maps to an integer of 123,456. To assign this agent to one of 256 feature columns, we compute 123456 mod 256 = 64. For this row of data, we put a value of one in column 64.\nPresumably, the number of possible categories is larger than the number of indicator columns we request. This means multiple predictor values will map to the same feature column. In cryptography, this is called a collision, the same as aliasing in statistical experimental design (Box, Hunter, and Hunter 1978). There is a trade-off between the number of feature columns and the number of collisions. As the number of columns increases, the probability of collisions decreases. However, the time required to train a model increases as the number of columns increases. Also, it is entirely possible that there are predictor columns where no corresponding categories, yielding all zeros.\nTo reduce collisions, the hashing function can produce integers that can be both positive and negative. When assigning the integer to a feature column, we can assign a value of -1 when the hashed integer value is negative. In this way, columns can have values of -1, 0, or 1. This means fewer collisions.\nTable 6.4 shows the results for several agents3 when ten signed feature hash columns are requested. The last row of the table shows the rate of non-zero values in each column. The average density is close to 10%, demonstrating that the hash function produces frequencies that emulate a uniform distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgent\n\nFeatures from Hashing\n\n\n\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n\n\n\n\nAaron Marquez\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n\n\nAlexander Drake\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n\n\nAllen Her\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n\n\nAnas El Bashir\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n\n\n:\n:\n:\n:\n:\n:\n:\n:\n:\n:\n:\n\n\nYa Eesh El Sadiq\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nYoonus Al Sultana\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nYoujia Tsuchiya\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nZachary Leedholm\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nDensity (%)\n10.7\n13.1\n7.4\n13.1\n7.4\n6.6\n9.8\n10.7\n9.8\n11.5\n\n\n\n\n\n\n\n\nTable 6.4: Signed indicators for agent via feature hashing.\n\n\n\n\nThe main downside of this method is that the use of hash values makes it impossible to explain the model. If the tenth feature column is critical, we can’t explain why this is the case for new data (since the hash function is practically non-reversible and may include collisions). This may be fine if the primary objective is prediction rather than interpretation. When the goal is to optimize predictive performance, then the number of hashing columns to use can be included as a tuning parameter. The model tuning process can then determine an optimal value of the number of hashing columns.\nOne unanticipated side effect of using feature hashing is that it might remove linear dependencies with other columns. This is because hashing distributes the predictor levels to hashing columns roughly uniformly. There is a high probability that this eliminates any existing relationship between other predictor values.\n\n\n\n6.4.3 Effect Encodings\nWe can also use a different supervised tool called effect encoding4 (Micci-Barreca 2001; Pargent et al. 2022). Recall the discussion of one-hot encodings for indicator variables. Table 6.3 showed that the model terms for such an encoding estimate the cell means for each category (when the outcome is numeric). For example, the effect on the outcome for contract customers in that table was 73€.\nEffect encodings use a simple, separate model to estimate the effect of each predictor category on the outcome and then use that value in place of the original data. For the example in Table 6.3, the rows corresponding to the group customers would be replaced with 63.8, transient customers with 94.6, and so on. Instead of making multiple binary columns out of the categorical predictor, a single numeric column would take its place.\nThis may seem somewhat problematic though; we are using the training data to estimate the effect of a predictor, putting the resulting effects back into the data, and using our primary model to (re)estimate the effect of the predictor. Since the same data are used, is this a circular argument? Also, there should be a worry about data quality. As previously mentioned, the number of bookings per agent can vary greatly. For example:\n\nAgent Chelsea Wilson had a single booking with an associated ADR of 193.6€. Using this single data point as the effect estimate is unsatisfactory since we suspect that the estimated ADR would not stay the same if the number of total bookings were larger5.\nAgent Alexander Drake had 967 bookings with a simple mean ADR estimate of 120.3€. We would judge the quality of this agent’s data to be better than cases with a handful of bookings.\n\nHow well can we estimate an effect if there are only one or two bookings for an agent in the data set?\nBoth of these concerns can be addressed by how we estimate the effect of the predictor. Recall that Table 6.3 showed how simple linear regression was used to calculate the mean for each customer type. The way that the indicator variables impact the parameter estimates is such that the rows for each customer type are the only data used to estimate that customer type’s effect. For example, the contract data do not directly affect the parameter estimate for the transient customers, and so on. This is often called a no pooling method of estimating parameters. In this context, “pooling” means combining data across multiple categories to estimate parameters.\nWe could compute the naive ADR estimate using sample means and produce a similar table. However, we can create better mean estimates for each agent by considering the data from all agents using a technique called partial pooling (Gelman et al. 2013; Johnson, Ott, and Dogucu 2022, chap. 15).\n\nIn the following two sub-sections, we’ll describe this technique in detail for cases where the outcome is a numeric value assumed to be normally distributed or or non-Gaussian. These sections will rely more on probability theory and statistical details than most others.\n\n\nNumeric Outcomes\nFor our ADR analysis, the outcome is a quantitative value. A linear regression model (see ?sec-reg-linear) is the most commonly used approach to model this type of outcome. Linear regression models the outcome as a function of a slope and intercept model. For our example, where we only consider estimating the effect of the agents using indicator columns, the model is:\n\\[\ny_{ij} =\\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_{121}x_{i 121} + \\epsilon_{ij} = \\mu_i  + \\epsilon_{ij}\n\\]\nwhere \\(y_{ij}\\) is the outcome (in this case, the ADR of booking \\(i\\) by agent \\(j\\)), \\(x_{ij}\\) is an indicator for reservation \\(i\\) from agent \\(j\\), \\(\\beta_0\\) is the estimated outcome of the reference cell, and we assume \\(\\epsilon_i \\sim N(0, \\sigma^2_\\epsilon)\\). This leads to the assumption that \\(y_{ij} \\sim N(\\mu_i, \\sigma^2_\\epsilon)\\). The \\(\\hat{\\beta}_{j}\\) parameters are the effect of the agents above and beyond the reference cell effect. The index \\(j\\) is from 1 to 121 since there were 122 agents in the training set with at least one booking.\nAgain, this approach is called no pooling since it only uses each agent’s data to estimate their effect. It treats the agent as a known set of values and cannot generalize to agents outside of those in the training set6.\nPartial pooling takes the viewpoint that the agents in the training set are a sample of a population of agents. From this, we might posit a probability distribution should be assigned to the agent-to-agent effects. Since the range of the \\(\\beta\\) parameters is the real line, we might propose a random intercept model:\n\\[\ny_{ij} = { \\color{darkseagreen} \\underbrace{\\color{black} (\\beta_0 + \\beta_{0j})}_{\\text{intercept(s)}} }  + \\epsilon_{ij} = \\mu_j + \\epsilon_{ij}\n\\tag{6.2}\\]\nwhere the \\(\\beta_{0j}\\) are per-agent differences from the population effect (\\(\\beta_{0}\\)) (\\(j = 1 \\ldots 122\\) this time). Note that the model has no \\(x_{ij}\\) terms. This is simply a notation difference between the models since most methods that estimate this model do not treat that variable as a traditional regression covariate.\nSeveral techniques can fit this model. For a linear mixed model approach to fitting a random intercept model (Laird and Ware 1982; Demidenko 2013), we assume that the model parameters have normal distributions with \\(\\beta_{0} \\sim N(\\beta, \\sigma^2)\\) and \\(\\beta_{0j} \\sim N(0, \\sigma_0^2)\\) and that residuals (\\(\\epsilon_{ij}\\)) follow a Gaussian distribution. When used in Bayesian analysis7, such distributions on parameters are referred to as prior distributions. We’ll stick with this terminology for Bayesian and non-Bayesian techniques. There are many strategies for estimating this model type, including maximum likelihood, restricted maximum likelihood, or Bayesian methods. See Stroup (2012) or Gelman et al. (2013) for more specifics.\nHowever, we might gain some intuition if we pause and consider a less complex version of this model. For simplicity, we’ll also make the completely unrealistic assumptions that all agents had the same number of bookings (\\(n_j = n\\)) and that we know that \\(\\sigma^2_\\epsilon = 1\\). From this, we can also say that sample means of the outcome data for each category are also normal with \\(\\bar{y}_{j} \\sim N(\\mu_i, 1/n)\\). These assumptions greatly simplify the math required to produce a formula for the estimates of the per-category means:\n\\[\n\\hat{y}_j = \\mu_0 + (\\bar{y}_j - \\mu_0) \\frac{\\sigma^2_0}{\\frac{1}{n} + \\sigma^2_0}\n\\tag{6.3}\\]\nThere are a few aspects of this equation8 to note. First, the effect for a specific category is estimated by a combination of the sample mean for each category (\\(\\bar{y}_j\\)) as well as our prior mean (\\(\\mu_0\\)). The difference demonstrates the possible shrinkage towards the prior mean.\nAlso, the shrinkage coefficient depends on \\(\\sigma^2_0\\) and the amount of data in the category. Suppose we choose a non-informative prior by making it extremely wide9 via \\(\\sigma^2_0 = 100\\). No matter how much data are observed in the category, the fraction will be close to 1, and our partial pooling estimate is essentially \\(\\bar{y}_j\\). If we had a highly opinionated prior10, \\(\\sigma^2_0 = 1\\), the weighting factor becomes \\(n / (n + 1)\\). For \\(n = 1\\), the partial pooling estimate has an even share of the sample mean and the prior mean. The effect of the prior mean decreases as \\(n\\) increases.\nThese example computations assumed that each category has the same number of rows in the training set (i.e., \\(n_j = n\\)). In practice, this is unrealistic. However, the insights into how the sample size changes the amount of shrinkage demonstrate the previous idea of how “poor data” can affect the estimates with partial pooling.\nWhile Equation 6.3 helps illustrate how partial pooling works, we should understand that the specific results hinge on knowing \\(\\sigma^2_\\epsilon = 1\\). We would seldom make such a critical assumption in practice.\nWe’ll use the non-Bayesian mixed model approach to estimate effects for our data since it is more computationally efficient. Using the training set data, maximum likelihood estimation produced \\(\\hat{\\beta}_0\\) = 76.9. Figure 6.2(a) shows the distribution of the 122 random intercept estimates \\(\\hat{\\beta}_{0j}\\). The estimated standard deviation of this distribution was \\(\\hat{\\sigma_0}\\) = 22.9€. Even though we assume that the prior distribution of the individual effects was symmetric, the histogram of the estimates shows a lack of symmetry. This may be due to a few outliers or indicate that our prior should not be symmetric.\n\n\n\n\n\n\n\n\nFigure 6.2: The effect encoding results for the hotel rate data. Panel (a) shows the distribution of the estimated per-agent effects. Panel (b) compares the two estimation methods for all unique data patterns in the training set.\n\n\n\n\n\nThe benefit of using this model, called a hierarchical model, is that it views the per-agent effects as a collection of values from a distribution. The mathematical consequence of this formulation is that the resulting parameter estimates use the data from all agents to estimate each agent’s estimates (hence the use of the phrase “partial pooling”). The intuition for that statistical method applies here: the estimates for agents with “poor data quality” have their results shrunken towards the overall population mean across all agents. Agents with “better” data are shrunken less during estimation. What does “poor data quality” mean? It primarily means high variance, which is usually a function of sample size and some baseline, the irreducible noise level in the system.\nRecall the previous discussion regarding agents Chelsea Wilson and Alexander Drake. Using the partial pooling approach:\n\nAgent Chelsea Wilson: partial pooling shrinks the single value (193.6€) back towards the estimated population mean (76.9€) to produce a more modest estimated effect of 95.3€. Because there is a single value for this agent, the prior distribution greatly influences the final estimate.\nAgent Alexander Drake: partial pooling only slightly adjusts the original sample mean (120.3€) to the final estimate (120.1€) since there are more data available. The prior has almost no influence on this estimate since the agent had 967 bookings in the training set.\n\nThe amount of shrinkage was driven mainly by the number of bookings per agent. Figure 6.2(b) shows the collection of effect estimates for each unique data pattern in the training set. The dotted line indicates the overall ADR sample mean, and the size of the points indicates the total number of bookings by an agent. The distribution of the shrunken effects (y-axis) is significantly attenuated compared to the no pooling effects estimates (x-axis). However, note that the points with large numbers of bookings line up along the diagonal green line (i.e., minimal shrinkage to the center); regardless of the value of their agent-specific ADR estimate (\\(\\bar{y}_j\\)).\nTo reiterate how these values are used for pre-processing this type of predictor, Table 6.5 shows the linear mixed model analysis results. The numeric column is our primary model’s data for representing the agent names. This avoids creating a large number of indicator variables for this predictor.\n\n\n\n\n\n\n\n\n\n\n\nAgent\nEffect Estimate (€)\n\n\n\n\nAaron Marquez\n92.2\n\n\nAlexander Drake\n120.1\n\n\nAllen Her\n73.7\n\n\n:\n:\n\n\nYoonus Al Sultana\n58.8\n\n\nYoujia Tsuchiya\n74.8\n\n\nZachary Leedholm\n85.6\n\n\n\n\n\n\n\n\nTable 6.5: Examples of the numeric values that are used in place of each agent’s data when effect encodings are used.\n\n\n\n\nAs an alternative to generalized linear mixed models to estimate the effects, a more general approach uses Bayesian analysis (Kruschke 2014; McElreath 2015). Here, we give all of the model parameters prior probability distributions, which reflect our prior knowledge of the parameters’ shape and range. “Priors” can be pretty vague or highly opinionated, depending on what we feel we know about the situation11. For example, to continue with the random intercept formulation, we might specify the following:\n\\[\n\\begin{align}\n\\beta_0 &\\sim N(0, \\sigma_0^2) \\notag \\\\\n\\beta_{0j} &\\sim t(2) \\notag \\\\\n\\sigma_0 &\\sim U(0, \\infty)\n\\end{align}\n\\]\nThe uniform prior in the standard deviation is incredibly vague and is considered a non-informative prior. Conversely, the t distribution (with two degrees of freedom) for the random effects reflects that we might expect that the distribution is symmetric but has heavy tails (e.g., potential outliers such as those seen in Figure 6.2(a)). Priors can be vague or highly opinionated.\nWhen estimating the Bayesian model, the prior distributions are combined with the observed data to produce a posterior distribution for each parameter. Like the previous mixed-model technique, the posterior mimics the prior when there is very little (or noisy) data. The posterior distribution favors the observed results when high-quality data are abundant. Using Bayesian methods to estimate the effects enables much more versatility in specifying the model but is often more computationally intensive.\n\n\nNon-Gaussian Outcomes\nIn many of these cases, effect encodings can be computed using Bayesian methods or via the generalized linear mixed model approach (Bolker et al. 2009; Stroup 2012) approach fitting a random intercept model.\nThe latter model has a similar right-hand side as Equation 6.2. For example, when the outcome (\\(y_{ij}\\)) is a count, the standard Poisson model would be\n\\[\n\\log(\\mu_{ij}) = \\beta_0 + \\beta_{0j}\n\\] where \\(\\mu_{ij}\\) is the theoretical effect in the original units.\nSuppose the data are binomial proportions or Bernoulli (i.e., binary 0/1 values). The canonical effects model is:\n\\[\n\\log\\left(\\frac{\\pi_{ij}}{1-\\pi_{ij}}\\right) = \\beta_0 + \\beta_{0j}\n\\] with \\(\\pi_{ij}\\) being the parameter value for the effects. This is a basic logistic regression formulation of the problem. Logistic regression is extensively discussed in Section 16.2.\nSimilar Bayesian approaches can be used. However, a completely different technique for binary outcomes is to assume again that the original data are binomial with rate parameter \\(\\pi\\) but, instead of the logistic regression formulation, assume that the actual rates come from a Beta distribution with parameters \\(\\alpha\\) and \\(\\gamma\\). With the Beta distribution as the prior, users can specify distributions with different shapes, such as uniform, bell-shaped, or “U” shaped, depending on the values chosen for \\(\\alpha\\) and \\(\\gamma\\). See Chapter 3 of Johnson, Ott, and Dogucu (2022). The Bayesian approach can be very flexible.\n\n\nConsequences of using Effect Encodings\nAs with feature hashing, effect encoding methods can naturally handle novel categories in future data. Without knowing anything about the new category, the most logical choice for this method is to assign the overall population effect (e.g., the mode of the posterior distribution) as the value.\nAs with feature hashing, effect encodings can eliminate linear dependencies between sets of indicators. Since the categorical data are being replaced with numeric summaries of the outcome data, it may be likely that there is no longer a redundancy in the data. Indicator columns reflect the identities of the predictor categories, while the effect encoding column measures the effectiveness of the categories. For example, a hypothetical scenario was mentioned where the agent was aliased with the customer type, yielding a linear dependency. Once the agent name column is converted to effect estimates, there would no longer be a relationship between the two predictors.\nLastly, applying effect encodings requires a robust data usage strategy. The risk of embedding over-fitted statistics into the model could cause problems that may not be observed until a new data set is predicted (see Chapter 9). Using resampling methods or a validation set (Chapter 10) is critical for using this tool. Also, when there is a temporal aspect to the data, we should try to ensure that historical data are not being predicted with future data (i.e., information leakage). For example, when an agent has multiple bookings, we kept the oldest data for an agent in the training set and newer bookings in the test set. Simple random sampling may result in the opposite occurring.\n\n\n\n\n6.4.4 Supervised Combining of Categories\nAnother approach for dealing with predictors with large numbers of categories is to collapse them into smaller groups that similarly affect the outcome data. For example, a cluster of agents may have similar performance characteristics (i.e., mean ADRs). If we can derive clusters of categories from the training set, a smaller group of cluster-level indicators might be a better approach than many indicator columns.\nThis approach closely mirrors the splitting process in tree-based models (discussed in Sections ?sec-cart-cls and ?sec-cart-reg), and we can use these models to find suitable groupings of predictor values. In summary, the categories are ordered by their estimated effects and then partitioned into two groups by optimizing some performance measures. For classification models, this is usually a measure of the purity of the classes in the resulting partition12. A measure of error is used for numeric outcomes, such as RMSE. Once the initial partition is made, the process repeats recursively within each division until no more splits can be made, perhaps due to insufficient data.\nFor the agent data, Figure 6.3 shows this process. The 122 agent are categorized into 12 groups. The top panel shows the ordered mean ADR values for each agent. The vertical lines indicate how the different agents are grouped at each step of the splitting process. The bottom panel shows a scaled estimate of the RMSE that uses the group-level mean ADR to predict the individual outcome results.\n\n\n\n\n\n\n\n\nFigure 6.3: The progression of using a tree-based method to collapse predictor categories into smaller sets.\n\n\n\n\n\nFor these data, there is no rational grouping or clustering of the agents (regarding their ADR) for the tree-based model to discover. The process produces increasingly granular groups, and the stopping point is somewhat arbitrary. For these reasons, it is probably not the best approach for this predictor in this particular data set.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-ordinal-encodings",
    "href": "chapters/categorical-predictors.html#sec-ordinal-encodings",
    "title": "6  Working with Categorical Predictors",
    "section": "6.5 Encodings for Ordinal Predictors",
    "text": "6.5 Encodings for Ordinal Predictors\nIn some cases, a categorical predictor will have levels with an inherent order. For example, in the Ames data, some house amenities were ranked in terms of quality with values ‘excellent’, ‘fair’, ‘good’, ‘typical’, and ‘poor’. These values do not imply a specific numerical translation (e.g., ‘excellent’ = 2 times ‘poor’). For this reason, there are different options for converting them to numeric features.\nFirst, the analysis could treat them as unordered categories and allow the model to try to determine how the values relate to the outcome. For example, if there is the belief that, as the quality of the heating system improves, the house price should increase. Using unordered categories would be agnostic to this assumption and the model would estimate whatever trend it saw in the data. This approach would essentially be treating the quality variable as a categorical variable with 5 levels.\nAnother choice is to use domain-specific knowledge to imply relationships between levels. For example, the quality values could be replaced with integers one through five. This does bake the assumed relationship into the features, and that is advantageous when the assumed pattern is correct.\nThere are also contrast methods, similar to the one discussed in Section 6.2, that can automatically create potential patterns using polynomial expansions (more on these in Section 8.2). For our quality example, there are five possible levels. Mathematically, we can create up to the number of unique levels (minus one) polynomial function. In this example, linear, quadratic, cubic, and quartic (\\(x^4\\)) polynomial functions can be used. Figure 6.4 shows these patterns using orthogonal polynomial values13.\n\n\n\n\n\n\n\n\nFigure 6.4: Polynomial contrasts for an ordered categorical predictor with five possible levels.\n\n\n\n\n\nIn many applications, higher-degree polynomials (&gt; 2) are unlikely to be effective predictors. For example, the cubic pattern in Figure 6.4 (“Feature 3”) implies that houses in Ames with ‘excellent’ quality heating systems would have lower sale prices than those with ‘fair’ or ‘poor’ systems. Utilizing higher order polynomials usually requires some specific knowledge about the problem at hand.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#chapter-references",
    "href": "chapters/categorical-predictors.html#chapter-references",
    "title": "6  Working with Categorical Predictors",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAntonio, N, A de Almeida, and L Nunes. 2019. “Hotel Booking Demand Datasets.” Data in Brief 22: 41–49.\n\n\nBolker, B, M Brooks, C Clark, S Geange, J Poulsen, H Stevens, and JS White. 2009. “Generalized Linear Mixed Models: A Practical Guide for Ecology and Evolution.” Trends in Ecology and Evolution 24 (3): 127–35.\n\n\nBox, GEP, W Hunter, and J. Hunter. 1978. Statistics for Experimenters. New York: Wiley.\n\n\nCerda, P, and G Varoquaux. 2022. “Encoding High-Cardinality String Categorical Variables.” IEEE Transactions on Knowledge and Data Engineering 34 (3): 1164–76.\n\n\nDemidenko, E. 2013. Mixed Models: Theory and Applications with R. John Wiley & Sons.\n\n\nEfron, B, and T Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.\n\n\nElswick, RK, C Gennings, V Chinchilli, and K Dawson. 1991. “A Simple Approach for Finding Estimable Functions in Linear Models.” The American Statistician 45 (1): 51–53.\n\n\nGelman, A, J Carlin, H Stern, and D Rubin. 2013. Bayesian Data Analysis. Chapman; Hall/CRC.\n\n\nJohnson, A, M Ott, and M Dogucu. 2022. Bayes Rules!: An Introduction to Applied Bayesian Modeling. Chapman; Hall/CRC.\n\n\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press.\n\n\nLaird, N, and J Ware. 1982. “Random-Effects Models for Longitudinal Data.” Biometrics, 963–74.\n\n\nMcElreath, R. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman; Hall/CRC.\n\n\nMicci-Barreca, D. 2001. “A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems.” ACM SIGKDD Explorations Newsletter 3 (1): 27–32.\n\n\nPargent, F, F Pfisterer, J Thomas, and B Bischl. 2022. “Regularized Target Encoding Outperforms Traditional Methods in Supervised Machine Learning with High Cardinality Features.” Computational Statistics, 1–22.\n\n\nRuberg, S. 1989. “Contrasts for Identifying the Minimum Effective Dose.” Journal of the American Statistical Association 84 (407): 816–22.\n\n\nStroup, W. 2012. Generalized Linear Mixed Models: Modern Concepts, Methods and Applications. CRC press.\n\n\nWeinberger, K, A Dasgupta, J Langford, A Smola, and J Attenberg. 2009. “Feature Hashing for Large Scale Multitask Learning.” In Proceedings of the 26th Annual International Conference on Machine Learning, 1113–20. ACM.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#footnotes",
    "href": "chapters/categorical-predictors.html#footnotes",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "In other words, we know that the vector of indicators (0, 0, 0) must represent the contract customers.↩︎\nThis is discussed in greater detail for one of the case studies in ?sec-reg-summary.↩︎\nThe original data uses codes to anonymize the agent names. The names shown here are randomly fabricated. The same is true of the company names.↩︎\nAlso called target encoding or likelihood encoding.↩︎\nThis demonstrates the concept of overfitting, discussed in Chapter 9 and subsequent chapters.↩︎\nIn the language of mixed effects models, the agent is a fixed effect in this parameterization.↩︎\nThere is a more general discussion of Bayesian methods in Section 12.5.↩︎\nEquation 6.3 is related to the well-known James-Stein estimator (Efron and Hastie 2016, chap. 7).↩︎\nMaking the range of the prior exceedingly wide indicates that almost any value is possible.↩︎\nIn other words, a very narrow distribution. There are other ways to create opinionated priors though. For instance, a skewed prior distribution for one of the \\(\\beta\\) regression slopes would be considered opinionated.↩︎\nThe linear mixed model approach mentioned above is often considered as an empirical Bayes approach with more restrictive options for the prior distributions. For empirical Bayes, one might say that you make your priors for your regression parameters any distribution you want, as long as they are Gaussians.↩︎\nThe notion of “purity” can mean a very tight distribution of numerical data (for regression models) or when the frequency distribution of qualitative has a very high probability for one category.↩︎\nThese features have characteristics that their values sum to zero and there is zero correlation between features.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html",
    "href": "chapters/embeddings.html",
    "title": "7  Embeddings",
    "section": "",
    "text": "7.1 Example: Predicting Barley Amounts\nWhen there are a multitude of predictors, it might be advantageous to condense them into a smaller number of artificial features. To be useful, this smaller set should represent what is essential in the original data. This process is often called feature extraction, dimension reduction, or manifold learning. We’ll use a more general term currently en vogue: embeddings. While this chapter focuses on feature extraction, embeddings can be used for other purposes, such as converting non-numeric data (e.g., text) into a more usable numeric format.\nThis chapter will examine several primary classes of embedding methods that can achieve multiple purposes. First, we’ll consider linear methods that take a numeric input matrix \\(\\boldsymbol{X}\\) that is \\(n \\times p\\) and create a different, probably smaller set of features \\(\\boldsymbol{X}^*\\) (\\(n \\times m\\)) using the transformation \\(\\boldsymbol{X}^* = \\boldsymbol{X}\\boldsymbol{Q}\\). We hope that we can find appropriate embeddings so that \\(m &lt;&lt; p\\).\nAfter describing linear methods, we will consider a different class of transformations that focuses on the distances between data points called multidimensional scaling (MDS). MDS creates a new set of \\(m\\) features that are not necessarily linear combinations of the original features but often use some of the same math as the linear techniques.\nFinally, some embedding techniques specific to classification are discussed. These are based on class centroids.\nBefore beginning, we’ll introduce another data set that will be used here and in forthcoming chapters.\nLarsen and Clemmensen (2019) and Fernández P et al. (2020) describe a data set where laboratory measurements are used to predict what percentage of a liquid was lucerne, soy oil, or barley oil1. An instrument is used to measure how much of particular wavelengths of light are absorbed by the mixture to help determine chemical composition. We will focus on using the lab measurements to predict the percentage of barley oil in the mixture. The distribution of these values is shown in Figure 7.1(a).\nFigure 7.1: (a) The distribution of the outcome for the entire data set. The bar colors reflect the percent barley distribution and are used in subsequent sections. (b) Selected training set spectra for four barley samples. Each line represents the set of 550 predictors in the data.\nNote that most of the data have very little barley oil. About 27% of the data are less than 1%, and the median barley oil percentage is 5.96%.\nThe 550 predictors are the light absorbance for sequential values in the light region of interest (believed to be from wavelengths between 1300 and 2398 nm). Figure 7.1(b) shows a selection of four samples from the data. The darker lines represent samples with lower barley content.\nThese predictor values, called spectra, have a very high serial correlation between predictors; median correlation between the predictors was 0.98. The high degree of between-predictor correlation can be a major complication for some models and can degrade predictive performance. Therefore, we need methods that will simultaneously decorrelate predictors while extracting useful predictive information for the outcome.\nAnalyses of similar data sets can be found in Section 9.1 of Kuhn and Johnson (2019) and Johnson and Kuhn (2024).\nIn the following computations, each predictor was standardized using the orderNorm transformation mentioned earlier (unless otherwise noted).\nThe data originated from a modeling competition to find the most accurate model and specific samples were allocated to training and test sets. However, there were no public outcome values for the test set; our analysis will treat the 6,915 samples in their training set as the overall pool of samples. This is enough data to split into separate training (\\(n_{tr} =\\) 4,839), validation (\\(n_{val} =\\) 1,035), and test sets (\\(n_{te} =\\) 1,041). The allocation of samples to each of the three data sets utilized stratified sampling based on the outcome data.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-linear-embed",
    "href": "chapters/embeddings.html#sec-linear-embed",
    "title": "7  Embeddings",
    "section": "7.2 Linear Transformations",
    "text": "7.2 Linear Transformations\nThe barley data set presents two common challenges for many machine learning techniques:\n\nThe number of original predictors (550) is fairly large .\nThe features are highly correlated. Techniques that require inverting the covariance matrix of the features, such as linear regression, will become numerically unstable or may not be able to be fit when features are highly correlated.\n\nWhat can we do if we desire to use a modeling technique that is adversely affected by either (or both) of these characteristics?\nDimensionality reduction is one technique that can help with the first issue (an abundance of columns). As we’ll see below, we might be able to extract new features (\\(X^*\\)) such that the new features optimally summarize information from the original data (\\(X\\)).\nFor the problem of highly correlated predictors, some embedding methods can additionally decorrelated the predictors by deriving embedded features with minimal correlation.\nThe three embedding methods we’ll discuss first are linear in a mathematical sense because they transform a table of numeric features into new features that are linear combinations of the original features. These new linear combinations are often called scores. This transformation uses the equation.\n\\[ \\underset{n\\times m}{\\boldsymbol{X}^*} = \\underset{n\\times p}{\\boldsymbol{X}}\\ \\underset{p\\times m}{\\boldsymbol{Q}} \\]\nwhere \\(\\boldsymbol{Q}\\) is a matrix that translates or embeds the original features into a potentially lower dimensional space (\\(m &lt; p\\)) without losing much information. It is easy to see why this matrix operation is linear when the elements of the \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Q}\\) matrices are expanded. For example, the first score for the \\(i^{th}\\) sample would be:\n\\[\nx^*_{i1} = q_{11} x_{i1} + q_{21}x_{i2} + \\ldots +  q_{p1}x_{ip}\n\\tag{7.1}\\]\nThe \\(q_{ij}\\) values are often referred to as the loadings for each predictor in the linear combination.\nIn this section, we will review principal components analysis (PCA), independent component analysis (ICA), and partial least squares (PLS), which are fundamental linear embedding techniques that are effective at identifying meaningful embeddings but estimate \\(A\\) in different ways. PCA and ICA extract embedded features using only information from the original features; they are unsupervised. Conversely, PLS uses the predictors and outcome; it is a supervised technique.\nThere are many other linear embedding methods that can be used. For example, non-negative matrix factorization (Lee and Seung 1999, 2000)is an embedding method that is useful when the data in \\(X\\) are integer counts or other values that cannot be negative.\nTo start, we’ll focus on the most often used embedding method: PCA.\n\n7.2.1 Principal Component Analysis\nKarl Pearson introduced PCA over a century ago, yet it remains a fundamental dimension reduction method (Jolliffe and Cadima 2016). Its relevance stems from the underlying objective of the technique: to find linear combinations of the original predictors that maximize amount of variation in the new features2.\n\nFrom a statistical perspective, variation is synonymous with information.\n\nSimultaneously, the scores produced by PCA (i.e., \\(X^*\\)) are required to be orthogonal to each other. The important side benefit of this technique is that PCA scores are uncorrelated. This is very useful for modeling techniques that need the predictors to be relatively uncorrelated: multiple linear regression, neural networks, support vector machines, and others.\nIn a way, PCA components have an order or hierarchy. The first principal component has a higher variance than any other component. The second component has a higher variance than subsequent components, and so on. We can think of this process as one of deflation; the first component extracts the largest sources of information/variation in the predictors set. The second component extracts as much as possible for whatever is left behind by the first, and so on. For this reason, we can track how much variation in the original data each component accounts for. We often use the phrase that “PCA chases variation” to achieve its goals.\nSuppose we start with \\(p\\) columns3 in our data. In that case, we can produce up to \\(p\\) PCA components, and their accumulated variation will eventually add up to the total variation in the original \\(X\\).\nWhile PCA can deliver new features with the desirable characteristics, the technique must be used with care. Specifically, we must understand that PCA seeks to find embeddings without regard to any further understanding of the original features (such as the response). Hence, PCA may generate embeddings that are ignorant of the modeling objective (i.e., they lack predictive information).\n\nPreparing for PCA\nBecause PCA seeks linear combinations of predictors that maximize variability, it will naturally first be drawn to summarizing predictors with more variation. If the original predictors are on measurement scales that differ in magnitude, then the first components will focus on summarizing the higher magnitude predictors. This means that the PCA scores will focus on the scales of the measurements (rather than their intrinsic value). If, by chance, the most relevant feature for predicting the response also has the largest numerical values, the new features created by PCA will retain the essential information for predicting the response. However, if the most important features are in the smallest units, then the top features created by PCA will be much less effective.\nIn most practical machine learning applications, features are on vastly different scales. We suggest using the tools from Section 5.3 to transform the data so that they have the same units. This prevents PCA from focusing dimension reduction simply on the measurement scale.\nAdditionally, the distributions of each feature may show considerable skewness or outliers. While there are no specific distributional requirements to use PCA, it is focused on variance and variance calculations often involve squared terms (e.g., \\((x_i - \\bar{x})^2\\)). This type of computation can be very sensitive to outliers and skewness; resolving these issues prior to PCA is recommended. See the methods previously described in Section 5.2. When outliers are a specific concern, the spatial-sign transformation might be a good idea (Visuri, Koivunen, and Oja 2000; Croux, Ollila, and Oja 2002).\n\n\nHow Does PCA Work?\nPrincipal component analysis is focused on understanding the relationships between the predictor columns. PCA is only effective when there are correlations between predictors. Otherwise, each new PCA feature would only highlight a single predictor, and you would need the full set of PCA features to approximate the original columns. Conversely, data sets with a handful of intense between-predictor relationships require very few predictors to represent the source columns.\nThe barley data set has abnormally high correlations between each predictor. These tend to be autoregressive; the correlations between predictors at adjacent wavelengths are very high, and the correlation between a pair of predictors diminishes as you move away from their locations on the spectrum. For example, the correlation between the first and second predictor (in their raw form) is essentially 1 while the corresponding correlation between the first and fiftieth columns is 0.67. We’ll explore the latter pair of columns below.\nBefore proceeding, let’s go on a small “side-quest” to talk about the mathematics of PCA.\n\nThe remainder of this subsection discussed a mathematical operation called the singular value decomposition (SVD). PCA and many other computations rely on on the SVD. It is a fundamental linear algebra calculation.\nWe’ll describe it loosely with a focus on how it is used. The concept of eigenvalues will come up again in the next section and in subsequent chapters. Banerjee and Roy (2014) and Aggarwal (2020) are thorough but helpful resources for learning more.\n\nThe SVD process is intended to find a way to rotate the original data in a way that the resulting columns are orthogonal to one another. It takes a square matrix as an input and can decompose it into several pieces:\n\\[\\underset{p\\times p}{A} = \\underset{p\\times p}{Q}\\quad\\underset{p\\times p}{\\Lambda}\\quad \\underset{p\\times p}{Q'}\\]\nThe results of this computation are the \\(p\\) eigenvectors \\(\\mathbf{q}_j\\) and eigenvalues \\(\\lambda_j\\). The eigenvectors tell you about the directions in which values of \\(A\\) are moving and the eigenvalues describe the corresponding magnitudes (e.g. how far they go in their corresponding direction).\nThe transformation works with PCA by using the covariance matrix as the input \\(A\\). We are trying to find trends in the relationships between variables and the SVD is designed to translate the original matrix to one that is orthogonal (that is, uncorrelated). This aspect of the SVD is how it connects to PCA. If we want to approximate our original data with new features that are uncorrelated, we need a transformation to orthogonality.\nThe matrix \\(Q\\) houses the \\(p\\) possible eigenvectors. These are the values by which you multiply the original matrix \\(A\\) to achieve an orthogonal version. For PCA, they are the loading values shown in Equation 7.1. The matrix \\(\\Lambda\\) is a diagonal matrix whose \\(p\\) values are the eigenvalues. It turns out that the eigenvalues represent the amount of variation (i.e., information) captured by each component.\nIf we conduct the SVD (and PCA) on the covariance matrix of the data, we are again subject to issues around potentially different units and scales of the predictors. Previously, we suggested to center and scale the data prior to PCA. The equivalent approach here is to conduct the SVD on the correlation matrix of the data. Similar to our recommendations on standardization, we recommend using the correlation as the input to PCA.\nOne interesting side-effect of using the covariance or correlation matrix as the input to PCA is related to missing data (discussed in more detail in Chapter 4). The conventional sample covariance (or correlation) matrices can be computed for each matrix element (i.e., without using matrix multiplication). Each covariance or correlation can be computed with whatever rows of the two predictors have pairwise-complete results. This means that we do not have to globally drop specific rows of the data when a small number of columns contain missing data.\n\n\nA Two Dimensional Example\nTo demonstrate, we chose two predictors in the barley data. They correspond to wavelengths4 1,300 and 1,398 shown in Figure 7.1(b). In the data, these two predictors were preprocessed using the ORQ procedure (Section 5.2). The standardized versions of the predictors have a correlation of 0.68 and are shown in Figure 7.2(a).\nPCA was performed on these data, and the results are animated in Figure 7.2(b). This visualization shows the original data (colored by their outcome data) and then rotates it around the center position. The PCA transformation corresponds to the rotation where the x-axis has the largest spread. The variance is largest at two angles (135\\(^{\\circ}\\) and 315\\(^{\\circ}\\)) since the solution for PCA is unique up to sign. Both possible solutions are correct, and we usually pick one. For one solution, the PCA loading coefficients are:\n\\[\n\\begin{align}\nx^*_{i1} &= -0.71 x_{i1} -0.71 x_{i2} \\notag \\\\\nx^*_{i2} &= -0.71 x_{i1} + 0.71 x_{i2} \\notag\n\\end{align}\n\\]\nBecause of this illustrative example’s low-dimensional aspect, this solution is excessively simple. Ordinarily, the loading values take a great many values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: A demonstration that PCA is just a rotation of the data. The original predictors (those at the first and fiftieth wavelengths) are in Panel (a). The animation in (b) shows the rotation of two of the predictors, which stops at the two optimal rotations corresponding to the principal components and the original data.\n\n\n\nAgain,the remarkable characteristic of this specific set of linear combinations is that they are orthogonal (i.e., uncorrelated) with one another.\n\nUsing PCA to transform the data into a full set of uncorrelated variables is often called “whitening the data.” It retains all of the original information and is more pliable to many computations. It can be used as a precursor to other algorithms.\n\nNow that we’ve seen PCA on a small scale, let’s look at an analysis of the full set of predictors\n\n\nPCA on the Barley Data\nWe’ll repeat the analysis in the preceding section; PCA was calculated on the training set using all predictors. There are 550 feature columns and the same number of possible principal components. The number of new features to retain is a parameter that must be determined. In practice, the number of new features is often selected based on the percentage of variability that the new features summarize relative to the original features. Figure 7.3 displays the new PCA feature number (x-axis) versus the percent of total variability across the original features (commonly called a “scree plot”). In this graph, three components summarize 99.4% of the total variability. We often select the smallest number of new features that summarize the greatest variability. Again, this is an unsupervised optimization. To understand how the number of components affects supervised predictors (e.g., RMSE), the number of features can be treated as a tuning parameter .\n\n\n\n\n\n\n\n\nFigure 7.3: A scree plot of the PCA component number versus the percent of variance explained for the first 10 components of the barley data.\n\n\n\n\n\nNote that the first component alone captured 92.3% of the variation in the 550 training data columns. This reflects the extreme correlation seen in the predictors.\nFigure 7.4 shows a scatterplot of the principal components colored by the percentage of barley oil. The figure reveals that the first PCA component delineates the higher barley oil samples from those with less oil. There also appear to be three different clusters of samples with very low (if any) barley oil. It might be helpful to investigate these samples to ascertain if they are artifacts or caused by some systematic, underlying factors not in the current set of columns. The pairing of the second and fourth components appears to help differentiate lower barely samples from the broader set. The pairing of the third and fourth components doesn’t offer much predictive power on their own.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-linear-scores\n#| out-width: \"80%\"\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggforce)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-linear-scores.R\")\n\napp\n\n\n\nFigure 7.4: A visualization of the four new features for different linear embedding methods. The data shown are the validation set results.\n\n\n\nOne note about this visualization. The axis scales are not common across panels (for ease of illustration). If we were to keep a common scale, the later components would appear to have very little effect due to the flatness of the resulting figures. When looking at the scores, we suggest keeping a common scale, which will most likely be the scale of the first component. This will help avoid over-interpreting patterns in the later components.\nFor PCA, it can be very instructive to visualize the loadings for each component. This can help in several different ways. First, it can tell which predictors dominate each specific new PCA component. If a particular component ends up having a strong relationship with the outcome, this can aid our explanation of how the model works. Second, we can examine the magnitude of the predictors to determine some of the relationships between predictors. If a group of predictors have approximately the same loading value, this implies that they have a common relationship with one another. This, on its own, can be a significant aid when conducting exploratory data analysis on a high-dimensional data set. Figure 7.5 shows the relationship between the PCA loadings and each predictor’s position on the spectrum.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-linear-loadings\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(dplyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\n  \"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-linear-loadings.R\"\n)\n\napp\n\n\n\nFigure 7.5: The loadings for the first four components of each linear embedding method as a function of wavelength.\n\n\n\nFor the second component, wavelengths at the low ends of the spectrum have a relatively small impact that increases as you get to about 1,400 nm. At this point, the predictors have a constant effect on that component. The first PCA component is almost the opposite; it emphasizes the smaller wavelengths while predictors above 1,400 nm fluctuate around zero. The third and fourth components emphasize different areas of the spectrum and are generally different than zero.\n\n\nHow to Incorporate PCA into the Model\nThe barley data demonstrates that PCA can effectively capture the training set information in a smaller predictor set when there is a correlation between predictors. The number of components to retain depends on the data. It is a good idea to optimize the number of components to retain via model tuning.\n\nNote that “dimension reduction” is not the same as feature selection. The PCA components described above are functions of all of the predictors. Even if you only need a handful of PCA components to approximate the training set, the original predictor set is still required when predicting new samples.\n\nAnother aspect of using PCA for feature extraction is the scale of the resulting scores. Figure 7.4 demonstrates that the first component has the largest variance, resulting in a wider range of training set values. For the validation set scores, the variance of PC1 is 14-fold larger than the variance of PC2. Even though the scores are unitless, there may be issues for models that expect the predictors to have a common scale, similar to the requirement for PCA itself. In these cases, it may be advisable to standardize the PCA components to have the same range before serving them to the supervised ML model.\n\n\nNon-Standard PCA\nVarious methods exist for estimating the PCA loadings. Some are well-suited to different dimensions of the predictor’s data. For example, “small \\(n\\), large \\(p\\)” data sizes can make PCA computations difficult. Wu, Massart, and De Jong (1997) and others describe a kernel method more appropriate for very wide data sets. Schölkopf, Smola, and Müller (1998) extended this even further but using nonlinear kernel methods5. Still, others use modified objective functions to compensate for some potential deficiencies of the canonical technique.\nFor example, the standard loading matrix (i.e., the eigenvectors) are dense. It is possible that the loadings for some predictors shouldn’t affect specific components. The SVD might estimate these to be close to zero but not exactly zero. Sparse PCA techniques can, through various means, estimate some loading values to be exactly zero, indicating that the predictor has no functional effect on the embedding.\nFor example, Shen and Huang (2008) take a penalization approach that puts a high price for loadings to have very large values. This regularization method can shrink the values towards zero and make some absolute zero. This is similar to the regularization approach seen in Section 6.4.3 and is directly related to techniques discussed in Section 16.2, Section 17.4, and ?sec-linear-reg. This option can coerce values across the loadings to be zero. It is unlikely to force a particular predictor’s values to be zero across all components. In other words, it may not completely erase the effect of a predictor from the embedding altogether.\nAnother method is based on a Bayesian approach (Ning and Ning 2021), where the loading values are assumed to come from a mixture of two different distributions. One has a sharp “spike” around zero, and the other is a flat, wide distribution that encompasses a wide range of values (called the “slab”). The method then estimates the parameters to favor one distribution or the other and sets some proportion of the loadings to zero.\nAdditionally, there are PCA variants for non-numeric data, such as categorical predictors. Probabilistic PCA (Tipping and Bishop 1999; Schein, Saul, and Ungar 2003) uses a PCA generalization to reduce the dimensions of qualitative data.\n\n\n\n\n7.2.2 Independent Component Analysis\nICA has roots in signal processing, where the observed signal that is measured is a combination of several different input signals.\nFor example, electroencephalography (EEG) uses a set of electrodes on a patient’s scalp to noninvasively measure the brain’s electrical activity over time. The electrodes are placed on specific parts of the scalp and measure different brain regions (e.g., prefrontal cortex etc.). However, since the electrodes are near others, they can often measure mixtures of multiple underlying signals.\nThe objective of independent component analysis is to identify linear combinations of the original features that are statistically independent of each other (Hyvärinen and Oja 2000; Nordhausen and Oja 2018).\n\nTwo variables can be uncorrelated but still have a relationship (most likely nonlinear). For example, suppose we have a variable \\(z\\) that is a sequence of evenly spaced values between -2 and 2. The correlation between \\(z\\) and \\(z^2\\) is very close to zero but there is a clear and precise relationship between them, therefore the variables are not independent.\n\nIn practice, satisfying the statistically independence requirement of ICA can be done several ways. One approach maximizes the “non-Gaussianity” of the resulting components. As shown by Hyvärinen and Oja (2000),\n\nThe fundamental restriction in ICA is that the independent components must be nongaussian for ICA to be possible.\n\nThere are different ways to measure non-Gaussianity. The fastICA algorithm uses an information theory statistic called negentropy (Hyvärinen and Oja 2000). Another approach called InfoMax uses neural networks to estimate a different information theory statistic.\nBecause ICA’s objective requires statistical independence among components, it will generate features different from those of PCA. Unlike PCA, which orders new components based on summarized variance, ICA’s components have no natural ordering. This means that ICA may require more components than PCA to find the ones related to the outcome.\nThe predictors should be centered (and perhaps scaled) before estimating the ICA loadings. Additionally, some ICA algorithms internally add a PCA step to “whiten” the data before computing the ICA loadings (using all possible PCA components). Since independent component analysis thrives on non-Gaussian data, we should avoid transformations that induce a more symmetric distribution. For this reason, we will not use the ORD transformation to preprocess the predictors. Finally, the ICA loadings are often initialized before training using random values. If this randomness is not controlled, different results will likely occur from training run to training run.\nFigure 7.4 contains a scatterplot matrix of the first 4 ICA components colored by the outcome. In this figure, Two components that appear to differentiate levels of barley by themselves (components two and three). Unsurprisingly, their interaction appears to be the one that visually differentiates the different amounts of barley oil in the validation set.\nThe loadings are shown in Figure 7.5 and tell an interesting story. The pattern of loadings one and three have similar patterns over the wavelengths. The same situation is the case for components two and four. These two pairs of trends in the loadings are somewhat oppositional to one another. Also, the patterns have little in common with the PCA loadings. For example, the first PCA loading was relatively constant across wavelengths. None of the ICA components show a constant pattern.\n\n\n\n7.2.3 Partial Least Squares\nBoth PCA and ICA focus strictly on summarizing information based on the features exclusively. When the outcome is related to the way that the unsupervised techniques extract the embedded features, then PCA and/or ICA can be effective dimension reduction tools. However, when the qualities of variance summarization or statistical independence are not related to the outcome, then these techniques may struggle to identify appropriate embedded features that are useful for predicting the outcome.\nPartial least squares (PLS) (Wold, Martens, and Wold 1983) is a dimension reduction technique that identifies embedded features that are optimally linearly related to the outcome. While the objective of PCA is to find linear combinations of the original features that best summarize variability, the objective of PLS is to do the same and to find the linear combinations that are correlated with the outcome (Stone and Brooks 1990). This process can be thought of as a constrained optimization problem in which PLS is forced to compromise between maximizing information from the predictors and simultaneously maximizing the prediction of the outcome. This means that the outcome is used to focus the dimension reduction process such that the new features have the greatest correlation with the outcome. For more detailed information, we suggest Geladi and Kowalski (1986) and Esposito Vinzi and Russolillo (2013).\nBecause one of the objectives of PLS is to summarize variance among the original features, the features should be pre-processed in the same way as PCA.\nFor the barley data, the first few sets of PLS loads are fairly similar to those generated by PCA. The first three components have almost identical values as their PCA analogs. The fourth component has different loadings, and the correlation between the fourth PCA and PLS scores is -0.63. The similarities in the first three scores and loadings between the methods can be seen in Figure 7.4 and Figure 7.5, respectively.\nIf our goal is to maximize predictive performance, fewer features are necessary when using PLS. As we’ll see below, we can sometimes achieve similar performance using PCA or ICA but we will need more features to match what PLS can do with less.\nLike principal component analysis, there are many modified versions of partial least squares. For example, Lê Cao et al. (2008) and Lê Cao et al. (2008) describe a sparse estimation routine that can set some loadings equal to absolute zero. Also, Barker and Rayens (2003) and Y. Liu and Rayens (2007) adapt the algorithm for classification problems (i.e., qualitative outcomes).\n\n\n\n7.2.4 Overall Comparisons\nHow do these three approaches differ in terms of predictive performance? To evaluate this, these embeddings were computed with up to 25 new features and used as the inputs for a linear regression model. After the embeddings and regression parameters were estimated, the validation set RMSE was computed. Figure 7.6 has the results. The bands around each curve are 90% confidence intervals for the RMSE.\n\n\n\n\n\n\n\n\nFigure 7.6: Results for linear regressions using three different linear embeddings.\n\n\n\n\n\nOverall, the patterns are similar. By the time 25 components are added, there is parity between the methods. However, PLS appears to be more efficient at finding predictive features since its curve has uniformly smaller RMSE values than the others . This isn’t surprising since it is the only supervised embedding of the three.\nPCA and PLS will be discussed in more detail in ?sec-colinearity.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-mds",
    "href": "chapters/embeddings.html#sec-mds",
    "title": "7  Embeddings",
    "section": "7.3 Multidimensional Scaling",
    "text": "7.3 Multidimensional Scaling\nMultidimensional scaling (Torgerson 1952) is a feature extraction tool that creates embeddings that try to preserve the geometric distances between training set points. In other words, the distances between points in the smaller dimensions should be comparable to those in the original dimensions.\nMany different distance metrics can be used, but we’ll postpone that discussion until Section 17.2. That said, we’ll denote a distance value between two vectors of predictors as \\(d(x_i, x_j)\\) and we’ll assume basic Euclidean distance unless otherwise noted. With distance metrics, the predictors should be standardized to equivalent units before those computations. As with PCA, we also recommend transformations to resolve skewness.\nTake Figure 7.7(a) as an example. There are ten points in two dimensions (colored by three outcome classes). If we were to project these points down to a single dimension, we’d like points that are close in the original two dimensions to remain close when projected down to a single dimension. Panel (c) shows two such solutions. Each does reasonably well with some exceptions (i.e., points six and nine are too close for non-Metric MDS).\n\n\n\n\n\n\n\n\nFigure 7.7: (a) A collection of samples associated with three outcome classes. (b) A diagram of the two nearest neighbors for each data point. (c) A one-dimensional projection using two different methods: non-metric MDS and Isomap.\n\n\n\n\n\nHere we present a few MDS methods, but there are many more. Ghojogh et al. (2023) has an excellent review of an assortment of methods and their nuances.\nSome MDS methods compute all pairwise distances between points and use this as the input to the embedding algorithm. This is similar to how PCA can be estimated using the covariance or correlation matrices. One technique, Non-Metric MDS (Kruskal 1964a, 1964b; Sammon 1969), finds embeddings that minimize an objective function called “stress”:\n\\[\n\\text{Stress} = \\sqrt{\\frac{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}\\left(d(x_i, x_j) - d(x^*_i, x^*_j)\\right)^2}{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}d(x_i, x_j)^2}}\n\\]\nThe numerator uses the squared difference between the pairwise distances in the original values (\\(x\\)) and the smaller embedded dimension (\\(x^*\\)). The summations only move along the upper triangle of the distance matrices to reduce redundant computations. Figure 7.7(c, top row) has the resulting one dimensional projection of our two-dimensional data.\nThis can be an effective dimension reduction procedure, although there are a few issues. First, the entire matrix of distances is required (with \\(n_{tr}(n_{tr}-1)/2\\) entries). For large training sets, this can be unwieldy and time-consuming. Second, like PCA, it is a global method that uses all data in the computations. We might be able to achieve more nuanced embeddings by focusing on local structures. Finally, it is challenging to apply metric MDS to project new data onto the space in which the original data was projected.\nFor these reasons, let’s take a look at more modern versions of multidimensional scaling.\n\n7.3.1 Isomap\nTo start, we’ll focus on Isomap (Tenenbaum, Silva, and Langford 2000). This nonlinear MDS method uses a specialized distance function to find the embedded features. First, the K nearest neighbors are determined for each training set point using standard functions, such as Euclidean distance. Figure 7.7(b) shows the K = 2 nearest neighbors for our example data. Many nearest-neighbor algorithms can be very computationally efficient and their use eliminates the need to compute all of the pairwise distances.\nThe connections between neighbors form a graph structure that qualitatively defines which data points are closely related to one another. From this, a new metric called geodesic distance can be approximated. For a graph, we can compute the approximate geodesic distance using the shortest path between two points on the graph. With our example data, the Euclidean distance between points four and five is not large. However, its approximate geodesic distance is greater because the shortest path is through points nine, eight, and seven. Ghojogh et al. (2023) use a wonderful analogy:\n\nA real-world example is the distance between Toronto and Athens. The Euclidean distance is to dig the Earth from Toronto to reach Athens directly. The geodesic distance is to move from Toronto to Athens on the curvy Earth by the shortest path between two cities. The approximated geodesic distance is to dig the Earth from Toronto to London in the UK, then dig from London to Frankfurt in Germany, then dig from Frankfurt to Rome in Italy, then dig from Rome to Athens.\n\nThe Isomap embeddings are a function of the eigenvalues computed on the geodesic distance matrix. The \\(m\\) embedded features are functions of the first \\(m\\) eigenvectors. Although eigenvalues are associated with linear embeddings (e.g., PCA), nonlinear geodesic distance results in a local nonlinear embedding. Figure 7.7(c, bottom row) shows the 1D results for the example data set. For a new data point, its nearest-neighbors in the training set are determined so that the approximate geodesic distance can be computed. The estimated eigenvectors and eigenvalues are used to project the new point into the embedding space.\nFor Isomap, the number of nearest neighbors and the number of embeddings are commonly optimized. Figure 7.8 shows a two-dimensional Isomap embedding for the barley data with varying numbers of neighbors. In each configuration, the higher barley values are differentiated from the small (mostly zero) barley samples. We again see the two or three clusters of data associated with small outcome values. The new features become more densely packed as the number of neighbors increases.\n\n\n\n\n\n\n\n\nFigure 7.8: Isomap for the barley data for different numbers of nearest neighbors. The training set was used to fit the model and these results show the projections on the validation set. Lighter colors indicate larger values of the outcome.\n\n\n\n\n\n\n\n\n7.3.2 Laplacian Eigenmaps\nThere are many other approaches to preserve local distances. One is Laplacian eigenmaps (Belkin and Niyogi 2001). Like Isomap, it uses nearest neighbors to define a graph of connected training set points. For each connected point, a weight between graph nodes is computed that becomes smaller as the distance between points in the input space increases. The radial basis kernel (also referred to as the “heat kernel”) is a good choice for the weighting function6:\n\\[\nw_{ij} = \\exp\\left(\\frac{-||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2}{\\sigma}\\right)\n\\]\nwhere \\(\\sigma\\) is a scaling parameter that can be tuned. If two points are not neighbors, or if \\(i = j\\), then \\(w_{ij} = 0\\). Note that the equation above uses Euclidean distance. For the 2-nearest neighbor graph shown in Figure 7.7(b) and \\(\\sigma = 1 / 2\\), the weight matrix is roughly\n\n\\[\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\n\\boldsymbol{W} = \\begin{bmatrix}\n\\0 & 0.1 & \\0 & 0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n& \\0 & 0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  & \\0 & 0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  &  & \\0 & \\0 & \\0 & 0.1 & \\0 & \\0 & \\0\\\\\n&  &  &  & \\0 & 0.4 & \\0 & \\0 & 0.1 & \\0\\\\\n&  & sym &  &  & \\0 & \\0 & \\0 & 0.1 & \\0\\\\\n&  &  &  &  &  & \\0 & 0.4 & \\0 & 0.1\\\\\n&  &  &  &  &  &  & \\0 & 0.1 & 0.3\\\\\n&  &  &  &  &  &  &  & \\0 & \\0\\\\\n&  &  &  &  &  &  &  &  & \\0\n\\end{bmatrix}\n\\]\n\nThe use of nearest neighbors means that the matrix can be very sparse and the zero values help define locality for each data point. Recall that samples 2 and 3 are fairly close to one another, while samples 1 and 2 are farther away. The weighting scheme gives the former pair a 4-fold larger weight in the graph than the latter pair.\nLaplacian eigenmaps rely heavily on graph theory. This method computes a graph Laplacian matrix, defined as \\(\\boldsymbol{L} = \\boldsymbol{D} - \\boldsymbol{W}\\) where the matrix \\(\\boldsymbol{D}\\) has zero non-diagonal entries and diagonals equal to the sum of the weights for each row. For our example data:\n\n\\[\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\nL = \\begin{bmatrix}\n0.3 & -0.1 & \\0 & -0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  0.5 & -0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  &  0.5 & -0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  &  &  0.4 & \\0 & \\0 & -0.1 & \\0 & \\0 & \\0\\\\\n&  &  &  &  0.5 & -0.4 & \\0 & \\0 & -0.1 & \\0\\\\\n&  & sym &  &  &  0.5 & \\0 & \\0 & -0.1 & \\0\\\\\n&  &  &  &  &  &  0.6 & -0.4 & \\0 & -0.1\\\\\n&  &  &  &  &  &  &  0.8 & -0.1 & -0.3\\\\\n&  &  &  &  &  &  &  &  0.3 & \\0\\\\\n&  &  &  &  &  &  &  &  &  0.4\n\\end{bmatrix}\n\\]\n\nThe eigenvalues and eigenvectors of this matrix are used as the main ingredients for the embeddings. Bengio et al. (2003) shows that, since these methods eventually use eigenvalues in the embeddings, they can be easily used to project new data.\n\n\n7.3.3 UMAP\nThe Uniform Manifold Approximation and Projection (UMAP) (Sainburg, McInnes, and Gentner 2020) technique is one of the most popular distance-based methods. Its precursors, stochastic neighbor embedding (SNE) (Hinton and Roweis 2002) and Student’s t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton 2008), redefined feature extraction, particularly for visualizations. UMAP borrows significantly from Laplacian eigenmaps and t-SNE but has a more theoretically sound motivation.\nAs with Laplacian eigenmaps, UMAP converts the training data points to a sparse graph structure. Given a set of K nearest neighbors, it computes values similar to the previously shown weights (\\(\\boldsymbol{W}\\) matrix), which we will think of as the probability that point \\(j\\) is a neighbor of point \\(i\\):\n\\[\np_{j|i} = \\exp\\left(\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i}\\right)\n\\]\nwhere \\(\\rho_i\\) is the distance from \\(\\boldsymbol{x}_i\\) to its closest neighbor, and \\(\\sigma_i\\) is a scale parameter that now varies with each sample (\\(i\\)). To compute \\(\\sigma_i\\), we can solve the equation\n\\[\n\\sum_{i=1}^K  \\exp\\left(\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i}\\right) = \\log_2(K)\n\\]\nUnlike the previous weighting system, the resulting \\(n \\times n\\) matrix may not be symmetric, so the final weights are computed using \\(p_{ij} = p_{j|i} + p_{i|j} - p_{j|i}p_{i|j}\\).\nUMAP performs a similar weight calculation for the embedded values \\(\\boldsymbol{x}^*\\). We’ll denote the probability that embedded points \\(\\boldsymbol{x}_i^*\\) and \\(\\boldsymbol{x}_j^*\\) are connected as \\(p_{ij}^*\\). We’d like the algorithm to estimate these values such that \\(p_{ij} \\approx p_{ij}^*\\).\nNumerical optimization methods7 used to estimate the \\(n \\times m\\) values \\(x^*_{ij}\\). The process is initialized using a very sparse Laplacian eigenmap, the first few PCA components, or random uniform numbers. The objective function is based on cross-entropy and attempts to make the graphs in the input and embedded dimensions as similar as possible by minimizing:\n\\[\nCE = \\sum_{i=1}^{n_{tr}}\\sum_{j=i+1}^{n_{tr}} \\left[p_{ij}\\, \\log\\frac{p_{ij}}{p_{ij}^*} + (1 - p_{ij})\\log\\frac{1-p_{ij}}{1-p_{ij}^*}\\right]\n\\]\nUnlike the other embedding methods shown in this section, UMAP can also create supervised embeddings so that the resulting features are more predictive of a qualitative or quantitative outcome value. See Sainburg, McInnes, and Gentner (2020).\nBesides the number of neighbors and embedding dimensions, several more tuning parameters exist. The optimization process’s number of optimization iterations (i.e., epochs) and the learning rate can significantly affect the final results. A distance-based tuning parameter, often called min-dist, specifies how “packed” points should be in the reduced dimensions. Values typically range from zero to one. However, the original authors state:\n\nWe view min-dist as an essentially aesthetic parameter governing the appearance of the embedding, and thus is more important when using UMAP for visualization.\n\nAs will be seen below, the initialization scheme is an important tuning parameter.\nFor supervised UMAP, there is an additional weighting parameter (between zero and one) that is used to balance the importance of the supervised and unsupervised aspects of the results. A value of zero specifies a completely unsupervised embedding.\nFigure 7.9 shows an interactive visualization of how UMAP can change with different tuning parameters. Each combination was trained for 1,000 epochs and used a learning rate of 1.0. For illustrative purposes, the resulting embeddings were scaled to a common range.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-umap\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(dplyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-umap.R\")\n\napp\n\n\n\nFigure 7.9: A visualization of UMAP results for the barley data using different values for several tuning parameters. The points are the validation set values.\n\n\n\nThere are a few notable patterns in these results:\n\nThe initialization method can heavily impact the patterns in the embeddings.\nAs with Isomap, there are two or three clusters of data points with small barley values.\nWhen the amount of supervision increases, one or more circular structures form that are associated with small outcome values.\nThe minimum distance parameter can drastically change the results.\n\nt-SNE and UMAP have become very popular tools for visualizing complex data. Visually, they often show interesting patterns that linear methods such as PCA cannot. However, they are computationally slow and unstable over different tuning parameter values. Also, it is easy to believe that the UMAP distances between embedding points are important or quantitatively predictive. That is not the case; the distances can be easily manipulated using the tuning parameters (especially the minimum distance).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-centroids",
    "href": "chapters/embeddings.html#sec-centroids",
    "title": "7  Embeddings",
    "section": "7.4 Centroid-Based Methods",
    "text": "7.4 Centroid-Based Methods\nAnother common approach to creating features is to calculate distances between landmarks or pairs of predictors. For example, the distance to desirable grade schools (or the university campus) could affect house prices in the Ames housing data. Potential predictors can be created that measure how close each home is to these points of interest. To calculate distance, a simple Euclidean metric can be used. However, for spatial data across longer distances, the Haversine metric (Sinnott 1984) is a better alternative because it takes into account the curvature of the earth.\nNote that distance-based features are often right skewed. When a metric produces a right-skewed distribution, the log-transformation often helps improve predictive performance when the predictor is truly informative.\nDistance-based features can also be effective for classification models. A centroid is another name for the multivariate mean of a collection of data. It is possible to compute class-specific centroids using only the data from each class. When a new sample is predicted, the distance to each class centroid can be used as a predictor. These features would be helpful when a model could be better at detecting/emulating linear class boundaries. An example of this would be tree-based models; these models have to work hard to approximate linear trends in the data. Supplementing the data with simple centroid features might improve performance.\nAgain, there are several choices for the distance metric. Mahalanobis distance is a good choice when there is not an overwhelming number of predictors:\n\\[\nD_c(\\boldsymbol{x}_0) = (\\boldsymbol{x}_0 - \\boldsymbol{\\bar{x}}_{c})' \\boldsymbol{S}^{-1}_c (\\boldsymbol{x}_0 - \\boldsymbol{\\bar{x}}_{c})\n\\]\nwhere \\(\\boldsymbol{x}_0\\) is the new data point being predicted, \\(\\boldsymbol{\\bar{x}}\\) is a vector of sample means, \\(\\boldsymbol{S}\\) is the estimated covariance matrix (the subscript of \\(c\\) denotes the class-specific statistics). This metric requires more data points within each class than the number of predictors being used. It also assumes that there are no linear dependencies between the predictors.\nWhen the model has many features, regularizing the centroid distances can be a good approach. This approach, similar to the tools described in Section 6.4.3, will shrink the class-specific centroids towards the overall (class-nonspecific) centroid at different rates. If a predictor does not have any discriminative ability in the training set, its contribution to the class-specific centroids can be removed.\nWe’ll let \\(x_{ij}\\) denote sample \\(i\\) (\\(i=1\\ldots n_{tr}\\)) for predictor \\(j\\) (\\(j=1\\ldots p\\)). The approach by Tibshirani et al. (2003) estimates the standardized difference between the class-specific centroid and the global centroid using the following:\n\\[\n\\begin{align}\n\\delta_{jc} &= \\frac{\\bar{x}_{jc} - \\bar{x}_j}{w_c s_j} &&\\text{ where } \\notag \\\\\n\\bar{x}_{jc} &= \\frac{1}{{n_{tr}^c}}\\sum_{i=1}^{{n_{tr}^c}} x_{ij}\\, I(y_i = c) && \\text{\\textcolor{grey}{(class-specific centroid elements)}} \\notag \\\\\n\\bar{x}_{j} &= \\frac{1}{n_{tr}}\\sum_{i=1}^{n_{tr}} x_{ij}  && \\text{\\textcolor{grey}{(global centroid elements)}}\\notag \\\\\nw_c &= \\sqrt{\\frac{1}{{n_{tr}^c}}  - \\frac{1}{n_{tr}}}  && \\text{\\textcolor{grey}{{(weights)}}} \\notag \\\\\ns_j &= \\frac{1}{n_{tr}-C}\\sum_{c=1}^C\\sum_{i=1}^{n_{tr}} \\left(x_{ij} - \\bar{x}_{jc}\\right)^2 I(y_i = c)  && \\text{\\textcolor{grey}{(pooled standard deviation for predictor $j$)}}\\notag\n\\end{align}\n\\]\nwhere \\(n_{tr}^c\\) is the number of training set points for class \\(c\\) and \\(I(x)\\) is a function that returns a value of one when \\(x\\) is true.\nTo shrink this difference towards zero, a tuning parameter \\(\\lambda\\) is used to create a modified version of each predictor’s contribution to the difference:\n\\[\\begin{equation}\n\\delta^*_{jc} = sign(\\delta_{jc})\\,h(|\\delta_{jc}| - \\lambda)\n\\end{equation}\\]\nwhere \\(h(x) = x\\) when \\(x &gt; 0\\) and zero otherwise. If the difference between the class-specific and global centroid is small (relative to \\(\\lambda\\)), \\(\\delta^*_{jc} = 0\\) and predictor \\(j\\) does not functionally affect the calculations for class \\(c\\). The class-specific shrunken centroid is then\n\\[\\begin{equation}\n\\bar{x}^*_{jc}= \\bar{x}_j + w_c\\, s_j\\, \\delta^*_{jc}\n\\end{equation}\\]\nNew features are added to the model based on the distance between \\(\\boldsymbol{x}_0\\) and \\(\\boldsymbol{\\bar{x}}^*_{c}\\). The amount of shrinkage is best optimized using the tuning methods described in later chapters. There are several variations of this specific procedure. Wang and Zhu (2007) describe several different approaches and Efron (2009) demonstrates the connection to Bayesian methods.\n\n\n\n\n\n\n\n\nFigure 7.10: An example of shrunken, class-specific centroids. Panel (a) shows data where there are three classes. Panel (b) demonstrates how, with increasing regularization, the centorids converge towards the global centroid (the black open circle).\n\n\n\n\n\nFigure 7.10 shows the impact of regularization for a simple data set with two predictors and three classes. The data are shown in Panel (a), while Panel (b) displays the raw, class-specific centroids. As regularization is added, the lines indicate the path each takes toward the global centroid (in black). Notice that the centroid for the first class eventually does not use predictor \\(x_1\\); the path becomes completely vertical when that predictor is removed from the calculations. As a counter-example, the centroid for the third class moves in a straight line towards the center. This indicates that both predictors showed a strong signal, and neither was removed as regularization increased.\nLike distance-based methods, predictors based on data depth (R. Liu, Parelius, and Singh 1999; Ghosh and Chaudhuri 2005; Mozharovskyi, Mosler, and Lange 2015) can be helpful for separating classes. The depth of the data was initially defined by Tukey (1975), and it can be roughly understood as the inverse of the distance to a centroid. For example, Mahalanobis depth would be:\n\\[\nDepth_c(\\boldsymbol{x}_0) = \\left[1 + D_c(\\boldsymbol{x}_0)\\right]^{-1}\n\\]\nThere are more complex depth methods, and some are known for their robustness to outliers. Again, like distances, class-specific depth features can be used as features in a model.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-other-embeddings",
    "href": "chapters/embeddings.html#sec-other-embeddings",
    "title": "7  Embeddings",
    "section": "7.5 Other Methods",
    "text": "7.5 Other Methods\nThere are numerous other methods to create embeddings such as autoencoders (Borisov et al. 2022; Michelucci 2022). Boykis (2023) is an excellent survey and discussion of modern embedding methods, highlighting methods for text and deep learning.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#chapter-references",
    "href": "chapters/embeddings.html#chapter-references",
    "title": "7  Embeddings",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAggarwal, C. 2020. Linear Algebra and Optimization for Machine Learning. Vol. 156. Springer.\n\n\nBanerjee, S, and A Roy. 2014. Linear Algebra and Matrix Analysis for Statistics. Vol. 181. CRC Press.\n\n\nBarker, M, and W Rayens. 2003. “Partial Least Squares for Discrimination.” Journal of Chemometrics: A Journal of the Chemometrics Society 17 (3): 166–73.\n\n\nBelkin, M, and P Niyogi. 2001. “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering.” Advances in Neural Information Processing Systems 14.\n\n\nBengio, Y, JF Paiement, P Vincent, O Delalleau, N Roux, and M Ouimet. 2003. “Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering.” In Advances in Neural Information Processing Systems, edited by S. Thrun, L. Saul, and B. Schölkopf. Vol. 16. MIT Press.\n\n\nBorisov, V, T Leemann, K Seßler, J Haug, M Pawelczyk, and G Kasneci. 2022. “Deep Neural Networks and Tabular Data: A Survey.” IEEE Transactions on Neural Networks and Learning Systems.\n\n\nBoykis, V. 2023. “What are embeddings?” https://doi.org/10.5281/zenodo.8015028.\n\n\nCroux, C, E Ollila, and H Oja. 2002. “Sign and Rank Covariance Matrices: Statistical Properties and Application to Principal Components Analysis.” In Statistical Data Analysis Based on the L1-Norm and Related Methods, 257–69. Springer.\n\n\nEfron, B. 2009. “Empirical Bayes Estimates for Large-Scale Prediction Problems.” Journal of the American Statistical Association 104 (487): 1015–28.\n\n\nEsposito Vinzi, V, and G Russolillo. 2013. “Partial Least Squares Algorithms and Methods.” Wiley Interdisciplinary Reviews: Computational Statistics 5 (1): 1–19.\n\n\nFernández P, J. A., A. Laborde, L. Lakhal, M. Lesnoff, M. Martin, Y. Roggo, and P. Dardenne. 2020. “The Applicability of Vibrational Spectroscopy and Multivariate Analysis for the Characterization of Animal Feed Where the Reference Values Do Not Follow a Normal Distribution: A New Chemometric Challenge Posed at the ’Chimiométrie 2019’ Congress.” Chemometrics and Intelligent Laboratory Systems 202: 104026.\n\n\nGeladi, P., and B Kowalski. 1986. “Partial Least-Squares Regression: A Tutorial.” Analytica Chimica Acta 185: 1–17.\n\n\nGhojogh, B, M Crowley, F Karray, and A Ghodsi. 2023. “Elements of Dimensionality Reduction and Manifold Learning.” In, 185–205. Springer International Publishing.\n\n\nGhosh, A K, and P Chaudhuri. 2005. “On Data Depth and Distribution-Free Discriminant Analysis Using Separating Surfaces.” Bernoulli 11 (1): 1–27.\n\n\nHinton, G, and S Roweis. 2002. “Stochastic Neighbor Embedding.” Advances in Neural Information Processing Systems 15.\n\n\nHyvärinen, Aapo, and Erkki Oja. 2000. “Independent Component Analysis: Algorithms and Applications.” Neural Networks 13 (4-5): 411–30.\n\n\nJohnson, K, and M Kuhn. 2024. “What They Forgot to Tell You about Machine Learning with an Application to Pharmaceutical Manufacturing.” Pharmaceutical Statistics n/a (n/a).\n\n\nJolliffe, I, and J Cadima. 2016. “Principal Component Analysis: A Review and Recent Developments.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 374 (2065): 20150202.\n\n\nKruskal, J. 1964a. “Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.” Psychometrika 29 (1): 1–27.\n\n\nKruskal, J. 1964b. “Nonmetric Multidimensional Scaling: A Numerical Method.” Psychometrika 29 (2): 115–29.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nLarsen, J, and L Clemmensen. 2019. “Deep Learning for Chemometric and Non-Translational Data.” https://arxiv.org/abs/1910.00391.\n\n\nLê Cao, KA, D Rossouw, C Robert-Granié, and P Besse. 2008. “A Sparse PLS for Variable Selection When Integrating Omics Data.” Statistical Applications in Genetics and Molecular Biology 7 (1).\n\n\nLee, D, and S Seung. 1999. “Learning the Parts of Objects by Non-Negative Matrix Factorization.” Nature 401 (6755): 788–91.\n\n\nLee, D, and S Seung. 2000. “Algorithms for Non-Negative Matrix Factorization.” Advances in Neural Information Processing Systems 13.\n\n\nLiu, R, J Parelius, and K Singh. 1999. “Multivariate Analysis by Data Depth: Descriptive Statistics, Graphics and Inference.” The Annals of Statistics 27 (3): 783–858.\n\n\nLiu, Y, and W Rayens. 2007. “PLS and Dimension Reduction for Classification.” Computational Statistics, 189–208.\n\n\nMichelucci, I. 2022. “An Introduction to Autoencoders.” arXiv.\n\n\nMozharovskyi, P, K Mosler, and T Lange. 2015. “Classifying Real-World Data with the DD\\(\\alpha\\)-Procedure.” Advances in Data Analysis and Classification 9 (3): 287–314.\n\n\nNing, B, and N Ning. 2021. “Spike and Slab Bayesian Sparse Principal Component Analysis.” arXiv.\n\n\nNordhausen, K, and H Oja. 2018. “Independent Component Analysis: A Statistical Perspective.” Wiley Interdisciplinary Reviews: Computational Statistics 10 (5): e1440.\n\n\nSainburg, T, L McInnes, and TQ Gentner. 2020. “Parametric UMAP: Learning Embeddings with Deep Neural Networks for Representation and Semi-Supervised Learning.” arXiv.\n\n\nSammon, J. 1969. “A Nonlinear Mapping for Data Structure Analysis.” IEEE Transactions on Computers 100 (5): 401–9.\n\n\nSchein, A, L Saul, and L Ungar. 2003. “A Generalized Linear Model for Principal Component Analysis of Binary Data.” In Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics, edited by C Bishop and B Frey, R4:240–47. Proceedings of Machine Learning Research.\n\n\nSchölkopf, B, A Smola, and KR Müller. 1998. “Nonlinear Component Analysis as a Kernel Eigenvalue Problem.” Neural Computation 10 (5): 1299–319.\n\n\nShen, H, and JZ Huang. 2008. “Sparse Principal Component Analysis via Regularized Low Rank Matrix Approximation.” Journal of Multivariate Analysis 99 (6): 1015–34.\n\n\nSinnott, R. 1984. “Virtues of the Haversine.” Sky and Telescope 68 (2): 158.\n\n\nStone, M, and R Brooks. 1990. “Continuum Regression: Cross-Validated Sequentially Constructed Prediction Embracing Ordinary Least Squares, Partial Least Squares and Principal Components Regression.” Journal of the Royal Statistical Society: Series B (Methodological) 52 (2): 237–58.\n\n\nTenenbaum, J, V Silva, and J Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290 (5500): 2319–23.\n\n\nTibshirani, R, T Hastie, B Narasimhan, and G Chu. 2003. “Class Prediction by Nearest Shrunken Centroids, with Applications to DNA Microarrays.” Statistical Science, 104–17.\n\n\nTipping, M, and C Bishop. 1999. “Probabilistic Principal Component Analysis.” Journal of the Royal Statistical Society Series B: Statistical Methodology 61 (3): 611–22.\n\n\nTorgerson, W. 1952. “Multidimensional Scaling: I. Theory and Method.” Psychometrika 17 (4): 401–19.\n\n\nTukey, J. 1975. “Mathematics and the Picturing of Data.” In Proceedings of the International Congress of Mathematicians, 2:523–31.\n\n\nVan der Maaten, L, and G Hinton. 2008. “Visualizing Data Using t-SNE.” Journal of Machine Learning Research 9 (11).\n\n\nVisuri, S, V Koivunen, and H Oja. 2000. “Sign and Rank Covariance Matrices.” Journal of Statistical Planning and Inference 91 (2): 557–75.\n\n\nWang, S, and J Zhu. 2007. “Improved Centroids Estimation for the Nearest Shrunken Centroid Classifier.” Bioinformatics 23 (8): 972–79.\n\n\nWold, S, H Martens, and H Wold. 1983. “The Multivariate Calibration Problem in Chemistry Solved by the PLS Method.” In Proceedings from the Conference on Matrix Pencils. Heidelberg: Springer-Verlag.\n\n\nWu, W, DL Massart, and S De Jong. 1997. “The Kernel PCA Algorithms for Wide Data. Part I: Theory and Algorithms.” Chemometrics and Intelligent Laboratory Systems 36 (2): 165–72.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#footnotes",
    "href": "chapters/embeddings.html#footnotes",
    "title": "7  Embeddings",
    "section": "",
    "text": "Retrieved from https://chemom2019.sciencesconf.org/resource/page/id/13.html↩︎\nAs is, this is far too general. We could maximize the variance by making every value of \\(Q\\) equal to \\(\\infty\\). PCA, and other methods, impose an implicit constraint to limit the elements of \\(Q\\). In this case, PCA constrains the loading vectors to have unit length.↩︎\nAssuming that the \\(p\\) columns are linearly independent.↩︎\nThese wavelengths correspond to the 1st and 50th predictors.↩︎\nWe will see similar methods in Section 17.6.↩︎\nA note about some notation… We commonly think of the norm notation as \\(\\|\\boldsymbol{x}\\|_p = \\left(|x_1|^p + |x_2|^p + \\ldots + |x_n|^p\\right)^{1/p}\\). So what does the lack of a subscript in \\(||\\boldsymbol{x}||^2\\) mean? The convention is the sum of squares: \\(||\\boldsymbol{x}||^2 = x_1^2 + x_2^2 + \\ldots + x_n^2\\).↩︎\nSpecifically gradient descent with a user-defined learning rate.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html",
    "href": "chapters/interactions-nonlinear.html",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "8.1 Interactions\nWe saw in previous chapters that preprocessing is necessary because models can be sensitive to specific predictor characteristics. For instance, some models:\nPreprocessing methods address aspects of the predictors that place them in a form so that models can be built. This is what the model needs to function.\nWhile preprocessing techniques allow models to be built, they do not necessarily transform them in ways that help the model to identify predictive relationships with the outcome. This is the fundamental concept of feature engineering which addresses the question: what can we do to make it easier for the model to understand and predict the outcome.\nIn this chapter we will discuss techniques that address this question. For example, if the relationship between a predictor is nonlinear, how can we represent that predictor so that the relationship can be modeled? Or if two predictors work in conjunction with each other, then how can this information be engineered for a model? While feature engineering is not necessary for models to be built, it contains a crucial set of tools for improving model performance.\nLet’s look at a simple example. Occasionally, predictor pairs work better in unison rather than as main effects. For example, consider the data in Figure 8.1(a), where two predictors are:\nIn the predictors’ original form, there is a significant overlap between the two classes of samples. However, when the ratio of the predictors is used, the newly derived predictor better discriminates between classes (shown in Figure 8.1(b)). While not a general rule, the three data characteristics above suggest that the modeler attempts to form ratios from two or more predictors 1.\nIt is essential to understand that these data require transformation. We can put the original predictors into a model as-is. The model won’t produce an error but won’t have good predictive performance. We can induce a separation of the classes when a joint feature is used.\nThis aspect of feature engineering depends on the data set so it is difficult to enumerate all possible techniques. In this chapter, we’ll describe a few commonly used methods: splines, interactions, and discretization (a.k.a. binning).\nWhen building models for prediction, the majority of variation in the outcome is generally explained by the cumulative effect of the important individual predictors. For many problems, additional variation in the response can be explained by the effect of two or more predictors working in conjunction with each other.\nThe healthcare industry has long understood the concept of interactions among drugs for treating specific diseases Altorki et al. (2021). As an example of an interaction, consider treatment for the disease non-small cell lung cancer (NSCLC). In a recent study, patients with an advanced stage of NSCLC with an EGFR mutation were given either osimertinib alone or osimertinib in combination with traditional chemotherapy (Planchard et al. 2023). Patients taking the combination treatment had a significantly longer progression-free survival time than patients taking osimertinib alone. Hence, the interaction of the treatments is more effective than the single treatment. To summarize, two (or more) predictors interact if their combined impact is different (less or greater) than what would be expected from the added impact of each predictor alone.\nWe’ve already encountered an example of an interaction in Section 2.3 where the relationship between delivery time and the time of order differed across days of the week. This trend is reproduced in Figure 8.2(a). The telltale sign of the interaction is that the trendlines are not parallel with one another; they have different rates of increase and cross.\nHow would this plot change if there was no interaction between the order time and day? To illustrate, we estimated trendlines in a way that coerced the nonlinear trend for order time to be the same. Figure 8.2(b) shows the results: parallel lines for each day of the week.\nFigure 8.2: Delivery times versus the time of the order, colored by the day of the week. (a) A duplicate of a panel from Figure 2.2. (b) The same data with trendlines where the underlying model did not allow an interaction between the delivery day and hour.\nThis example demonstrates an interaction between a numeric predictor (hour of order) and a categorical predictor (day of the week). Interactions can also occur between two (or more) numeric or categorical predictors. Using the delivery data, let’s examine potential interactions solely between categorical predictor columns.\nFor regression problems, one visualization technique is to compute the means (or medians) of the outcome for all combinations of variables and then plot these means in a manner similar to the previous figure. Let’s look at the 27 predictors columns for whether a specific item was included in the order. The original column contains counts, but the data are mostly zero or one. We’ll look at two variables at a time and plot the four combinations of whether the item was ordered at all (versus not at all). Figure 8.3 shows two potential sets of interactions. The x-axis indicates whether Item 1 was in the order or not. The y-axis is the mean delivery time with 90% confidence intervals2.\nThe left panel shows the joint effect of items 1 and 9. The lines connecting the means are parallel. This indicates that each of these two predictors affects the outcome independently of one another. Specifically, the incremental change in delivery time is the same when item 9 is or is not included with item 1. The right panel has means for items 1 and 10. The mean delivery times are very similar when neither is contained in the order. Also, when only one of the two items is in the order, the average time is similarly small. However, when both are included, the delivery time becomes much larger. This means that you cannot consider the effect or either item 1 or 10 alone; their effect on the outcome occurs jointly.\nFigure 8.3: Interaction examples with two categorical predictors. Items 1 and 9 (left panel) do not interaction, while items 1 and 10 appear to have a strong interaction (right panel).\nAs a third example, interactions may occur between continuous predictors. It can be difficult to discover the interaction via visualization for two numeric predictors without converting one of the predictors to categorical bins. To illustrate the concept of an interaction between two numeric predictors, \\(x_1\\) and \\(x_2\\), let’s use a simple linear equation:\n\\[\ny = \\beta_0 + \\beta_{1}x_1 + \\beta_{2}x_2 + \\beta_{3}x_{1}x_{2} + \\epsilon\n\\tag{8.1}\\]\nThe \\(\\beta\\) coefficients represent the overall average response (\\(\\beta_0\\)), the average rate of change for each individual predictor (\\(\\beta_1\\) and \\(\\beta_2\\)), and the incremental rate of change due to the combined effect of \\(x_1\\) and \\(x_2\\) (\\(\\beta_3\\)) that goes beyond what \\(x_1\\) and \\(x_2\\) can explain alone. The parameters for this equation can be estimated using a technique such as ordinary least squares (REF). The sign and magnitude of \\(\\beta_3\\) indicate how and the extent to which the two predictors interact:\nTo understand this better, Figure 8.4 shows a contour plot of a predicted linear regression model with various combinations of the model slope parameters. The two predictors are centered at zero with values ranging within \\(x_j \\pm 4.0\\)). The default setting shows a moderate synergistic interaction effect since all of the \\(\\beta_j = 1.0\\)). In the plot, darker values indicate smaller predicted values.\nInteraction effects between predictors are sometimes confused with correlation between predictors. They are not the same thing. Interactions are defined by their relationship to the outcome while between-predictors correlations are unrelated to it. An interaction could occur independent of the amount of correlation between predictors.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-interactions",
    "href": "chapters/interactions-nonlinear.html#sec-interactions",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "When \\(\\beta_3\\) is positive, the interaction is synergistic since the response increases beyond the effect of either predictor alone.\n\nAlternatively, when \\(\\beta_3\\) is negative, the interaction is antagonistic since the response decreases beyond the effect of either predictor alone.\n\nA third scenario is when \\(\\beta_3\\) is essentially zero. In this case, there is no interaction between the predictors and the relationship between the predictors is additive.\nFinally, for some data sets, we may find that neither predictor is important individually (i.e., \\(\\beta_1\\) and \\(\\beta_2\\) are zero). However, the coefficient on the interaction term is not zero. Because this case occurs very infrequently, it is called atypical.\n\n\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-interaction-contours\n#| viewerHeight: 600\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-interaction-contours.R\")\n\napp\n\n\n\nFigure 8.4: Prediction contours for a linear regression model with Equation 8.1 with \\(\\beta_0 = 0\\). Darker values indicate smaller predictions.\n\n\n\n\n\n\n8.1.1 How Likely are Interactions?\nWithin the context of statistical experimental design, Hamada and Wu (1992) discuss some probabilistic aspects of predictor importance. The effect sparsity principle is that there are often few predictors that are relevant for predicting the outcome. Similarly, the effect hierarchy principle states that “main effects” (i.e. a feature involving only predictor) are more likely to occur than interactions. Also, as more predictors are involved in the interaction, the less likely they become. Finally, the heredity principle conjectures that if an interaction is important, it is very likely that the corresponding main effects are likely too. Chipman (1996) further expands this principle.\nThe original context of these principles was envisioned for screening large numbers of predictors in experimental designs; they are still very relevant for the analysis of tabular data.\n\n\n8.1.2 Detecting Interactions\nAn ideal scenario would be that we would know which predictors interact before modeling the data. If this would be the case, then these terms could be included in a model. This would be ideal because the model could more easily find the relationship with the response, thus leading to better predictive performance. Unfortunately, knowledge of which predictors interact is usually not available prior to initiating the modeling process.\nIf meaningful interactions are unknown before modeling, can models still discover and utilize these potentially important features? Recent studies using various advanced modeling techniques have shown that some methods can inherently detect interactions. For example, tree-based models (Elith, Leathwick, and Hastie 2008), random forests (García-Magariños et al. 2009), boosted trees (Lampa et al. 2014), and support vector machines (Chen et al. 2008) are effective at uncovering them.\nIf modern modeling techniques can naturally find and utilize interactions, then why is it necessary to spend any time uncovering these relationships? The first reason is to improve interpretability. Recall the trade-off between prediction and interpretation that was discussed in Section TODO. More complex models are less interpretable and generally more predictive, while simpler models are more interpretable and less predictive. Therefore, if we know which interaction(s) are important, we can include these in a simpler model to enable a better interpretation. A second reason to spend time uncovering interactions is to help improve the predictive performance of models.\nWhat should we do if we have a candidate set of interactions? If we are using a more formal statistical model, such as linear or logistic regression, the traditional tool for evaluating whether additional model terms in a set of nested models are worth including is the conventional analysis of variance (ANOVA). This uses the statistical likelihood value as the objective function, which is equivalent to comparing the RMSE values between models for linear regression 3. The error reduction and how many additional terms are responsible for the improvement are computed. Using these results and some probability assumptions about the data, we can formally test the null hypothesis that the additional parameters all have coefficient values of zero.\nLooking at the delivery data, our model in Section 2.4 included a single set of interactions (e.g., hour-by-day). What if we included one more interaction: item 1 \\(\\times\\) item 10? Table 8.1 shows the ANOVA results. The RMSE computed on the training set is listed in the first and second columns. The reduction by including this additional model term is 0.18 (decimal minutes). Assuming normality of the model residuals, the p-value for this test is exceedingly small. This indicates that there is no evidence that this parameter is truly zero. In other words, there is strong evidence that the inclusion of the interaction helps explain the response. This statistical difference may not make much of a practical difference. However, machine learning models often behave like “a game of inches” where every small improvement adds up to an overall improvement that matters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Set RMSE\n\n\n\nValidation Set\n\n\n\nInteractions\nDecimal\nTime\nDeg. Free.\np-Value\nMAE\nRMSE\n\n\n\n\nHour x Day\n2.30\n(2m, 17s)\n5,890\n\n1.61\n2.29\n\n\nAdd One Item Interaction\n2.12\n(2m, 6s)\n5,889\n10-219.8\n1.56\n2.10\n\n\nAdd All Item Interactions\n2.06\n(2m, 3s)\n5,539\n10-18.4\n1.60\n2.13\n\n\n\n\n\n\n\n\nTable 8.1: ANOVA table and validation set statistics for three models with different interaction sets.\n\n\n\n\n\n\n\nWhat about the other two-way interactions between the other item predictors? Since the training set is fairly large (compared to the current number of model parameters), it is feasible to include the remaining 350 pairwise interactions and use the ANOVA method to validate this choice. These results are contained in the third row of Table 8.1 where the RMSE dropped further by 0.05 minutes. The p-value is also very small, indicating that there is no evidence that all of the 350 parameter estimates are zero. More extensive testing would be required to determine which actually are zero. This can be tedious and a potentially dangerous “fishing expedition” that could result in serious bias creeping into the modeling process.\nOne problem with the ANOVA method is that it calculates the model error using the training set, which we know may not indicate what would occur with previously unseen data. In fact, it is well known that, for linear regression via ordinary least squares estimation, it is impossible for the training set error to ever increase when adding new model terms. Therefore, was the drop in RMSE when all interactions were included due to this fact? Or was it due to other important interactions that were included? To understand this we can turn to the validation set to provide confirmation. You can see this in Table 8.1 where the validation set RMSE and MAE values are included. Note that, when the large set of interactions were added, these metrics both increase in the validation set, a result contrary to what the ANOVA results tell us.\nThe observed and predicted visualizations for each model in Table 8.1 are shown in Figure 8.5. Adding the additional interaction yielded a slight numerical improvement. However, the visualization in the middle panel shows fewer very large residuals. The figure also shows the model results that include the full set of 351 two-way interactions; this panel shows no significant reduction in large residuals, further casting doubt on using the entire set.\n\n\n\n\n\n\n\n\nFigure 8.5: Validation set predicted vs. observed delivery times for linear regression models with and without additional interactions.\n\n\n\n\n\nSo far, we have used visualizations to unearth potential interactions. This can be an effective strategy when the number of potential interactions is large, but the visual nature of this process is subjective. There are some specialized quantitative tools for identifying interactions. For example, Lim and Hastie (2015) used regularized generalized linear models (sections TODO) to estimate all possible two-way interactions and use a penalization method to determine which should be retained. Miller (1984) and Kuhn and Johnson (2019) (Section 7.4.3) describe the feasible solution algorithm, an iterative search method for interaction discovery. Another, which we will now describe in more detail, is Friedman’s H-statistic (Friedman and Popescu 2008).\nWe can estimate the joint effect of a set of predictors on the outcome as well as the effect of individual predictors. If we thought that only main effects and two-factor interactions were possible, we could factor out the individual effects from the joint effect. The leftover predictive ability would then be due to interactions. Consider the linear model in Equation 8.1. The joint effect would include all possible model terms associated with a predictor. We can also create main effects too:\n\\[\n\\begin{align}\nf(x_1, x_2) &= \\beta_{1}x_1 + \\beta_{2}x_2 + \\beta_{3}x_{1}x_{2} \\notag \\\\\nf(x_1) &= \\beta_{1}x_1  \\notag \\\\\nf(x_2) &= \\beta_{2}x_2 \\notag\n\\end{align}\n\\]\nTo isolate the potential interaction effect:\n\\[\nf(x_1\\times x_2) = f(x_1, x_2) - f(x_1) - f(x_2) = \\beta_{3}x_{1}x_{2}\n\\tag{8.2}\\]\nThis shows that, for this situation, we can isolate the effect of an interaction by removing any other systematic effects in the data4. Equation 8.2 is based on a simple parametric linear model. For models with more complex prediction equations, we can’t analytically pick out which model parameters should be used to investigate potential interactions.\nHowever, for any model, the joint and marginal effects can be quantified using partial dependence profiles (PDP) (Molnar (2020), Section 8.1). First, we determine a sequence of values covering the observed range of the predictor(s) of interest. Then we randomly sample a data point, perhaps from the training set, and over-ride the value of the predictor of interest with values from the grid. This produces a prediction profile over the grid. We can repeat this process many times to approximate \\(f(\\cdot)\\) by averaging the multiple prediction values for each grid point.\nFigure 8.6 visualizes the PDP data derived from using the ensembles of regression trees for \\(f(hour)\\), \\(f(day)\\), and \\(f(hour, day)\\). The first panel shows the random realizations of the relationship between delivery time and order hour from 1,000 randomly sampled rows of the training set. The results are unsurprising; we can see a similarity between these results and the initial visualizations in Figure 2.2. The single trend line in panel (b) is the average of these profiles for each value of the predictor (delivery hour). There appears to be, on average, a nonlinear effect of the delivery hour. The day of the week is an informative predictor and the joint effect profile in panel (d) shows that its effect induces different patterns in the delivery hour. If this were not the case, the patterns in panels (b) and (c) would, when combined, approximate the pattern in panel (d).\n\n\n\n\n\n\n\n\nFigure 8.6: Partial dependence profiles for the food delivery hour and day, derived for the tree-based ensemble. Panel (a) shows the individual grid predictions from 1,000 randomly sampled points in the training set. The other panels show the average trends.\n\n\n\n\n\nFriedman’s H-statistic can quantify the effect of interaction terms using partial dependence profiles. When investigating two-factor interactions between predictors \\(j\\) and \\(j'\\), the statistic is\n\\[\nH^2_{jj'}=\\frac{\\sum_\\limits{i=1}^B\\left[\\hat{f}(x_{ij},x_{ij'})-\\hat{f}(x_{ij})-\\hat{f}(x_{ij'})\\right]}{ \\sum_\\limits{i=1}^B\\hat{f}^2(x_{ij}, x_{ij'})}\n\\]\nwhere the \\(\\hat{f}(x_{ij})\\) term represents the partial dependence profile for predictor \\(j\\) for sample point \\(i\\) along the grid for that predictor and \\(B\\) is the number of training set samples. The denominator captures the total joint effect of predictors \\(j\\) and \\(j'\\) so that \\(H^2\\) can be interpreted as the fraction of the joint effect explained by the potential interaction.\nFor a set of \\(p\\) predictors, we could compute all \\(p(p-1)/2\\) pairwise interactions and rank potential interactions by their statistic values. This can become computationally intractable at some point. One potential shortcut suggested by the heredity principle is to quantify the importance of the \\(p\\) predictors and look at all pairwise combinations of the \\(p^*\\) most important values. For \\(p^* = 10\\), Figure 8.7 shows the top five interactions detected by this procedure. We’ve already visually identified the hour \\(\\times\\) day interaction, so seeing this in the rankings is comforting. However, another large interaction effect corresponds to the variables for items #1 and #10. Discovering this led us to visually confirm the effect back in Figure 8.3.\n\n\n\n\n\n\n\n\nFigure 8.7: \\(H^2\\) statistics for the top ten pairwise interaction terms.\n\n\n\n\n\nNote that the overall importance of the features are being quantified. From this analysis alone, we are not informed that the relationships between the hour and distance predictors and the outcome are nonlinear. It also doesn’t give any sense of the direction of the interaction (e.g., synergistic or antagonistic). We suggest using this tool early in the exploratory data analysis process to help focus visualizations on specific variables to understand how they relate to the outcome and other predictors.\nThere is a version of the statistic that can compute how much a specific predictor is interacting with any other predictor. It is also possible to compute a statistical test to understand if the H statistic is different than zero. In our opinion, it is more helpful to use these values as diagnostics rather than the significant/insignificant thinking that often accompanies formal statistical hypothesis testing results.\nInglis, Parnell, and Hurley (2022) discuss weak spots in the usage of the H-statistic, notably that correlated predictors can cause abnormally large values. This is an issue inherited from the use of partial dependence profiles (Apley and Zhu 2020).\nAlternatively, Greenwell, Boehmke, and McCarthy (2018) measures the potential for variables to interact by assessing the “flatness” over regions of the partial dependence profile. The importance of a predictor, or a pair of predictors, is determined by computing the standard deviation of the flatness scores over regions. For example, Figure 8.3 shows a flat profile for the item 1 \\(\\times\\) item 9 interaction while the item 1 \\(\\times\\) item 10 interaction has different ranges of means across values of item 1. Figure 8.8 shows ten interactions with the largest flatness importance statistics. As with the \\(H^2\\) results, the hour-by-day interaction. The two top ten lists have 6 other interactions in common.\n\n\n\n\n\n\n\n\nFigure 8.8: Importance statistics for the top ten pairwise interaction terms.\n\n\n\n\n\nOther alternatives can be found in Hooker (2004), Herbinger, Bischl, and Casalicchio (2022), and Oh (2022).\nThe two PDP approaches we have described apply to any model capable of estimating interactions. There are also model-specific procedures for describing which joint effects are driving the model (if any). One method useful for tree-based models is understanding if two predictors are used in consecutive splits in the data. For example, Figure 2.5 showed a shallow regression tree for the delivery time data. The terminal nodes had splits with orderings \\(\\text{hour}\\rightarrow\\text{hour}\\) and \\(\\text{hour}\\rightarrow\\text{day} \\rightarrow\\text{distance}\\) (twice). This implies that these three predictors have a higher potential to be involved in interactions with one another.\nKapelner and Bleich (2013) describe an algorithm that counts how many times pairs of variables are used in consecutive splits in a tree. They demonstrate this process with Bayesian Adaptive Regression Trees (BART, Section 19.3) which creates an ensemble of fairly shallow trees. Figure 8.9 shows the top ten pairwise interactions produced using this technique. Note that the BART implementation split categorical predictors via single categories. For the delivery data, this means that the tree would split on a specific day of the week (.i.e., Friday or not) instead of splitting all of the categories at once. This makes a comparison between the other two methods difficult. However, it can be seen that the hour \\(\\times\\) day interaction has shown to be very important in all three methods.\n\n\n\n\n\n\n\n\nFigure 8.9: BART model statistics identifying interactions by how often they occur in the tree rules.\n\n\n\n\n\nCaution should be exercised with this method. First, some tree ensembles randomly sample a subset of predictors for each split (commonly known as \\(m_{try}\\)). For example, if \\(m_{try} = 4\\), a random set of four predictors are the only ones considered for that split point. This deliberately coerces non-informative splits; we are not using the best possible predictors in a split. This can dilute the effectiveness of counting predictors in successive splits. Second, as discussed in Chapter 18, many tree-based splitting procedures disadvantage predictors with fewer unique values. For example, in the delivery data, the distance and hour predictors are more likely to be chosen for splitting than the item predictors even if they are equally informative. There are splitting procedures that correct for this bias, but users should be aware of the potential impartiality.\nThe H-statistic and its alternatives can be an incredibly valuable tool for learning about the data early on as well as suggesting new features that should be included in the model. They are, however, computationally expensive.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-polynomials",
    "href": "chapters/interactions-nonlinear.html#sec-polynomials",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.2 Polynomial Basis Expansions",
    "text": "8.2 Polynomial Basis Expansions\nIn the food delivery data, we have thoroughly demonstrated that there are quantitative predictors that have nonlinear relationships with the outcome. In Section 2.4, two of the three models for these data could naturally estimate nonlinear trends. Linear regression, however, could not. To fix this, the hour predictor spawned multiple features called spline terms and adding these to the model enables it to fit nonlinear patterns. This approach is called a basis expansion. In this section, we’ll consider (global) polynomial basis expansions.\nThe most traditional basis function is the polynomial expansion. If we start with a simple linear regression model with a single predictor:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\epsilon_i\n\\]\nthe expansion would add additional terms with values of the predictor exponentiated to the \\(p^{th}\\) power5. For a cubic polynomial model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_{i}^3 + \\epsilon_i\n\\tag{8.3}\\]\nThis is a linear model in the sense that it is linear in the statistical parameters (the \\(\\beta\\) values), so we can use standard parameter estimation procedures (e.g., least squares).\nFigure 8.10 shows data from Bralower et al. (1997) which was collected to understand the relationship between the age of a fossil and the ratio of two radioactive isotopes. The data set contains 106 data points for these two variables. The relationship is highly nonlinear, with several regions where the data have significant convex or concave patterns.\nFor simplicity, a fit from a cubic model Equation 8.3 is shown as the default. While the pattern is nonlinear, the line does not conform well to the data. The lines above the plot show the three basis functions that, for \\(p=3\\), correspond to linear (\\(x\\)), quadratic (\\(x^2\\)), and cubic (\\(x^3\\)) terms. The effects of these terms are blended using their estimated slopes (\\(\\hat{\\beta}_j\\)) to produce the fitted line. Notice that the cubic fit is very similar to the cubic basis shown at the top of the figure. The estimated coefficient for the cubic term is much larger than the linear or quadratic coefficients\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-global-polynomial\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(splines2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(aspline)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\n# source(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-polynomial.R\")\n\n# Requires the sources in shiny-setup.R\n\ndata(fossil)\n\nui &lt;- page_fillable(\n  # theme = grid_theme,\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-3, 6, -3)),\n    sliderInput(\n      \"global_deg\",\n      label = \"Polynomial Degree\",\n      min = 1L,\n      max = 20L,\n      step = 1L,\n      value = 3L,\n      ticks = TRUE\n    ) # sliderInput\n  ), # layout_columns\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 10, -1)),\n    as_fill_carrier(plotOutput('global'))\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  maybe_lm &lt;- function(x) {\n    try(lm(y ~ poly(x, input$piecewise_deg), data = x), silent = TRUE)\n  }\n  \n  names_zero_padded &lt;- function(num, prefix = \"x\", call = rlang::caller_env()) {\n    rlang:::check_number_whole(num, min = 1, call = call)\n    ind &lt;- format(seq_len(num))\n    ind &lt;- gsub(\" \", \"0\", ind)\n    paste0(prefix, ind)\n  }\n  \n  expansion_to_tibble &lt;- function(x, original, prefix = \"term \") {\n    cls &lt;- class(x)[1]\n    nms &lt;- names_zero_padded(ncol(x), prefix)\n    colnames(x) &lt;- nms\n    x &lt;- as_tibble(x)\n    x$variable &lt;- original\n    res &lt;- tidyr::pivot_longer(x, cols = c(-variable))\n    if (cls != \"poly\") {\n      res &lt;- res[res$value &gt; .Machine$double.eps, ]\n    }\n    res\n  }\n  \n  mult_poly &lt;- function(dat, degree = 4) {\n    rng &lt;- extendrange(dat$x, f = .025)\n    grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n    grid_df &lt;- tibble(x = grid)\n    feat &lt;- poly(grid_df$x, degree)\n    res &lt;- expansion_to_tibble(feat, grid_df$x)\n    \n    # make some random names so that we can plot the features with distinct colors\n    rand_names &lt;- lapply(\n      1:degree,\n      function(x) paste0(sample(letters)[1:10], collapse = \"\")\n    )\n    rand_names &lt;- unlist(rand_names)\n    rand_names &lt;- tibble(name = unique(res$name), name2 = rand_names)\n    res &lt;- dplyr::inner_join(res, rand_names, by = dplyr::join_by(name)) %&gt;%\n      dplyr::select(-name) %&gt;%\n      dplyr::rename(name = name2)\n    res\n  }\n  \n  col_rect &lt;- ggplot2::element_rect(fill = \"#fcfefe\", colour = \"#fcfefe\")\n  theme_light_bl &lt;- function() {\n    ggplot2::theme(\n      panel.background = col_rect,\n      panel.border = col_rect,\n      legend.background = col_rect,\n      legend.key = col_rect,\n      plot.background = col_rect\n    )\n  }\n  \n  # ------------------------------------------------------------------------------\n  \n  spline_example &lt;- tibble(x = fossil$age, y = fossil$strontium.ratio)\n  rng &lt;- extendrange(fossil$age, f = .025)\n  grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n  grid_df &lt;- tibble(x = grid)\n  alphas &lt;- 1 / 4\n  line_wd &lt;- 1.0\n  \n  base_p &lt;- spline_example %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point(alpha = 3 / 4, pch = 1, cex = 3) +\n    labs(x = \"Age\", y = \"Isotope Ratio\") +\n    lims(x = rng) +\n    theme_bw()\n  \n  output$global &lt;- renderPlot({\n    poly_fit &lt;- lm(y ~ poly(x, input$global_deg), data = spline_example)\n    poly_pred &lt;- predict(\n      poly_fit,\n      grid_df,\n      interval = \"confidence\",\n      level = .90\n    ) %&gt;%\n      bind_cols(grid_df)\n    \n    global_p &lt;- base_p\n    \n    if (input$global_deg &gt; 0) {\n      global_p &lt;- global_p +\n        geom_ribbon(\n          data = poly_pred,\n          aes(y = NULL, ymin = lwr, ymax = upr),\n          alpha = 1 / 15\n        ) +\n        geom_line(\n          data = poly_pred,\n          aes(y = fit),\n          col = \"black\",\n          linewidth = line_wd\n        ) +\n        theme(\n          plot.margin = margin(t = -20, r = 0, b = 0, l = 0),\n          panel.background = col_rect,\n          plot.background = col_rect,\n          legend.background = col_rect,\n          legend.key = col_rect\n        )\n      \n      feature_p &lt;- poly(grid_df$x, input$global_deg) %&gt;%\n        expansion_to_tibble(grid_df$x) %&gt;%\n        ggplot(aes(variable, y = value, group = name, col = name)) +\n        geom_line(show.legend = FALSE) + # , linewidth = 1, alpha = 1 / 2\n        theme_void() +\n        theme(\n          plot.margin = margin(t = 0, r = 0, b = -20, l = 0),\n          panel.background = col_rect,\n          plot.background = col_rect,\n          legend.background = col_rect,\n          legend.key = col_rect\n        ) +\n        scale_color_viridis(discrete = TRUE, option = \"turbo\")\n      \n      p &lt;- (feature_p / global_p) + plot_layout(heights = c(1.5, 4))\n    }\n    \n    print(p)\n  })\n}\n\napp &lt;- shinyApp(ui, server)\n\napp\n\n\n\nFigure 8.10: A global polynomial approach to modeling data from Bralower et al. (1997) along with a representation of the features (above the data plot). The black dashed line is the true function and the shaded region is the 90% confidence intervals around the mean.\n\n\n\nWe might improve the fit by increasing the model complexity, i.e., adding additional polynomial terms. An eight-degree polynomial seems to fit better, especially in the middle of the data where the pattern is most dynamic. However, there are two issues.\nFirst, we can see from the 90% confidence bands around the line that the variance can be very large, especially at the ends. The fitted line and confidence bands go slightly beyond the range of the data (i.e., extrapolation). Polynomial functions become erratic when they are not close to observed data points. This becomes increasingly pathological as the polynomial degree increases. For example, in Figure 8.10, choosing degrees greater than 14 will show extreme uncertainty in the variance of the mean fit (the solid line). This is a good example of the variance-bias tradeoff discussed in Section 8.4 below.\nDecreasing the polynomial degree will result in less complex patterns that tend to underfit the data. Increasing the degree will result in the fitted line being closer to the data, but, as evidenced by the confidence interval in the shaded region, the uncertainty in the model explodes (especially near or outside of the range range). This is due to severe overfitting. Eventually, you can increase the degree until the curve passes through every data point. However, the fit for any other values will be wildly inaccurate and unstable.\nSecond, the trend is concave for age values greater than 121 when it probably should be flat. The issue here is that a global polynomial pattern is stipulated. The effect of each model term is the same across the entire data range6. We increased the polynomial degree to eight to improve the fit via additional nonlinearity. Unfortunately, this means that some parts of the data range will be too nonlinear than required, resulting in poor fit.\nA global polynomial is often insufficient because the nonlinear relationships are different in different data sections. There may be steep increases in one area and gradual increases in others. Rather than using a global polynomial, what if we used different basis expansions in regions with more consistent trends rather than a global polynomial?\nFigure 8.11 shows a crude approach where three different regions have different polynomial fits with the same degree. The pre-set degree and data ranges are probably as good as can be (after much manual fiddling) but the approach isn’t terribly effective. The curves do not connect. This discontinuity is visually discordant and most likely inconsistent with the true, underlying pattern we are attempting to estimate. Also, there are large jumps in uncertainty when predicting values at the extremes of each region’s range.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-piecewise-polynomials\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(splines2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(aspline)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-piecewise.R\")\n\napp\n\n\n\nFigure 8.11: A piecewise polynomial approach to model the data from Figure 8.10.\n\n\n\nIt turns out that we can make sure that the different fitted curves can connect at the ends and that they do that smoothly. This is done via a technique called splines which yield better results and are more mathematically elegant.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-splines",
    "href": "chapters/interactions-nonlinear.html#sec-splines",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.3 Spline Functions",
    "text": "8.3 Spline Functions\nLet’s assume we use piecewise cubic polynomials in the regional curve fits. To make the curves connect, we can add additional features that isolate local areas of \\(x\\) via a new feature column such as \\(h(x - \\xi)^3\\) where\n\\[\nh(u)  =\n\\begin{cases} x & \\text{if $u &gt; 0$,}\n\\\\\n0 & \\text{if $u \\le 0$.}\n\\end{cases}\n\\]\nand \\(\\xi\\) defines one of the boundary points.\nThis zeros out parts of the predictor’s range that are greater than \\(\\xi\\). A smooth, continuous model would include a global polynomial expansion (i.e., \\(x\\), \\(x^2\\), and \\(x^3\\)) and these partial features for each value that defines the regions (these breakpoints, denoted with \\(\\xi\\), are called knots7). For example, if a model has three regions, the basis expansion has 3 + 2 = 5 feature columns. For the fossil data, a three region model with knots at \\(\\xi_1 = 107\\) and \\(\\xi_2 = 114\\) would have model terms\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x^2 + \\beta_3x^3 + \\beta_4h(x_i - 107)^3 + \\beta_5h(x_i - 114)^3 + \\epsilon_i\n\\tag{8.4}\\]\nThe model fit, with the default interior knots, shown in Figure 8.12 looks acceptable. As with global polynomials, there is a significant increase in the confidence interval width as the model begins to extrapolate slightly beyond the data used to fit the model.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-simple-spline\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(aspline)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-spline-manual-knots.R\")\n\napp\n\n\n\nFigure 8.12: A simple cubic spline function with three regions. By default, the regions are partitions at age values of 107 and 114. The fitted curve has the same form as Equation 8.4. The two curves above the data plot indicate the range and values of the partial featured truncated using the function \\(h(\\cdot)\\).\n\n\n\nLet’s consider how each model parameter affects the predictor’s space by labeling the regions A, B, and C (from left to right). For region A, the model is a standard cubic function (using \\(\\beta_0\\) though \\(\\beta_3\\)). Region B adds the effect of the first partial function by including \\(\\beta_4\\). Finally, region C is a function of all five model parameters. This representation of a spline is called a truncated power series and is mostly used to demonstrate how spline functions can flexibly model local trends.\n\nThe word “spline” is used in many different contexts. It’s confusing, but we’ll try to clear it up.\nWe’ll define a “spline” or “spline function” as a collection of features that represent continuous and smooth piece-wise polynomial features of a single-number predictor (with some specific properties).\n“Regression splines” usually describe the use of splines of one or more predictors in a model fit such as ordinary least squares (Wood 2006; Arnold, Kane, and Lewis 2019).\nThe term “smoothing splines” refers to a procedure that encapsulates both a spline function as well as an estimation method to fit it to the data (usually via regularization). There is typically a knot for each unique data point in \\(x\\) (Wang 2011).\nAlso, there are many types of spline functions; we’ll focus on one but describe a few others at the end of this section.\n\nA different method for representing spline basis expansions is the B-spline (De Boor 2003). The terms in the model are parameterized in a completely different manner. Instead of the regions using increasingly more model parameters to define the regional fit, B-spline terms have more localized features such that only a few are “active” (i.e., non-zero) at a particular point \\(x_0\\). This can be seen in Figure 8.13 which contains the basis function values for the truncated power series and B-spines\n\n\n\n\n\n\n\n\nFigure 8.13: Two representations of a cubic spline for the fossil data with knots at 107 and 114. The top panel illustrates the model terms defined by Equation 8.4. The bottom panel shows the B-spline representation. All lines have been scaled to the same y-axis range for easy visualization.\n\n\n\n\n\nB-spline parameterizations are used primarily for their numerical properties and, for this reason, we’ll use them to visualize different types of splines.\nBefore considering another type of spline function, let’s look at one practical aspect of splines: choosing the knots.\n\n8.3.1 Defining the Knots\nHow do we choose the knots? We can select them manually, and some authors advocate for this point of view. As stated by Breiman (1988):\n\nMy impression, after much experimentation, is the same - that few knots suffice providing that they are in the right place.\n\nA good example is seen in Figure 8.12 with the fossil data. Using the knot positioning widget at the top, we can find good and bad choices for \\(\\xi_1\\) and \\(\\xi_2\\); knot placement can matter. However, hand-curating each feature becomes practically infeasible as the number of knots/features increases. Otherwise, they can be set algorithmically.\nThere are two main choices for automatic knot selection. The first uses a sequence of equally-spaced values encompassing the predictor space. The second is to estimate percentiles of the predictor data so that regions have about the same number of values they capture. For example, a fourth degree of freedom spline would have three split points within the data arranged at the 25th, 50th, and 75th percentiles (with the minimum and maximum values bracketing the outer regions).\nIt would be unwise to place knots as evenly spaced values between the minimum and maximum values. Without taking the distribution of the predictor into account, it is possible that the region between some knots might not contain any training data. Since there are usually several overlapping spline features for a given predictor value, we can still estimate the model despite empty regions (but it should be avoided).\nMany, but not all, splines specify how complex the fit should be in the area between the knots. However, cubic splines are a common choice because they allow for greater flexibility than linear or quadratic fits, but are not overly flexible, which could lead to over-fitting. Increasing the polynomial degree beyond three has more disadvantages than advantages. Recall that, in Figure 8.10, variance exploded at the ends of the predictor distribution when the polynomial degree was very large.\n\n\n8.3.2 Natural Cubic Splines\nThe type of spline that we suggest using by default is the natural cubic spline8 (Stone and Koo 1985; Arnold, Kane, and Lewis 2019; Gauthier, Wu, and Gooley 2020). It uses cubic fits in the interior regions and linear fits in regions on either end of the predictor distribution. For a simple two-knot model similar to Equation 8.4, there are only three slope parameters (the global quadratic and cubic terms are not used):\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2h(x_i - \\xi_1)^3 + \\beta_3h(x_i - \\xi_2)^3 + \\epsilon_i\n\\tag{8.5}\\]\nFor the fossil data, Figure 8.14 shows how this model performs with different numbers of knots chosen using quantiles. The 90% confidence bands show an increase in uncertainty when extrapolating, but the increase is orders of magnitude smaller than a global polynomial with the same number of model terms.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-natural-cubic-spline\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(splines2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(aspline)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-spline-natural.R\")\n\napp\n\n\n\nFigure 8.14: A natural cubic spline fit to the data previously shown in Figure 8.12. Good fits occur when approximately nine degrees of freedom are used. The top curves are the B-spline representation of the model terms.\n\n\n\n\nGiven the simplicity of this spline function, the fixed polynomial degree, and its stability in the tails of the predictor’s distribution, we suggest choosing natural cubic splines for most problems.\n\nSplines can be configured in different ways. Two details to specify are how many regions should be separately modeled and how they are apportioned. The number of regions is related to how many features are used in the basis expansion. As the number of regions increases, so does the ability of the spline to adapt to whatever trend might be in the data. However, the risk of overfitting the model to individual data points increases as the flexibility increases.\nFor this reason, we often tune the amount of complexity that the spline will accommodate. Generally, since the number of regions is related to the number of features, we’ll refer to the complexity of the basis function via the number of degrees of freedom afforded by the spline. We don’t necessarily know how many degrees of freedom to use. There are two ways to determine this. First, we can treat the complexity of the spline as a tuning parameter and optimize it with the tuning methods mentioned in Chapters 11 and 12. Another option is to over-specify the number of degrees of freedom and let specialized training methods solve the potential issue of over-fitting. From Wood (2006):\n\nAn alternative to controlling smoothness by altering the basis dimension, is to keep the basis dimension fixed, at a size a little larger than it is believed it could reasonably be necessary, but to control the model’s smoothness by adding a “wiggliness” penalty to the least squares fitting objective.\n\nThis approach is advantageous when there are separate basis expansions for multiple predictors. The overall smoothness can be estimated along with the parameter estimates in the model. The process could be used with general linear models (Chapters ?sec-reg-linear and 16.2) or other parametric linear models.\nSplines are extremely useful and are especially handy when we want to encourage a simple model (such as linear regression) to approximate the predictive performance of a much more complex black-box model (e.g., a neural network or tree ensemble). We’ll also see splines and spline-like features used within different modeling techniques, such as generalized additive models (Sections 16.4 and 16.4), multivariate adaptive regression splines (?sec-reg-mars), and a few others.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-variance-bias",
    "href": "chapters/interactions-nonlinear.html#sec-variance-bias",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.4 Sidebar: The Variance-Bias Tradeoff",
    "text": "8.4 Sidebar: The Variance-Bias Tradeoff\nOur discussion of basis expansions presents an excellent time for a segue to discuss an essential idea in statistics and modeling: the variance-bias tradeoff. This idea will be relevant in upcoming sections for resampling and specific models. At the end of this section, we’ll also connect it to Section 6.4.3.\nWhat does variance mean in terms of a machine learning model? In this context, it would quantify how much the fitted model changes if we slightly change the data. For example, would the curve change much if we repeat the data collection that produces the values in Section 8.2 and fit the sample model (for a fixed sample size and polynomial degree)? We can also look at the variance of prediction: for a specific new data point \\(x_0\\), how much intrinsic uncertainty is there? Similar to the discussion regarding extrapolation of global polynomials, we might want to compare how much the uncertainty changes as we move outside the training set’s range.\nBias is the difference between some estimate, like an average or a model prediction, and its true value. The true value is not the same as the data we collect; it is the unknowable theoretical value. For this reason, bias is often difficult to compute directly. For machine learning, the most pertinent idea of bias relates to how well a model can conform to the patterns we see in the data (hoping that the observed data are a good representation of the true values). Back in Figure 8.10, we saw that a linear model didn’t fit the data well. This is due to simple linear regression being a high-bias model because, without additional feature engineering, it cannot replicate nonlinear trends. Adding polynomial or spline terms decreased the model’s bias since it was more flexible.\n\nWe can think of bias in terms of the question: “How close are we aiming to the center of the target?” and variance as: “How much do our results vary when shooting at the target?”\n\nThe concepts of variance and bias are paired because they are often at odds with one another. For models that predict, we’d like a low variance, low bias model. In many cases, that can be exceedingly difficult to achieve. We can often lower bias by adding model complexity. However, increased complexity usually comes at the expense of stability (i.e., high variance). We will often be in the position of trying to find an acceptable compromise between the two. This was discussed with global polynomials; linear models were bad for our data, but adding too many polynomial terms adds unnecessary complexity and an explosion of variance (reflected in the confidence bands).\nThis leads us to the mean squared error (MSE). We’ve seen the root mean squared error already where we used it as a measure of accuracy for regression models. More generally, we can write it in terms of some unknown parameter \\(\\theta\\) and some estimate \\(\\hat{\\theta}\\) based on statistical estimation from data: \\[MSE = E\\left[(\\theta - \\hat{\\theta})^2\\right]\\]\nIt turns out that the MSE is a combination of model variance and (squared) bias (\\(\\theta - \\hat{\\theta}\\)):\n\\[MSE = E\\left[(\\theta - \\hat{\\theta})^2\\right] = Var[\\hat{\\theta}] + (\\theta - \\hat{\\theta})^2 + \\sigma^2\\]\nwhere \\(\\sigma^2\\) represents some unknown amount of “irreducible noise.” Because of this, MSE offers a statistic to minimize that accounts for both properties. This can offer a compromise between the two.\nTo illustrate this, we simulated a simple nonlinear model:\n\\[y_i = x_i^3 + 2\\exp\\left[-6(x_i - 0.3)^2\\right] + \\epsilon_i\\]\nwhere the error terms are \\(\\epsilon_i \\sim N(0, 0.1)\\) and the predictor values were uniformly spaced across [-1.2, 1.2]. Since we know the true values, we can compute the model bias.\nTo create a simulated data set, 31 samples were generated. Of these, 30 were used for the training set and one was reserved for the estimating the variance and bias. The training set values were roughly equally spaced across the range of the predictor. One simulated data set is shown in Figure 8.15, along with the true underlying pattern.\n\n\n\n\n\n\n\n\nFigure 8.15: A simulated data set with a nonlinear function. The test set point is the location where our simulation estimates the bias and variance.\n\n\n\n\n\n\n#&gt; Warning: executing %dopar% sequentially: no parallel backend registered\n\nA linear model was estimated using ordinary least squares and a polynomial expansion for each simulated data set. The test set values were predicted, and the bias, variance, and root mean squared error statistics were computed. This was repeated 1,000 times for polynomial degrees ranging from one to twenty.\nFigure 8.16 shows the average statistic values across model complexity (i.e., polynomial degree). A linear model performs poorly for the bias due to underfitting (as expected). In panel (a), adding more nonlinearity results in a substantial decrease in bias because the model fit is closer to the true equation. This improvement plateaus at a sixth-degree polynomial and stays low (nearly zero). The variance begins low; even though the linear model is ineffective, it is stable. Once additional terms are added, the variance of the model steadily increases then explodes around a 20th degree polynomial9. This shows the tradeoff; as the model becomes more complex, it fits the data better but eventually becomes unstable.\n\n\n\n\n\n\n\n\nFigure 8.16: Simulation results showing variances, squared biases, and mean squared errors.\n\n\n\n\n\nThe mean squared error in panel (b) shows the combined effect of variance and bias. It shows a steep decline as we add more terms, then plateaus between seven and fifteenth-degree models. After that, the rapidly escalating variance dominates the MSE as it rapidly increases.\nThe variance-bias tradeoff is the idea that we can exploit one for the other. Let’s go back to the basic sample mean statistic. If the data being averaged are normally distributed, the simple average is an unbiased estimator: it is always “aiming” for the true theoretical value. That’s a great property to have. The issue is that many unbiased estimators can have very high variance. In some cases, a slight increase in bias might result in a drastic decrease in variability. The variance-bias tradeoff helps us when one of those two quantities is more important.\nFigure 8.17 shows a simple example. Suppose we have some alternative method for estimating the mean of a group that adds some bias while reducing variance. That tradeoff might be worthwhile if some bias will greatly reduce the variance. In our figure, suppose the true population mean is zero. The green curve represents the sample mean for a fixed sample size. It is centered at zero but has significant uncertainty. The other curve might be an alternative estimator that produces a slightly pessimistic estimate (its mean is slightly smaller than zero) but has 3-fold smaller variation. When estimating the location of the mean of the population, the biased estimate will have a smaller confidence interval for a given sample size than the unbiased estimate. While the biased estimate may slightly miss the target, the window of the location will be much smaller than the unbiased estimate. This may be a good idea depending on how the estimate will be used 10.\n\n\n\n\n\n\n\n\nFigure 8.17: A simple example of the variance-bias tradeoff.\n\n\n\n\n\nThe categorical encoding approach shown in Equation 6.3 from Section 6.4.3 is another example. Recall that \\(\\bar{y}_j\\) was the average daily rate for agent \\(j\\). That is an unbiased estimator but has high variance, especially for agents with few bookings. The more complex estimator \\(\\hat{y}_j\\) in Equation 6.3 is better because it is more reliable. That reliability is bought by biasing the estimator towards the overall mean (\\(\\mu_0\\)).\nAs mentioned, we’ll return to this topic several times in upcoming sections.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#discretization",
    "href": "chapters/interactions-nonlinear.html#discretization",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.5 Discretization",
    "text": "8.5 Discretization\nDiscretization11 is the process of converting quantitative data into a set of qualitative groups (a.k.a “bins”). The model uses these values instead of the original predictor column (perhaps requiring an additional step to convert them into binary indicator columns). Sadly, Figure 8.18 illustrates numerous analyses that we have witnessed. This example uses the food delivery data and breaks the order hour and distance predictors into six and three groups, respectively. It also converts the delivery time outcome into three groups. This visualization, colloquially known as the “Wall of Pie,” tries to explain how the two predictors affect the outcome categories, often with substantial subjectivity.\n\n\n\n\n\n\n\n\nFigure 8.18: An unfortunate visualization of two predictors and the outcome in the food delivery data. The colors of the pie chart reflect the binned delivery times at cut points of 20 and 30 minutes with lighter blue indicating earlier times.\n\n\n\n\n\nOur general advice, described in more detail in Kuhn and Johnson (2019), is that the first inclination should never be to engineer continuous predictors via discretization12. Other tools, such as splines, are both theoretically and practically superior to converting quantitative data to qualitative data. In addition, if a predictor can be split into regions that are predictive of the response, then methods that use recursive partitioning will be less arbitrary and more effective.\nThe literature supporting this is extensive, such as: Cohen (1983), Altman (1991), Maxwell and Delaney (1993), Altman et al. (1994), Buettner, Garbe, and Guggenmoos-Holzmann (1997), Altman (1998), Taylor and Yu (2002), MacCallum et al. (2002), Irwin and McClelland (2003), Owen and Froman (2005), Altman and Royston (2006), Royston, Altman, and Sauerbrei (2006), van Walraven and Hart (2008), Fedorov, Mannino, and Zhang (2009), Naggara et al. (2011), Bennette and Vickers (2012), Kuss (2013), Kenny and Montanari (2013), Barnwell-Menard, Li, and Cohen (2015), Fernandes et al. (2019), as well as the references shown in Harrell (2015). These articles identify the main problems of discretization as follows:\n\nArbitrary (non-methodological) choice of breaks for binning can lead to significant bias.\nThe predictor suffers a significant loss of information, making it less effective. Moreover, there is reduced statistical power to detect differences between groups when they exist.\nThe number of features are increased, thus exacerbating the challenge of feature selection.\nCorrelations between predictors are inflated due to the unrealistic reduction in the variance of predictors.\n\nPettersson et al. (2016) shows differences in analyses with and without discretization. Their Fig. 1 shows a common binning analysis: a continuous outcome and one or more predictors are converted to qualitative formats and a grid of pie charts is created. Inferences are made from this visualization. One main problem is related to uncertainty. The noise in the continuous data is squashed so that any visual signal that is seen appears more factual than it is in reality13. Also, the pie charts do not show measures of uncertainty; how do we know when two pie charts are “significantly different”?\nAlternatively, Figs. 4 and 5 of their paper shows the results of a logistic regression model where all predictors were left as-is and splines were used to model the probability of the outcome. This has a much simpler interpretation and confidence bands give the reader a sense that the differences are real.\nWhile it is not advisable to discretize most predictors, there are some cases when discretization can be helpful. As a counter-example, one type of measurement that is often appropriate to discretize is date. For example, Kuhn and Johnson (2019) show a data set where daily ridership data was collected for the Chicago elevated train system. The primary trend in the data was whether or not the day was a weekday. Ridership was significantly higher when people commute to work. A simple indicator for Saturday/Sunday (as well as major holiday indicators) was the driving force behind many regression models on those data. In this case, making qualitative versions of the date was rational, non-arbitrary, and driven by data analysis.\nNote that several models, such as classification/regression trees and multivariate adaptive regression splines, estimate cut points in the model-building process. The difference between these methodologies and manual binning is that the models use all the predictors to derive bins based on a single objective (such as maximizing accuracy). They evaluate many variables simultaneously and are usually based on statistically sound methodologies.\nIf it is the last resort, how should one go about discretizing predictors? First, topic specific expertise of the problem can be used to create appropriate categories when categories are truly merited as in the example of creating an indicator for weekend day in the Chicago ridership data. Second, and most important, any methodology should be well validated using data that were not used to build the model (or choose the cut points for binning). To convert data to a qualitative format, there are both supervised and unsupervised methods.\nThe most reliable unsupervised approach is to choose the number of new features and use an appropriate number of percentiles to bin the data. For example, if four new features are required, the 0, 25%, 50%, 75%, and 100% quantiles would be used. This ensures that each resulting bin contains about the same number of samples from the training set.\nIf you noticed that this is basically the same approach suggested for choosing spline knots in the discussion earlier in this chapter, you are correct. This process is very similar to using a zero-order polynomial spline, the minor difference being the placement of the knots. A zero-order model is a simple constant value, usually estimated by the mean. This is theoretically interesting but also enables users to contrast discretization directly with traditional spline basis expansions. For example, if a B-spline was used, the modeler could tune over the number of model terms (i.e., the number of knots) and the spline polynomial degree. If binning is the superior approach, the tuning process would select that approach as optimal. In other words, we can let the data decide if discretization is a good idea14.\nA supervised approach would, given a specific number of new features to create, determine the breakpoints by optimizing a performance measure (e.g., RMSE, classification accuracy, etc.). A good example is a tree-based model (very similar to the process shown in Figure 6.3). After fitting a single tree or, better yet, an ensemble of trees, the split values in the trees can be used as the breakpoints.\nLet’s again use the fossil data from Bralower et al. (1997) illustrated in previous section on basis functions. We can fit a linear regression with qualitative terms for the age derived using:\n\nAn unsupervised approach using percentiles at cut-points.\nA supervised approach where a regression tree model is used to set the breakpoints for the bins.\n\nIn each case, the number of new features requires tuning. Using the basic grid search tools described in Chapter 11, the number of required terms was set for each method (ranging from 2 to 10 terms) by minimizing the RMSE from a simple linear regression model. The results are that both approaches required the same number of new features and produced about the same level of performance; the unsupervised approach required 6 breaks to achieve an RMSE of 0.0000355 and the supervised model has an RMSE of 0.0000302 with 6 cut points. Figure 8.19 shows the fitted model using the unsupervised terms.\n\n\n\n\n\n\n\n\nFigure 8.19: The estimated relationship between fossil age and the isotope ratio previously shown in Figure 8.11, now using discretization methods.\n\n\n\n\n\nThe results are remarkably similar to one another. The blocky nature of the fitted trend reflects that, within each bin, a simple mean is used to estimate the sale price.\nAgain, we want to emphasize that arbitrary or subjective discretization is almost always suboptimal.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#chapter-references",
    "href": "chapters/interactions-nonlinear.html#chapter-references",
    "title": "8  Interactions and Nonlinear Features",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAltman, D G. 1991. “Categorising Continuous Variables.” British Journal of Cancer 64 (5): 975.\n\n\nAltman, D G. 1998. “Suboptimal Analysis Using ’Optimal’ Cutpoints.” British Journal of Cancer 78 (4): 556–57.\n\n\nAltman, D G, B Lausen, W Sauerbrei, and M Schumacher. 1994. “Dangers of Using \"Optimal\" Cutpoints in the Evaluation of Prognostic Factors.” Journal of the National Cancer Institute 86 (11): 829.\n\n\nAltman, D G, and P Royston. 2006. “The Cost of Dichotomising Continuous Variables.” BMJ 332 (7549): 1080.\n\n\nAltorki, Nasser K, Timothy E McGraw, Alain C Borczuk, Ashish Saxena, Jeffrey L Port, Brendon M Stiles, Benjamin E Lee, et al. 2021. “Neoadjuvant Durvalumab with or Without Stereotactic Body Radiotherapy in Patients with Early-Stage Non-Small-Cell Lung Cancer: A Single-Centre, Randomised Phase 2 Trial.” The Lancet Oncology 22 (6): 824–35.\n\n\nApley, D, and J Zhu. 2020. “Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.” Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (4): 1059–86.\n\n\nArnold, T, M Kane, and B Lewis. 2019. A Computational Approach to Statistical Learning. Chapman; Hall/CRC.\n\n\nBarnwell-Menard, JL, Q Li, and A Cohen. 2015. “Effects of Categorization Method, Regression Type, and Variable Distribution on the Inflation of Type-I Error Rate When Categorizing a Confounding Variable.” Statistics in Medicine 34 (6): 936–49.\n\n\nBennette, C, and A Vickers. 2012. “Against Quantiles: Categorization of Continuous Variables in Epidemiologic Research, and Its Discontents.” BMC Medical Research Methodology 12: 21.\n\n\nBralower, T, P Fullagar, C Paull, G Dwyer, and R Leckie. 1997. “Mid-Cretaceous Strontium-Isotope Stratigraphy of Deep-Sea Sections.” Geological Society of America Bulletin 109 (11): 1421–42.\n\n\nBreiman, L. 1988. “Monotone Regression Splines in Action (Comment).” Statistical Science 3 (4): 442–45.\n\n\nBuettner, P, C Garbe, and I Guggenmoos-Holzmann. 1997. “Problems in Defining Cutoff Points of Continuous Prognostic Factors: Example of Tumor Thickness in Primary Cutaneous Melanoma.” Journal of Clinical Epidemiology 50 (11): 1201–10.\n\n\nChen, SH, J Sun, L Dimitrov, A Turner, T Adams, D Meyers, BL Chang, et al. 2008. “A Support Vector Machine Approach for Detecting Gene-Gene Interaction.” Genetic Epidemiology 32 (2): 152–67.\n\n\nChipman, H. 1996. “Bayesian Variable Selection with Related Predictors.” Canadian Journal of Statistics 24 (1): 17–36.\n\n\nCohen, J. 1983. “The Cost of Dichotomization.” Applied Psychological Measurement 7 (3): 249–53.\n\n\nDe Boor, C. 2003. A Practical Guide to Splines. Springer-Verlag.\n\n\nElith, J, J Leathwick, and T Hastie. 2008. “A Working Guide to Boosted Regression Trees.” Journal of Animal Ecology 77 (4): 802–13.\n\n\nFedorov, V, F Mannino, and R Zhang. 2009. “Consequences of Dichotomization.” Pharmaceutical Statistics 8 (1): 50–61.\n\n\nFernandes, A, C Malaquias, D Figueiredo, E da Rocha, and R Lins. 2019. “Why Quantitative Variables Should Not Be Recoded as Categorical.” Journal of Applied Mathematics and Physics 7 (7): 1519–30.\n\n\nFriedman, J, and B Popescu. 2008. “Predictive Learning via Rule Ensembles.” The Annals of Applied Statistics 2 (3): 916–54.\n\n\nGarcía-Magariños, M, I López-de-Ullibarri, R Cao, and A Salas. 2009. “Evaluating the Ability of Tree-Based Methods and Logistic Regression for the Detection of SNP-SNP Interaction.” Annals of Human Genetics 73 (3): 360–69.\n\n\nGauthier, J, QV Wu, and TA Gooley. 2020. “Cubic Splines to Model Relationships Between Continuous Variables and Outcomes: A Guide for Clinicians.” Bone Marrow Transplantation 55 (4): 675–80.\n\n\nGreenwell, B, B Boehmke, and A J McCarthy. 2018. “A Simple and Effective Model-Based Variable Importance Measure.” arXiv.\n\n\nHamada, M, and CF Wu. 1992. “Analysis of Designed Experiments with Complex Aliasing.” Journal of Quality Technology 24 (3): 130–37.\n\n\nHarrel, F, K Lee, and B Pollock. 1988. “Regression Models in Clinical Studies: Determining Relationships Between Predictors and Response.” JNCI: Journal of the National Cancer Institute 80 (15): 1198–1202.\n\n\nHarrell, F. 2015. Regression Modeling Strategies. Springer.\n\n\nHerbinger, J, B Bischl, and G Casalicchio. 2022. “REPID: Regional Effect Plots with Implicit Interaction Detection.” In International Conference on Artificial Intelligence and Statistics, 10209–33.\n\n\nHooker, G. 2004. “Discovering Additive Structure in Black Box Functions.” In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 575–80.\n\n\nInglis, A, A Parnell, and C Hurley. 2022. “Visualizing Variable Importance and Variable Interaction Effects in Machine Learning Models.” Journal of Computational and Graphical Statistics 31 (3): 766–78.\n\n\nIrwin, J R, and G H McClelland. 2003. “Negative Consequences of Dichotomizing Continuous Predictor Variables.” Journal of Marketing Research 40 (3): 366–71.\n\n\nKapelner, A, and J Bleich. 2013. “bartMachine: Machine Learning with Bayesian Additive Regression Trees.” arXiv.\n\n\nKenny, P W, and C A Montanari. 2013. “Inflation of Correlation in the Pursuit of Drug-Likeness.” Journal of Computer-Aided Molecular Design 27 (1): 1–13.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nKuss, O. 2013. “The Danger of Dichotomizing Continuous Variables: A Visualization.” Teaching Statistics 35 (2): 78–79.\n\n\nLampa, E, L Lind, P Lind, and A Bornefalk-Hermansson. 2014. “The Identification of Complex Interactions in Epidemiology and Toxicology: A Simulation Study of Boosted Regression Trees.” Environmental Health 13 (1): 57.\n\n\nLim, M, and T Hastie. 2015. “Learning Interactions via Hierarchical Group-Lasso Regularization.” Journal of Computational and Graphical Statistics 24 (3): 627–54.\n\n\nMacCallum, R C, S Zhang, K J Preacher, and D Rucker. 2002. “On the Practice of Dichotomization of Quantitative Variables.” Psychological Methods 7 (1): 19–40.\n\n\nMaxwell, S, and H Delaney. 1993. “Bivariate Median Splits and Spurious Statistical Significance.” Psychological Bulletin 113 (1): 181–90.\n\n\nMiller, A. 1984. “Selection of Subsets of Regression Variables.” Journal of the Royal Statistical Society. Series A (General), 389–425.\n\n\nMokhtari, Reza Bayat, Tina S Homayouni, Narges Baluch, Evgeniya Morgatskaya, Sushil Kumar, Bikul Das, and Herman Yeger. 2017. “Combination Therapy in Combating Cancer.” Oncotarget 8 (23): 38022–43.\n\n\nMolnar, C. 2020. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. Lulu.com.\n\n\nNaggara, O, J Raymond, F Guilbert, D Roy, A Weill, and D G Altman. 2011. “Analysis by Categorizing or Dichotomizing Continuous Variables Is Inadvisable: An Example from the Natural History of Unruptured Aneurysms.” AJNR. American Journal of Neuroradiology 32 (3): 437–40.\n\n\nOh, S. 2022. “Predictive Case-Based Feature Importance and Interaction.” Information Sciences 593: 155–76.\n\n\nOwen, S, and R Froman. 2005. “Why Carve up Your Continuous Data?” Research in Nursing and Health 28 (6): 496–503.\n\n\nPettersson, M, XiXnjun Hou, M Kuhn, T T Wager, G W Kauffman, and P R Verhoest. 2016. “Quantitative Assessment of the Impact of Fluorine Substitution on P-Glycoprotein (P-gp) Mediated Efflux, Permeability, Lipophilicity, and Metabolic Stability.” Journal of Medicinal Chemistry 59 (11): 5284–96.\n\n\nPlanchard, D, P Jänne, Y Cheng, J Yang, N Yanagitani, SW Kim, S Sugawara, et al. 2023. “Osimertinib with or Without Chemotherapy in EGFR-Mutated Advanced NSCLC.” New England Journal of Medicine 389 (21): 1935–48.\n\n\nRoyston, P, D G Altman, and W Sauerbrei. 2006. “Dichotomizing Continuous Predictors in Multiple Regression: A Bad Idea.” Statistics in Medicine 25 (1): 127–41.\n\n\nSingh, Nina, and Pamela J Yeh. 2017. “Suppressive Drug Combinations and Their Potential to Combat Antibiotic Resistance.” The Journal of Antibiotics 70 (11): 1033–42.\n\n\nStone, C, and CY Koo. 1985. “Additive Splines in Statistics.” Proceedings of the American Statistical Association 45: 48.\n\n\nTaylor, J M G, and M Yu. 2002. “Bias and Efficiency Loss Due to Categorizing an Explanatory Variable.” Journal of Multivariate Analysis 83 (1): 248–63.\n\n\nvan Walraven, C, and R Hart. 2008. “Leave ’Em Alone - Why Continuous Variables Should Be Analyzed as Such.” Neuroepidemiology 30 (3): 138–39.\n\n\nWang, Y. 2011. Smoothing Splines: Methods and Applications. CRC Press.\n\n\nWood, S. 2006. Generalized Additive Models: An Introduction with R. Chapman; Hall/CRC.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#footnotes",
    "href": "chapters/interactions-nonlinear.html#footnotes",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "Alternatively, PCA after skewness-correcting transformations may be another good option.↩︎\nThese were computed using the bootstrap, as in Section 2.3.↩︎\nIt so happens that the Gaussian likelihood function is equivalent to the sums of squared errors (SSE). That, in turn, is equivalent to the RMSE. That simplification does not automatically occur for other probability distributions.↩︎\nThis approach works because the model in Equation 8.1 is capable of estimating the interaction. There are many models that do not have the ability to measure interaction effects, and, for this case, it would be impossible to isolate the interaction term(s). However, tree-based ensembles are good at estimating interactions, as are other complex black-box models such as neural networks and support vector machines. The tools described below only work with “interaction capable” models.↩︎\nIn actuality, a more computationally preferred method is to use orthogonal polynomials which rescale the variables before we exponentiate them. We’ll denote these modified versions of the predictor in equation Equation 8.3 as just \\(x\\) to reduce mathematical clutter. We’ve seen these before in Figure 6.4↩︎\nThis can be seen in the basis functions above the plot: each line covers the entire range of the data.↩︎\nActually, they are the interior knots. The full set of knots includes the minimum and maximum values of \\(x\\) in the training set.↩︎\nAlso called a restricted cubic spline in Harrel, Lee, and Pollock (1988).↩︎\nIn this particular case, the variance becomes very large since the number of parameters is nearly the same as the number of training set points (30). This makes the underlying mathematical operation (matrix inversion) numerically unstable. Even so, it is the result of excessive model complexity.↩︎\nThis specific example will be referenced again when discussing resampling in Chapter 10.↩︎\nAlso known as binning or dichotomization.↩︎\nIn the immortal words12 of Lucy D’Agostino McGowan and Nick Strayer: “Live free or dichotomize.”↩︎\nKenny and Montanari (2013) does an excellent job illustrating this issue.↩︎\nBut is most likely not a good idea.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html",
    "href": "chapters/overfitting.html",
    "title": "9  Overfitting",
    "section": "",
    "text": "9.1 Model Complexity and Overfitting\nThis chapter describes the most crucial concept in predictive modeling: overfitting. When a model fails, it is almost always due to overfitting of some sort. The problem is that you may only realize that there is an issue once you finish the development phase of modeling and are exposed to completely new data.\nTo get started, we’ll discuss model complexity and how it can be both good and bad.\nMany predictive models have parameters. For example, the linear regression model initially shown in Equation 8.3 contains slopes and intercepts (the \\(\\beta\\) values) estimated during model training using the predictor and outcome data. In other words, it is possible to write the equations to solve to estimate these values.\nHowever, in that same equation, a series of polynomial terms were used. The number of additional features is also a parameter but, unlike the \\(\\beta\\) coefficients, it cannot be directly estimated from the data. Equation 8.3 arbitrarily used three terms for the predictor \\(x\\).\nTwo other models were discussed in Chapter 2. First, the tree-based model shown in Figure 2.5 had five splits of the data. This led to a model fit that resulted in six regions of the \\(x\\) space, each with its specific predicted value. We could have continued to split the training set into finer and finer partitions so that the fitted curve more effectively modeled the observed data. As such, the depth of the tree was a parameter that needed to be set before the model fit could be estimated.\nSimilarly, the neural network required the number of hidden units to be defined (among other values). We weren’t sure what that should be, so a set of candidate values were assessed, and the one with the best results was used for the final model fit (see Figure 2.7). We’ve also seen specific tuning parameters in Section 6.4.2, Section 6.4.4, and also for the embedding methods in Chapter 7.\nIn each of these cases, there was a tuning parameter (a.k.a., hyperparameter) that defined the structure of the model and could not be directly estimated from the data. Increasing the parameter’s value for each model made the model’s potential structure more elaborate. The effect of this increase in the model’s complexity is additional capability to adapt to any pattern seen in the training data. The straight line fit associated with a linear regression model without polynomial terms would have done an abysmal job on such a nonlinear data set. Adding additional complexity allowed it to better conform to the training set data.\nThe problem is that added complexity can also add risk. Being more capable of adapting to subtle trends in the training data is only a good thing when such trends generalize to other data. Recall in Section 6.4.3 where we were estimating the effect of an agent on the price of a hotel room. One agent had a single booking. The naive estimate of the ADR wouldn’t generalize well. As they accrue more and more bookings, their mean would undoubtedly change to a more realistic value. Using the raw ADR value would “overfit to the training set data.”\nFor many sophisticated models, the complexity can be modulated via one or more tuning parameters that detect inconsequential patterns in the training data. The problem is that they may go too far and over-optimize the model fit.\nAs an example, Figure 9.1 shows a scenario where there are two predictors (A and B) and each point belongs to one of two classes (vermilion circles or blue triangles). These data were simulated and panel (a) shows a thick grey line representing the true class boundary where there is a 50% chance that a data point belongs to either class. Points to the left of the grey curve are more likely to be circles than points to the right. The curve itself has a smooth, parabolic shape.\nWe can fit a model to these data and estimate the class boundary. The model we used1 has a tuning parameter called the cost value. For now, you can think of this parameter as one that discourages the model from making an incorrect prediction (i.e., placing it on the wrong side of the estimated boundary). In other words, as the cost parameter increases, the model is more and more incentivized to be accurate on the training data.\nWhen the cost is low, the results are often boundaries with low complexity. Figure 9.1(b) shows the result where the fitted boundary is in black; points on the left of the boundary would be classified as circles. There is some curvature and, while correctly classifying the points in the middle of the training set, it could do a better job of emulating the true boundary. This model is slightly underfit since more complexity would make it better.\nFigure 9.1: Examples of how a model can overfit the training data across different levels of complexity.\nWhen the cost is increased, the complexity also increases (Figure 9.1(c)). The boundary is more parabolic, and a few more training set points are on the correct side of the estimated line.\nFigure 9.1(d) shows what occurs when the model is instructed that it is incredibly bad when incorrectly predicting a data point in the training set. Its boundary contorts in a manner that attempts to gain a few more points of accuracy. Two specific points, identified by black arrows, appear to drive much of the additional structure in the black curve. Since we know the actual boundary, it’s easy to see that these contortions will not reproduce with new data. For example, it is implausible that a small island of vermilion circles exists in the mainstream of blue triangles.\nAnother sign that a model probably has too much complexity is the additional component of the decision boundary at the bottom of Figure 9.1(d) (red arrow). Points inside this part of the boundary would be classified as triangles even though no triangles are near it. This is a sort of “blowback” of an overly complex model where choices the model makes in one part of the predictor space add complexity in a completely different part of the space. This blowback is often seen in areas where there are no data. It’s unnecessary and points to the idea that the model is trying to do too much.\nThis boundary demonstrates how added complexity can be a bad thing. A test set of data was simulated and is shown in Figure 9.2. The performance for this overly complex model is far from optimal since the extra areas induced by the high cost value do not contain the right class of points.\nFigure 9.2: The high complexity fit from Figure 9.1(d) superimposed over a test set of data.\nOur goal is to find tuning parameter values that are just right: complex enough to accurately represent the data but not complex enough to over-interpret it.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-how-to-overfit",
    "href": "chapters/overfitting.html#sec-how-to-overfit",
    "title": "9  Overfitting",
    "section": "9.2 The Ways Models Overfit",
    "text": "9.2 The Ways Models Overfit\nThis leads us to a formal definition of overfitting:\n\nOverfitting is the situation where a model over-interprets trends and patterns in the training set that do not generalize to new data.\nThese irreproducible data trends result in models that do very well on the training set but greatly underperform on the validation set.\n\nWe can overinterpret the training set data in a few different ways. For example, ?sec-cls-imbalance will discuss class imbalance problems that occur when one or more outcome classes have a very low probability of occurrence. As models are allowed more complexity, they will often maximize accuracy (or some proxy for it) by simply declaring that all data points belong to the majority class. If there is a 1% event rate, 99% accuracy is easily achievable by overfitting to the majority class.\nAnother example is overfitting the predictor set. The “low \\(N\\), high \\(P\\)” problem (Johnstone and Titterington 2009) occurs when data points are expensive to obtain, but each comes with abundant information. For example, in high-dimensional biology data, it is common to have dozens of data points and hundreds of thousands of predictors. Many models cannot be estimated with this imbalance of dimensions, and often, the first step is to filter out uninformative predictors.\nSuppose some statistical inferential method, such as a t-test, is used to determine if a predictor can differentiate between two classes. Small sample sizes make the chance of uninformative predictors falsely passing the filter large. If the same data are then used to build and evaluate the model, there is a significant probability that an ineffective model will appear to be very predictive (Ambroise and McLachlan 2002; Kuhn and Johnson 2019). Again, using a data set external to the model training and development process shows you when this occurs.\nFinally, it is possible to overfit a model via sample filtering. For example, in some machine learning competitions, participants might sub-sample the training set to be as similar to the unlabelled test set as possible. This will most likely improve their test set performance, but it generally handicaps a model when predicting a new yet-to-be-seen data set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-external-validation",
    "href": "chapters/overfitting.html#sec-external-validation",
    "title": "9  Overfitting",
    "section": "9.3 External Data to Measure Effectiveness",
    "text": "9.3 External Data to Measure Effectiveness\nBy now, it should seem clear that the best way of avoiding overfitting is to quantify the model performance using a separate data set. How do we do that? We’ve said that most modeling projects create an initial partition of the data into training and testing sets. Can the testing set detect overfitting?\nWe’ll again use the classification data from Figure 9.1 to demonstrate, this time with a different model. A boosted tree2 is an ensemble model. The model creates a sequence of decision trees, each depending on the previous, to create a large ensemble of individual trees. For a new sample, each tree is used to predict the outcome, and a weighted average is used to create the predicted class probabilities.\nThe model has tuning parameters related to the tree (e.g., tree depth) and ones related to the process of building the ensemble (e.g., the number of trees in the ensemble). We’ll use 100 trees to create our ensemble. However, we don’t know how deep the trees should be. One method of controlling the number of splits in a tree is to control how much data should be in a partition to continue splitting. For example, in Figure 2.5, the left-most terminal node in the tree contained \\(n_{tr}\\) = 971 data points after all splits are applied. If we keep splitting with no constraint, at some point, the amount of data in the node is too small to continue.\nThis tuning parameter, we’ll call it \\(n_{min}\\), is used to moderate the tree-growing process. Small values increase the risk of overfitting; it’s possible to have a single data point left in the terminal node. Large values prevent the tree from accurately modeling the data, and underfitting occurs. What does the trade-off between complexity and performance look like for these data?\nWe took the training data and fit models using values \\(n_{min}\\) = 3, 4, …, 80. To understand performance, the Brier score (Brier 1950; Steyerberg 2009) is used. It is an error metric for classification models (similar to RMSE) and measures how close the predicted probabilities are to their binary (i.e., 0/1) values. A Brier score of zero is a perfect model, and ineffectual models tend to have values \\(\\ge\\) 0.25. See Section 15.7.2 for more details.\nFigure 9.3 shows the results. The yellow curve illustrates the Brier score when the training set is re-predicted. There is a sharp increase in error as the tuning parameter values increase. This line suggests that we use very small values. The smallest Brier score is associated with \\(n_{min}\\) = 3 (producing an effectively zero Brier score).\n\n\n\n\n\n\n\n\nFigure 9.3: An example to show how complexity and model error, measured two ways, interact. The tuning parameter on the x-axis is \\(n_{min}\\) used for moderating tree growth.\n\n\n\n\n\nThe purple line shows the Brier score trend on the test set. This tells a different story where small \\(n_{min}\\) values overfit, and the model error improves as larger values are investigated. The Brier score is lowest around \\(n_{min}\\) = 39 with a corresponding Brier score estimate of 0.0928. From here, increasing the tuning parameter results in shallow trees that underfit. As a result the Brier score slowly increases.\nContrasting these two curves, data that are external to the training set gives a much more accurate picture of performance. However:\n\nDo not use the test set to develop your model.\n\nWe’ve only used it here to show that an external data set is needed to effectively optimize model complexity. If we only have training and testing data, what should we do?\nThere are two ways to solve this problem:\n\nUse a validation set.\nResample the training set Chapter 10.\n\nBoth of these options are described in detail in the next chapter.\nAs previously discussed, validation sets are a third split of the data, created at the same time as the training and test sets. They can be used as an external data set during model development (as in Chapter 2). This approach is most applicable when the project starts with abundant data.\nResampling methods take the training set and make multiple variations of it, usually by subsampling the rows of the data. For example, 10-fold cross-validation would make ten versions of the training set, each with a different 90% of the training data. We would say that, for this method, there are ten “resamples” or “folds.” To measure performance, we would fit ten models on the majority of the data in each fold, then predict the separate 10% that was held out. This would generate ten different Brier scores, which are averaged to produce the final resampling estimate of the model. If we are trying to optimize complexity, we will apply this process for every candidate tuning parameter value.\nFigure 9.4 shows the results when 10-fold cross-validation is used to pick \\(n_{min}\\). The points on the additional curve are the averages of 10 Brier scores from each fold for each value of \\(n_{min}\\). The resampling curve doesn’t precisely mimic the test set results but does have the same general pattern: there is a real improvement in model error as \\(n_{min}\\) is increased but it begins to worsen due to underfitting. In this case, resampling finds that \\(n_{min}\\) = 56 is numerically best and the resampling estimate of the Brier score was 0.104, slightly more pessimistic when compared to the test set result.\n\n\n\n\n\n\n\n\nFigure 9.4: Model complexity versus model error with external validation via resampling.\n\n\n\n\n\nWe suggest using either of these two approaches (validation set or resampling) to measure performance during model development. Before going to the next chapter, let’s give a preview of Chapters 11 and 12.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-optimizing-complexity",
    "href": "chapters/overfitting.html#sec-optimizing-complexity",
    "title": "9  Overfitting",
    "section": "9.4 How Should We Optimize Complexity?",
    "text": "9.4 How Should We Optimize Complexity?\nNow that we know how to appropriately estimate metrics for evaluating effectiveness, how do we choose tuning parameter values? Do we pick them at random?\nOperationally, there are two main classes of search routines for optimizing model complexity via tuning parameters. The oldest is grid search. For each tuning parameter in the model, we define a reasonable set of specific candidate values to investigate. When there is more than one tuning parameter, there are various ways to create a multidimensional grid (these are discussed in Chapter 11). Each candidate in the grid is evaluated and the “best” combination of parameter values is chosen as the one to use (if we think the model works well enough).\nOne tuning parameter used in tree-based ensembles and neural networks is the learning rate. This parameter typically governs how quickly a model adapts during iterations of model fitting (i.e., training). In gradient descent, the learning rate specifies how far we proceed in the optimal direction. For machine learning models, the parameter must be greater than zero and usually has a maximum value around 0.1 to 1.0. When evaluating different learning rates, it is common to think of them in logarithmic units (typically, base 10). This is because the effect of the parameter on model performance is often nonlinear; a per-unit change of 0.05 has different effects on where in the total range that it occurs.\nSuppose that the performance goal is to minimize some notion of error (e.g., RMSE, the Brier score, etc.). Figure 9.5 shows an example of how the learning rate can affect the model error3. At low rates, the error is high. Increasing the rate results in a drop in error that reaches a trough of optimal performance. However, increasing the learning rate at some point causes a different type of underfitting4, increasing the error.\nThe solid points illustrate a very simple five-point grid of errors whose y-axis value is the resampled error. While this grid does not pick the absolute best value, it does result in a model with good performance (relatively speaking) very quickly. If a larger grid were used, we would have placed a grid point much closer to the optimal value.\n\n\n\n\n\n\n\n\nFigure 9.5: An example of grid search containing five candidates for a learning rate parameter.\n\n\n\n\n\nGrid search pre-defines the grid points and all of them are tested before the results can be analyzed. There are good and bad aspects of this method, and it has been (unjustly) criticized for being inefficient. For most models, it can be very efficient and there are additional tools and tricks to make it even faster. See Chapter 11 for more details.\nThe other type of search method is iterative. These tools start with one or more initial candidate points and conduct analyses that predict which tuning parameter value(s) should be evaluated next. The most widely used iterative tuning method is Bayesian optimization (Močkus 1975; Gramacy 2020; Garnett 2023). After each candidate is evaluated, a Bayesian model is used to suggest the next value and this process repeats until a pre-defined number of iterations is reached. Any search method could be used, such as simulated annealing, genetic algorithms, and others.\nFigure 9.6 has an animation to demonstrate. First, three initial points were sampled (shown as open circles). A Bayesian model is fit to these data points and is used to predict the probability distribution of the metric (e.g. RMSE, Brier score, etc.). A tool called an acquisition function is utilized to choose which learning rate value to evaluate on the next search iteration based on the mean and variance of the predicted distribution. In Figure 9.6, this process repeats for a total of 9 iterations.\n\n\n\n\n\n\n\n\nFigure 9.6: An example of iterative optimization. The open circles represent the three initial points and the solid circles show the progress of the Bayesian optimization of the learning rate.\n\n\n\n\n\nThe animation shows that the search evaluated very disparate values, including the lower and upper limits that were defined. However, after a few iterations, the search focuses on points near the optimal value. Again, this method discussed in more detail in Chapter 12.\nAll of our optimization methods assume that a reasonable range of tuning parameters is known. The range is naturally bounded in some cases, such as the number of PCA components. Otherwise, you (and/or the ML community) will develop a sense of appropriate ranges of these parameters. We will suggest basic ranges here. For example, for the learning rate, we know it has to be greater than zero but the fuzzy upper bound of about 0.1 is a convention learned by trial and error.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-permutations",
    "href": "chapters/overfitting.html#sec-permutations",
    "title": "9  Overfitting",
    "section": "9.5 Sidebar: How Bad Can the Model Be?",
    "text": "9.5 Sidebar: How Bad Can the Model Be?\nWhen a model is overfit, the results can be unduly optimistic. Let’s take a minute to consider the opposite: ff we compute some performance measure, what’s the worst value we can achieve?\nFor some metrics, the possible range of values is well-defined. For example, for the coefficient of determination (a.k.a. R2), a value of zero means that the model explains none of the variation in the outcome. In classification, when computing the area under the ROC curve Section 15.7.3, a value of 0.5 indicates that the model has no ability to discriminate between the classes. In these instances, the range of metric values is not dependent on the outcome data distribution.\nAs a counter-example, the mean absolute deviation (MAE) used in Chapter 2 to measure the effectiveness of our time-to-delivery models does depend on the outcome data. We know a value of zero is best, but what is the worst it can be?\nThe worst performance occurs when no relationship exists between the observed and predicted outcomes. To translate this to a worst-case metric value, we can use a permutation strategy where we break the relationship by randomly shuffling the outcome data and then compute the metric.\nSince it is possible to get “a bad shuffle” that doesn’t completely break the relationship, we can do this multiple times and then take the average of these values.\nFor example, we’ve said above that the best Brier score is zero and that poor models “have values \\(\\ge\\) 0.25.” We used the predictions from the high complexity fit from Figure 9.1 and shuffled the outcome 50 times. The average permuted Brier score for this data set was 0.321.\nPermuted performance values will show up several times in subsequent chapters.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#chapter-references",
    "href": "chapters/overfitting.html#chapter-references",
    "title": "9  Overfitting",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBrier, G. 1950. “Verification of Forecasts Expressed in Terms of Probability.” Monthly Weather Review 78 (1): 1–3.\n\n\nGarnett, R. 2023. Bayesian Optimization. Cambridge University Press.\n\n\nGramacy, R. 2020. Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.\n\n\nJohnstone, I, and M Titterington. 2009. “Statistical Challenges of High-Dimensional Data.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 367 (1906): 4237–53.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nMočkus, J. 1975. “On Bayesian Methods for Seeking the Extremum.” In Optimization Techniques IFIP Technical Conference Novosibirsk, edited by G Marchuk, 400–404. Springer Berlin Heidelberg.\n\n\nSteyerberg, E. 2009. Clinical Prediction Models. Springer.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#footnotes",
    "href": "chapters/overfitting.html#footnotes",
    "title": "9  Overfitting",
    "section": "",
    "text": "A support vector machine. See Section 17.6.↩︎\nThese are described in more detail in ?sec-cls-boosting↩︎\nThis was taken from a real example.↩︎\nA very high learning rate for tree-based ensembles can result in models that produce near-constant predicted values (as opposed to many unique predicted values that are inaccurate).↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html",
    "href": "chapters/resampling.html",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "10.1 The Resampling Process and Nomenclature\nIn Section 9.3, it was shown that using the same data to estimate and evaluate our model can produce inaccurate estimates of how well it functions to predict new samples. It also described using separate partitions of the data to fit and evaluate the model.\nThere are two general approaches for using external data for model evaluation. The first is a validation set, which we’ve seen before in Section 3.7. This is a good idea if you have a lot of data on hand. The second approach is to resample the training set. Resampling is an iterative approach that reuses the training data multiple times based on sound statistical methodology. We’ll discuss both validation sets and resampling in this chapter.\nFigure 10.1 shows a standard data usage scheme that incorporates resampling. After an initial partition, resampling creates multiple versions of the training set. We’ll use special terminology1 for the data partitions within each resample which serve the same function as the training and test sets:\nLike the training and test sets, the analysis and assessment sets are mutually exclusive data partitions and are different for each of the B iterations of resampling.\nFigure 10.1: A general data usage scheme that includes resampling. The colors denote the data used to train the model (in tan) and the separate data sets for evaluating the model (colored periwinkle).\nThe resampling process fits a model to an analysis set and predicts the corresponding assessment set. One or more performance statistics are calculated from these held-out predictions and saved. This process continues for B iterations, and, in the end, there is a collection of B statistics of efficacy. These are averaged to produce the overall resampling estimate of performance. To formalize this, we’ll consider a mapping function \\(M(\\mathfrak{D}^{tr}, B)\\) that takes the training set as input and can output \\(B\\) sets of analysis and assessment sets.\nAlgorithm 10.1 describes this process.\nThere are many different resampling methods, such as cross-validation and the bootstrap. They differ in how the analysis and assessment sets are created in line 7 of Algorithm 10.1. Subsequent sections below discuss a number of mapping functions.\nTwo primary aspects of resampling make it an effective tool. First, the separation of data used to create and appraise the model avoids the data reuse problem described in Section 9.3.\nSecond, using multiple iterations (B &gt; 1) means that you are evaluating your model under slightly different conditions (because the analysis and assessment sets are different). This is a bit like a “multiversal” science fiction story: what would have happened if the situation were slightly different before I fit the model? This lets us directly observe the model variance; how stable is it when the inputs are slightly modified? An unstable model (or one that overfits) will have a high variance.\nIn this chapter, we’ll describe some conceptual aspects of resampling. Then, we’ll define and discuss various methods. Finally, the last section lists frequently asked questions we often hear when teaching these tools.\nUp until Section 10.7, we will assume that each row of the data are independent. We’ll see examples of non-independent in the later sections.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampling-basics",
    "href": "chapters/resampling.html#sec-resampling-basics",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "The analysis set (of size \\(n_{fit}\\)) estimates quantities associated with preprocessing, model training, and postprocessing.\nThe assessment set (\\(n_{pred}\\)) is only used for prediction so that we can compute measures of model effectiveness (e.g., RMSE, classification accuracy, etc).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{algorithm} \\begin{algorithmic} \\State $\\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$ \\State $B$: number of resamples \\State $M(\\mathfrak{D}^{tr}, B)$: a mapping function to split $\\mathfrak{D}^{tr}$ for each of $B$ iterations. \\State $f()$: model pipeline \\Procedure{Resample}{$\\mathfrak{D}^{tr}, f, M(\\mathfrak{D}^{tr}, B)$} \\For{$b =1$ \\To $B$} \\State Partition $\\mathfrak{D}^{tr}$ into $\\{\\mathfrak{D}_b^{fit}, \\mathfrak{D}_b^{pred}\\}$ using $M_b(\\mathfrak{D}^{tr}, B)$. \\State Train model pipeline $f$ on the analysis set to produce $\\hat{f}_{b}(\\mathfrak{D}_b^{fit})$. \\State Generate assessment set predictions $\\hat{y}_b$ by applying model $\\hat{f}_{b}$ to $\\mathfrak{D}_b^{pred}$. \\State Estimate performance statistic $\\hat{Q}_{b}$. \\EndFor \\State Compute resampling estimate $\\hat{Q} = \\sum_{b=1}^B \\hat{Q}_{b}$. \\Return $\\hat{Q}$. \\Endprocedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nAlgorithm 10.1: Resampling models to estimate performance.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Algorithm 10.1, \\(f()\\) is the model pipeline described in Section 1.5. Any estimation methods, before, during, or after the supervised model, are executed on the analysis set B times during resampling.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampling-goals",
    "href": "chapters/resampling.html#sec-resampling-goals",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.2 What is Resampling Trying to Do?",
    "text": "10.2 What is Resampling Trying to Do?\nIt’s important to understand some of the philosophical aspects of resampling. It is a statistical procedure that tries to estimate some true, unknowable performance value (let’s call it \\(Q\\)) associated with the same population of data represented by the training set. The quantity \\(Q\\) could be RMSE, the Brier score, or any other performance statistic.\nThere have been ongoing efforts to understand what resampling methods do on a theoretical level. Bates, Hastie, and Tibshirani (2023) is an interesting work focused on linear regression models with ordinary least squares. Their conclusion regarding resampling estimates is that they\n\n… cannot be viewed as estimates of the prediction error of the final model fit on the whole data. Rather, the estimate of prediction error is an estimate of the average prediction error of the final model across other hypothetical data sets from the same distribution.\n\nIt is a good idea to think that this will also be true for more complex machine learning models. The important idea is that resampling is measuring performance on training sets similar to ours (and the same size).\nTo understand different methods, let’s examine some theoretical properties of resampling estimates that matter to applied data analysis.\nLet’s start with analogy; consider the usual sample mean estimator used in basic statistics (\\(\\bar{x}\\)). Based on assumptions about the data, we can derive an equation that optimally estimates the true mean value based on some criterion. Often, the statistical theory focuses on our estimators’ theoretical mean and variance. For example, if the data are independent and follow the same Gaussian distribution, the sample mean attempts to estimate the actual population mean (i.e., it is an unbiased estimator). Under the same assumptions we can also derive what the estimator’s theoretical variance. These properties, or some combination of them, such as mean squared error2, help guide us to the best estimator.\nThe same is true for resampling methods. We can understand how their bias and variance change under different circumstances and choose an appropriate technique. To demonstrate resampling methods, we’ll focus on bias and variance as the main properties of interest.\nFirst is bias: how accurately does a resampling technique estimate the true population value \\(Q\\)? We’d like it to be unbiased, but to our knowledge, that is nearly impossible. However, some things we do know. Figure 10.1 shows that the training set (of size \\(n_{tr}\\)) is split into B resampling partitions which are then split into analysis and assessment sets of size \\(n_{fit}\\) and \\(n_{pred}\\), respectively. It turns out that, as \\(n_{pred}\\) becomes larger, the bias increases in a pessimistic direction3. In other words, if our training set has 1,000 samples, a resampling method where the assessment set has \\(n_{pred} = 100\\) data points has a smaller bias than one using \\(n_{pred} = 500\\). Another thing that we know is that increasing the number of resamples (B) can’t significantly reduce the bias. Therefore, we must think carefully about the way the data is partitioned in the resampling process to reduce potential bias.\nThe variance (often called precision) of resampling is also important. We want to get a performance estimate that gives us consistent results if we repeat it. The resampling precision is driven mainly by B and \\(n_{tr}\\). With some resampling techniques, if your results are too noisy, you can resample more and stabilize or reduce the estimated variance (at the cost of increased computational time). We’ll examine the bias and precision of different methods as we discuss each method.\nLet’s examine a few specific resampling methods, starting with the most simple: a single validation set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-validation",
    "href": "chapters/resampling.html#sec-validation",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.3 Validation Sets",
    "text": "10.3 Validation Sets\nWe’ve already described a validation set; it is typically created via a three-way initial split4 of the data (as shown in Figure 10.2). For time series data, it is common to use the most recent data for the test set. Similarly, the validation set would include the most recent data (once the test set partition is created). The training set would include everything else.\n\n\n\n\n\n\n\n\nFigure 10.2: An initial data splitting scheme that incorporates a validation set.\n\n\n\n\n\nWhile not precisely the same, a validation set is extremely similar to an approach described below called Monte Carlo cross-validation (MCCV). If we used a single MCCV resample, the results would be effectively the same as those of a validation set; the difference is not substantive.\nOne aspect of using the validation set is what to do with it after you’ve made your final decision about the model pipeline. Ordinarily, the entire training set is used for the final model fit.\nData can be scarce and, in some cases, an argument can be made that the final model fit could include the training and validation set. Doing this does add some risk; your validation set statistics are no longer completely valid since they measured how well the model works with similar training sets of size \\(n_{tr}\\). If you have an abundance of data, the risk is low, but, at the same time, the model fit won’t change much by adding \\(n_{val}\\) data points. However, if your training data set is not large, adding more data could have a profound impact, and you risk using the wrong model for your data5.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-cv-mc",
    "href": "chapters/resampling.html#sec-cv-mc",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.4 Monte Carlo Cross-Validation",
    "text": "10.4 Monte Carlo Cross-Validation\nMonte Carlo cross-validation emulates the initial train/test partition. For each one of B resamples, it takes a random sample of the training set (say about 75%) to use as the analysis set6. The remainder is used for model assessment. Each of the B resamples is created independently of the others, so some of the training set points are included in multiple assessment sets. Figure 10.3 shows the results of \\(M(\\mathfrak{D}^{tr}, 3)\\) for MCCV with a data set with \\(n_{tr} = 30\\) data points and 80% of the data are allocated to the analysis set. Note that samples 16 and 17 are in multiple assessment sets.\n\n\n\n\n\n\n\n\nFigure 10.3: A schematic of three Monte Carlo cross-validation resamples created from an initial pool of 30 data points.\n\n\n\n\n\nHow many resamples should we use, and what proportion of the data should be used for the analysis set? These choices are partly driven by computing power and training set size. Clearly, using more resamples means better precision.\nA thought experiment can be useful for any resampling method. Based on the proportion of data going into the assessment set, how confident would you feel in a performance metric being computed for this much data? Suppose that we are computing the RMSE for a regression model. If our assessment set contained \\(n_{pred} = 10\\) (on average), we might think our metric’s mean is excessively noisy. In that case, we could either increase the proportion held out (better precision, worse bias) or resample more (better precision, same bias, long computational time).\nEach resampling method has different trade-offs between bias and variance. Let’s look at some simulation results to help understand the trade-off for MCCV.\n\nAnother Simulation Study\nSection 9.1 described a simulated data set that included 200 training set points. In that section, Figure 9.1 illustrates the effect of overfitting using a particular model (KNN). This chapter will use the same training set but with a simple logistic regression model where a four-degree-of-freedom spline was used with each of the two predictors.\nSince these are simulated data, an additional, very large data set was simulated and used to approximate the model’s true performance, once again evaluated using a Brier score. Our logistic model has a Brier score value of \\(Q \\approx\\) 0.0898, which demonstrates a good fit. Using this estimate, the model bias can be computed by subtracting the resampling estimate from 0.0898.\nThe simulation created 500 realizations of this 200 sample training set7. We resampled the logistic model for each and then computed the corresponding Brier score estimate from MCCV. This process was repeated using the different analysis set proportions and values of B shown in Figure 10.4. The left panel shows that the bias8 decreases as the proportion of data in the analysis set becomes closer to the amount in the training set. It is also apparent that the bias is unaffected by the number of resamples (B). The panel on the right shows the precision, estimated using the standard error, of the resampled estimates. This decreases nonlinearly as B increases; the cost-benefit ratio of adding more resamples shows eventual diminishing returns. The amount retained for the analysis set shows that values close to one have higher precision than the others. Regarding computational costs, the time to resample a model increases with both parameters (amount retained and B).\n\n\n\n\n\n\n\n\nFigure 10.4: Variance and bias statistics for simulated data using different configurations for Monte Carlo cross-validation. The range of the y-axes are common across similar plots below for different resampling techniques.\n\n\n\n\n\nThe results of this simulation indicate that, in terms of precision, there is little benefit in including more than 70% of the training set in the assessment set. Also, while more resamples always help, there is incremental benefit in using more than 50 or 60 resamples (for this size training set). Regarding bias, the decrease somewhat slows down near 80% of the training set retained. Holdout proportions of around 75% - 80% might be a reasonable rule of thumb (these are, of course, subjective and based on this simulation).",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-cv",
    "href": "chapters/resampling.html#sec-cv",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.5 V-Fold Cross-Validation",
    "text": "10.5 V-Fold Cross-Validation\nV-fold cross-validation (sometimes called K-fold cross-validation) (Stone 1974) is the most well-known resampling method9. It randomly allocates the training data to one of V groups of about equal size, called a “fold” (a stratified allocation can also be used). There are B = V iterations where the analysis set comprises V -1 of the folds, and the remaining fold defines the assessment set. For example, for 10-fold cross-validation, the ten analysis sets consist of 90% of the data, and the ten assessment sets contain the remaining 10% of the data and are used to quantify performance. As a visual illustration, Figure 10.5 shows the process for V = 3 (for brevity), for a data set with 30 samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.5: An example mapping with \\(V = 3\\)-fold cross-validation.\n\n\n\nArlot and Celisse (2010) is a comprehensive survey of cross-validation. Let’s look at two special cases.\n\nLeave-One-Out Cross-Validation\nLeave-one-out cross-validation (LOOCV, also called the jackknife) sets \\(V = n_{tr}\\). A single sample is withheld and, over \\(n_{tr}\\) iterations, a set of models is created, each with an analysis set of \\(n_{tr} - 1\\) samples. The resampling estimate is created by applying the metric function to the resulting \\(n_{tr}\\) predicted values. There are no replicate performance values; only a single \\(\\hat{Q}\\). As one might expect, the bias is nearly zero for this method. Although it would be difficult to quantify, the standard error of the estimator should be very large.\nThis method is extremely computationally expensive10 and is not often used in practice unless computational shortcuts are available.\n\n\nRepeated Cross-Validation\nOne way to improve precision of this method is to use repeated V-fold cross-validation. In this case, V-fold CV is repeated multiple times using different random number seeds. For example, two repeats of 10-fold cross-validation11 create two sets of V folds, which are treated as a collection of twenty resamples (e.g., B = V \\(\\times\\) R for R repeats). Again, increasing the number of resamples does not generally change the bias.\n\n\nVariance and Bias for v-Fold Cross-Validation\nThe discussions of LOO and repeated cross-validation beg the question: what value of V should we use? Sometimes the choice of V is driven by computational costs; smaller values of V require less computation time. A second consideration for choice of V is metric performance in terms of bias and variance. As we will see later in this chapter, the degree of bias is determined by how much data are retained in the assessment sets (as defined by V). As V increases, bias decreases; V=5 has substantially more bias than V=10. Fushiki (2011) describes post hoc methods for reducing the bias for this resampling method.\nCompared to other resampling methods, V-fold cross-validation generally has a smaller bias, but relatively poor precision (that would be improved via repeats).\nTo understand the trade-off, the same simulation approach from the previous section is used. Figure 10.6 shows bias and variance results, where the x-axis is the total number of resamples \\(V\\times R\\). As expected, the bias decreases with V and is constant over the number of resamples. Five-fold cross-validation stands out as particularly bad with a percent bias of roughly 3.9%. Using ten folds decreases this to about 2.2%. The precision values show that smaller values of V have better precision; 10-fold CV needs substantially more resamples to have the same standard error than V = 5.\n\n\n\n\n\n\n\n\nFigure 10.6: Variance and bias statistics for simulated data using different configurations for \\(V\\)-fold cross-validation.\n\n\n\n\n\nIs there any advantage to using V &gt; 10? Not much. The decrease in bias is small and many more replicates are required to reach the same precision as V = 10. For example, twice as many resamples are required for 20-fold CV to match the variance of 10-fold CV.\nFor this simulation, the properties are about the same when MCCV and 10-fold CV are matched in terms of number of resamples and the amount allocated to the assessment set.\nOur recommendation is to almost always use V = 10. The bias and variance improvements are both good with 10 folds. Reducing bias in 5-fold cross-validation is difficult but the precision for 10-fold can be improved by increased replication.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-bootstrap",
    "href": "chapters/resampling.html#sec-bootstrap",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.6 The Bootstrap",
    "text": "10.6 The Bootstrap\nThe bootstrap (Efron 1979, 2003; Davison and Hinkley 1997; Efron and Hastie 2016) is a resampling methodology originally created to compute the sampling distribution of statistics using minimal probabilistic assumptions12. In this chapter, we’ll define a bootstrap sample and show how it can be used to measure fit quality for predictive models.\nFor a training set with \\(n_{tr}\\) data points, a bootstrap resample takes a random sample of the training set that is also size \\(n_{tr}\\). It does this by sampling with replacement; when each of the \\(n_{tr}\\) samples is drawn, it has no memory of the prior selections. This means that each row can be randomly selected again. For example, Figure 10.7 shows another schematic for three bootstrap samples. In this figure, the thirteenth training set sample was selected to go into the first analysis set three times. For this reason, a bootstrap resample will contain multiple replicates of some training set points, while others will not be selected at all. The data that were never selected are used to create the assessment set. This means that while the training set will have the same number of data points, the assessment sets will have varying numbers of data points.\nFor the bootstrap, we’ll refer to the mean estimate (line 12 in Algorithm 10.1) as the “ordinary” estimator (others will follow shortly).\n\n\n\n\n\n\n\n\nFigure 10.7: A schematic of three bootstraps resamples created from an initial pool of 30 data points.\n\n\n\n\n\nThe probability that a data point will be picked is \\(1 / n_{tr}\\) and, from this, the probability that a training set point is not selected at all is\n\\[\\prod_{i=1}^{n_{tr}} \\left(1 - \\frac{1}{n_{tr}}\\right) = \\left(1 - \\frac{1}{n_{tr}}\\right)^{n_{tr}}\\approx e^{-1} = 0.368\\]\nSince the bootstrap sample contains the selected data, each training point is selected with probability \\(1 - e^{-1} \\approx 0.632\\). The implication is that, on average, the analysis set, contains about 63.2% unique training set points and the assessment set includes, on average, 36.8% of the training data. When comparing the number of unique training set points excluded by the bootstrap method to those excluded by V-fold cross-validation, the bootstrap method is roughly equivalent to using \\(V=3\\) in cross-validation.\nFigure 10.8 shows bias and variance for the simulated data. As one might expect, there is considerable bias in the bootstrap estimate of performance. In comparison to the corresponding plot for MCCV, the bootstrap bias is worse than the MCCV curve where 60% were held out. The curve is also flat; the bias doesn’t go away by increasing B.\nHowever, the precision is extremely good, even with very few resamples.\n\n\n\n\n\n\n\n\nFigure 10.8: Variance and bias statistics for simulated data using different configurations for the bootstrap.\n\n\n\n\n\n\nCorrecting for Bias\nThere have been some attempts to de-bias the ordinary bootstrap estimate. The “632 estimator” (Efron 1983) uses the ordinary bootstrap estimate (\\(\\hat{Q}_{bt}\\)) and the resubstitution estimate (\\(\\hat{Q}_{rsub}\\)) together:\n\\[\\hat{Q}_{632} = e^{-1}\\, \\hat{Q}_{rsub} + (1 - e^{-1})\\, \\hat{Q}_{bt} = 0.368\\,\\hat{Q}_{rsub} + 0.632\\,\\hat{Q}_{bt}\\] Figure 10.8 shows that there is a significant drop in the bias when using this correction. The 632 estimator combines two different statistical estimates, and we only know the standard error of \\(\\hat{Q}_{bt}\\). Therefore, the right-hand panel does not show a standard error curve for the 632 estimator.\nLet’s look at an average example from the simulated data sets with B = 100. Table 10.1 has the estimator values and their intermediate values. Let’s first focus on the values for the column labeled “Logistic.” The ordinary bootstrap estimate was \\(\\hat{Q}_{bt}\\) = 0.101 and repredicting the training set produced \\(\\hat{Q}_{rsub}\\) = 0.0767. The 632 estimate shifts the Brier score downward to \\(\\hat{Q}_{632}\\) = 0.0919. This reduces the pessimistic bias; our large sample estimate of the true Brier score is 0.0898 so we are closer to that value.\nAnother technique, the 632+ estimator (Efron and Tibshirani 1997), uses the same blending strategy but uses dynamic weights based on how much the model overfits (if at all). It factors in a model’s “no-information rate”: the metric value if the predicted and true outcome values were independent. The author gives a formula for this, but it can also be estimated using a permutation approach where we repeatedly shuffle the outcome values and compute the metric (Section 9.5). We will denote this value as \\(\\hat{Q}_{nir}\\). For our simulated data set \\(\\hat{Q}_{nir}\\) = 0.427; we believe that this is the worst case value for our metric.\nWe then compute the relative overfitting rate (ROR) as\n\\[ ROR = \\frac{\\hat{Q}_{bt}-\\hat{Q}_{rsub}}{\\hat{Q}_{nir} -\\hat{Q}_{rsub}} \\]\nThe denominator measures the range of the metric (from most optimistic to most pessimistic). The numerator measures the optimism of our ordinary estimator. A value of zero implies that the model does not overfit, and values of one indicate the opposite. For our logistic model, the ratio is 0.024 / 0.35 = 0.0686, indicating that the logistic model is not overinterpeting the training set.\nThe final 632+ estimator uses a different weighting system to combine estimates:\n\\[\n\\begin{align}\n\\hat{Q}_{632+} &= (1 - \\hat{w})\\, \\hat{Q}_{rsub} + \\hat{w}\\, \\hat{Q}_{bt} \\quad \\text{with}\\notag \\\\\n\\hat{w} &= \\frac{0.632}{1 - 0.368ROR} \\notag\n\\end{align}\n\\] Plugging in our values, the final weight on the ordinary estimator (\\(w\\) = 0.648) is very close to what is used by the regular 632 estimator (\\(w\\) = 0.632). The final estimate is also similar: \\(\\hat{Q}_{632+}\\) = 0.0923.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrier Score\n\n\n\n\nLogistic\n1 NN\n\n\n\n\nEstimates\n\n\n(truth)\n0.090\n0.170\n\n\nresubstitution\n0.077\n0.000\n\n\nsimple mean\n0.101\n0.174\n\n\nIntermediates\n\n\nno information rate\n0.427\n0.496\n\n\nrelative overfitting rate\n0.069\n0.351\n\n\nweights\n0.648\n0.726\n\n\nFinal Estimates\n\n\n632\n0.092\n0.110\n\n\n632+\n0.092\n0.126\n\n\n\n\n\n\n\n\n\n\n\nTable 10.1: Bias correction values for two models on a simulated data set.\n\n\n\nThe simulations studies shown here in Figure 10.8 replicated what Molinaro (2005) found; the 632+ estimator has bias properties about the same as 10-fold cross-validation.\nHow do these bias corrected estimators work when there is extreme overfitting? One way to tell is to consider a 1-nearest neighbor model. In this case, re-predicting the training set predicts every data point with its outcome value (i.e., a “perfect” model). For the Brier score, this means \\(\\hat{Q}_{rsub}\\) is zero. Table 10.1 shows the rest of the computations. For this model \\(Q\\approx\\) 0.17 and the ordinary estimate comes close: \\(\\hat{Q}_{bt}\\) = 0.174.\nThe 632 estimate is \\(\\hat{Q}_{632} = 0.632\\,\\hat{Q}_{bt} =\\) 0.11; this is too much bias reduction as it overshoots the true value by a large margin. The 632+ estimator is slightly better. The ROR value is higher (0.351) leading to a higher weight on the ordinary estimator to produce \\(\\hat{Q}_{632+}\\) = 0.126.\nShould we use the bootstrap to compare models? Its small variance is enticing, but the bias remains an issue. The bias is likely to change as a function of the magnitude of the metric. For example, the bootstrap’s bias is probably different for models with 60% and 95% accuracy. If we think our models will have about the same performance, the bootstrap (with a bias correction) may be a good choice. Note that Molinaro (2005) found that the 632+ estimator may not perform well when the training set is small (and especially of the number of predictors is large).\nLet’s take a look at a few specialized resampling methods.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-time-series-resampling",
    "href": "chapters/resampling.html#sec-time-series-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.7 Time Series Data",
    "text": "10.7 Time Series Data\nTime series data (Hyndman and Athanasopoulos 2024) are sequential observations ordered over time. The most common example is daily stock prices. These data are a special case due to autocorrelation: rows of the data set are not independent. This can occur for many different reasons (e.g. underlying seasonal trends, etc.), and this dependency complicates the resampling process in two primary ways. We need to think more carefully about how analysis and assessment sets are created.\n\nRandomly allocating samples to these sets would deteriorate the relationships among the samples due to autocorrelation. If the autocorrelation is important information related to the response, then random sampling will negatively impact a model’s ability to uncover the predictive relationship with the response.\nPlacing newer data in the analysis set and older data in the assessment set would not make sense because of the time trends in the data.\n\nRolling forecasting origin splitting (Tashman 2000) is a resampling scheme that mimics the train/test splitting pattern from Section 3.4 where the most recent data are used in the assessment set. We would choose how much data (or what range of time) to use for the analysis and assessment sets, then slide this pattern over the whole data set. Figure 10.9 illustrates this process where each block could represent some unit of time. For example, we might build the model on six months of data and then predict the next month. The next resample iteration could bump that forward by a month or some other period (so that the assessment sets can be distinct or overlapping).\n\n\n\n\n\n\n\n\nFigure 10.9: A schematic of rolling origin resampling for time series data. Each box corresponds to a unit of time (e.g., day, month, year, etc.).\n\n\n\n\n\nWe also have the choice to cumulatively expand the analysis data from the start of the training set to the end of the current slice. For example, in Resample 2 of Figure 10.9, the model would be fit using samples 1-9; in Resample 3, the model would be fit using samples 1-10; and so on. This approach may be useful when the number of periods of time are smaller and we would like to include as much of the historical data as possible in each resample.\nWe’ll use rolling forecasting origin resampling in one of the upcoming case studies using the hotel rate data previously discussed in Section 6.1.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-spatial-resampling",
    "href": "chapters/resampling.html#sec-spatial-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.8 Spatial Data",
    "text": "10.8 Spatial Data\nSection 3.9 described the problem of spatial correlation between locations and a method for creating an initial split for the forestation data.\nLike time-series data, the resampling scheme for this type of data emulates the initial splitting process. We could create groupings for the training set using the following methods:\n\nClustering methods: These methods create partitions of the data where points with spatial coordinates are closer to each other within the same cluster than to points in other clusters.\nGrid-based methods: A grid of equally sized blocks, either rectangular or hexagonal, is created to encompass the training set points.\n\nFrom these groupings, we can apply procedures similar to previous cross-validation variations:\n\nLeave-one-group-out resampling: This method creates as many resamples as there are groups. In each resample, data points from one group are used as the assessment set, while the remaining groups are used to train the model. This process repeats for each group.\nV-fold cross-validation: This method assigns each group to one of V meta-groups (e.g., folds). In each of the V iterations, one fold is left out as the assessment set, while the remaining folds are used to train the model.\nRepeated V-fold cross-validation: This method involves restarting the grouping process with a different random number seed, allowing for multiple rounds of V-fold cross-validation.\n\nEach of these approaches can be augmented using a buffer to exclude very close neighbors from being used.\nGoing back to our forestry data, we previously used hexagonal blocks to split the data (see Figure 3.5). The same blocking/buffering scheme was re-applied to the training set in order to create a 10-fold cross-validation. Recall that a 25 x 25 grid was used. For the training set, some of these hexagons are missing all of their data (since they were previously added to the test set). Of the remaining grid points, we systematically assign them to the ten folds. Figure 10.10 shows this process for the first fold. The figure shows the blocks of data, along with the corresponding buffer for this iteration of cross-validation. The purple blocks are, for the most part, not adjacent to one another. These are combined into the first assessment set. The other locations that are not in the buffers are pooled into the first analysis set.\n\n\n\n\n\n\n\n\nFigure 10.10: An example of one split produced by block cross-validation on the forestation data. The analysis (magenta) and assessment (purple) sets are shown with locations in the buffer in black. The empty spaces represent the data reserved for the test set (see Figure 3.5).\n\n\n\n\nOur training set contains 4,832 locations. Across each of the ten folds, every location appears once in the analysis and assessment sets.\nJust as with standard V-fold cross-validation, the number of folds can be chosen based on bias and variance concerns. Unlike standard resampling, any investigation into this topic would depend on a specific spatial autocorrelation structure. As such, we have decided to follow the recommendations for non-spatial data and use V=10. If we find that the variation in our statistics estimated from ten resamples is too large, we can employ the same process of repeating the block resampling process using different random number seeds. For our data, the average number of locations in the assessment sets is 483 and this appears large enough to produce sufficiently precise performance statistics.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-multilevel-resampling",
    "href": "chapters/resampling.html#sec-multilevel-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.9 Grouped or Multi-Level Data",
    "text": "10.9 Grouped or Multi-Level Data\nWhen rows of data are correlated for other reasons, such as those discussed in Section 3.8, we can extend the techniques described above. Consider the previously mentioned example of having the purchase histories of many users from a retail database. If we had a training set of 10,000 instances that included 100 customers, we could conduct V-fold cross-validation to determine which customers would go into the analysis and assessment sets. As with the initial split, any rows associated with specific customers would be placed into the same partition (i.e., analysis or assessment).",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampling-faq",
    "href": "chapters/resampling.html#sec-resampling-faq",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.10 Frequently Asked Questions",
    "text": "10.10 Frequently Asked Questions\nThe general concept of resampling seems clear and straightforward. However, we regularly encounter confusion and commonly asked questions about this process. In this section we will address these common questions and misconceptions.\n\nIs it Bad to Get Different Results from Each Resample?\nThe point of resampling is to see how the model pipeline changes when the data changes. There is a good chance that some important estimates differ from resample to resample. For example, when algorithmically filtering predictors, you might get B different predictor lists, and that’s okay. It does give you a sense of how much noise is in the part of your model pipeline.\nIt can be good to examine the distribution of resampling performance. Occasionally, we may find a resample that has unusual performance with respect to the rest of the resamples. This may merit further investigation as to why the resample is unusual.\n\n\nCan/Should I Use the Individual Resampling Results to Change the Model Pipeline?\nWe often hear questions about how to use the cross-validated results to define the model. For example, using the top X% of predictors selected during resampling as the final predictor set in a new model pipeline seems intuitive.\nOne important thing to consider is that you now have a different model pipeline than what was just resampled. To understand the performance of your newly informed model, you need to resample a model pipeline that algorithmically selects the X% of predictors and resample it.\nNote that resampling the specific set of top predictors determined from the resamples is not appropriate. We have seen this done many times before in the hopes of improving predictive performance. Performance often improves for the assessment sets. But the performance cannot be generalized to the test set or for other new samples. The rule to determine the top set should be resampled, not the specific output of that rule.\nFor the original pipeline, the final predictor set is determined when you fit that pipeline on the training set. The resampling results tell you about what could have happened; the training set results show what did happen.\n\n\nWhat Happens to the b Trained Models? Which One Should I Keep?\nThe B models fit within resampling are only used to estimate how well the model fits the data. You don’t need them after that. You can retain them for diagnostic purposes though. As mentioned above, using the replicate models to look into how parts of the model change can be helpful.\nAlso, if you are not using a single validation set, recall that none of these models are trained with the unaltered training set, so none would be the final model.\n\n\nIs this some Sort of Ensemble?\nNo. We’ll talk more about ensembles in ?sec-ensembles and how the assessment set predictions can be used to create an ensemble.\n\n\nCan I Accidentally Resample Incorrectly?\nUnfortunately, yes. As previously mentioned, improper data usage is one of the most frequent ways that machine learning models silently fail; that is, you won’t know that there is a problem until the next set of labeled data. A good example discussed in Section 13.3 is Ambroise and McLachlan (2002).\nIf you are\n\ntraining every part of your model pipeline after the initial data split and using only the training set and,\nevaluating performance with external data\n\nthe risk of error is low.\n\n\nWhat is Nested Resampling?\nThis is a version of where an additional layer of resampling occurs. For example, suppose you are using 10-fold cross-validation. Within each of the 10 iterations, you might add 20 bootstrapping iterations (that resamples the analysis set). That ends up training 200 distinct models13.\nFor a single model, that’s not very useful. However, as discussed in Section 11.4, there are situations where the analysis set is being used for too many purposes. For example, during model tuning, you might use resampling to find the best model and measure its performance. In certain cases, that can lead to bad results for the latter task.\nAnother example is recursive feature selection, where we are trying to rank predictors, sequentially remove them, and determine how many to remove. In this case, it’s a good idea to use an outer resampling loop to determine where to stop and an inner loop for the other tasks.\nWe will see nested resampling in the next chapter.\n\n\nIs Resampling Related to Permutation Tests?\nNot really. Permutation methods (Section 9.5) are similar to resampling only in that they perform multiple calculations on different versions of the training set.\nWe used permutation methods in the bootstrapping section to estimate the no-information rate. Otherwise, they won’t be used to evaluate model performance.\n\n\nAre Sub-Sampling or Down-Sampling the Same as Resampling?\nNo. These are techniques to modify your training set (or analysis set) to rebalance the data when a classification problem has rare events. They’ll be discussed in ?sec-cls-imbalance.\n\n\nWhy Do I Mostly Hear About Validation Sets?\nAs discussed in Section 1.6, stereotypical deep learning models are trained on very large sets of data. There is very little reason to use multiple resamples in these instances, and deep learning takes up a lot of space in the media and social media.\nGenerally, every data set that is not massive could benefit from multiple resamples.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#chapter-references",
    "href": "chapters/resampling.html#chapter-references",
    "title": "10  Measuring Performance with Resampling",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nArlot, S, and A Celisse. 2010. “A Survey of Cross-Validation Procedures for Model Selection.” Statistics Surveys 4: 40–79.\n\n\nBates, S, T Hastie, and R Tibshirani. 2023. “Cross-Validation: What Does It Estimate and How Well Does It Do It?” Journal of the American Statistical Association, 1–12.\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nEfron, B. 1983. “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.” Journal of the American Statistical Association, 316–31.\n\n\nEfron, B. 2003. “Second Thoughts on the Bootstrap.” Statistical Science, 135–40.\n\n\nEfron, B, and T Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.\n\n\nEfron, B, and R Tibshirani. 1997. “Improvements on Cross-Validation: The 632+ Bootstrap Method.” Journal of the American Statistical Association 92 (438): 548–60.\n\n\nFushiki, T. 2011. “Estimation of Prediction Error by Using k-Fold Cross-Validation.” Statistics and Computing 21: 137–46.\n\n\nHyndman, RJ, and G Athanasopoulos. 2024. Forecasting: Principles and Practice, 3rd Edition. Otexts.\n\n\nMolinaro, A. 2005. “Prediction Error Estimation: A Comparison of Resampling Methods.” Bioinformatics 21 (15): 3301–7.\n\n\nStone, M. 1974. “Cross-Validatory Choice and Assessment of Statistical Predictions.” Journal of the Royal Statistical Society: Series B (Methodological) 36 (2): 111–33.\n\n\nTashman, L. 2000. “Out-of-Sample Tests of Forecasting Accuracy: An Analysis and Review.” International Journal of Forecasting 16 (4): 437–50.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#footnotes",
    "href": "chapters/resampling.html#footnotes",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "These names are not universally used. We only invent new terminology to avoid confusion; people often refer to the data in our analysis set as the “training set” because it has the same purpose.↩︎\nThis discussion is very similar to the variance-bias tradeoff discussed in Section 8.4. The context here differs, but the themes are the same.↩︎\nMeaning that performance looks worse than it should. For example, smaller \\(R^2\\) or inflated Brier score.↩︎\nThe split can be made using the same tools already discussed, such as completely random selection, stratified random sampling, etc.↩︎\nAlso, if \\(n_{tr}\\) is not “large,” you shouldn’t use a validation set anyway.↩︎\nAgain, this could be accomplished using any of the splitting techniques described in Chapter 3.↩︎\nThe simulation details, sources, and results can be found at https://github.com/topepo/resampling_sim.↩︎\nThe percent bias is calculated as \\(100(Q_{true} - \\hat{Q})/Q_{true}\\) for metrics where smaller is better.↩︎\nArlot and Celisse (2010) provides a comprehensive survey of cross-validation↩︎\nHowever, for linear regression, the LOOCV predictions can be quickly computed without refitting a large number of models.↩︎\nThis isn’t the same as using V = 20. That scheme has very different bias properties than two repetitions of V = 10.↩︎\nThis application will be discussed in Section 14.2.↩︎\nWe will refer to the first resample (10-fold CV) as the outer resample and the bootstrap as the inner resample.↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html",
    "href": "chapters/grid-search.html",
    "title": "11  Grid Search",
    "section": "",
    "text": "11.1 Regular Grids\nGrid search is a method to optimize a model pipeline’s tuning parameters. It creates a pre-defined set of candidate values and computes performance for each. From there, the numerically best candidate could be chosen, or the relationship between the tuning parameter(s) and model performance can be inspected to see if the model might benefit from additional optimization.\nSuppose there is a single tuning parameter, as in the \\(n_{min}\\) example of Section 9.3; each candidate takes a scalar value (quantitative or qualitative). For other models, there are multiple tuning parameters. For example, support vector machine models can have two or three tuning parameters, the Elastic Net model has two tuning parameters, and the boosted tree model has multiple tuning parameters. We’ll look at two of the boosted tree parameters in detail below.\nThe previous chapter demonstrated that using external data to evaluate the model is crucial (be it resampling or a validation set). Grid search has no free lunch: we cannot simply fit the model to each candidate set and evaluate them by simply re-predicting the same data.\nAlgorithm 11.1 formally describes the grid search process. For a model with \\(m\\) tuning parameters, we let \\(\\Theta\\) represent the collection of \\(s\\) candidate values. For each specific combination of parameters (\\(\\theta_j\\)), we resample the model to produce some measure of efficacy (e.g., \\(R^2\\), accuracy, etc.)1. From there, the best value is chosen, or additional work is carried out to find a suitable candidate.\nTo demonstrate, this chapter will initially focus on grids for a boosted tree model with two tuning parameters. Recall that a boosted tree is a collection of individual decision trees created sequentially. One parameter related to the process of creating the ensemble is the learning rate: the next tree uses information from the last tree to improve it. An important parameter is how much, or how fast it learns. It could be that some data sets need a large number of trees that evolve slowly (i.e., low learning rate) to find an optimal value of the performance metric. Alternatively, other data sets require fewer trees that change rapidly for optimal predictive performance. The learning rate parameter must be greater than zero, and is typically in the range of 10-5 to 10-1. Because the range of this parameter is very large, it is best conceptualized in log units.\nWe’ll illustrate different strategies of grid search using this model setup with varying configurations of the learning rate and the number of trees in the ensemble.\nThere are two main classes of grids: regular and irregular. We can view generating a collection of candidate models as a statistical design of experiments (DOE) problem (Box, Hunter, and Hunter 1978). There is a long history of DOE, and we’ll invoke relevant methods for each grid type. Santner, Williams, and Notz (2018) and Gramacy (2020) have excellent overviews of the DOE methods discussed in this chapter.\nThe following two sections describe different methods for creating the candidate set \\(\\Theta\\). Subsequent sections describe strategies for making grid search efficient using tools such as parallel processing and model racing.\nA regular grid starts with a sequence or set of candidate values for each tuning parameter and then creates all combinations. In statistics, this is referred to as a factorial design. The number of values per tuning parameter does not have to be the same.\nTo illustrate a regular grid, we’ll use five values of the number of trees (1, 500, 1000, 1500, and 2000) and three values of the learning rate (10-3, 10-2, and 10-1). This grid of 15 candidates is shown in Figure 11.1. The grid covers the entire space with significant lacuna in between.\nFigure 11.1: Three types of grids (in columns) are illustrated with tuning parameters from a boosting model. Each grid contains 15 candidates. The space-filling design was created using the Audze-Eglais method described in Section 11.2.\nWe’ll use the simulated training set in Section 9.1 with the same 10-fold cross-validation scheme described there. Once again, Brier scores were used to measure how well each of the 15 configurations of boosted tree model predicted the data. Figure 11.2 shows the results: a single tree is a poor choice (due to underfitting), and there are several learning rate values that work well. Even though this is not a diverse set of candidate values, we can probably pick out a reasonable candidate with a small Brier score, such as 500 trees and a learning rate of 10-2 (although there are a few other candidates that would be good choices).\nFigure 11.2: The boosted tree tuning results for the regular grid shown in Figure 11.1.\nThe pattern shown in this visualization is interesting: the trajectory for the number of trees is different for different values of the learning rate. The two larger learning rates are similar, showing that the optimal number of trees is in the low- to mid-range of our grid. The results for the smallest learning rate indicate that better performance might be found using more trees than were evaluated. This indicates that there is an interaction effect in our tuning parameters (just as we saw for predictors in Section 8.1). This might not affect how we select the best candidate value for the grid, but it does help build some intuition for this particular model that might come in handy when tuning future models.\nGaining intuition is much easier for regular grids than other designs since we have a full set of combinations. As we’ll see shortly, this interaction is very hard to see for a space-filling design.\nThe primary downside to regular grids is that, as the number of tuning parameters increases, the number of points required to fill the space becomes extremely large (due to the curse of dimensionality). However, for some models and pre-processing methods, regular grids can be very efficient despite the number of tuning parameters (see the following section that describes the “submodel trick”).",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-irregular-grid",
    "href": "chapters/grid-search.html#sec-irregular-grid",
    "title": "11  Grid Search",
    "section": "11.2 Irregular Grids",
    "text": "11.2 Irregular Grids\nIrregular grids are not factorial in nature. A simple example is a random grid where points are randomly placed using a uniform distribution on an appropriate range for each tuning parameter. A design of size 15 is shown in Figure 11.1. There are some gaps and clustering of the points, but the space for each individual dimension is covered well. The tuning results are shown in the top panels of Figure 11.3. Because it is an irregular design, we can’t use the same visualization as in Figure 11.2. Instead, a “marginal” plot is shown, where each numeric tuning parameter is plotted against performance in a separate panel.\nThe top left panel suggests that a very small learning rate is bad, but otherwise, there is not a strong trend. For the number of trees (in the top right panel), a small number of trees should be avoided, and perhaps the maximum number tested would be a good choice. From this plot, it is impossible to discover the interaction effect seen with the regular grid. However, this marginal plot is the main technique for visualizing the results when there are a moderate to large number of tuning parameters. The numerically best candidate was not too dissimilar from the regular grid: 1,898 trees and a learning rate of 10-2.7\n\n\n\n\n\n\n\n\nFigure 11.3: The tuning results for the irregular grids shown in Figure 11.1. The lines are spline smooths.\n\n\n\n\n\nAnother type of irregular grid is a space-filling design (Joseph 2016), where the goal is to make sure that the tuning parameter space is covered and that there is minimal redundancy in the candidate values. There are a variety of methods for achieving this goal.\nFor example, Figure 11.1 shows a 15 point Audze-Eglais space-filling design. The space is covered more compactly than the regular design of the same size and is much more uniform than the 15 point random design. The lower panels of Figure 11.3 show the tuning results. The results are somewhat cleaner than the random grid results but show similar trends. Here, the numerically best candidate was: 286 trees and a learning rate of 10-2.\nThere are many types of space-filling designs, and it is worth taking the time to take a quick tour of some of them.\nThe Latin hypercube design (LHD) is the most popular method for constructing space-filling designs (Husslage et al. 2011; Viana 2016). These designs have a simple definition. Suppose our hyperparameter space is rectangular and partitioned into smaller (hyper)cubes. From this, a LHD is a set of distinct points where no one dimension has multiple values in any bins2. We desire candidates that fill the space of each parameter and are not close to one another.\nThe most basic approach to creating a Latin hypercube design is random sampling (Mckay, Beckman, and Conover 2000). If there are \\(m\\) parameters and we request \\(s\\) candidate values, the parameter space is initially divided into \\(s^m\\) hypercubes of equal size. For each tuning parameter, \\(s\\) regions in its dimension are selected at random, and a value is placed in this box (also at random). This process repeats for each dimension. Suppose there are ten bins for a parameter that ranges between zero and one. If the first design point selects bin two, a random uniform value is created in the range [0.1 0.2). Figure 11.4 shows three such designs, each generated with different random numbers.\n\n\n\n\n\n\n\n\nFigure 11.4: Three replicate Latin hypercube sampling designs of size ten for two parameters. Each design uses different random numbers. The grid lines illustrate the 100 hypercubes used to generate the values.\n\n\n\n\n\nTechnically, these designs cover the space of each predictor uniformly. However, large multivariate regions can be empty. For example, one design only samples combinations along the diagonal. Additional constraints can make the design more consistent with our desires.\nFor example, we could choose points to maximize the minimum pairwise distances between the candidates. These designs are usually referred to as MaxiMin designs (Pronzato 2017). Comparing the two irregular designs in Figure 11.1, the random grid has a maximum minimum distance between candidates of 0.09. In contrast, the corresponding space-filling design’s value (0.26) is 2.9-fold larger3. The latter design was optimized for coverage, and we can see far less redundancy in this design.\nA similar method, initially proposed by Audze and Eglais (1977), maximizes a function of the inverse distances between \\(s\\) candidate points:\n\\[\ncriterion = \\sum_{i=1}^s \\sum_{j=1,\\;i\\ne j}^s\\frac{1}{dist(\\theta_i, \\theta_j)^2}\n\\]\nBates, Sienz, and Toropov (2004) devised search methods to find optimal designs for this criterion.\nSome other space-filling designs of note:\n\nMaximum entropy sampling selects points based on assumptions related to the distributions of the tuning parameters and their covariance matrix (Shewry and Wynn 1987; Joseph, Gul, and Ba 2015).\nUniform designs (Fang et al. 2000; Wang, Sun, and Xu 2022) optimally allocate points so that they are uniformly distributed in the space.\n\nWhile these methods can be generally constructed by sampling random points and using a search method to optimize a specific criterion, there has been scholarship that has pre-optimized designs for some combination of the number of tuning parameters and the requested grid size.\nThe advantage of space-filling designs over random designs is that, for smaller designs, the candidates do a better job covering the space and have a low probability of producing redundant points. Also, it is possible to create a space-filling design so that the candidates for each numerical parameter are nearly equally spaced (as was done in Figure 11.1).\nMany designs assume that all the tuning parameter values are quantitative. That may not always be the case. For example, K-nearest neighbors can adjust its predictions by considering how far the cost are from a new point; more distant points should not have the same influence as close cost. A weighting function can be used for this purpose. For example, weights based on the inverse of the distance between neighbors may produce better results. Equal weighting is often called a “rectangular weighting” function. The type of algebraic function used for weighting is a qualitative tuning parameter.\nA simple workaround for creating a space-filling design is to repeat the unique parameter values as if they were \\(s\\) distinct values when making the design. This allows them to be used with a Latin hypercube design and any space-filling design that uses this approach is not technically optimal. Practically speaking, this is an effective approach for tuning models. However, there are sliced LHD that can accomplish the same goal (Qian 2012; Ba, Myers, and Brenneman 2015).\nWe recommend space-filling designs since they are more efficient than regular designs. Regular designs have a lot of benefits when using an unfamiliar modeling methodology since you will learn a lot more about the nuances of how the tuning parameters affect one another.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-efficient-grid",
    "href": "chapters/grid-search.html#sec-efficient-grid",
    "title": "11  Grid Search",
    "section": "11.3 Efficient Computations for Conventional Grid Search",
    "text": "11.3 Efficient Computations for Conventional Grid Search\nThe computational cost of grid search can become large, depending on the resampling strategy and the number of candidates under consideration. In our example, a total of 150 boosted tree models are evaluated (5 values of the number of trees, 3 values of the learning rate, and 10-fold cross-validation) before determining which candidates are most favorable to our data.\nWe’ll look at three approaches to making grid search more efficient. One method (submodels) happens automatically for some types of models, another approach (parallel processing) uses software engineering tools, and the last tool (racing) is a statistical solution.\nTo illustrate these more efficient search approaches, a 100 grid point space-filling design will be used with a large parameter range (1 to 3000 trees) and learning rates between 10-5 and 10-1.\n\n11.3.1 Submodels\nLet’s begin by understanding the submodel trick. For some models a single training model can be used to predict many tuning parameter candidates. For example, the boosted tree model that we have been using creates a sequential ensemble of decision trees, each depending on the previous. Suppose that a boosting model with 3,000 trees is created. Most implementations of boosted trees can use this model to predict the outcomes of any smaller ensemble size. Therefore, we only need to build the boosted tree model using the largest number of trees for each specific combination of the remaining tuning parameters. For a regular grid, this can effectively drop a dimension of the computations. Depending on the model and grid, the speed-up for using this approach can be well into double digits.\nSome different models and pre-processors can have this quality, including the glmnet model (Section 16.2.4), partial least squares (Section 7.2.3), principal component feature extraction (Section 7.2.1), and others.\nUnfortunately, irregular designs cannot exploit the submodel trick since they are not factorial in nature. A hybrid design could be used where a dense sequence of \\(s_1\\) parameter values is created for the tuning parameter associated with submodels and a separate space-filling design of size \\(s_2\\) for the other parameters. These two grids can be crossed so that the space-filling design is replicated for each value of the submodel parameter. This produces \\(s_1\\times s_2\\) grid points but, effectively, only \\(s_2\\) models are fit.\nFor our larger grid, we created a similar regular grid of 100 points with the same expanded ranges of parameters. There were 10 unique, evenly spaced values for both parameters. To evaluate this grid, we only need to train 10 models. Compared to the analogous space-filling design, the regular grid was 2.8 times faster to evaluate.\n\n\n11.3.2 Parallel Processing\nFortunately, none of the 1,000 models in our large grid depend on one another and can be computed separately. Since almost all modern computers have GPUs and multiple CPUs, we can break the computations into different “chunks” of work and execute them simultaneously on distinct processors (or separate computers entirely). The parallel processing of models can significantly reduce the time it takes to tune using grid search.\nFor example, consider the space-filling designs with 15 candidates evaluated across 10 resamples. We spread the 150 model fits across 10 worker processes (on the same computer). Despite the meager computation costs of training each boosted tree, there was still a 7.7-fold speedup (35s versus 4.5s) when run in parallel.\nHowever, there are some important nuances that we should consider before initiating parallel processing.\nFirst, it is an excellent idea to parallelize the “longest loop” (literally or figuratively). Looking back at Algorithm 11.1, there is a loop across the \\(s\\) candidates in line 6. Line 7 contains the resampling loop across \\(B\\) resamples. If the data set is large, it may be optimal to invert the loop where we parallel process across the \\(B\\) resamples and execute the \\(s\\) models within each. Keeping each resample on a single worker means less input/output traffic across the workers. See Kuhn and Silge (2022) Section 13.5.2. Alternatively, if the data are not large, it could be best to “flatten” the two loops into a single loop with \\(s\\times B\\) iterations (with as many workers as possible).\nAdditionally, if expensive preprocessing is used, a naive approach where each pipeline is processed in parallel might be counterproductive because we unnecessarily repeat the same preprocessing. For example, suppose we are tuning a supervised model along with UMAP preprocessing. By sending each of the \\(s\\) candidates to a worker, the same UMAP training occurs for each set of candidates that correspond to the supervised model. Instead, a conditional process can be used where we loop across the UMAP tuning parameter combinations and, for each, run another loop across the parameters associated with the supervised model.\nAnother consideration is memory. If the data used to train and evaluate the model are copied for each worker, the number of workers can be restricted to fit within system memory. A well thought out process can avoid this unnecessary restriction.\nWhen we understand the computational aspects of parallel processing, we can almost always use it to greatly reduce the time required for model tuning.\n\n\n\n11.3.3 Racing\nA third way we can improve the model tuning process is through Racing (Maron and Moore 1997). This technique adaptively resamples the data during grid search. The goal is to cull tuning parameter combinations that have no real hope of being optimal before they are completely resampled. For example, in our previous tuning of the boosting model using a regular grid, it is clear that a single tree performs very poorly. Unfortunately, we will not know this until all the computations are finished.\nRacing tries to circumvent this issue by doing an interim statistical analysis of the tuning results. From this, we can compute a probability (or score) that measures how likely each candidate will be the best (for some single metric). If a candidate is exceedingly poor, we usually can tell that after just a few resamples4.\nThere are a variety of ways to do this. See Kuhn (2014), Krueger, Panknin, and Braun (2015), and Bergman, Purucker, and Hutter (2024). The general process is shown in Algorithm 11.2. The resampling process starts normally for the first \\(B_{min}\\) resamples. Lines 7-12 can be conducted in parallel for additional computation speed. After the initial resampling for all candidates, the interim analysis at iteration \\(b\\) results in a potentially smaller set of \\(s_b\\) candidates. Note that parallel processing can be incorporated into this process for each loop and the speed-up from using submodels also occurs automatically.\nAt each resampling estimate beyond the first \\(B_{min}\\) iterations, the current candidate set is evaluated for a single resample, and the interim analysis potentially prunes tuning parameter combinations.\n\n\n\n\n\n\n\n\n\n\\begin{algorithm} \\begin{algorithmic} \\State $\\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$ \\State $B$: number of resamples \\State Initial number of resamples $1 \\lt B_{min} \\lt B$ executed prior to analysis \\State $M(\\mathfrak{D}^{tr}, B)$: a mapping function to split $\\mathfrak{D}^{tr}$ for each of $B$ iterations. \\State $f()$: model pipeline \\State $\\Theta$: Parameter set ($s \\times m$) with candidates $\\theta_j$ \\For{$j=1$ \\To $s$} \\For{$b=1$ \\To $B_{min}$} \\State Generate $\\hat{Q}_{jb} =$ \\Call{Resample}{$\\mathfrak{D}^{tr}, f(\\cdot;\\theta_j), M_b(\\mathfrak{D}^{tr}, B)$} \\EndFor \\State Compute $\\hat{Q}_{j} = 1/B_{min}\\sum_b \\hat{Q}_{jb}$. \\EndFor \\State Eliminate candidates to produce $\\Theta^b$ ($s_b \\times m$) \\For{$b = B_{min} + 1$ \\To $B$} \\For{$j=1$ \\To $s$} \\State Generate $\\hat{Q}_{jb} =$ \\Call{Resample}{$\\mathfrak{D}^{tr}, f(\\cdot;\\theta_j), M_b(\\mathfrak{D}^{tr}, B)$} \\State Update candidate subset $\\Theta^b$ by applying the filtering analysis \\Endfor \\Endfor \\State Determine $\\hat{\\theta}_{opt}$ that optimizes $\\hat{Q}_j^k$. \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nAlgorithm 11.2: Using racing for grid search tuning parameter optimization.\n\n\n\nThe details are in the analysis used to discard tuning parameter combinations. For simplicity, we’ll focus on a basic method that uses a form of analysis of variance (ANOVA), a standard statistical tool for determining whether there are differences between conditions (Kutner, Nachtsheim, and Neter 2004). In our context, the conditions are the tuning parameter candidates; we ignore their values and treat them as qualitative samples from the distribution of all possible tuning parameter combinations. The ANOVA model uses the candidate conditions as the predictor and the metric of choice as the numeric outcome.\nThere are two statistical considerations that we need to understand when using an ANOVA model for evaluating Racing results.\nFirst, the ANOVA model is used for statistical inference. Specifically, we’ll use it to compute confidence intervals on differences in performance. This means that the probabilistic assumptions about our data matter. The ANOVA method requires that the errors follow a Gaussian distribution. That distribution can be strongly influenced by the distribution of the outcome, and, in our case, this is a performance metric. Previously, we used the Brier score, which has non-negative values and might be prone to follow a right-skewed statistical distribution. However, each resampled Brier score is the average of differences, and the Central Limit Theorem suggests that as the number of data points used to compute the score increases, the sampling distribution will become more Gaussian. If we use this approach to assess parameter candidates, then we should check the normality assumptions of the errors.\nThe second statistical complication is related to the resamples. Basic ANOVA methods require the data to be independent of one another, which is definitely not the case for resampling results. A “within-resample” correlation occurs since some resamples are “easier” to predict than others. This means that the metrics associated with each resample are more similar to one another than to the metrics from other resamples.\nThis extra correlation means that a simple ANOVA model cannot be used. Instead, our interim analysis should instead use a hierarchical random effects model. This is the same methodology used in Section 6.4.3 for effect encodings. We’ll treat our set of resamples as a random sample of possible resampling indices. The ANOVA model itself is:\n\\[\nQ_{ij} =(\\beta_0 + \\beta_{0i}) + \\beta_1x_{i1} + \\ldots +  \\beta_{s-1}x_{i(s-1)}+ \\epsilon_{ij}\n\\tag{11.1}\\]\nfor \\(i=1,\\ldots, B\\) resamples and \\(j=1, \\ldots, s\\) candidates. This random intercept model assumes that the ranking of candidates is the same across resamples and only the magnitude of the pattern changes from resample to resample.\nThis model’s form is the reference cell parameterization discussed in Section 6.2. For each interim analysis, the reference cell will be set to the current best candidate. This means that the \\(\\hat{\\beta}_j\\) parameter estimates represent the loss of performance relative to the current best. A one-sided confidence interval is constructed to determine if a candidate should be removed. We can stop considering candidates whose interval does not contain zero.\nTo demonstrate the racing procedure, we re-evaluated our larger a space-filling design. The order of the folds was randomized, and after 3 resamples, the ANOVA method was used to analyze the results. Figure 11.5 shows the one-sided 95% confidence intervals for the loss of Brier score relative to the current best configuration (2030 trees and a learning rate of 10-2.78). Sixty-Six were eliminated at this round.\n\n\n\n\n\n\n\n\nFigure 11.5: The first interim analysis in racing using a larger space-filling design for the boosted tree model.\n\n\n\n\n\nIn the end, the racing process eliminated all but 7 candidates. The process eliminated candidates at iterations three (66 configurations), four (14 configurations), five (7 configurations), six (1 configuration), seven (2 configurations), eight (2 configurations), and nine (1 configuration). In all, 404 models were fit out of the possible 1,000 (40.4%). The numerically best candidate set for racing and basic grid search were the same: 576 trees and a learning rate of 10-2.3.\nThe model in Equation 11.1 allows us to estimate the within-resample correlation coefficient. This estimates how similar the performance metric values are to one another relative to values between resamples. In our example, the estimate of within resample correlation is 0.6, and is a good example of why we should use Equation 11.1 when culling parameter candidates.\nIn summary, racing can be an effective tool for screening a large number of models while using comprehensive grids.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-nested-resampling",
    "href": "chapters/grid-search.html#sec-nested-resampling",
    "title": "11  Grid Search",
    "section": "11.4 Optimization Bias and Nested Resampling",
    "text": "11.4 Optimization Bias and Nested Resampling\nIn the last section, we found that the numerically best candidate was 576 trees and a learning rate of 10-2.3. The corresponding resampling estimate of the Brier score was 0.0783 with a standard error of 0.0053. If someone were to ask us how well the optimal boosted tree performs, we would probably give them these performance estimates.\nHowever, there is an issue in doing so that may affect our decision for selecting the optimal tuning parameters. We are using grid search and resampling to find the best combination of parameters and to estimate the corresponding performance. The problem is that we don’t know how accurate or precise the estimate of the optimal candidate may be. In some situations, this dual use of the model tuning process can introduce optimization bias where, to some degree, our performance statistics are optimistic.\nThis issue has been studied extensively in the high-dimensional biology literature, where there could be dozens of samples but thousands of predictors. In this case, feature selection is of paramount importance. However, the process of selecting important predictors in this situation can be very difficult and often leads to unstable models or preprocessing methods that overfit the predictor set to the data. Discussions of these issues can be found in Varma and Simon (2006), Boulesteix and Strobl (2009), Bischl et al. (2012), and Bernau, Augustin, and Boulesteix (2013).\nIn this section, we’ll introduce a few methods to quantify and correct for optimization bias. These approaches are not specific to traditional grid search; they can be used with racing, iterative search, or any algorithm that we use to optimize a model. Some of the concepts can be difficult and, for this reason, we’ll use a simplified grid search scenario. Let’s optimize our boosted tree by fixing the number of boosting iterations to 500 and only optimize the learning rate. But the tools that we are about to discuss are not limited to a single tuning parameter. Simplifying the task in this way allows us to visualize and explain the process5.\nUsing the same 10-fold cross-validation scheme, Figure 11.6 shows the results of a conventional grid search over the learning rate where 100 values of that parameter were evaluated. As usual, the line indicates the average of the Brier scores produced by the 10 assessment sets. The results of this process indicates that the smallest Brier score is achieved with a learning rate of 10-2.23. The Brier score that corresponds to this tuning parameter value is estimated to be 0.0782 with corresponding standard error of 0.00536.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.6: Basic grid search for learning rate with 100 grid points. The dotted vertical line shows the optimal learning rate using the overall resampling estimates. The colored rug values show the learning rates that were optimal within each resample. There were 2 learning rates selected more than once: 10-2.27 and 10-1.91.\n\n\n\nHow can we estimate the potential bias in the process that led to this performance metric? Tibshirani and Tibshirani (2009) describes a simple analytical estimator applicable when multiple resamples are used to estimate performance. First, we obtain the performance estimates corresponding to the optimal tuning parameter candidate within each fold. Let’s call these \\(\\hat{Q}_j^*\\) (where \\(j=1\\ldots B\\)). They are represented in Figure 11.6 as the colored “rug” lines at the bottom of the figure. We know that our conventional analysis of this grid search finds that a learning rate of 10-2.23 to be optimal. We can find the metric values associated with the global optimum for each resample (denoted as \\(\\hat{Q}_j^{opt}\\)). For each resample, we now have matched performance estimates for the local optima as well as the global optimum. The difference between these values is an estimate of the bias in optimization. If different resamples have very different optimal performance metrics compared to the optimal performance determined using the averages of the resamples (as shown in Figure 11.6), bias can increase. The estimate of the bias is then:\n\\[\\widehat{bias} = \\frac{1}{B}\\sum_{j=1}^B\\left[\\hat{Q}_j^{opt} - \\hat{Q}_j^*\\right]\\] For this particular example, the bias is fairly small (0.000931 with standard error 0.000304). To correct the conventional estimator, we add the bias; the Brier score is adjusted from 0.0782 to 0.0791. A more complex analytical method can be found in Tibshirani and Rosset (2019).\nWe can also estimate optimization bias with more complex resampling schemes. Recall that the issue is that we are overextending the conventional resampling scheme by doing too much with the same data (i.e., estimating overall performance assessment and the optimal performance value). Nested resampling (Varma and Simon 2006; Boulesteix and Strobl 2009) prevents this overuse by using two layers of resampling: the “inner resamples” are used to estimate the optimal candidate values, and the “outer resamples” estimate performance at those values.\nFor example, our current resampling scheme is 10-fold cross-validation. Since the training set has 2,000 samples, each fold uses 1,800 to fit the model and a separate 200 for assessment. Nested resampling would create 10 more independent resampling schemes within each of the 1,800-point analysis sets. If we once again used 10-fold cross-validation for the inner resamples, each would contain 10 analysis sets of size 1,620 with corresponding assessment sets of 180 data points.\nThe process starts with the inner resamples. The same model tuning procedure is used (basic grid search in our example), and each of the inner resamples estimates its own optimal candidate \\(\\hat{\\theta}_k\\). The outer resample takes this candidate value and estimates its performance using the assessment sets from the outer resampling loop. These outer estimates, whose data were never used to tune the model, are averaged to produce the nested resampling estimate of performance.\nConsider Figure 11.7. The colored lines in the left-hand panel show the results of the 10 inner resamples. Each line is made up of the averages of the inner 10 assessment sets of size 180. The filled circles along these lines indicate the optimal learning rate for that inner resample. Each outer resample takes its corresponding learning rate, trains that model using its analysis set of 1,800 points, and computes performance using its 200 assessment samples. These 10 statistics are averaged to get the final performance estimate, which should be free of any optimization bias.\n\n\n\n\n\n\n\n\nFigure 11.7: Each line corresponds to a complete resampled grid search that uses the inner resamples. The points indicate the estimates of the optimal learning rate as determined by the inner results. The panel on the right contrasts the Brier score for the inner assessment set with the outer assessment sets.\n\n\n\n\n\nWe can see that the resample-specific optimal learning rates vary but are within a consistent region. There are a few resamples that found the same optimal value. There is some variation in the y-axis too; different assessment sets produce different values. The standard error of these inner statistics, 0.000537, is much smaller than the value of the conventional estimate (0.00536).\nThe panel on the left shows boxplots of the Brier scores for the inner resamples and the corresponding outer resample estimates. The standard error of the outer resamples is very similar to the the level of noise in the conventional estimate.\nIn the end, the nested resampling estimate of the Brier score was estimated as 0.0784; a value very close to the single 10-fold cross-validation result shown in Figure 11.6.\n\nIt is important to emphasize that nested resampling is for verification, not optimization. It provides a better estimate of our model optimization process; it does not replace it.\n\nWe might best communicate the results of nested resampling like this:\n\nWe tuned the learning rate of our boosted classification tree using grid search and found that a rate of 10-2.23 to be best. We think that this model has a corresponding Brier score, measured without optimization bias, of 0.0784.\n\nWe can look at the candidates produced by the inner resamples to understand the stability of the optimization process and potentially diagnose other issues. We would not choose “the best” inner resampling result and move forward with its candidate value6.\nIt is a good idea to use nested resampling when the training set is small, the predictor set is very large, or both. The lack of significant bias in the analysis above does not discount the problem of optimization bias. It exists but can sometimes be within the experimental noise.\nIt is a good idea to choose an inner resampling scheme that is as close as possible to the single resampling scheme that you used for optimization. The outer scheme can vary depending on your needs. Care should be used when the outer resampling method is the bootstrap. Since it replicates training set points in its analysis sets, the inner resamples need to use the unique rows of the original training set. Otherwise the same data point might end up in both the inner analysis and assessment sets.\nThe primary downside to nested resampling is the computational costs. Two layers of resampling have a quadratic cost, and only the inner resamples can be executed in parallel. Using 10 workers, the analysis in Figure 11.7 took 6.6-fold longer to compute than the basic grid search that produced Figure 11.6.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-parameter-ranges",
    "href": "chapters/grid-search.html#sec-parameter-ranges",
    "title": "11  Grid Search",
    "section": "11.5 Setting Parameter Ranges",
    "text": "11.5 Setting Parameter Ranges\nSpecifying the range of parameters used to define the grid may involve some guesswork. For some parameter types, there is a well understood and defined range. For others, such as learning rate, there is a lower bound (zero) and a loosely defined upper bound mostly based on convention and prior experience. For grids, we want to avoid configurations with few unique values and a wide range. This might not sample the section of the parameter space that includes nonlinearity and the region of optimal results. Figure 9.6 shows an example of this for the learning rate. The initial grid included three points that missed the regions of interest. Figure 9.5 did a better job with more grid points. In high dimensional parameter space, the likelihood of a poor grid increases, especially for relatively “small” grid sizes.\nWe often envision the relationship between a predictor and model performance as being a sharp peak. If we cannot find the pinnacle, the model optimization would fail. Luckily, as seen in Figure 11.7, there are often substantial regions of parameter space with good performance. Model optimization may not be superficially easy but it is often not impossible. We only need to sample the optimal region once to be successful.\nIf we worry that our tuning grid did not produce good results, another strategy is to increase the parameter ranges and use an iterative approach that can naturally explore the parameter space and let the current set of results guide the exploration about the space. These methods are discussed in the next chapter.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#chapter-references",
    "href": "chapters/grid-search.html#chapter-references",
    "title": "11  Grid Search",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAudze, P, and V Eglais. 1977. “New Approach to Planning Out of Experiments.” Problems of Dynamics and Strengths 35: 104–7.\n\n\nBa, S, R Myers, and W Brenneman. 2015. “Optimal Sliced Latin Hypercube Designs.” Technometrics 57 (4): 479–87.\n\n\nBates, S, J Sienz, and V Toropov. 2004. “Formulation of the Optimal Latin Hypercube Design of Experiments Using a Permutation Genetic Algorithm.” In 45th AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics & Materials Conference, 2011.\n\n\nBergman, E, L Purucker, and F Hutter. 2024. “Don’t Waste Your Time: Early Stopping Cross-Validation.” arXiv.\n\n\nBernau, C, T Augustin, and AJ Boulesteix. 2013. “Correcting the Optimal Resampling-Based Error Rate by Estimating the Error Rate of Wrapper Algorithms.” Biometrics 69 (3): 693–702.\n\n\nBischl, B, O Mersmann, H Trautmann, and C Weihs. 2012. “Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation.” Evolutionary Computation 20 (2): 249–75.\n\n\nBoulesteix, AL, and C Strobl. 2009. “Optimal Classifier Selection and Negative Bias in Error Rate Estimation: An Empirical Study on High-Dimensional Prediction.” BMC Medical Research Methodology 9: 1–14.\n\n\nBox, GEP, W Hunter, and J. Hunter. 1978. Statistics for Experimenters. New York: Wiley.\n\n\nFang, KT, D Lin, P Winker, and Y Zhang. 2000. “Uniform Design: Theory and Application.” Technometrics 42 (3): 237–48.\n\n\nGramacy, R. 2020. Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.\n\n\nHusslage, B, G Rennen, E Van Dam, and D Den Hertog. 2011. “Space-Filling Latin Hypercube Designs for Computer Experiments.” Optimization and Engineering 12: 611–30.\n\n\nJoseph, V. 2016. “Space-Filling Designs for Computer Experiments: A Review.” Quality Engineering 28 (1): 28–35.\n\n\nJoseph, V, E Gul, and S Ba. 2015. “Maximum Projection Designs for Computer Experiments.” Biometrika 102 (2): 371–80.\n\n\nKrueger, T, D Panknin, and ML Braun. 2015. “Fast Cross-Validation via Sequential Testing.” Journal of Machine Learning Research 16 (1): 1103–55.\n\n\nKuhn, M. 2014. “Futility Analysis in the Cross-Validation of Machine Learning Models.” arXiv.\n\n\nKuhn, M, and J Silge. 2022. Tidy Modeling with R. O’Reilly Media, Inc.\n\n\nKutner, M, C Nachtsheim, and J Neter. 2004. Applied Linear Regression Models. McGraw-Hill/Irwin.\n\n\nMaron, O, and AW Moore. 1997. “The Racing Algorithm: Model Selection for Lazy Learners.” Artificial Intelligence Review 11: 193–225.\n\n\nMckay, M, R Beckman, and W Conover. 2000. “A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code.” Technometrics 42 (1): 55–61.\n\n\nPronzato, L. 2017. “Minimax and Maximin Space-Filling Designs: Some Properties and Methods for Construction.” Journal de La Société Française de Statistique 158 (1): 7–36.\n\n\nQian, P. 2012. “Sliced Latin Hypercube Designs.” Journal of the American Statistical Association 107 (497): 393–99.\n\n\nSantner, T, B Williams, and W Notz. 2018. The Design and Analysis of Computer Experiments. Springer.\n\n\nShewry, M, and H Wynn. 1987. “Maximum Entropy Sampling.” Journal of Applied Statistics 14 (2): 165–70.\n\n\nTibshirani, RJ, and S Rosset. 2019. “Excess Optimism: How Biased Is the Apparent Error of an Estimator Tuned by SURE?” Journal of the American Statistical Association 114 (526): 697–712.\n\n\nTibshirani, RJ, and R Tibshirani. 2009. “A Bias Correction for the Minimum Error Rate in Cross-Validation.” The Annals of Applied Statistics 3 (2): 822–29.\n\n\nVarma, S, and R Simon. 2006. “Bias in Error Estimation When Using Cross-Validation for Model Selection.” BMC Bioinformatics 7: 1–8.\n\n\nViana, F. 2016. “A Tutorial on Latin Hypercube Design of Experiments.” Quality and Reliability Engineering International 32 (5): 1975–85.\n\n\nWang, Y, F Sun, and H Xu. 2022. “On Design Orthogonality, Maximin Distance, and Projection Uniformity for Computer Experiments.” Journal of the American Statistical Association 117 (537): 375–85.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#footnotes",
    "href": "chapters/grid-search.html#footnotes",
    "title": "11  Grid Search",
    "section": "",
    "text": "This process was illustrated in Algorithm 10.1.↩︎\nOther applications may want the points to have properties such as orthogonality, symmetry, etc.↩︎\nUsing Euclidean distance.↩︎\nNote that racing requires multiple assessment sets. A single validation set could not be used with racing.↩︎\nUnfortunately, this is an example that is not prone to optimization bias. We’ll see examples later on this website where we can more effectively use these tools.↩︎\nTo reiterate, the nested resampling process is used to characterize our optimization process. If it becomes our optimization process, we would need to nest it inside another nested resample.↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html",
    "href": "chapters/iterative-search.html",
    "title": "12  Iterative Search",
    "section": "",
    "text": "12.1 Example: Predicting Barley Amounts using Support Vector Machines\nGrid search is a static procedure; we predetermine which candidates will be evaluated before beginning. How can we adaptively optimize tuning parameters in a sequential manner? Perhaps more importantly, when is this a good approach?\nPreviously, we’ve seen that a plateau of good performance in the parameter space is possible. This is often the case but will not always be true. If the region of optimal performance is small, we must create a large space-filling design to find it. When there are many tuning parameters, the computational expense can be unreasonable. An even worse situation is one where the previously described speed-up techniques (such as the submodel trick from Section 11.3.1) are not applicable. Racing can be very efficient, but if the training set is very large, it may be that using a validation set is more appropriate than multiple resamples; in this case, racing cannot be used. Finally, there are practical considerations when the data set is very large. Most parallel processing techniques require the data to be in memory for each parallel worker, restricting their utility.\nGenerally, we are not often constrained by these issues for models used for tabular data. However, there are a few models that might be better optimized sequentially. The first two that come to mind are large neural networks1. and support vector machines2 (SVMs) (Schölkopf and Smola 2001; Shawe-Taylor and Cristianini 2004). For the former, as the number of layers increases, so does the number of tuning parameters. Neural networks also have many important tuning parameters associated with the model’s training, such as the learning rate, regularization parameters, etc. These models also require more preprocessing than others; they are suspectable to the effects of uninformative predictors, missing data, and collinearity. Depending on the data set, there can be many pre-model tuning parameters.\nSupport vector machines are models with fewer tuning parameters than neural networks but with similar preprocessing requirements. Unfortunately, the tuning parameter space can have large areas of poor performance with “islands” where the model works well. The location of these will change from data set to data set. We’ll see an example of this shortly where two model parameters are tuned.\nThese two models, in particular, are more likely to benefit from an optimization method that chooses candidates as the process evolves.\nIn theory, any iterative optimization procedures can be used. In general, gradient methods, such as steepest descent or Newton’s method, are the most commonly used technique for nonlinear optimization. These tools are suboptimal when it comes to parameter tuning but play important parts in other areas of machine learning and, for this reason, they will be discussed later.\nDerivative-free techniques can be helpful for model tuning. Traditional examples are genetic algorithms, simulated annealing, particle swarm optimization, etc. We’ll consider the first two of these in Sections 12.3 and 12.4. Currently, the most well-known iterative tool for tuning models is Bayesian optimization. This will be examined in Section 12.6. However, before this, Section 12.5 will flesh out what it means for a model to be Bayesian.\nTo get started, let’s revisit a data set.\nWe’ll return to the data previously seen in Section 7.1 where we want to predict the percentage of barley oil in a mixture. Recall that the predictors are extremely correlated with one another. In Chapter 7, we considered embedding methods like PCA to preprocess the data and these models were able to achieve RMSE values of about 6% (shown in Figure 7.6).\nIn this chapter, we’ll model these data in two different scenarios. First, we’ll use them as a “toy problem” where only two tuning parameters are optimized for a support vector machine model. This is a little unrealistic, but it allows us to visualize how iterative search methods work in a 2D space. Second, in Section 12.7, we’ll get serious and optimize a larger group of parameters for neural networks simultaneously with additional parameters for a specialized preprocessing technique.\nLet’s start with the toy example that uses an SVM regression model. This highly flexible nonlinear model can represent the predictor data in higher dimensions using a kernel transformation (Hofmann, Schölkopf, and Smola 2008). This type of function combines numeric predictor vectors from two data points using a dot product. There are many different types of kernel functions, but for a polynomial kernel, it is:\n\\[\nk(\\boldsymbol{x}_1, \\boldsymbol{x}_2) = (a\\boldsymbol{x}_1'\\boldsymbol{x}_2 + b)^q\n\\tag{12.1}\\]\nwhere \\(a\\) is called the scaling factor, \\(b\\) is a constant offset value, and \\(q\\) is the polynomial degree. The dot product of predictor vectors (\\(\\boldsymbol{x}_i\\)) measures both angle and distance between points. Note that for the kernel function to work in an appropriate way, the two vectors must have elements with consistent units (i.e., they have been standardized).\nThe kernel function operates much like a polynomial basis expansion; it projects a data point into a much larger, nonlinear space. The idea is that a more complex representation of the predictors might enable the model to make better predictions.\nFor our toy example, we’ll take the 550 predictors, project them to a reduced space of 10 principal components, and standardize those features to have the same mean and standard deviation. A quartic polynomial with zero offset is applied, and the scale parameter, \\(a\\), will be tuned. This parameter helps define how much influence the dot product has in the polynomial expansion.\nThe most commonly adjusted parameter in SVMs is the cost value, which is independent of the chosen kernel function. This parameter determines how strongly the model is penalized for incorrect predictions on the training set. A higher cost value pushes the SVM to create a more complex model. As shown earlier in Figure 9.1, small cost values result in the SVM making less effort to classify samples correctly, leading to underfitting. In contrast, extremely high cost values cause the model to overfit to the training set.\nJust as in Chapter 7, the RMSE will be computed from a validation set and these statistics will be used to guide our efforts.\nLet’s define a wide space for our two tuning parameters: cost will vary from 2-10 to 210 and the scale factor3 is allowed to range from 10-10 to 10-0.1.\nFigure 12.1 visualizes the RMSE across these ranges4 where darker colors indicate smaller RMSE values. The lower left diagonal area is a virtual “dead zone” with very large RMSE results that don’t appear to change much. There is also a diagonal wedge of good performance (symbolized by the darker colors). The figure shows the location of the smallest RMSE value and a diagonal ridge of parameter combinations with nearly equal results. Any points on this line would produce good models (at least for this toy example). Note that there is a small locale of excessively large RMSE values in the upper right. Therefore, increasing both tuning parameters to their upper limits is a bad idea.\nFigure 12.1: A visualization of model performance when only the SVM cost and scaling factor parameters are optimized. It shows the combination with the smallest RMSE and a ridge of candidate values with nearly equivalent performance.\nIn the next section, we explore gradient descent optimization and explain why it may not be suitable for model tuning. Following that, we delve into two traditional global search methods—simulated annealing and genetic algorithms—and their application in parameter space exploration. We then shift our focus to Bayesian optimization, concluding with an in-depth analysis of the barley prediction problem.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-gradient-opt",
    "href": "chapters/iterative-search.html#sec-gradient-opt",
    "title": "12  Iterative Search",
    "section": "12.2 Sidebar: Gradient-Based Optimization",
    "text": "12.2 Sidebar: Gradient-Based Optimization\nTo formalize the concept of optimization, we need an objective function that defines what we are trying to optimize. This function, often called a loss function (specifically to be minimized), is denoted by \\(\\psi()\\). The parameters that modify \\(\\psi()\\) are represented by \\(\\boldsymbol{\\theta}\\), a \\(p \\times 1\\) vector of real numbers. For simplicity, we will assume that \\(\\psi()\\) is smooth and generally differentiable. Additionally, without loss of generality, we will assume that smaller values of \\(\\psi()\\) are better.\nWe’ll denote the first derivative (\\(\\psi'(\\boldsymbol{\\theta})\\)), for simplicity, as \\(g(\\boldsymbol{\\theta})\\) (\\(p\\times 1\\)). The matrix of second deriviatives, called the Hessian matrix, is symbolized as \\(H(\\boldsymbol{\\theta})\\) (\\(p\\times  p\\)).\nWe start with an initial guess, \\(\\boldsymbol{\\theta}_0\\), and compute the gradient at this point, yielding a \\(p\\)-dimensional directional vector. To get to our next parameter value, simple gradient descent uses the update:\n\\[\n\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\alpha\\:g(\\boldsymbol{\\theta}_i)\n\\tag{12.2}\\]\nThe value \\(\\alpha\\) defines how far to move in the chosen direction. It can either be a fixed constant5 or adjusted using a secondary method called a line search. In a line search, \\(\\alpha\\) is incrementally increased until the objective function worsens.\nWe proceed to iterate this process until some measure of convergence is achieved. For example, the optimization could be halted if the objective function does not improve more than a very small value. Lu (2022) and Zhang (2019) are excellent introductions to gradient-based optimization that is focused on training models.\nAs a simple demonstration, Figure 12.2 shows \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\) for values of \\(\\theta\\) between \\(\\pm 10.0\\). We want to minimize this function. There is a global minimum at about \\(\\theta \\approx 6.85\\) while there are local minima at \\(\\theta = -10.0\\) and \\(\\theta \\approx -1.72\\). These are false solutions where some search procedures might become trapped.\n\n\n\n\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-grad-descent\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-gd.R\")\n\napp\n\n\n\nFigure 12.2: An example of simple gradient descent for the function \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\).\n\n\n\n\n\n\n\nThe figure enables the choice of \\(\\theta_0\\), the value of \\(\\alpha\\), and how many iterations to used. Consider a few configurations:\n\nStarting at \\(\\theta = 3\\), a learning rate of \\(\\alpha = 2.0\\) is inappropriate. The search initially moves towards the global minimum but then reverses course and jumps past the best value, then becomes trapped around one of the local optima.\nIf we keep \\(\\theta = 3\\) and decrease learning rate to \\(\\alpha = 1.0\\), we quickly find the best result.\nHowever, if we tried decreasing the learning rate too low, say \\(\\alpha = 0.01\\), the optimization moves too slowly.\n\nThe learning rate plays a crucial role in the optimization process. Additionally, the starting value is significant; values below 2.0 consistently fail to reach the optimum.\nThis gradient descent algorithm outlined above is extremely basic. There are far more complex versions, the most well-known of which is the Newton-Raphson method (a.k.a. Newton’s Method) that incorporates the second derivative matrix \\(H(\\boldsymbol{\\theta})\\) in the updating formula6.\nWe often know when gradient-based optimization will work well. If we know the equation for the objective function, we can determine its properties, such as whether it is a convex function, and theory can tell us if a global optimum can be found via the use of gradients. For example, squared error loss \\(\\psi(\\boldsymbol{\\theta}) = (y-\\hat{y})^2\\), is convex when the relationship for the predicted value \\(\\hat{y}\\) is a well-behaved function of \\(\\boldsymbol{\\theta}\\) (such as in linear regression).\nHowever, there are additional considerations when it comes to optimizations for predictive models. First, since data are not deterministic, the loss function is not only a random variable but can be excessively noisy. This noise can have a detrimental effect on how well the optimization proceeds.\nSecond, our objective function is often a performance metric, such as RMSE. However, not all metrics are mathematically well-behaved—they may lack smoothness or convexity. In the case of deep neural networks, even when the objective function is simple (e.g., squared error loss), the model equations can create a non-convex optimization landscape. As a result, the optimized parameters may correspond to a local optimum rather than a global one.\nThis issue arises because traditional gradient-based methods are inherently greedy. They optimize by moving in the direction that appears most favorable based on the current estimates of \\(\\boldsymbol{\\theta}\\). While this approach is generally effective, it can lead to the optimization process becoming trapped in a local optimum, particularly with complex or non-standard objective functions.\nThere are additional complications when the tuning parameters, \\(\\boldsymbol{\\theta}\\), are not real numbers. For instance, the number of spline terms is an integer, but the update equation might produce a fractional value for this number. Other parameters are qualitative, such as the choice of activation function in a neural network, which cannot be represented as continuous numerical values.\nDuring model tuning, we know to avoid repredicting the training set (due to overfitting). Procedures such as resampling make the evaluation of \\(\\psi()\\) computationally expensive since multiple models should be trained. Unless we use symbolic gradient equations, the numerical process of approximating the gradient vector \\(g(\\boldsymbol{\\theta})\\) can require a large number of function evaluations.\nLater, we’ll discuss stochastic gradient descent (SGD) (Prince 2023, chap. 6). This method is commonly used to train complex networks with large amounts of training data. SGD approximates the gradient by using only a small subset of the data at a time. Although this introduces some variation in the direction of descent, it can be beneficial as it helps the algorithm escape from local minima. Additionally, when dealing with large datasets, computer memory may not be able to store all the data at once. In such cases, SGD is often the only viable option because it doesn’t require the entire dataset to be loaded into memory and can also be much faster.\nUntil then, the next three chapters will describe different stochastic optimization, gradient-free methods that are well-suited for parameter tuning since they can sidestep some of the issues described above.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-sim-anneal",
    "href": "chapters/iterative-search.html#sec-sim-anneal",
    "title": "12  Iterative Search",
    "section": "12.3 Simulated Annealing",
    "text": "12.3 Simulated Annealing\nNon-greedy search methods are not constrained to always proceed in the absolute best direction (as defined by the gradient). One such method is simulated annealing (SA)(Kirkpatrick, Gelatt, and Vecchi 1983; Spall 2005). It is a controlled random search that moves in random directions but with some amount of control over the path. It can also incorporate restarts if the algorithm moves into clearly poor regions.\nGiven an initial solution, simulated annealing (SA) creates a random perturbation of the current candidate solution, typically within a small local neighborhood. The objective function is then evaluated for the new candidate and compared to the previous solution. If the new candidate results in an improvement, the process moves forward by using it to make the next step. If the new candidate is worse, there are two options:\n\nWe can accept the current solution as “suboptimal” and use it as the basis for the next perturbation, or\nwe can discard the current solution and treat it as if it never occurred. The next candidate point will then be a perturbation of the last “acceptable” solution.\n\nFor our SVM example, suppose we start with a candidate where\n\\[x_0 = \\left[log_2 (cost), log_{10} (scale)\\right] = [-10, -0.1]\\]\nand had an associated RMSE value of 6.10%. For the next candidate, a perturbation of this values is created, say \\(x_1 = [-7.11, -0.225]\\). Suppose that the corresponding RMSE was measured at 5.97%. Since \\(x_1\\) has a better performance metric it is automatically accepted and is used to create the next parameter.\nHowever, suppose that the RMSE value for \\(x_1\\) was 7.00%, meaning that the new candidate did worse than the initial one. In this case, simulated annealing generates a probability threshold for accepting the worse solution. Typically, the probability depends on two quantities:\nSimulated annealing generates a probability threshold for accepting the worse solution. Typically, the probability depends on two quantities:\n\nThe difference between the current and previous objective function values. If the new candidate is nearly as good as the current solution, the probability of acceptance will be higher compared to a candidate that is significantly worse.\nThe probability of acceptance should decrease over time as the search progresses. This is often achieved using an exponentially decaying function, known as the “cooling schedule.”\n\nAn often used equation for the probability, assuming that smaller values are better, is\n\\[\nPr[accept] = \\exp\\Bigl[-i\\bigl(\\hat{Q}(\\boldsymbol{\\theta}_{i}) - \\hat{Q}(\\boldsymbol{\\theta}_{i-1})\\bigr)\\Bigr]\n\\]\nwhere \\(i\\) is the iteration number. To compare 6.1% versus 7.0%, the acceptance probability is 0.407. To make the determination, a random uniform number \\(\\mathcal{U}\\) is generated and, if \\(\\mathcal{U} \\le Pr[accept]\\), we accept \\(\\boldsymbol{\\theta}_{i}\\) and use it to make \\(\\boldsymbol{\\theta}_{i+1}\\). Note that the probability “cools” over time; if this difference were to occur at a later iteration, say \\(i = 5\\), the probability would drop to 0.0111.\nOne small matter is related to the scale of the objective function. The difference in the exponent is very sensitive to scale. If, for example, instead of percentages we were to use the proportions 0.61 and 0.70, the probability of acceptance would change from 40.7% to 91.4%. One way to mitigate this issue is to use a normalized difference by dividing the raw difference by the previous objective function (i.e., (0.70-0.61) / 0.70). This is the approach used in the SA analyses here.\nThis process continues until either a pre-defined number of iterations is reached or there is no improvement after a certain number of iterations. The best result found during the optimization process is used as the final value, as there is no formal concept of “convergence” for this method. Additionally, as mentioned earlier, a restart rule can prevent simulated annealing from getting stuck in suboptimal regions if no better results have been found within a certain timeframe. When the process is restarted, it can either continue from the best candidate found in previous iterations or start from a random point in the parameter space.\nHow should the candidates be perturbed from iteration to iteration? When the tuning parameters are all numeric, we can create a random distance and angle from the current values. A similar process can be used for integers by “flooring” them to the nearest whole number. For qualitative tuning parameters, a random subset of parameters is chosen to change to a different value chosen at random. The amount of change should be large enough to search the parameters space and potentially get out of a local optimum.\nIt is important to perform computations on the parameters in their transformed space to ensure that the full range of possible values is treated equally. For example, when working with the SVM cost parameter, we use its log (base 2) scale. When perturbing the parameter values, we make sure to adjust them in the log space rather than the original scale. This approach applies to all other search methods discussed in this chapter.\nTo illustrate, a very small initial space-filling design with three candidates was generated and evaluated. The results are in Table 12.1. To start the SA search7, we will start with the candidate with the smallest RMSE and proceed for 50 iterations without a rule for early stopping. A restart to the last known best results was enforced after eight suboptimal iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRMSE (%)\nCost (log-2)\nScale (log-10)\n\n\n\n\n6.10\n-10\n-0.1\n\n\n7.33\n10\n-5.0\n\n\n17.04\n0\n-10.0\n\n\n\n\n\n\n\n\nTable 12.1: The initial set of candidates used for iterative search with the toy example from Figure 12.1. One candidate does poorly while the other two have relatively similar results.\n\n\n\n\n\n\n\nFigure 12.3 contains an animation of the results of the SA search. In the figure, the initial points are represented by open circles, and a grey diagonal line shows the ridge of values that corresponds to the best RMSE results.\n\n\n\n\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-sa-example\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A two-dimensional parameter space with axes for the scale parameter and the SVM cost is shown. Three initial points are shown. The SA algorithm progresses from a point with low cost and a large value of the scale factor to meander to the ridge of optimal performance, starting several times along the way. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-sa.R\")\n\napp\n\n\n\nFigure 12.3: An example of how simulated annealing can investigate the tuning parameter space. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n\n\n\n\n\n\nDuring the search, there were 4 iterations where a new global best result was discovered (iterations 1, 14, 15, and 40). There were also 5 restarts at iterations 9, 23, 31, 39, and 48. In the end, the best results occurred with a cost value of 2-2.5 and a scale factor of 10-0.98. The corresponding validation set RMSE was 5.78%. With this random seed, the search gets near the ridge of best performance shown in Figure 12.1 but only lingers there for short times. It does spend a fair amount of time meandering in regions of poor performance.\nSimulated annealing has several attractive qualities. First, the process of generating new candidates works with any type of parameter, whether real, integer, or qualitative. This is not true for the other two iterative methods we’ll discuss. Additionally, the perturbation process is very fast, meaning there is minimal computational overhead to compute the next objective function value. Since we are effectively generating new candidate sets, we can also apply constraints to individual parameters or groups of parameters. For example, if a tuning parameter is restricted to odd integers, this would not pose a significant problem for simulated annealing.\nThere are a few downsides to this method. Compared to the other search methods, SA makes small incremental changes. If we start far away from the optimum, many iterations might be required to reach it. One way to mitigate this issue is to do a small space-filling design and start from the best point (as we did). In fact, applying SA search after a grid search (perhaps using racing) can be a good way to verify that the grid search was effective.\nAnother disadvantage is that a single candidate is processed at a time. If the training set size is not excessive, we could parallel process the multiple candidates simultaneously. We could make a batch of perturbations and pick the best value to keep or apply the probabilistic process of accepting a poor value.\nThis optimization took 31.5s per candidate to execute for these data.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-genetic-algo",
    "href": "chapters/iterative-search.html#sec-genetic-algo",
    "title": "12  Iterative Search",
    "section": "12.4 Genetic Algorithms",
    "text": "12.4 Genetic Algorithms\nGenetic algorithms (GAs) (Mitchell 1996; Eiben and Smith 2015) are an optimization method that mimics the process of evolution through natural selection. While GAs are not ideally suited for parameter tuning in our case—since a standard GA search typically requires hundreds to millions of objective function evaluations—we will limit the search to a smaller scale for practical reasons. Nevertheless, genetic algorithms can often find solutions near the optimal value fairly quickly. Additionally, as previously mentioned, there is usually a region of acceptable candidates. Finally, GAs are highly unlikely to become trapped in a locally optimal solution.\nInstead of search iterations, genetic algorithms are counted in generations. A generation is a group of candidate values that are evaluated at the same time (as a batch)8. Once their corresponding performance metrics are computed, a small set of the best candidates is selected and is used to create the next generation via reproduction. Reproduction would entail combining a pair of “parent” candidates by swapping information9, and then random mutations can be applied. Once the next generation is created, the process continues for some pre-defined time limit or maximum number of generations.\nHow, exactly, does this work? The first step is to pick a numerical representation for each candidate. Let’s consider methods for our main types of data.\n\nReal-Valued Parameters\nFor real numbers, there are two encoding methods: one option is to keep them as-is (i.e., floating point values) and another converts the values to a binary encoding.\nWhen keeping the values as real numbers, we can make two children from a pair of well-performing candidates via a linear combination. For candidate vectors \\(\\boldsymbol{\\theta}_j\\), we can use:\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}^{\\:kid}_1 &= \\alpha \\boldsymbol{\\theta}^{\\:par}_1 + (1 - \\alpha) \\boldsymbol{\\theta}^{\\:par}_2 \\notag \\\\\n\\boldsymbol{\\theta}^{\\:kid}_2 &= (1-\\alpha) \\boldsymbol{\\theta}^{\\:par}_1 + \\alpha \\boldsymbol{\\theta}^{\\:par}_2 \\notag \\\\\n\\end{align}\n\\]\nwhere \\(\\alpha\\) is a random standard uniform number. Notice that the two children’s candidate values will always be in-between the values of their parents.\nFor mutation, a candidate’s values are either locally perturbed or simulated with rate \\(\\pi_m\\). For example, we might mutate the log cost value by simulating a random number across the range that defines its search space.\nBinary encodings for real numbers were suggested in the early days of genetic algorithms. In this case, each candidate value is encoded as a set of binary integers. For example, consider a log2 cost value of -2.34. To convert this to binary, we multiply it by 100 (assuming a limit of two decimal places) to convert it to an integer. If we use 8 binary digits (a.k.a. “bits”) to represent 234, we get 11101010. If the candidates could have both positive and negative numbers, we can add an extra bit at the start that is 1 when the value is positive, yielding: 011101010.\nThe method of cross-over was often used for the reproduction of binary representations. For a single cross-over, a random location between digits was created for each candidate value, and the binary digits were swapped. For example, a representation with five bits might have parents ABCDE and VWXYZ. If they were crossed over between the second and third elements, the children would be ABXYZ and VWCDE. There are reproduction methods that use multiple cross-over points to create a more granular sharing of information.\nThere are systematic bias that can occur when crossing binary representations as described by Rana (1999) and Soule (2009). For example, there are positional biases. For example, if the first bits (A and V) capture the sign of the value, the sign is more likely to follow the initial bits than the later bits to an offspring.\nFor mutating a binary representation, each child’s bit would be flipped at a rate of \\(\\pi_m\\).\nAfter these reproduction and mutation, the values are decoded into real numbers.\n\n\nInteger Parameters\nFor integers, the same approach can be used as real numbers, but after the usual operations, the decimal values are coerced to integers via rounding. If a binary encoding is used, the same process can be used for integers as real numbers; they are all just bits to the encoding process.\n\n\nQualitative Parameters\nFor qualitative tuning parameters, one (inelegant) approach is to encode them into values on the real line or as integers. For example, for a parameter with values “red,”, “blue,” and “green”, we could map them to bins of [0, 1/3), [1/3, 2/3), and [2/3, 1]. From here, we reproduce and mutate them as described above, then convert them back to their non-numeric categories. This is more palatable when there is a natural ordering of the values but is otherwise a workable but unfortunate approach. Mutation is simple though; at rate \\(\\pi_m\\), a value is flipped to a random selection of the possible values.\n\n\n12.4.1 Assembling Generations\nNow that we know how to generate new candidates, we can form new generations. The first step is to determine the population size. Typically, the minimum population size within a generation is 25 to 50 candidates. However, this may be infeasible depending on the computational cost of the model and the characteristics of the training set, such as its size. While fewer candidates can be used, the risk is that we may fail to sample any acceptable results. In such cases, combining the best results will only produce more mediocre candidates. Increasing the mutation rate can help mitigate this issue. For the first iteration, random sampling of the parameter space is commonly used. It is highly recommended to use a space-filling design for the initial candidate set to ensure a more comprehensive exploration.\nFinally, there is the choice of which candidates to reproduce at each generation. There are myriad techniques for selecting which candidates are used to make the next generation of candidates. The simplest is to pick two parents by randomly selecting them with probabilities that are proportional to their performance (i.e., the best candidates are chosen most often). There is also the idea of elitism in selection. Depending on the size of the generation, we could retain a few of the best-performing candidates from the previous generation. The performance values of these candidates would not have to be recomputed (saving time), and their information is likely to persist in a few good candidates for the next generation.\n\n\n12.4.2 Two Parameter Example\nTo illustrate genetic algorithms in two dimensions, a population size of 8 was used for 7 generations. These values are not optimal defaults but were selected to align with the previous SA search and the optimization method discussed in the next section. The two tuning parameters were kept as floating-point values. Parental selection was based on sampling weights proportional to the RMSE values (with smaller values being better). The mutation rate was set at 10%, and elitism was applied by retaining the best candidate from each generation. All computations within a generation were performed in parallel. On average, the optimization took 17.2s per candidate. Figure 12.4 illustrates the results of the search.\n\n\n\n\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-ga-example\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A two-dimensional parameter space with axes for the scale parameter and the SVM cost is shown. Eight initial points are shown, one near the ridge of best results. As generations increase, new generations are gathered around the ridge. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-ga.R\")\n\napp\n\n\n\nFigure 12.4: Several generations of a genetic algorithm that search tuning parameter space. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n\n\n\n\n\n\nA space-filling design was used to ensure that the initial population was diverse. Fortuitously, one design point was very close to the ridge, with an RMSE of 5.83%. After this, there were 4 generations with better candidates (with almost identical performance): 5.82%, 5.772%, 5.765%, and 5.764%. We can see that, after three generations, the search is concentrated around the ridge of optimal performance. In later generations, some candidates have outliers in one dimension; this is the effect of mutation during reproduction.\n\n\n12.4.3 Summary\nThis small toy example is not ideal for demonstrating genetic algorithms. The example’s low parameter dimensionality and the relatively low computational cost per candidate might give the impression that genetic algorithms are a universal solution to optimization problems. While genetic algorithms are versatile and powerful tools for global optimization, they come with limitations. Like simulated annealing, they have minimal overhead between generations, but in most real-world applications, there are more than just two parameters. This typically means that larger populations and more generations are needed. If the dataset is not excessively large, parallel processing can help manage the increased computational demands.10.\n\nNow we’ll take a look at what makes a Bayesian model Bayesian.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes",
    "href": "chapters/iterative-search.html#sec-bayes",
    "title": "12  Iterative Search",
    "section": "12.5 Sidebar: Bayesian Models",
    "text": "12.5 Sidebar: Bayesian Models\nWe’ve superficially described an application of Bayesian analysis in Section 6.4.3. Before discussing Bayesian optimization, we should give a general description of Bayesian analysis, especially since it will appear again several times after this chapter.\nMany models make probabilistic assumptions about their data or parameters. For example, a linear regression model has the form\n\\[\ny_i = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p + \\epsilon_i\n\\]\nUsing ordinary least squares estimation, we can make assumptions regarding the model errors (\\(\\epsilon_i\\)). We can assume that the residuals are independent of one another and follow a Gaussian distribution with zero mean and a constant standard deviation. From there, it follows that the regression parameters (\\(\\beta\\) coefficients) also follow Gaussian distributions.\nBased on these assumptions, our objective function is the Gaussian likelihood function, generally denoted as \\(\\ell(z|\\theta)\\), although we often maximize the log of this value. We fix the outcome and predictor data and try to find values of \\(\\sigma\\) and the \\(\\beta\\) parameters that maximize the objective function (log \\(\\ell(z|\\theta)\\)). This process is maximum likelihood estimation.\n\nOne important point is that each model parameter is treated as a single value. Our maximum likelihood estimate (MLE) is a point estimate and, based on our assumptions about the residuals, we know the distribution of the MLEs.\n\nThe consequence of this is that we cannot make inferences about the true, unknown model parameters since they are considered to be single points. Instead, our inference focuses on the MLEs. This leads to the circuitous explanation of hypothesis tests and confidence intervals. For example, the explaination of a 90% confidence interval is:\n\n“We believe that if we were to repeat this experiment a large number of times, the true parameter value would fall between \\(L\\) and \\(U\\) 90% of the time.”\n\nA Bayesian approach takes a different perspective on probability assumptions. It assumes that the unknown parameters are drawn from a prior distribution that represents our beliefs or knowledge about them before observing the data. We denote this prior distribution as \\(\\pi(\\theta)\\). The term “prior” is crucial because the modeler should define this distribution before seeing the observed data.\nFor example, consider our model for the time to deliver food in Equation 1.1. A reasonable prior for the regression parameter associated with distance would have a distribution that assigns zero probability to negative values, since we would never expect (log) delivery times to decrease with distance. If we worked at the restaurant, we could be more specific based on our experience. We might believe that, all other factors being equal, each additional mile from the restaurant doubles the delivery time. Using this assumption, we could define a probability distribution that reflects this belief.\nOnce we have our prior distributions for our parameters and assumptions regarding our data distributions, we can write down the equations required to estimate our results. Bayes’ Rule is a basic probability statement that combines the prior distribution with our likelihood function:\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) \\ell(x|\\theta)} {\\pi(x)}\n\\tag{12.3}\\]\n\\(\\pi(\\theta|x)\\) is the posterior distribution: the probability distribution of our parameters, given the observed data. This is the endpoint for any Bayesian analysis.\nAnother important point is that Bayesian estimation has a much more difficult goal than maximum likelihood estimation. The latter needs to find point estimates of its parameters while Bayesian estimation has to estimate the entire posterior distribution \\(\\pi(\\theta|x)\\). In a moment, we’ll look at a simple problem with a simple solution. However, in most other cases, the computational requirements for Bayesian estimation are considerably higher. That’s the bad news.\nThe good news is that, once we find the posterior distribution, it is incredibly useful. First, we can make direct statements about parameter values. Unlike confidence intervals, Bayesian methods allow us to say things like\n\n“We believe that there is a 90% probability that the true value of the parameter is between \\(L\\) and \\(U\\).”\n\nIt also lets us easily make similar statements regarding more complex combinations of parameters (such as ratios, etc).\nFinally, we should mention the effect of the prior on the computations. It does pull the likelihood \\(\\ell(x|\\theta)\\) towards it. For example, suppose that for Equation 1.1 we used a highly restrictive prior for \\(\\beta_1\\) that was a uniform distribution between [0.7, 0.9]. In that case, the posterior would be confined to this range11. That said, the effect of the prior on the posterior decreases as our training set size increases. We saw this in Section 6.4.3 where two travel agents were contrasted; one with many reservations in the data and another with very few.\nTo illustrate, let’s look at a very simple example.\n\n12.5.1 A Single Proportion\nFor the forestry data discussed earlier in Section 10.8, the outcome is categorical, with the values “yes” and “no” representing the question “Is this location forested?” We can use the training dataset to estimate the probability (\\(\\pi\\)) of the event occurring. The simplest estimate is the sample proportion, which is calculated by dividing the number of occurrences of the event (e.g., “yes” responses) by the total number of data points. This gives the estimate \\(\\hat{\\pi}\\) = 56.167%).\nTo estimate this rate using maximum likelihood estimation, we might assume that the data follow a binomial distribution with theoretical probability \\(\\pi\\). From there we can solve equations that find a value of \\(\\pi\\) that correspond to the largest likelihood. It turns out that the sample proportion is also the maximum likelihood estimate.\nInstead of treating the unknown parameter as a single value, Bayesian methods propose that the parameter comes from a distribution of possible values. This distribution reflects our prior understanding or belief about the parameter based on what we know of the situation. For this reason, it is called a prior distribution. If we think that locations in Washington state are very likely to be forested, we would choose a distribution that has more area for higher probabilities.\nFor a binomial model, an example of a prior is the Beta distribution. The Beta distribution is very flexable and has values range between zero and one. It is indexed by two parameters: \\(\\alpha\\) and \\(\\beta\\). Figure 12.5 shows different versions of the Beta distribution for different values.\nTo settle on a specific prior, we would posit different questions such as:\n\n“Are there rate values that we would never believe possible?”\n“What is the most likely value that we would expect and how certain are we of that?”\n“Do we think that the distribution of possible values is symmetric?”\n\nand so on.\nFrom these answers, we would experiment with different values of \\(\\alpha\\) and \\(\\beta\\) until we find a combination that encapsulates what we believe. If we think that larger probabilities of forestation are more likely, we might choose a Beta prior that has values of \\(\\alpha\\) that are larger than \\(\\beta\\), which places more mass on larger values (as seen below). This is an example of an informative prior, albeit a weak one. As our prior distribution is more peaked it reflects that, before seeing any data, we have strong beliefs. If we honestly have no idea, we could choose a uniform distribution between zero and one using \\(\\alpha = \\beta = 1\\).\n\n\n\n\n\n\n\n\nFigure 12.5: Examples of the Beta distribution for different values of \\(\\alpha\\) and \\(\\beta\\).\n\n\n\n\n\nWe’ll use values of \\(\\alpha = 5\\) and \\(\\beta = 3\\). If the training set has \\(n_{tr}\\) points and \\(n_+\\) of them are known to be forested, the ordinary sample proportion estimate is \\(\\hat{p} = n_+ / n_{tr}\\). In a Bayesian analysis, the final estimate is a function of both the prior distribution and our observed data. For a Beta prior, the Bayesian estimate is\n\\[\n\\hat{p}_{BB} = \\frac{n_+ +\\alpha}{n_{tr} + \\alpha + \\beta}\n\\tag{12.4}\\]\nIf, before seeing the data, we had chosen \\(\\alpha = 5\\) and \\(\\beta = 1\\), we estimate that \\(\\hat{p}_{BB} = 56.201\\)%. This is pretty close to our simple estimate because there is so much data in the training set (\\(n_{tr}\\) = 4,832) that the influence of the prior is severely diminished. As a counter-example, suppose we took a very small sample that resulted in \\(n_y\\) = 1 and \\(n_{tr}\\) = 3, the prior would pull the estimate from the MLE of 33.3% to 54.5%.\nNow suppose that for regulatory purposes, we were required to produce an interval estimate for our parameter. From these data and our prior, we could say that the true probability of forestation has a 90% chance of being between 55% and 57.3%.\nWe did mention that it can be very difficult to compute the posterior distribution. We deliberately chose the Beta distribution because, if we assume a binomial distribution for our data and a \\(Beta(\\alpha, \\beta)\\) prior, the posterior is \\(Beta(\\alpha + n_y, n_{tr} - n_y + \\beta)\\). More often than not, we will not have a simple analytical solution for the posterior. That said, we’ll see this happen again in Section 12.6.2.\nFor a Bayesian model, the predictions also have a posterior distribution, reflecting the probability of a wide range of values. As with non-Bayesian models, we often summarize the posterior using the most likely value (perhaps using the mean or mode of the distribution). We can also measure the uncertainty in the prediction using the estimated standard deviation.\n\n\n12.5.2 What is not Bayesian?\nMost other methods fall into the category of “Frequentist.” The terms “Frequentist” and “Bayesian” correspond to different philosophies of interpreting probability. They are not the only two philosophies, but between them, they account for the vast majority of data analytic methods.\nMaximum likelihood estimation is a good example of a Frequentist approach. As we said, when training a model, the optimal parameters are the ones that maximize the probability that we actually did get the data that we observed. If we flip a coin 100 times and 90 of them come up heads, MLE would not choose parameter values that are inconsistent with the results from our 100 observed results.\nCoin flipping does present a good window into how both philosophies operate. Consider the probability of getting heads when tossing a specific coin. The two points of view are:\nFrequentists:\n\nThe probability of being heads is a single unknown value representing the long-term rate at which one would get heads with that coin after infinite tosses. Once we toss the coin, there is no uncertainty. If it is heads, the probability that it is heads is exactly 100%.\n\nBayesians:\n\nThe probability of heads is a random variable with an unknown distribution. We can incorporate our beliefs about that distribution into our analyses, often making them somewhat biased. We update our beliefs with data so that our inferences and predictions are a combination of both. We can make direct probabilistic statements about unknown parameters so if a coin was heads, we can still estimate the probability that it would be heads.\n\nIf you are unsure about the tool that you are using, terms like “maximum likelihood estimation”, “confidence intervals”, and “hypothesis testing” should tell you that it takes a Frequentist view. Terms such as “prior,” “posterior,” or “credible interval” are more indicative of a Bayesian methodology.\nBland and Altman (1998) and Fornacon-Wood et al. (2022) are fairly non-technical discussions of the two camps, while Wagenmakers et al. (2008) discusses more mathematical differences. Discussions on this subject can be vigorous. Gill, Sabin, and Schmid (2005)’s discussion of how medical doctors make decisions is interesting and elicited numerous letters to the British Medical Journal (Chitty 2005; Hutchon 2005; McCrossin 2005). Others have had pragmatic, if not emphatic, thoughts on the subject (Breiman 1997).\n\nTo summarize:\n\nBayesian models require a prior distribution for all of our model parameters.\nThe prior and observed data are combined into a posterior distribution.\nDifferent statistics can be estimated from the posterior, including individual predictions.\n\n\n\nWe’ll encounter even more Bayesian methods in subsequent chapters. For example, in Chapter 14, both Frequentist and Bayesian approaches to inference are discussed. In Chapter 15, we’ll describe the process of developing a prior for muticlass problem based on Wordle scores and how priors can affect our results.\nNow that we know more about Bayesian statistics, let’s see how they can be used to tune models.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes-opt",
    "href": "chapters/iterative-search.html#sec-bayes-opt",
    "title": "12  Iterative Search",
    "section": "12.6 Bayesian Optimization",
    "text": "12.6 Bayesian Optimization\nBayesian optimization (BO) is a search procedure that uses an overarching statistical model to predict the next set of candidate values based on past performance. It is currently the most used iterative search routine for optimizing models. The technique originated with Močkus (1975) and was further developed by Jones, Schonlau, and Welch (1998). It is often tied to a particular Bayesian predictive model: the Gaussian process (GP) regression model (Binois and Wycoff 2022).\nBayesian optimization and Gaussian proceses have been vibrant research areas and our discussions will scratch the surface. More recent surveys, Gramacy (2020) and Garnett (2023), are comprehensive resources for these methodologies.\nHowever, before diving into the details of Gaussian proceseses, we should discuss how any Bayesian method can be used for optimization. Similar to the racing model from Equation 11.1, the Bayesian model’s outcome will be our estimated performance statistic (i.e., \\(\\hat{Q}\\)). The predictors for this model will be the tuning parameter values (\\(\\boldsymbol{\\theta}\\)):\n\\[\nQ_j = f(\\boldsymbol{\\theta}_j)+ \\epsilon_{j}\n\\tag{12.5}\\]\nwhere \\(j\\) is the iteration number, \\(f(\\cdot)\\) is a surrogate model, and \\(\\epsilon\\) is an error term. For the barley data, we’ve used RMSE as our primary metric. Using a Bayesian model for \\(f(\\cdot)\\) enables us to predict what we expected the RMSE to be for different candidate points.\nSince a Bayesian model produces a posterior (predictive) distribution, we can obtain the distribution of the performance metric for a potential new candidate. We often summarize this predictive distribution with the mean and standard deviation. Figure 12.6 shows two hypothetical candidates and their respective predictive distributions. The first candidate has a slightly better predicted RMSE (on average). The area under the distribution curve to the right of the vertical line reflects the probability of being worse than the current solution which, for this candidate, is 16%.\n\n\n\n\n\n\n\n\nFigure 12.6: An example of a choice between two candidates where performance is gauged by RMSE. One candidate has slightly better predicted mean performance and another where the mean RMSE is predicted to be much lower but with high uncertainty.\n\n\n\n\n\nThe second candidate has a mean RMSE that is predicted to have about a 2-fold improvement over the first but comes with considerably large uncertainty. On average, this is a better choice but there is more area to the right of the vertical line. The probability of the second candidate having worse results than the current best is 26%.\nWhich option is better? The first candidate is a safer choice, while the second comes with higher risk but also a greater potential reward. The decision often depends on our perspective. However, it’s important to understand why the second candidate has such a large variance in outcomes.\nWe’ve previously discussed the variance-bias tradeoff and demonstrated that large uncertainty in the performance metric can often be explained by model variance. Recall that models might vary due to predictive instability caused by a model that is far more complex than is required, such as the polynomial example in Section 8.4.\nHowever, there is another reason that the noise in our performance statistic can be large. Back in Section 3.9, we described spatial autocorrelation, where objects are more similar to closer objects than to objects further away. If our Bayesian model reflects spatial variability, the increased uncertainty can be attributed to how each candidate relates spatially to the existing set. If our model’s variance in Figure 12.6 is based on spatial effects, it implies that the first candidate is closer to the current collection of tuning parameter candidates. Conversely, the large uncertainty for the second candidate suggests that it is placed far away from existing results.\nThis example illustrates two different search strategies: exploitation and extrapolation.\n\nExploitation involves focusing the search near existing data, leveraging known results to refine and optimize solutions within familiar territory.\nExtrapolation emphasizes exploring new, untested areas more aggressively in search of novel candidates that could lead to model improvements.\n\nUsing the Bayesian model in Equation 12.5 is beneficial because it allows for both mean and variance predictions, which can be weighted differently when evaluating potential new candidates. If our surrogate Bayesian model effectively captures spatial effects, we can leverage this to optimize the model. The next section outlines the overall optimization process.\n\n12.6.1 How Does Bayesian Optimization Work?\nThe search process begins with an initial set of candidates (\\(s_0\\)) and their corresponding performance statistics (_j$). These data are used to build a surrogate model, where the performance metric serves as the outcome and the candidate values act as predictors. The model then predicts the mean and standard deviation of the metric for new candidate values.\nTo guide the search, we use an acquisition function—an objective function that combines the predicted mean and standard deviation into a single value (Garnett 2023, chap. 7). The next candidate to be evaluated is the one that optimizes this acquisition function. This process is then repeated iteratively.\nOne of the most well-known acquisition functions is based on the notion of expected improvement (Jones, Schonlau, and Welch 1998, sec. 4). If the current best metric value is denoted as \\(\\hat{Q}_{opt}\\), the expected improvement is the positive part of \\(\\delta(\\hat{\\boldsymbol{\\theta}}) = Q_{opt} - \\mu(\\hat{\\boldsymbol{\\theta}})\\), where \\(\\mu(\\hat{\\boldsymbol{\\theta}})\\) is the mean of the posterior prediction (assuming that smaller metrics are better). They found that, probabilistically, the expected improvement is\n\\[\nEI(\\hat{\\boldsymbol{\\theta}}; Q_{opt}) = \\delta(\\hat{\\boldsymbol{\\theta}}) \\Phi\\left(\\frac{\\delta(\\hat{\\boldsymbol{\\theta}})}{\\sigma(\\hat{\\boldsymbol{\\theta}})}\\right) + \\sigma(\\hat{\\boldsymbol{\\theta}}) \\phi\\left(\\frac{\\delta(\\hat{\\boldsymbol{\\theta}})}{\\sigma(\\hat{\\boldsymbol{\\theta}})}\\right)\n\\tag{12.6}\\]\nwhere \\(\\Phi(\\cdot)\\) is the cumulative standard normal and \\(\\phi(\\cdot)\\) is the standard normal density.\nExpected improvement balances both mean and variance, but their influence can shift throughout the optimization process. Consider a parameter space where most regions have already been sampled to some extent. If the Bayesian model is influenced by spatial variation, the remaining unexplored areas will generally have low variance (\\(\\sigma\\)), causing one term in Equation 12.6 to dominate the optimization. In contrast, during the early iterations, variance tends to be much higher. Unless there is a strong mean effect, spatial variance will dominate, leading the search to focus more on extrapolation.\nAdditionally, the expected improvement can be altered to include a tradeoff factor \\(\\xi\\) so that improvement is the positive part of\n\\[\n\\delta(\\hat{\\boldsymbol{\\theta}}) = Q_{opt} - \\mu(\\hat{\\boldsymbol{\\theta}}) + \\xi\n\\]\nThis effectively inflates the value of the current best and the resulting value of \\(\\delta(\\hat{\\boldsymbol{\\theta}})\\) helps emphasize extrapolation. \\(\\xi\\) could be a single constant or a function that changes the tradeoff value as a function of iterations.\nSimilar to simulated annealing, sampling continues until reaching a predefined limit on iterations or computational time. Additionally, an early stopping rule can be used to end the search early if no new best result is found within a set number of iterations.\nLet’s see how this approach works in a single dimension. Figure 12.8 shows how expected improvement can be used to find the global minimum of \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\). The light grey line in the lower panel represents the true value of \\(\\psi(\\theta)\\) across the parameter space. The process starts with \\(s_0 = 2\\) candidate points at -8.0 and 3.0. From these two points, the surrogate model is fit and the dark line in the lower panel shows the predicted objective function values across the space. The magnitude of the line represents the mean of the posterior. Notice that the predicted line fails to capture the true trend, which is not unexpected with two data points.\nIf we optimized only on the mean prediction, we would select the next point near \\(\\theta = 3\\) since it has the lowest predicted value. This approach relies entirely on exploitation.\nThe color of the line depicts the predicted variance, which drops to zero near the existing points and explodes in the regions in between our two points. If we focused solely on the variance of the objective function, we would choose the next point in an area far from the two sampled points, such as the interval \\(-6 \\le \\theta \\le 1\\) or \\(\\theta \\ge 4\\).\nBy combining the mean and variance predictions through expected improvement, we strike a balance between the two. In the first iteration, two areas near (but not exactly at) \\(\\theta = 3.0\\) are predicted to have large expected improvements. The region on the left is sampled first, and once \\(\\psi(\\theta = 3)\\) is evaluated, the next point moves closer to the true global optimum. This process continues, alternating between sampling near the best result and exploring regions with few nearby candidate values. After 15 iterations, the search is very close to the true global optimum.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-bayes-opt\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A single curve is shown with alternating peaks and valleys. The x-axis is the parameter, and the y-axis shows the objective function. Above this is another curve where the y-axis is expected improvement. As the optimization proceeds, the top curve shows several peaks of expected improvement, and the candidate points begin to cluster around the global minimum. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\nload(url(\"https://raw.githubusercontent.com/aml4td/website/main/RData/bayesian_opt_1d.RData\"))\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-bo-1d.R\")\n\napp\n\n\n\nFigure 12.7: An example of Bayesian optimization for the function \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\). The top plot shows the expected improvement for the current iteration. The bottom panel shows the true curve in grey and the mean prediction line (colored by the predicted standard deviation). The vertical line indicates the location of the next candidate.\n\n\n\nNumerous other acquisition functions can be used. For example, instead of expected improvement, we can maximize the probability of improvement:\n\\[\nPI(\\hat{\\boldsymbol{\\theta}}) = \\Phi\\left(\\frac{\\tau - \\mu(\\hat{\\boldsymbol{\\theta}})}{\\sigma(\\hat{\\boldsymbol{\\theta}})}\\right)\n\\]\nwhere \\(\\tau\\) is some performance metric goal (e.g., an R2 of 80%). This is generally deprecated in favor of EI for a few reasons. Most importantly, PI does not directly incorporate the current best value. Additionally, the value of \\(\\tau\\) is arbitrary and the search can be inconsistent across values of \\(\\tau\\) (Jones 2001).\nAnother acquisition function is based on confidence bounds (Cox and John 1992). For minimizing the performance statistic, the lower bound would be computed:\n\\[\nCB(\\hat{\\boldsymbol{\\theta}}) = \\mu(\\hat{\\boldsymbol{\\theta}}) - \\kappa\\: \\sigma(\\hat{\\boldsymbol{\\theta}})\n\\]\nwhere \\(\\kappa\\) sets the confidence level and often has values that are much smaller than range used for statistical inference (e.g., centered around 1.64). This multiplier also modulates between extrapolation (large \\(\\kappa\\)) and exploitation (small \\(\\kappa\\)) for the search.\nBayesian optimization offers more than just different acquisition functions; there are also various other modifications that can enhance its efficiency. For instance, similar to simulated annealing, the default approach samples one candidate at a time. While this works, it can be computationally inefficient, especially when models can be fitted in parallel. To improve this, modifications allow for multiple candidates to be simulated at once. In this approach, the candidate with the highest predicted acquisition value is chosen, and its predicted performance is treated as if it were derived from an actual assessment. A new GP model is then created using this imputed data point, and the process continues by acquiring the next point. This cycle repeats until the desired number of new candidates is obtained (see Azimi, Fern, and Fern (2010)). Alternatively, we can focus on the most promising areas of the parameter space and sample several different points. For example, in iteration five of Figure 12.7, where there are three peaks showing improvement, we could sample each of these peaks and create a batch of three new candidates to evaluate all at once, rather than individually.\nNow that the general optimization process has been outlined, let’s focus on the most popular model associated with Bayesian optimization.\n\n\n12.6.2 Gaussian Process Models\nGPs (Williams and Rasmussen 2006; Garnett 2023) are often motivated as special cases of stochastic processes, which are the collection of random variables that are indexed by one or more variables. The most common example of a stochastic process is the stock market. A stock price is a random variable that is indexed by time. A Gaussian process is just a stochastic process whose data are assumed to be multivariate normal.\nGaussian processes are different from basic multivariable normal distributions. Let’s say that we collected the heights and weights of 100 students in a specific grade. If we collect the heights and weights into a two-dimensional vector \\(\\boldsymbol{x} = [height, weight]\\), we might assume that the data is multivariate normal, with a 2D population mean vector and a 2 \\(\\times\\) 2 covariance matrix, i.e., \\(\\boldsymbol{x} \\sim N(\\boldsymbol{\\mu}, \\Sigma)\\). In this case, the data are a sample of people at a static time point.\nHowever, a Gaussian process is a sequence of data so we have to specify their means and covariances dynamically; there are mean and covariance functions instead of vectors and matrices. The mean function is often denoted as \\(\\mu(\\boldsymbol{x})\\). One way of writing the covariance function is \\(\\Sigma(\\boldsymbol{x}, \\boldsymbol{x}') = k(\\boldsymbol{x}, \\boldsymbol{x}') + \\sigma^2_{\\epsilon}I\\) where \\(k(\\cdot, \\cdot)\\) is a kernel function and \\(\\sigma_{\\epsilon}\\) is a constant error term.\nWe’ve seen kernels before, back in Equation 12.1 when they were associated with support vector machine models. For Gaussian processes, the kernel will define the variance between two data points in the process. Some kernels can be written as a function of the difference \\(\\delta_{ii'} = ||\\boldsymbol{x} - \\boldsymbol{x}'||\\). A common kernel is the squared exponential:\n\\[\nk(\\boldsymbol{x}, \\boldsymbol{x}') = \\exp\\left(-\\sigma d_{ii'}^2 \\right)\n\\tag{12.7}\\]\nNote that this covariance function has it’s own parameter \\(\\sigma\\), similar to \\(a\\), \\(b\\), and \\(q\\) in Equation 12.1 (\\(\\sigma\\) is unrelated to \\(\\sigma_{\\epsilon}\\)).\nThe squared exponential function shows that as the two vectors become more different from one another (i.e., further away), the exponentiated difference becomes large, indicating high variance (apart from \\(\\sigma^2_{\\epsilon}\\)). For two identical vectors, this covariance function yields a variance of \\(\\sigma^2_{\\epsilon}\\). Now, we can see why the previous section had a strong emphasis on spatial variability for Bayesian optimization. Figure 12.7 used this kernel function.\nA more general covariance function uses the Matern kernel:\n\\[\nk_{\\nu }(\\delta_{ii'})=\n{\\frac {2^{1-\\nu }}{\\Gamma (\\nu )}}{\\Bigg (}{\\sqrt {2\\nu }}{\\frac {\\delta_{ii'}}{\\rho }}{\\Bigg )}^{\\nu }\\mathcal{K}_{\\nu }{\\Bigg (}{\\sqrt {2\\nu }}{\\frac {\\delta_{ii'}}{\\rho }}{\\Bigg )}\n\\tag{12.8}\\]\nwhere \\(\\mathcal{K}(\\cdot)\\) is the Bessel function, \\(\\Gamma(\\cdot)\\) is the Gamma function, and \\(\\rho\\) and \\(\\nu\\) are tuning parameters.\nSo far, our covariance functions have assumed that all tuning parameters are numeric. A common, though simplistic, approach for incorporating qualitative parameters into a Gaussian process is to convert them into binary indicators and treat them as generic numeric predictors. However, this method is problematic because it’s unlikely that these binary indicators follow a Gaussian distribution. Additionally, if the qualitative tuning parameters have many levels, this approach can rapidly increase the dimensionality of the Gaussian space, which in turn raises the computational costs of training the GP model.\nAlternatively, special “categorical kernels” take integers corresponding to levels of a categorical predictor as inputs. For example, one based on an exchangeable correlation structure is:\n\\[\nk(x, x') = \\rho I(x \\ne x') + I(x = x')\n\\tag{12.9}\\]\nwhere \\(\\rho &lt; 1\\) (Joseph and Delaney 2007). More extensive discussions of models with mixed types can be found in Qian and Wu (2008), Zhou, Qian, and Zhou (2011), and Saves et al. (2023).\nMultiple kernels can be used to accommodate a pipeline with both quantitative and qualitative tuning parameters. Depending on the situation, an overall kernel can be created via addition or multiplication.\nGenton (2001), Part II of Shawe-Taylor and Cristianini (2004), and Chapter 4 of Garnett (2023) contain overviews of kernel functions, their different classes, and how they can be used and combined for data analysis.\nHow does this relate to iterative tuning parameter optimization? Suppose that our multivariate random variable is the concatenation of an outcome variable and one or more predictors, our vector of random variable could be thought of as \\(\\boldsymbol{d} = [y, x_1, ..., x_p]\\). In a model, we’d like to predict the value of \\(y\\) is for a specific set of \\(x\\) values. Mathematically, this is \\(Pr[y | \\boldsymbol{x}]\\) and, since this collection of data is multivariate normal, there are some nice linear algebra equations that can be used to compute this. This ties into Bayesian analysis since we can write Equation 12.3 in terms of probabilities:\n\\[\nPr[y | \\boldsymbol{x}] = \\frac{Pr[y] Pr[\\boldsymbol{x} | y]}{Pr[\\boldsymbol{x}]} = \\frac{(prior)\\times (likelihood)}{(evidence)}\n\\tag{12.10}\\]\nThe GP model is Bayesian because we have specified the distributions for the outcome and predictors (\\(Pr[y]\\) and \\(Pr[\\boldsymbol{x}]\\), respectively) simultaneously. The kernel function is the primary driver of the prior distribution of Gaussian processes.\nIn theory, each of these is easy to compute with a simple multivariate normal. However, we don’t know the value of \\(\\sigma^2_{\\epsilon}\\) nor do we know the values of any kernel parameters used by \\(k(\\cdot,\\cdot)\\). The error term can be estimated a variety of ways, including maximum likelihood, cross-validation, and others (see Ameli and Shadden (2022) for a survey of methods). For the kernel tuning parameters, it is common to maximize the marginal likelihood to estimate these structural parameters (Williams and Rasmussen 2006, chap. 5). The Gaussian nature of this model enables a number of relatively straightforward procedures for these purposes (that does not involve another layer of resampling).\nHowver, training Gaussian process models can be difficult and the computation cost of training these models increases cubically with the number of data points. In our application, the training set size is the number of tuning parameter candidates with performance statistics. However, after the initial GP model has been trained, there are updating algorithms that can prevent the model from being completely re-estimated from scratch thus reducing training time.\n\nIn summary, Gaussian process models are nonlinear models that can use tuning parameter values to make predictions on future performance statistics. They can predict the mean and variance of performance, the latter being mostly driven by spatial variability.\n\nIn Figure 12.7 we’ve seen how well Bayesian optimization works for a simple one-dimensional function. Let’s apply it in context by using it to optimize our two SVM tuning parameters.\n\n\n12.6.3 A Two Dimensional Illustration\nReturning the the SVM example, we initialize the GP model using the same three points shown in Table 12.1. We again use the squared exponential kernel in Equation 12.7 to measure variability.\nFigure 12.8 shows the results that help us select the first new candidate value. Due to the small number of points used to fit the GP, the EI surface is virtually flat. The flatness of the acquisition function results in a fairly uninformative choice: the first point is very close to the best candidate in the initial set.\nOnce this new point is sampled, the surrogate GP model produces a clear trend in EI indicating a strong preference for higher cost values (and no effect of the scale parameter). The selected candidate is closer to the ridge of optimal performance. After acquiring this point, the next iteration shows that EI is focused on the scale parameter value and ignores the cost parameter. By iteration 13, a candidate is chosen that is nearly optimal.\nThis example shows that Bayesian optimization can make enormous leaps in parameter space, frequently choosing values of parameters at their minimum or maximum values. This is the opposite of SA, which meanders through local parameter space.\n\n\n\n\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-bo-example\n#| viewerHeight: 630\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A two-dimensional parameter space with axes for the scale parameter and the SVM cost is shown. Three initial points are shown. The BO algorithm progresses from a point with low cost and a large scale factor value, making large jumps to different regions of the parameter space; several iterations come close to the optimal ridge. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-bo.R\")\n\napp\n\n\n\nFigure 12.8: An illustration of Bayesian optimization. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n\n\n\n\n\n\nThe complete search produced new optimal values at iterations iterations 4, 9, 13, 42, and 44. The decrease in RMSE shows few meaningful improvements among negligible decreases: 6.098%, 5.993%, 5.947%, 5.773%, 5.771%, and 5.77%. The search process was able to effectively tune the SVM model and find very good parameters.\nThe cost of the Gaussian process fit does become more expensive as new candidates are sampled, making the process somewhat less efficient than the previous approaches. The average time to compute each candidate was 56.2s, although the cost of each increases with iterations.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes-opt-nnet",
    "href": "chapters/iterative-search.html#sec-bayes-opt-nnet",
    "title": "12  Iterative Search",
    "section": "12.7 Example: Tuning a Neural Network",
    "text": "12.7 Example: Tuning a Neural Network\nWe revisit the barley data, where the percentage of barley oil in a mixture is estimated using spectrographic measurements. Our previous work on these data addressed the high correlation between predictors via feature engineering with principal components.\nIn this chapter, we’ll take a more sophisticated approach to preprocessing this type of data using signal processing techniques, specifically the Savitzky-Golay (SG) method (Rinnan, Van Den Berg, and Engelsen 2009; Schafer 2011). This technique incorporates two main goals: to smooth the data from location to location (i.e., across the x-axis in Figure 7.1(b)) and to difference the data. Smoothing attempts to reduce measurement noise without diminishing the data patterns that could predict the outcome. Differencing can remove the correlation between locations and might also mitigate other issues, such as systematic background noise.\nFor smoothing, the SG procedure uses a moving window across a sample’s location and, within each window, uses a polynomial basis expansion to produce a smoother representation of the data. The window width and the polynomial degree are both tuning parameters.\nDifferencing has a single tuning parameter: the difference order. A zero-order difference leaves the data as-is (after smoothing). A first-order difference is the smoothed value minus the smoothed value for the next location (and so on). Savitzky and Golay’s method uses the polynomial estimates to estimate the differences/derivatives, instead of manually subtracting columns after smoothing.\nFigure 12.9 shows an example of a small set of locations from a single spectra in Figure 7.1. The top row shows the effect of smoothing when differencing is not used. For a linear smoother (i.e., a polynomial degree of one), the process can over-smooth the data such that the nonlinear portions of the raw data lose significant amounts of information. This issue goes away as the polynomial order is increased. However, the higher degree polynomial nearly interpolates between data points and may not be smoothing the data in any meaningful way.\nThe bottom panel shows the data when a first-order difference is used. The pattern is completely different since the data now represent the rate of change in the outcome rather than the absolute measurement. Again, larger window sizes oversmooth the data, and the highest degree of polynomial provides minimal smoothing.\n\n\n\n\n\n\n\n\nFigure 12.9: Demonstrations of the Savitzky-Golay preprocessing method for a small region for one specific sample previously shown in Figure 7.1.\n\n\n\n\n\nWe’ll want to optimize these preprocessing parameters simultaneously with the parameters for the supervised model. With this example, the supervised model of choice is a neural network with a single layer of hidden units. We’ll tune over the following parameters, described at length in Section 17.4:\n\nThe number of hidden units in the layer (2 to 100).\nThe type of nonlinear function that connects the input layer and the hidden layer, called the “activation function”. Possible functions are “tanh”, exponential linear unit (“ELU”) functions, rectified linear unit (“ReLU”) functions, and log-sigmoidal functions.\nThe number of epochs of degraded performance before early stopping is enacted (3 to 20).\nThe total amount of penalization for the model (-10 to 0, in log-10 units).\nThe proportion of \\(L_1\\) penalty (a.k.a. Lasso penalization) (0 to 1).\nThe initial learning rate used during gradient descent (-2 to -0.5, in log-10 units).\nThe “scheduling” function to adjust the learning rate over iterations: “constant”, “cyclic”, or “exponential decay”.\n\nIn all, there are ten tuning parameters, two of which are qualitative.\nA space-filling design with 12 candidate points were initially processed to use as the starting point for Bayesian optimization and simulated annealing. When constructing the grid, there are two constraints on the preprocessing parameters:\n\nThe smoothing polynomial degree must be greater or equal to the differentiation order.\nThe window size must be greater than the polynomial degree.\n\nAn initial grid was created without constraints, and candidates who violated these rules were eliminated. The two constraints will also be applied when simulated annealing and Bayesian optimization create new candidates.\nTable 12.2 shows the settings and their corresponding validation set RMSE values, which ranged between 4.3% and 14.4%. At least two of the settings show good performance results, especially compared to the RMSE values previously achieved when embeddings and/or SVMs were used.\n\n\n\n\n\n\n\n\n\nRMSE (%)\nWindow\nDiff. Order\nDegree\nUnits\nPenalty (log-10)\nActivation\nLearn Rate (log-10)\nRate Schedule\nStop Iter.\nL1 Mixture\n\n\n\n\n4.32\n19\n2\n10\n27\n-7.89\nelu\n-1.84\nnone\n12\n42.11\n\n\n4.57\n19\n3\n7\n84\n-9.47\ntanh\n-0.82\ndecay\n8\n31.58\n\n\n4.72\n15\n2\n3\n2\n-2.63\ntanh\n-1.45\nnone\n16\n5.26\n\n\n4.87\n15\n1\n1\n69\n-5.79\nelu\n-1.29\ndecay\n4\n0.00\n\n\n4.91\n9\n2\n8\n100\n-7.37\nelu\n-1.05\ncyclic\n5\n94.74\n\n\n5.88\n17\n0\n3\n79\n-6.84\nrelu\n-1.92\ncyclic\n19\n73.68\n\n\n6.16\n11\n4\n6\n94\n-3.68\nelu\n-1.21\ndecay\n20\n10.53\n\n\n6.47\n13\n0\n9\n38\n-5.26\nlog sigmoid\n-0.74\ncyclic\n14\n15.79\n\n\n7.01\n13\n0\n5\n7\n-6.32\ntanh\n-1.37\ndecay\n3\n89.47\n\n\n7.72\n21\n1\n5\n53\n-1.58\nlog sigmoid\n-0.50\nnone\n15\n78.95\n\n\n10.57\n5\n1\n4\n17\n-0.53\nelu\n-1.68\ndecay\n13\n68.42\n\n\n14.38\n21\n3\n7\n58\n0.00\nrelu\n-1.53\ncyclic\n4\n52.63\n\n\n\n\n\n\n\n\nTable 12.2: The initial set of candidates used for iterative search methods.\n\n\n\nCan we improve on these results using iterative optimization? To begin, a simulated annealing search was initialized using the best results in Table 12.2 and ran for 50 iterations with no early stopping. If a new globally best result was not found within eight iterations, the search restarted from the previously best candidate. The search was restarted 3 times at iterations 15, 31, and 47 and improved candidates were found at iterations 2, 7, 23, 39. The overall best candidate had an RMSE value of 3.65%, with a 90% confidence interval of (3.41%, 3.9%). The improvement appears to be a legitimate improvement over the initial best configuration. The top panel of Figure 12.10 shows the RMSE values over iterations and Table 12.3 lists the best candidate value.\n\n\n\n\n\n\n\n\nFigure 12.10: Validation set RMSE results for the optimization. The validation set RMSE is shown on the y-axis with 90% confidence intervals. Orange points indicate iterations where a new optimal value was found and vertical lines indicate where the algorithm restarted from the previous best candidate.\n\n\n\n\n\nA Bayesian optimization was also used with a squared exponential kernel for the covariance function and used expected improvement to select candidates at each of the 50 iterations. This search found new best results at 3 iterations: 1, 10, and 47. Like the SA search, the best RMSE was substantially different than the one found in the initial grid: 3.58% with 90% confidence interval, (3.3%, 3.87%). The iterations and final values also be seen in Figure 12.10 and Table 12.3, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInitial Grid\nSimulated Annealing\nBayesian Optimzation\n\n\n\n\nDifferentiation Order\n2\n2\n1\n\n\nPolynomial Degree\n10\n4\n8\n\n\nWindow Size\n19\n19\n21\n\n\nHidden Units\n27\n27\n53\n\n\nActivation\nelu\nrelu\ntanh\n\n\nPenalty (log10)\n-7.89\n-9.00\n-8.22\n\n\nL1 Proportion\n0.421\n0.212\n0.112\n\n\nLearning Rate (log10)\n-1.84\n-1.39\n-1.97\n\n\nRate Schedule\nnone\ncyclic\ncyclic\n\n\nStopping Rule\n12\n14\n14\n\n\nRMSE\n4.32%\n3.65%\n3.58%\n\n\nConf. Int.\n(4.03%, 4.6%)\n(3.41%, 3.9%)\n(3.3%, 3.87%)\n\n\n\n\n\n\n\n\nTable 12.3: The best results from the initial space-filling design and the two iterative searches for a neural network using Savitzky-Golay preprocessing.\n\n\n\n\n\n\n\nMany of the parameter values for the final candidates from each search are very similar. In particular, the learning rates, regularization penalties, and SG window sizes where nearly the same.\nIn terms of computing time, simulated annealing took 1m 55.9s on average for each candidate and compared to 3m 22.3s required for BO (a 1.7-fold difference).",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-iterative-summary",
    "href": "chapters/iterative-search.html#sec-iterative-summary",
    "title": "12  Iterative Search",
    "section": "12.8 Summary",
    "text": "12.8 Summary\nThere are several different ways to tune models sequentially. For tabular data, these approaches are rarely the only efficient method for model optimization, but they may be preferable for certain models and large data sets.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#chapter-references",
    "href": "chapters/iterative-search.html#chapter-references",
    "title": "12  Iterative Search",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmeli, S, and S Shadden. 2022. “Noise Estimation in Gaussian Process Regression.” arXiv.\n\n\nAzimi, J, A Fern, and X Fern. 2010. “Batch Bayesian Optimization via Simulation Matching.” Advances in Neural Information Processing Systems 23.\n\n\nBinois, M, and N Wycoff. 2022. “A Survey on High-Dimensional Gaussian Process Modeling with Application to Bayesian Optimization.” ACM Transactions on Evolutionary Learning and Optimization 2 (2): 1–26.\n\n\nBland, J, and D Altman. 1998. “Bayesians and Frequentists.” Bmj 317 (7166): 1151–60.\n\n\nBreiman, L. 1997. “No Bayesians in Foxholes.” IEEE Expert 12 (6): 21–24.\n\n\nChitty, R. 2005. “Why Clinicians Are Natural Bayesians: Is There a Bayesian Doctor in the House?” BMJ: British Medical Journal 330 (7504): 1390.\n\n\nCox, D, and S John. 1992. “A Statistical Method for Global Optimization.” In [Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics, 1241–46. IEEE.\n\n\nEiben, A, and J Smith. 2015. Introduction to Evolutionary Computing. Springer.\n\n\nFornacon-Wood, I, H Mistry, C Johnson-Hart, C Faivre-Finn, J O’Connor, and G Price. 2022. “Understanding the Differences Between Bayesian and Frequentist Statistics.” International Journal of Radiation Oncology, Biology, Physics 112 (5): 1076–82.\n\n\nGarnett, R. 2023. Bayesian Optimization. Cambridge University Press.\n\n\nGenton, M. 2001. “Classes of Kernels for Machine Learning: A Statistics Perspective.” Journal of Machine Learning Research 2 (Dec): 299–312.\n\n\nGill, C, L Sabin, and C Schmid. 2005. “Why Clinicians Are Natural Bayesians.” Bmj 330 (7499): 1080–83.\n\n\nGramacy, R. 2020. Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.\n\n\nHofmann, T, B Schölkopf, and A Smola. 2008. “Kernel Methods in Machine Learning.” The Annals of Statistics 36 (3): 1171–1220.\n\n\nHutchon, D. 2005. “Why Clinicians Are Natural Bayesians: Bayesian Confusion.” BMJ: British Medical Journal 330 (7504): 1390.\n\n\nJones, D. 2001. “A Taxonomy of Global Optimization Methods Based on Response Surfaces.” Journal of Global Optimization 21: 345–83.\n\n\nJones, D, M Schonlau, and W Welch. 1998. “Efficient Global Optimization of Expensive Black-Box Functions.” Journal of Global Optimization 13: 455–92.\n\n\nJoseph, R, and J Delaney. 2007. “Functionally Induced Priors for the Analysis of Experiments.” Technometrics 49 (1): 1–11.\n\n\nKirkpatrick, S, D Gelatt, and M Vecchi. 1983. “Optimization by Simulated Annealing.” Science 220 (4598): 671–80.\n\n\nLu, J. 2022. Gradient Descent, Stochastic Optimization, and Other Tales. Eliva Press.\n\n\nMcCrossin, R. 2005. “Why Clinicians Are Natural Bayesians: Clinicians Have to Be Bayesians.” BMJ: British Medical Journal 330 (7504): 1390.\n\n\nMitchell, M. 1996. An Introduction to Genetic Algorithms. MIT Press.\n\n\nMočkus, J. 1975. “On Bayesian Methods for Seeking the Extremum.” In Optimization Techniques IFIP Technical Conference, 400–404. Springer.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.\n\n\nQian, P, and CF Jeff Wu. 2008. “Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments.” Technometrics 50 (2): 192–204.\n\n\nRana, S. 1999. “The Distributional Biases of Crossover Operators.” In Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation-Volume 1, 549–56.\n\n\nRinnan, A, F Van Den Berg, and S Engelsen. 2009. “Review of the Most Common Pre-Processing Techniques for Near-Infrared Spectra.” Trends in Analytical Chemistry 28 (10): 1201–22.\n\n\nSaves, P, Y Diouane, N Bartoli, T Lefebvre, and J Morlier. 2023. “A Mixed-Categorical Correlation Kernel for Gaussian Process.” Neurocomputing 550: 126472.\n\n\nSchafer, R. 2011. “What Is a Savitzky-Golay Filter?” IEEE Signal Processing Magazine 28 (4): 111–17.\n\n\nSchölkopf, B, and A Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. The MIT Press.\n\n\nShawe-Taylor, J., and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.\n\n\nSoule, T. 2009. “Crossover and Sampling Biases on Nearly Uniform Landscapes.” In Genetic Programming Theory and Practice VI, 1–15. Springer US.\n\n\nSpall, J. 2005. Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control. John Wiley & Sons.\n\n\nWagenmakers, E, M Lee, T Lodewyckx, and G Iverson. 2008. “Bayesian Evaluation of Informative Hypotheses.” In, edited by H Hoijtink, I Klugkist, and P Boelen, 181–207. New York, NY: Springer New York.\n\n\nWilliams, C, and C Rasmussen. 2006. Gaussian Processes for Machine Learning. MIT press Cambridge, MA.\n\n\nZhang, J. 2019. “Gradient Descent Based Optimization Algorithms for Deep Learning Models Training.” arXiv.\n\n\nZhou, Q, P Qian, and S Zhou. 2011. “A Simple Approach to Emulation for Computer Models with Qualitative and Quantitative Factors.” Technometrics 53 (3): 266–73.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#footnotes",
    "href": "chapters/iterative-search.html#footnotes",
    "title": "12  Iterative Search",
    "section": "",
    "text": "Sections 17.6 and ?sec-reg-svm↩︎\nSections 17.4 and ?sec-reg-nnet↩︎\nWhy do these two parameters use different bases? It’s mostly a convention. Both parameters have valid values that can range across many orders of magnitude. The change in cost tends to occur more slowly than the scaling factor, so a smaller base of 2 is often used.↩︎\nThese are not simulated data, so this surface is an approximation of the true RMSE via the validation set using a very large regular grid. The RMSE results are estimates and would change if we used different random numbers for data splitting.↩︎\nWe’ve seen \\(\\alpha\\) before when it was called the learning rate (and will revisit it later in this chapter).↩︎\nHowever, computing the Hessian matrix increases the computational cost by about 3-fold.↩︎\nThese values will be also used as the initial substrate for Bayesian optimization.↩︎\nThink of the candidates within a generation as a population of people.↩︎\nAnalogous to chromosomes.↩︎\nIf you have access to high-performance computing infrastructure, the parallel processing can run the individuals in the population in parallel and, within these, any resampling iterations can also be parallelized.↩︎\nWe’ll see how the prior can affect some calculations later in Section 17.1.1. ↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html",
    "href": "chapters/feature-selection.html",
    "title": "13  Feature Selection",
    "section": "",
    "text": "13.1 The Cost of Irrelevant Predictors\nIn Chapters 5 through 8, we described different methods for adding new predictors to the model (e.g., interactions, spline columns, etc.). We can add any predictors (or functions of predictors) based on first principles or our intuition or contextual knowledge.\nWe are also motivated to remove unwanted or irrelevant predictors. This can decrease model complexity, training time, computer memory, the cost of acquiring unimportant predictors, and so on. For some models, the presence of non-informative predictors can decrease performance (as we’ll see in Figure 13.1).\nWe may want to remove predictors solely based on their distributions; unsupervised feature selection focuses on just the predictor data.\nMore often, though, we are interested in supervised feature selection1: the removal of predictors that do not appear to have a relationship to the outcome. This implies that we have a statistic that can compute some measure of each predictor’s utility for predicting the outcome.\nWhile it is easy to add predictors to a model, it can be very difficult (and computationally expensive) to properly determine which predictors to remove. The reason comes back to the misuse (or over-use) of data. We’ve described the problem of using the same data to fit and evaluate a model. By naively re-predicting the training set, we can accidentally get overly optimistic performance statistics. To compensate for this, resampling repeatedly allocates some data for model fitting and other data for model evaluation.\nFor feature selection, we have the same problem when the same data are used to measure importance, filter the predictors, and then train the model on the smaller feature set. It is similar to repeatedly taking the same test and then measuring our knowledge using the grade for the last test repetition. It doesn’t accurately measure the quantity that is important to us. This is the reason that this chapter is situated here instead of in Part 2; understanding overfitting is crucial to effectively remove predictors.\nThis chapter will summarize the most important aspects of feature selection. This is a broad topic and a more extensive discussion can be found in Kuhn and Johnson (2019). First, we’ll use an example to quantify the effect of irrelevant predictors on different models. Next, the four main approaches for feature selection are described and illustrated.\nDoes it matter if the model is given extra predictors that are unrelated to the outcome? It depends on the model. To quantify this, data were simulated using the equation described in Hooker (2004) and used in Sorokina et al. (2008):\n\\[\ny_i = \\pi^{x_{i1} x_{i2}}  \\sqrt{ 2 x_{i3} } - asin(x_{i4}) + \\log(x_{i3}  + x_{i5}) - (x_{i9} / x_{i10}) \\sqrt{x_{i7} / x_{i8}} - x_{i2} x_{i7} + \\epsilon_i\n\\tag{13.1}\\]\nwhere predictors 1, 2, 3, 6, 7, and 9 are standard uniform while the others are uniform on [0.6, 1.0]. Note that \\(x_6\\) is not used in the equation. The errors \\(\\epsilon_i\\) are Gaussian with mean zero and a standard deviation2 of 0.25. Training and testing data sizes where simulated to be 1,000 and 100,000, respectively.\nTo assess the effect of useless predictors, between 10 and 100 extra columns of standard normal data were generated (unrelated to the outcome). Models were fit, tuned, and the RMSE of the test set was computed for each model. Twenty simulations were run for each combination of models and extra features3.\nThe percent difference from baseline RMSE was computed (baseline being the model with no extra columns). Figure 13.1 shows the results for the following models:\nThese models are described in detail in subsequent chapters. The colors of the points/lines signifies whether the model automatically removes unused predictors from the model equation (see Section 13.6 below).\nFigure 13.1: For different regression models, the percent increase in RMSE is shown as a function of the number of additional non-informative predictors.\nThe results show that a few models, K-nearest neighbors, neural networks, and support vector machines, have severely degraded performance as the number of predictors increases. This is most likely due to the use of cross- and/or dot-products of the predictor columns in their calculations. The extra predictors add significant noise to these calculations, and that noise propagates in a way that inhibits the models from accurately determining the underlying relationship with the outcome.\nHowever, several other models use these same calculations without being drastically affected. This is due to the models automatically performing feature selection during model training. Let’s briefly describe two sets of models that were largely unaffected by the extra columns.\nThe other unaffected models are tree-based. As seen in Figure 2.5, these types of models make different rules by selecting predictors to split the data. If a predictor was not used in any split, it is functionally independent of the outcome in the prediction equation. This provides some insensitivity to the presence of non-informative predictors, although there is slight degradation in RMSE as the number of columns increases.\nIn summary, irrelevant columns minimally affect ML models that automatically select features. Models that do not have this attribute can be crippled under the same circumstances.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-irrelevant-predictors",
    "href": "chapters/feature-selection.html#sec-irrelevant-predictors",
    "title": "13  Feature Selection",
    "section": "",
    "text": "Bagged regression trees\nBayesian additive regression trees (BART)\nBoosted trees (via lightGBM)\nCubist\nGeneralized additive models (GAMs)\nK-nearest neighbors (KNN)\nMultivariate adaptive regression splines (MARS)\nPenalized linear regression\nRandom forest\nRuleFit\nSingle-layer neural networks\nSupport vector machines (radial basis function kernel)\n\n\n\n\n\n\nThe first set includes penalized linear regression, generalized additive models, and RuleFit. These add a penalty to their calculations, restricting the model parameters from becoming abnormally large unless the underlying data warrant a large coefficient. This effectively reduces the impact of extra predictors by keeping their corresponding model coefficient at or near zero.\nThe other set o model models include MARS and Cubist. They selectively include predictors in their regression equations, only including those specifically selected as having value.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-feature-levels",
    "href": "chapters/feature-selection.html#sec-feature-levels",
    "title": "13  Feature Selection",
    "section": "13.2 Different Levels of Features",
    "text": "13.2 Different Levels of Features\nWhen we talk about selecting features, we should be clear about what level of computation we mean. There are often two levels:\n\nOriginal predictors are the unmodified version of the data.\nDerived predictors refer to the set of columns that are present after feature engineering.\n\nFor example, if our model requires all numeric inputs, a categorical predictor is often converted to binary indicators. A date-time column is the original predictor, while the binary indicators for individual days of the week are the derived predictor. Similarly, the 550 measurement predictors in the barley data are the original predictors, and embedded values, such as the PCA components, are the derived predictors.\nDepending on the context, our interest in feature selection could be at either level. If the original predictors are expensive to estimate, we would be more interested in removing them at the original level; if any of their derived predictors are important, we need the original.\nThis idea will arise again in ?sec-importance when we discuss variable importance scores for explaining models.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-selection-overfitting",
    "href": "chapters/feature-selection.html#sec-selection-overfitting",
    "title": "13  Feature Selection",
    "section": "13.3 Overfitting the Feature Set",
    "text": "13.3 Overfitting the Feature Set\nWe’ve previously discussed overfitting, where a model finds patterns in the training data that are not reproducible. It is also possible to overfit the predictor set by finding the training set predictors that appear to be connected to the outcome but do not show the same relationship in other data.\nThis is most likely to occur when the number of predictors (\\(p\\)) is much larger than the training set size (\\(n_{tr}\\)) or when no external validation is used to verify performance. Ambroise and McLachlan (2002) shows reanalyses of high-dimensional biology experiments using a backward selection algorithm for removing predictors. In some cases, they could find a model and predictor subset with perfect accuracy even when the training set outcome data were randomly shuffled.\nRecall back in Section 1.5 an argument was made that preprocessing methods and the supervised model encompassed the entire model pipeline. Technically, feature selection (supervised or unsupervised) falls into the definition of preprocessing. It is important to use data that is different from the data used to train the pipeline (i.e., select the predictors) is different from the data used to evaluate how well it worked.\nFor example, if we use a resampling scheme, we have to repeat the feature selection process within each resample. If we used 10-fold cross-validation, each of the 10 analysis sets (90% of the training set) would have its own set of selected predictors.\nAmbroise and McLachlan (2002) demonstrated that when feature selection was enacted once, and then the model with this subset was resampled, they could produce models with excessively optimistic performance metrics. However, when the performance statistics were computed with multiple realizations of feature selection, there was very little false optimism.\nAnother improper example of data usage that can be found in the literature, as well as in real-life analyses, occurs when the entire data set is used to select predictors then the initial split is used to create the training and test sets. This “bakes in” the false optimism from the start, the epitome of information leakage4.\nHowever, performing feature selection in each resample can multiplicatively increase the computational cost, especially if the model has many tuning parameters. For this reason, we tend to focus on methods that automatically select features during the normal process of training the model (as discussed below).\nOne scheme to understand if we are overfitting to the predictors is to create one or more artificial “sentinel predictors” that are random noise. Once the analysis that ranks/selects predictors is finished, we can see how high the algorithm ranks these deliberately irrelevant predictors. It is also possible to create a rule where predictors that rank higher than the sentinels are selected to be included in the model.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-unsupervised-selection",
    "href": "chapters/feature-selection.html#sec-unsupervised-selection",
    "title": "13  Feature Selection",
    "section": "13.4 Unsupervised Selection",
    "text": "13.4 Unsupervised Selection\nThere are occasions where the values of the predictors are unfavorable or possibly detrimental to the model. The simplest example is a zero-variance column, where all of the values of a predictor in the training set have the same value. There is no information in these predictors and, although the model might be able to tolerate such a predictor, taking the time to identify and remove such columns is advisable.\nBut what about situations where only one or two rows have non-zero values? It is difficult to filter on variance values since the predictors could have different units or ranges. The underlying problem occurs when very few unique values exist (i.e., “coarse data”), and one or two predictor values capture most of the rows. Perhaps we can identify this situation.\nConsider a hypothetical training set of 500 data points with a count-based predictor that has mostly zero counts. However, there are three data points with a count of 1 and one instance with a count of 2. The variance of these data is low, but it would be difficult to determine a cutpoint for removal. Kuhn (2008) developed an algorithm to find near-zero variance predictors, as defined by two criteria:\n\nThe freqeuncy ratio is the frequency of the most prevalent value over the second most frequent value. For our example, the ratio was 496/3 = 165.3.\nThe proportion of unique values, which is 3 / 500 = 0.006.\n\nWe can determine thresholds for both of these criteria to remove the predictor. The default behavior is to remove the column of the frequency ratio is greater than 19 and the proportion of unique values is less than 0.1. For the example, the predictor would be eliminated.\nAnother example of an unsupervised filter reduces between-predictor correlations. A high degree of correlation can reduce the effectiveness of some models. We can filter out predictors by choosing a threshold for the (absolute) pairwise correlations and finding the smallest subset of predictors to meet this criterion.\nA good example is Figure 13.2, which visualizes the correlation matrix for the fifteen predictors in the forestry data set (from Section 3.9). Predictors for the January minimum temperature and the annual mean temperature are highly correlated (corr = 0.93), and only one might be sufficient to produce a better model. Similar issues occur with other predictor pairs. If we apply a threshold of 0.50, then six predictors (dew temperature, annual precipitation, annual maximum temperature, annual mean temperature, annual minimum temperature, and maximum vapor) would be removed before model training.\n\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n#&gt; ℹ The deprecated feature was likely used in the dendextend package.\n#&gt;   Please report the issue at &lt;https://github.com/talgalili/dendextend/issues&gt;.\n\n\n\n\n\n\n\nFigure 13.2: A heatmap of a correlation matrix for fifteen predictors spatial, ordered using a hierarchical clustering algorithm.\n\n\n\n\nSimilar redundancy can occur in categorical predictors. In this case, measures of similarity can be used to filter the predictors in the same way.\nUnsupervised filters are only needed in specific circumstances. Some models (such as trees) are resistant to the distributions of the predictor’s data5, while others are excessively sensitive.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-selection-methods",
    "href": "chapters/feature-selection.html#sec-selection-methods",
    "title": "13  Feature Selection",
    "section": "13.5 Classes of Supervised Methods",
    "text": "13.5 Classes of Supervised Methods\nThere are three main strategies for supervised feature selection. The first and most effective was discussed: automatic selection occurs when a subset of predictors is determined during model training. We’ll discuss this more in the next section.\nThe second supervised strategy is called wrappers. In this case, a sequential algorithm proposes feature subsets, fits the model with these subsets, and then determines a better subset from the results. This sounds similar to iterative optimization because it is. Many of the same tools can be used. Wrappers are the most thorough approach to searching the space of feature subsets, but this can come with an enormous computational cost.\nA third strategy is to use a filter to screen predictors before adding them to the model. For example, the analysis shown in Figure 2.3 computed how each item in a food order impacted the delivery time. We could have applied a filter where we only used food item predictors whose lower confidence interval for the increase in time was greater than one. The idea behind the filter is that it is applied once prior to the model fit.\nThe next few sections will describe the mechanics of these techniques. However, how we utilize these algorithms is tricky. We definitely want to avoid overfitting the predictor set to the training data.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-automatic-selection",
    "href": "chapters/feature-selection.html#sec-automatic-selection",
    "title": "13  Feature Selection",
    "section": "13.6 Automatic Selection",
    "text": "13.6 Automatic Selection\nTo more closely demonstrate how automatic feature selection works, let’s return to the Cubist model fit to the delivery data in Section 2.4. That model created a series of regression trees, and converted them to rules (i.e., a path through the tree), then fit regression models for every rule. Recall that there was a sizable number of rules (2,007) each with its own linear regression. Despite the size of the rule set, only 13 of the original 30 predictors in the data set were used in rules or model fits; 17 were judged to be irrelevant for predicting delivery times.\n\nThis shows that we can often reduce the number of features for free.\n\nDifferent models view the training data differently, and the list of relevant predictors will likely change from model to model. When we fit a MARS model (?sec-reg-mars) to the data, only 9 original predictors were used. If we use automatic selection methods to understand what is important, we should fit a variety of different models and create a consensus estimate of which predictors are “active” for the data set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-wrappers",
    "href": "chapters/feature-selection.html#sec-wrappers",
    "title": "13  Feature Selection",
    "section": "13.7 Wrapper Methods",
    "text": "13.7 Wrapper Methods\nWrapper methods (Kohavi and John 1998) use an overarching search method to optimize binary indicators, one for each potential predictor, that control which predictors are given to the model.\nThe most popular approach is recursive feature elimination (RFE), which starts with the complete set of predictors and uses a mechanism to rank each in terms of their utility for predicting the outcome (Guyon et al. 2002). Using this ordering, predictors are successively removed while performance is monitored. The performance profile determines the optimal subset size, and the most important predictors are included in the model. RFE is greedy, though; it predetermines the order that the predictors are eliminated and will never consider a subset that is inconsistent with this order.\nTwo global search methods described in Chapter 12, genetic algorithms (GA) and simulated annealing, can also be used. In this instance, the search space is the predictor space represented as binary indicators. The same process applies, though. For example, a GA would contain a population of different subsets, each producing a fitted model. The performance values enable the creation of the next generation via selection, reproduction, and mutation. In the case of simulated annealing, an initial subset is created and evaluated, and perturbations are created by randomly altering a small set of indicators (to create a “nearby” subset). Suboptimal subsets can be accepted similarly to our application for tuning parameters.\nHowever, for each of these cases, the details matter. First, we should probably use separate data to\n\nestimate performance for each candidate subset and\ndetermining how far the search should proceed.\n\nIf we have large data sets, we can partition them into separate validation sets for each purpose. Otherwise, resampling is probably the answer to solve one or both of these problems.\nThe need to optimize tuning parameters can greatly complicate this entire process. A set of optimal parameter values for large subset sizes may perform very poorly for small subsets. In some cases, such as \\(m_{try}\\) and the number of embedding dimensions, the tuning parameter depends on the subset size.\nIn this case, the traditional approach uses a nested resampling procedure similar to the one shown in Section 11.4. An inner resampling scheme tunes the model, and the optimized model is used to predict the assessment set from the outer resample. Even with parallel processing, this can become an onerous computational task.\nHowever, let’s illustrate a wrapper method using an artificial data set from the simulations described in Section 13.1. We’ll use Equation 13.1 and supplement the eight truly important predictors with twenty one noise columns. For simplicity, we’ll use a neural network to predict the outcome with a static set of tuning parameters. The training set consisted of 1,000 data points.\nSince feature selection is part of the broader model pipeline, 10-fold cross-validation was used to measure the effectiveness of 150 iterations of simulated annealing (Kuhn and Johnson 2019 Sect 12.2). A separate SA run is created for each resample; we use the analysis set to train the model on the current subset, and the assessment set is used to measure the RMSE.\nFigure 13.3 shows the results, where each point is the average of the 10 RMSE values for the SA at each iteration. The results clearly improve as the selection process mostly begins to include important predictors and discards a few of the noise columns. After approximately 120 iterations, the solution stabilizes. This appears to be a very straightforward search process that clearly indicates when to stop the SA search.\n\n\n\n\n\n\n\n\nFigure 13.3: The progress of using simulated annealing to select features for.a neural network. The dashed green line indicators the best RMSE value achievable for these simulated data.\n\n\n\n\n\nHowever, looks can be deceiving. The feature selection process is incredibly variable, and adding or removing an important predictor can cause severe changes in performance. Figure 13.3 is showing the average change in performance, and the relative smoothness of this curve is somewhat deceiving.\nFigure 13.4 shows what could have happened by visualizing each of the 10 searches. The results are much more dynamic and significantly vary from resample to resample. Most (but not all) searches contain one or two precipitous drops in the RMSE with many spikes of high errors (caused by a bad permutation of the solution). These cliffs and spikes do not reproducibly occur at the same iteration or in the same way.\n\n\n\n\n\n\n\n\nFigure 13.4: The individual SA runs that make up the data in Figure 13.3.\n\n\n\n\n\nWe should keep in mind that Figure 13.3 lets us know where, on average, the search should stop, but our actual search result (that uses the entire training set) is more likely to look like what we see in Figure 13.4.\nFor this particular search, the numerically best result occurred at iteration 113 and the final model contained 17 predictors where eight of the twenty noise columns are still contained in the model. Our resampled estimate of the RMSE, at this iteration, was 0.302 (a perfect model would have a value of 0.25). However, the plots of the SA performance profiles indicate a range, probably above 125 iterations, where the results are equivocal.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-filters",
    "href": "chapters/feature-selection.html#sec-filters",
    "title": "13  Feature Selection",
    "section": "13.8 Filter Methods",
    "text": "13.8 Filter Methods\nFilters can use any scoring function that quantifies how effectively the predictor predicts the outcome (Kohavi and John 1998; Duch 2006). One method is to use a univariate statistic to evaluate each predictor independently. For example, the area under the ROC curve (Section 15.7.3) can be computed for a binary classification outcome and a set of numeric predictors. The filter could be applied using these values by pre-defining a threshold for the statistic that defines “important enough.” Alternatively, the top \\(p^*\\) predictors can be retained and used in the supervised model. This works well unless predictors have ties in their statistics.\nThe example of using ROC curves raises an interesting question. Should we rank the predictors by their area under the curve or should we convert that analysis to a p-value to see if the area under the curve is greater than 0.5 (Hanley and McNeil 1983)? Our advice is to do the former. Hypothesis tests and p-values have their uses but the AUC estimates themselves are probably more numerically stable. To factor in uncertainty, we could filter on the lower confidence bound of the AUC. This is generally true for almost any ranking statistic that has a corresponding p-value.\nMultivariate methods can also be used. These simultaneously compute measures of importance for all predictors. This is usually preferred since univariate statistics can be compromised when the predictors are highly correlated and/or have important interaction effects.\nA variety of models have built-in methods for measuring importance. For example:\n\nMARS (Friedman 1991) and decision trees can compute the importance of each predictor by aggregating the improvement in the objective function when each predictor is used in a split or an artificial feature.\nSome neural networks can use the values of their coefficients in each layer to compute an aggregate measure of effectiveness for each feature (Garson 1991; Gevrey, Dimopoulos, and Lek 2003; Olden, Joy, and Death 2004).\nSimilarly, partial least squares models can compute importance scores from their loading estimates (Wold, Johansson, and Cocchi 1993; Farrés et al. 2015).\nAs mentioned in Section 8.1.2, BART (and H-statistics) can measure the importance of individual predictors using the same techniques to discover interaction effects.\n\nThere are also model-free tools for computing importance scores from an existing model based on permuting each predictor and measuring how performance metrics change. This approach was initially popularized by the random forest model (Section 19.2) but can be easily extended to any model. See Breiman (2001), Strobl et al. (2007), and ?sec-importance for more details. There are a few other methods worth mentioning:\n\nThe Relief allgorithm (Kira and Rendell 1992; Kononenko 1994) and its descendant methods randomly select training set points and use the outcome values of their nearest neighbors to aggregate statistics to measure importance.\nMinimum redundancy, maximum relevance analysis (MRMR) combines two statistics, one for importance and another for between-predictor correlation, to find a subset that has the best of both quantities (Peng, Long, and Ding 2005).\n\nThere are also tools to help combine multiple filters into a compound score (Karl et al. 2023). For example, we may want to blend the number of active predictors and multiple performance characteristics into a single score used to rank different tuning parameter candidates. One way to do this is to use desirability functions. These will be described in ?sec-multi-objectives, Also, Section 12.3.2 of Kuhn and Johnson (2019) shows an example of how desirability functions can be incorporated into feature selection.\nOnce an appropriate scoring function is determined, a tuning parameter can be added for how many predictors are to be retained. This is like RFE, but we optimize all the tuning parameters simultaneously and evaluate subsets in a potentially less greedy way.\nThere are two potential downsides, though. First, there is still the potential for enormous computational costs, especially if the ranking process is expensive. For example, if we use a grid search, there is significant potential for the same subset size to occur more than once in the grid6. In this case, we are repeating the same importance calculation multiple times.\nThe second potential problem is that we will use the same data set to determine optimal parameters and how many features to retain. This could very easily lead to overfitting and/or optimization bias, especially if our training set is not large.\nWe measured the predictors using the standard random forest importance score using the same data as the previous simulated annealing search. The number of features to retain was treated as an additional tuning parameter. Cross-validation was used to select the best candidate from the space-filling design with 50 candidate points. Figure 13.5(a) shows the tuning results. The number of selected predictors appears to have the largest effect on the results; performance worsens as we remove too many features (which probably includes informative ones). It also appears that too much penalization has a negative effect on reducing the RMSE.\nThe model with the smallest RMSE selected the top 41.8% most important predictors, 8 hidden units, a penalty value of 10-1.63, a learning rate of 10-0.602, and tanh activation. The resampled RMSE associated with the numerically best results was 0.291. When that model was fit on the entire training set, the holdout RMSE estimate was 0.288. One reason that these two estimates of performance are so close is the relatively high \\(n_{tr}/p\\) ratio (1,000 / 29 \\(\\approx\\) 34.5).\n\n\n\n\n\n\n\n\nFigure 13.5: Panel (a): The results of tuning a variance importance filter and the parameters from a single-layer neural network for predicting the simulated data from Equation 13.1. Panel (b): The frequency distribution of predictor selection in the 10 folds for the model with the smallest RMSE.\n\n\n\n\n\nRecall that the importance scores are computed on different analysis sets. For the best model, Figure 13.5(b) contains the frequency at which each predictor was selected. For the most part, the importance scores are pretty accurate; almost all informative features are consistently selected across folds. When the final model was fit on the entire training set, 4 noise predictors were included in the model (by random chance) along with true predictors \\(x_{1}\\), \\(x_{2}\\), \\(x_{3}\\), \\(x_{4}\\), \\(x_{5}\\), \\(x_{7}\\), \\(x_{9}\\), and \\(x_{10}\\). These results are unusually clean, mostly due to the large sample size and relatively small predictor set.\nSimilar to the discussion in Section 10.10, the different predictor sets discovered in the resamples can give us a sense of our results’ consistency. They are “realizations” of what could have occurred. Once we fit the final model, we get the actual predictor set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#chapter-references",
    "href": "chapters/feature-selection.html#chapter-references",
    "title": "13  Feature Selection",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nDuch, W. 2006. “Filter Methods.” In Feature Extraction: Foundations and Applications, 89–117. Springer.\n\n\nFarrés, M, S Platikanov, S Tsakovski, and R Tauler. 2015. “Comparison of the Variable Importance in Projection (VIP) and of the Selectivity Ratio (SR) Methods for Variable Selection and Interpretation.” Journal of Chemometrics 29 (10): 528–36.\n\n\nFriedman, J. 1991. “Multivariate Adaptive Regression Splines.” The Annals of Statistics 19 (1): 1–67.\n\n\nGarson, D. 1991. “Interpreting Neural Network Connection Weights.” Artificial Intellegence Expert.\n\n\nGevrey, M, I Dimopoulos, and S Lek. 2003. “Review and Comparison of Methods to Study the Contribution of Variables in Artificial Neural Network Models.” Ecological Modelling 160 (3): 249–64.\n\n\nGuyon, I, J Weston, S Barnhill, and V Vapnik. 2002. “Gene Selection for Cancer Classification Using Support Vector Machines.” Machine Learning 46: 389–422.\n\n\nHanley, J, and B McNeil. 1983. “A Method of Comparing the Areas Under Receiver Operating Characteristic Curves Derived from the Same Cases.” Radiology 148 (3): 839–43.\n\n\nHooker, G. 2004. “Discovering Additive Structure in Black Box Functions.” In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 575–80.\n\n\nKarl, F, T Pielok, J Moosbauer, F Pfisterer, S Coors, M Binder, L Schneider, et al. 2023. “Multi-Objective Hyperparameter Optimization in Machine Learning—an Overview.” ACM Transactions on Evolutionary Learning and Optimization 3 (4): 1–50.\n\n\nKira, K, and L Rendell. 1992. “A Practical Approach to Feature Selection.” In Machine Learning Proceedings 1992, 249–56. Elsevier.\n\n\nKohavi, R, and G John. 1998. “The Wrapper Approach.” In Feature Extraction, Construction and Selection: A Data Mining Perspective, 33–50. Springer.\n\n\nKononenko, I. 1994. “Estimating Attributes: Analysis and Extensions of RELIEF.” In European Conference on Machine Learning, 171–82. Springer.\n\n\nKuhn, M. 2008. “Building Predictive Models in R Using the caret Package.” Journal of Statistical Software 28: 1–26.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nOlden, J, M Joy, and R Death. 2004. “An Accurate Comparison of Methods for Quantifying Variable Importance in Artificial Neural Networks Using Simulated Data.” Ecological Modelling 178 (3-4): 389–97.\n\n\nPeng, H, F Long, and C Ding. 2005. “Feature Selection Based on Mutual Information Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy.” IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (8): 1226–38.\n\n\nSorokina, D, R Caruana, M Riedewald, and D Fink. 2008. “Detecting Statistical Interactions with Additive Groves of Trees.” In Proceedings of the 25th International Conference on Machine Learning, 1000–1007.\n\n\nStrobl, C, AL Boulesteix, A Zeileis, and T Hothorn. 2007. “Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution.” BMC Bioinformatics 8: 1–21.\n\n\nWold, S, E Johansson, and M Cocchi. 1993. “PLS: Partial Least Squares Projections to Latent Structures.” In 3D QSAR in Drug Design: Theory, Methods and Applications., 523–50. Kluwer ESCOM Science Publisher.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#footnotes",
    "href": "chapters/feature-selection.html#footnotes",
    "title": "13  Feature Selection",
    "section": "",
    "text": "Recall that we use the terms “predictors” and “features” interchangeably. We favor the former term, but “features” is more commonly used when discussing the filtering of columns.↩︎\nFor this value of the simulated standard deviation, the best possible RMSE value is also 0.25.↩︎\nDetails and code can be found at https://github.com/topepo/noise_features_sim.↩︎\nRecall out mantra that “Always have a separate piece of data that can contradict what you believe.”↩︎\nZero-variance predictors do not harm these models, but determining such predictors computationally will speed up their training.↩︎\nThe same issue can also occur with iterative search↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html",
    "href": "chapters/comparing-models.html",
    "title": "14  Comparing Models",
    "section": "",
    "text": "14.1 Resampled Data Sets\nSo far, we have established several key fundamentals of building predictive models. To create an effective model, we must employ strategies to reduce overfitting, use empirical validation techniques to accurately measure predictive performance, and systematically search a tuning parameter space to identify values that optimize model quality. By following these approaches, we can have confidence that our model generalizes well to new, unseen data and provides a realistic estimate of its efficacy.\nWith this foundation in place, the next step is to select the best model for a given data set. When presented with new data, we typically evaluate multiple machine learning models, each with different sets of tuning parameters. Our goal is to select the model and parameter combination that yields the best results. But how do we determine which is truly the best?\nMaking this decision requires considering several factors. As briefly discussed in Chapter 2, practical characteristics such as model complexity, interpretability, and deployability play an important role. Predictive ability, however, is an objective measure that can be evaluated quantitatively. Comparing metrics across models raises an important question: How can we be sure that one model’s quality is truly superior to another’s? Or, conversely, how can we conclude that multiple models have no significant difference in performance?\nTo address these questions, we will examine both graphical and statistical techniques for comparing the performance statistics of different models. During the model tuning and training process, these comparisons may involve assessing various configurations of tuning parameters within a single model or comparing fundamentally different modeling approaches.\nEmpirical validation (e.g., resampling) serves as the backbone of the tuning and training process. It systematically partitions the data into multiple subsets, enabling repeated estimation of performance metrics. The variability from the different assessment sets provides critical information for selecting optimal tuning parameters or identifying the best-performing model.\nIn the first part of this chapter, we will focus on methods for evaluating models when replicate estimates of performance metrics are available. This includes techniques grounded in Frequentist and Bayesian frameworks. After that, we will explore the case of a single data set. This is relevant when a single holdout set is used during model development (i.e., a validation set) and also when the test set is evaluated.\nAs a refresher, resampling produces multiple variations of the training set. These are used to metric estimates, such as accuracy or R2, that should mimic the results we would see on data sets similar to the training set.\nThe tools in this section are very similar to those described for model racing in Section 11.3.3, and we’ll use the same notation as that section. Similarly, we’ve discussed the two main philosophies of statistical analysis (Frequentist and Bayesian) in Section 6.4.3 and Section 12.5.2. Many of those ideas play out again in this chapter.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-resamples",
    "href": "chapters/comparing-models.html#sec-compare-resamples",
    "title": "14  Comparing Models",
    "section": "",
    "text": "14.1.1 Statistical Foundations\nRegardless of whether we take a Frequentist or Bayesian philosophies to comparing model performance, both methods aim to identify key factors that influence the outcome, such as model-to-model effects. We seek to determine whether the effect of these factors is statistically significant—that is, unlikely to be due to random chance.\nTo illustrate these statistical techniques, we will use the same data that will be used in later chapters on classification: the Washington State forestation data introduced in Chapter 3. Our goal is to predict whether a location is classified as forested or non-forested. The data set includes predictors such as elevation, annual precipitation, longitude, latitude, and so on.\nWithout going into much detail, we’ll focus on comparing three modeling strategies that will be described in Part 4:\n\nA boosted tree ensemble using C5.0 trees as the base learner. This model was tuned over the number of trees in the ensemble and the amount of data required to make additional splits (a.k.a. \\(n_{min}\\)). Grid search found that 60 trees in the ensemble were optimal, in conjunction with \\(n_{min}\\) = 20 training set points.\nA logistic regression model that was trained using penalization (Section 16.2.4). Feature engineering included an effect encoding parameterization of the county associated with locations, a small set of interaction terms (described in Section 16.1), and ten spline terms for several of its numeric predictors. The optimal tuning parameter values were a penalization value of 10-4.67 and 0% Lasso penalization.\nA naive Bayes model (Section 17.1.1) was used where the conditional distributions were modeled using a nonparametric density estimate. No feature preprocessing was used.\n\nOne difference between these analyses and those to come in Part 4 is that we’ve chosen to use classification accuracy as our metric1. Some of these results may differ from upcoming analyses. Previously, the spatially aware resampling scheme that was used generated 10 resamples of the training set. For this reason, there are 10 different accuracy “replicates” for each candidate value that was evaluated.\nSimilar to previous sections, we’ll produce models where the replicated performance statistics (denoted as \\(\\widehat{Q}_{ij}\\)) are the outcomes of the model (for resample \\(i\\) and model \\(j\\)). This is the same general approach shown in Equation 11.1 and Equation 12.5. We’ll assume that there are \\(M\\) models to compare (three in our case).\n\n\n14.1.2 Frequentist Hypothesis Testing Methods\nLet’s begin by building towards an appropriate statistical model that will enable us to formally compare the efficacy of models. To begin this illustration, let’s suppose that each of these models was built separately using different 10-fold partitions of the training data. In this scenario, the metrics on the first holdout fold would be unique, or independent, for each model. If this were the case, then we could represent the desired performance metric with the following linear statistical model:\n\\[\nQ_{ij} = \\beta_0 + \\sum_{j=1}^{M-1}\\beta_jx_{ij} + \\epsilon_{ij}\n\\tag{14.1}\\]\nIn this equation \\(\\beta_0\\) is the overall average performance statistic across all assessment sets and models2. The term \\(\\beta_{j}\\) represents the incremental impact on the outcome due to the \\(j^{th}\\) model (\\(j\\) = 1, 2, …, \\(M-1\\) = total number of models), and \\(x_{ij}\\) is an indicator function for the \\(j^{th}\\) model. In this example, the \\(\\beta_j\\) will be considered as a fixed effect. This indicates that these are the only models that we are considering. If we were to have taken a random sample of models, then the \\(\\beta_j\\) would be treated a random effect which would have a corresponding statistical distribution. The distinction between fixed and random effects will come into play shortly. Finally, \\(\\epsilon_{ij}\\)) represents the part of the performance that is not explainable by the \\(i^{th}\\) assessment set and \\(j^{th}\\) model. We’ll assume these residuals follow a Gaussian distribution with a mean of zero and an unknown variance3. The model, along with the statistical assumption about the distribution of the residuals, provides the statistical foundation for formally comparing models.\nFigure 14.1 illustrates these separate components for the assessment set with the largest accuracy value for the boosting ensemble, where the overall intercept is determined by the logistic regression. The horizontal dashed line represents the overall mean of the accuracy estimates, and the blue bar shows the mean of the boosting estimates alone. It is slightly higher than the average. For that model, the orange line shows the residual (\\(\\epsilon_{ij}\\) for the largest boosted tree accuracy replicate. The totality of such errors is used to measure the overall error in the model.\n\n\n\n\n\n\n\n\nFigure 14.1: An illustration of how variation is partitioned based on the effect of the model (blue) and unexplained causes (orange) when distinct cross-validation partitions are used to train each model.\n\n\n\n\n\nEquation 14.1 enables us to distinctly partition the total variability in the outcome data into two components:\n\nExplained variability: The portion attributable to differences among models, represented by the blue bar. This is the signal in the data set.\nUnexplained variability: The portion due to random or unaccounted-for factors, represented by the orange bar, is commonly called the noise.\n\nOur key question is whether the observed differences in the mean performance for each model reflect genuine differences among models or if they could simply be due to random variation. The Frequentist approach addresses this question using hypothesis testing. In this framework:\n\nThe null hypothesis (\\(H_0\\)) represents the assumption that all models have the same average performance. Formally, this can be written as:\n\\[\nH_0: \\beta_k = \\beta_m,\n\\] for all \\(k = m\\).\nThe alternative hypothesis (\\(H_A\\)) asserts that at least two models have significantly different average performance:\n\\[\nH_A: \\beta_{k} \\neq \\beta_{m},\n\\]\nfor any \\(k \\neq m\\).\n\nTo determine whether we have sufficient evidence to reject the null hypothesis, we compare:\n\nBetween-group variability (signal): Differences in mean performance across models.\n\nWithin-group variability (noise): Residual variation not explained by the models.\n\nThis comparison is conducted using a test whose statistic follows an F distribution (Kutner, Nachtsheim, and Neter 2004), which assesses whether the signal is large enough relative to the noise to conclude that differences among models are statistically significant.\nIn this case, the F-statistic is (3.06, 2, 27)4, with a corresponding p-value of 0.0633. The p-value is small. However, it is not small enough to conclude that there is strong evidence to reject the null hypothesis. Therefore, we would not conclude that there are differences in accuracy among the three models. However, remember that we treated these data as if the resamples were completely different, which was not the case. As you’ll see shortly, this is a seriously flawed analysis of our data.\nIn practice, when tuning and training models, we use the same resampling sets to evaluate their performance. This is a more efficient approach since we only need to set up the resampling scheme once. Moreover, the structure generated by using the same assessment sets will enable us to detect differences in quality between models more effectively.\nIn this setting, each assessment set forms what is known as a block, which is a group of units that are similar to each other. This grouping is an important piece of information that will aid in assessing whether models have statistically different performance. The predictive performance when tuning multiple models using the same cross-validation assessment sets can now be represented as:\n\\[\nQ_{ij} = (\\beta_0 + \\beta_{0i}) + \\sum_{j=1}^{M-1}\\beta_jx_{ij} + \\epsilon_{ij}\n\\tag{14.2}\\]\nNotice that Equation 14.2 is nearly the same as Equation 14.1, with the additional term \\(\\beta_{0i}\\). This term represents the incremental impact on performance due to the \\(i^{th}\\) assessment set (\\(i\\) = 1, 2,…, \\(B\\) resamples). In this model, the impact of the \\(B\\) resamples, represented by \\(\\beta_{0i}\\) are a random effect. The reason the resamples are considered as random effects is because the 10 folds were selected at random from an overall distribution of resamples. The impacts of each model are still considered as fixed effects. When a model contains both fixed and random effects, it is called a mixed effect model.\nFigure 14.2 illustrates these separate components for the assessment set with the largest accuracy value for the boosted tree. In this figure, the points are connected based on the fold. The green line shows that the residual for the best boosting accuracy can be explained by the fact that it comes from this specific fold (i.e., resample). For whatever reason, this specific assessment set was easier to predict since all models had the highest accuracies. The resample that is associated with each accuracy is a systematic and explainable effect in these data. It’s also a “nuisance parameter” because we don’t care about this specific trend.\nThe mixed model can quantify the effect of this nuisance parameter; we can subtract its effect from the overall error, yielding a much smaller value of \\(\\epsilon_{ij}\\) as evidenced by the updated vertical orange bar. Notice that the total amount of variability explained by the model is identical to Figure 14.1. However, the previously unexplainable variability from Figure 14.2 is now mostly explained by the variation due to the fold, thus reducing the unexplainable variability. We have made our analysis much more precise by accounting for the resample effect5.\n\n\n\n\n\n\n\n\nFigure 14.2: An illustration of how variation is partitioned based on the effect of the model (blue), folds (green), and unexplainable causes (orange) when the same cross-validation partitions are used to train each model. The lines connecting the points indicate the results for the same cross-validation fold.\n\n\n\n\n\nTo understand if there are differences between models, we again compare the variability explained by the different models with the unexplainable variability. The signal relative to the noise is now much stronger, enabling us to more easily see differences in model performance. The F-statistic from this model is (20.42, 2, 18) and has a corresponding p-value of &lt; 10-4. By utilizing the same cross-validation scheme across models, there is now strong evidence of a difference in performance among the models.\nThis result does not tell us which models are different. To understand that, we need to perform additional tests to compare each pair of models. These tests are referred to as post hoc tests because they are performed after the initial hypothesis test among all means. To learn more details about post hoc tests and the field of multiple comparisons, see Hsu (1996) and/or Dudoit and Van Der Laan (2008) .\n\n\n14.1.2.1 Post Hoc Pairwise Comparisons and Protecting Against False Positive Findings\nAfter detecting a significant effect in a one-way analysis of variance (ANOVA), the next step is to determine which specific groups differ from each other. This is achieved using post hoc pairwise comparisons, which assess all possible pairwise differences while controlling for false positive findings.\nWhen conducting multiple comparisons, the risk of making at least one Type I error (false positive) increases. If we compare many pairs of groups separately, the probability of incorrectly rejecting at least one null hypothesis accumulates, leading to spurious findings. If this issue is not addressed, then we will eventually conclude that models are different when they actually are not. To mitigate this, statistical procedures adjust for multiple comparisons to ensure that the overall false positive rate remains controlled. Two methods for this purpose are Tukey’s Honestly Significant Difference (HSD) (Tukey 1949) and the False Discovery Rate (FDR) procedures (Efron and Hastie 2016, chap. 15).\nTukey’s HSD test focuses on controlling the probability of making at least one false discovery; this is known as the familywise error rate (FWER). The HSD test is specifically designed for one-way ANOVA post hoc comparisons and ensures that the FWER remains at a pre-determined significance level (\\(\\alpha\\) = 0.05).\nThe test considers all possible pairwise differences while maintaining a constant error rate across comparisons. To contrast models \\(i\\) and \\(j\\), the test statistic is calculated as:\n\\[\nT_{HSD} = \\frac{\\bar{Q}_i - \\bar{Q}_j}{\\sqrt{MS_{within} / n}}\n\\]\nwhere, \\(\\bar{Q}_i\\) and \\(\\bar{Q}_j\\) are the sample means of each model’s outcome data, \\(MS_{within}\\) is the mean square error from the ANOVA, and \\(n\\) is the number of observations per group. The critical value for \\(T_{HSD}\\) is obtained from the studentized range distribution, which depends on the number of groups and the degrees of freedom from the ANOVA error term.\nWhen we desire to compare the performance of each pair of models, then Tukey’s HSD procedure will provide the appropriate adjustments to the p-values to protect against false positive findings as it becomes overly conservative.\nIn settings where many hypotheses are tested simultaneously, we may be more interested in the false discovery rate (FDR): the expected proportion of false positives among all rejected hypotheses. With many comparisons, this can be more appropriate than controlling the familywise error rate.\nThere are numerous FDR procedures in the literature. The method by Benjamini and Hochberg (1995) is the simplest. It ranks the p-values from all pairwise comparisons from smallest to largest (\\(p_1\\), \\(p_2\\), …, \\(p_m\\)) and compares them to an adjusted threshold:\n\nRank the p-values in ascending order, assigning each a rank \\(i\\).\nCompute the critical value for each comparison: \\[\np_{crit} = \\frac{i}{m} \\alpha\n\\] where \\(m\\) is the total number of comparisons and \\(\\alpha\\) is the desired FDR level.\nFind the largest \\(i\\) where \\(p_i \\leq p_{crit}\\), and reject all hypotheses with p-values at or below this threshold.\n\nUnlike Tukey’s method, FDR control is more appropriate when performing many comparisons. This procedure offers a balance between sensitivity (detecting true differences) and specificity (limiting false positives).\nNow, let’s compare how these methods perform when evaluating each pair of models in our example. Table 14.1 presents the estimated differences in accuracy, standard error, degrees of freedom, raw, unadjusted p-values, and the p-values adjusted using the Tukey and FDR methods for each pairwise comparison. Notice that the Tukey adjustment results in less significant p-values than the FDR adjustment. Regardless of the adjustment method, the results indicate that each model’s performance differs statistically significantly from the others. Specifically, the boosting model has a higher accuracy than the other two models.\n\n\n\n\n\n\n\n\n\n\n\nComparison\nEstimate\nSE\nDF\n\np-value Adjustment\n\n\n\nUnadjusted\nTukey\nFDR\n\n\n\n\nBoosting - Logistic\n0.79%\n0.67%\n18\n0.25544\n0.48276\n0.25544\n\n\nBoosting - Naive Bayes\n4.04%\n0.67%\n18\n0.00001\n0.00003\n0.00003\n\n\nLogistic - Naive Bayes\n3.25%\n0.67%\n18\n0.00013\n0.00036\n0.00019\n\n\n\n\n\n\n\n\nTable 14.1: Pairwise comparisons among the three models for predicting forestation classification. The table includes the estimated difference in accuracy, standard error, degrees of freedom, the unadjusted p-value, and the corresponding Tukey and FDR adjusted p-values.\n\n\n\n\nIn this table, we can see that we have no evidence that boosting and logistic regression have different accuracies, but both of these models are statistically different from naive Bayes (no matter the adjustment type).\n\n\n\n14.1.2.2 Comparing Performance using Equivalence Tests\nIn the previous section, we explored how to determine whether one model significantly outperforms another in a metric of choice. However, there are cases where we may instead want to assess whether the models perform similarly in a practically meaningful way. To do this, we use an equivalence test, which differs from the traditional hypothesis test, which aimed to detect differences between models.\nEquivalence testing is based on the concept that two models can be considered practically equivalent if their difference falls within a predefined margin of indifference, often denoted as \\(\\theta\\). This threshold represents the maximum difference that is considered practically insignificant. To illustrate equivalence testing, let’s begin by reviewing the hypothesis testing context for assessing differences between models. Let \\(\\beta_1\\) and \\(\\beta_2\\) represent the mean quality of two models. The null hypothesis for a standard two-sample test of difference is:\n\\[\nH_O:  \\beta_1 = \\beta_2\n\\]\nwhile the alternative hypothesis is:\n\\[\nH_A:  \\beta_1 \\neq \\beta_2\n\\]\nHowever, an equivalence test reverses this logic. The null hypothesis is that the models are not equivalent, and the alternative hypothesis is that the models are practically insignificant:\n\\[\nH_0:  |\\beta_1 - \\beta_2| \\geq \\theta\n\\tag{14.3}\\]\n\\[\nH_A:  |\\beta_1 - \\beta_2| \\lt \\theta\n\\tag{14.4}\\]\nUsing this framework, we might reject the null hypothesis and conclude that the models are equivalent within the predefined margin of indifference.\nA commonly used approach for equivalence testing is the Two One-Sided Test (TOST) procedure (Schuirmann 1987). This method involves conducting two separate one-sided hypothesis tests as defined in Equation 14.3 and Equation 14.4. Specifically, one test assesses the lower bound comparison while the other test assesses the upper bound comparison as follows:\nLower bound:\n\\[\nH_0:  \\beta_1 - \\beta_2 \\geq -\\theta\n\\]\n\\[\nH_A:  \\beta_1 - \\beta_2 &gt; -\\theta\n\\] Upper bound:\n\\[\nH_0:  \\beta_1 - \\beta_2 \\leq \\theta\n\\] \\[\nH_A:  \\beta_1 - \\beta_2 &gt; \\theta\n\\]\nEach test is conducted at a desired significance level \\(\\alpha\\), and if both null hypotheses are rejected, we conclude that the model performance is equivalent based on the predefined margin of indifference.\nSchuirmann showed that the two one-sided hypotheses can be practically evaluated by constructing a \\(100(1-2\\alpha)\\%\\) confidence interval for \\(\\beta_1 - \\beta_2\\). If this interval is contained within the boundaries of (\\(-\\theta\\), \\(\\theta\\)), then we can reject the null hypothesis and conclude that the two models have equivalent performance.\nSelecting an appropriate margin of indifference is crucial and should be determined prior to conducting the statistical test. What value should we choose? The answer to this question is problem-specific and depends on what we believe makes a sensible difference in performance.\nFrom the previous section, we saw that the largest expected difference between any pair of models was 4%. For the sake of this illustration, let’s suppose that the margin of practical indifference between models is at least \\(\\pm\\) 3 percent in accuracy.6\nTo perform the equivalence tests, we use the same ANOVA model as when testing for differences in Equation 14.2 and estimate the 90% confidence intervals about each pairwise difference. Because we are making multiple pairwise comparisons, the intervals need to be adjusted to minimize the chance of making a false positive finding. This adjustment can be done using either the Tukey or FDR procedures.\nFigure 14.3 presents the Tukey-adjusted 90% confidence intervals for each pairwise difference between models. For the forestation data, we would only conclude that the boosting ensemble and the logistic regression are equivalent.\n\n\n\n\n\n\n\n\nFigure 14.3: Pairwise comparisons of models using equivalence testing. The Tukey-adjusted 90% confidence interval of each pairwise difference is illustrated with vertical bands. For these comparisons, the margin of practical equivalence was set to +/- 3% accuracy.\n\n\n\n\n\n\n\n\n\n14.1.3 Comparisons Using Bayesian Models\nHow would we use a Bayesian approach to fit the model from Equation 14.2? To start, we can again assume a Gaussian distribution for the errors with standard deviation \\(\\sigma\\). One commonly used prior for this parameter is the exponential distribution; it is highly right-skewed and has a single parameter. Our strategy here is to use very uninformative priors so that the data have more say in the final posterior estimates than the prior. If our model were useless, the standard deviation would be very large and would probably be bounded by the standard deviation of the outcome. For our data, that value for the accuracy is 4.1%. We’ll specify an exponential prior to use this as the mean of its distribution.\nWe’ll also need to specify priors for the three \\(\\beta\\) parameters. A fairly wide Gaussian would suffice. We’ll use \\(\\beta_j \\sim N(0, 5)\\) for each.\nFor the random effects \\(\\beta_{0i}\\), the sample size is fairly low (10 resamples per model). Let’s specify a bell-shaped distribution with a wide tail: a \\(t\\)-distribution with a single degree of freedom. This would enable the Bayesian model to have some resample-to-resample effects that might be considered outliers if we assumed a Gaussian distribution.\nWe’ll discuss training Bayesian models in Section 16.2.5, so we’ll summarize here. Given our priors, there is no analytically defined distribution for the posterior. To train the model, we’ll use a Markov-chain Monte Carlo (MCMC) method to slowly iterate the posterior to an optimal location, not unlike the simulated annealing process previously described. To do this, we create chains of parameters. These are a controlled random walk from a starting point. Once that walk appears to converge, we continue generating random data points that are consistent with the optimized posterior. These random points will numerically approximate the posterior, and from these, we can make inferences. We’ll create 10 independent chains, and each will have a warm-up phase of 5,000 iterations and then another 5,000 samples to approximate the posterior. If all goes well, we will have a random sample of 50,000 posterior samples.\nThe resulting posterior distributions for the mean accuracy for each of the three models are shown in Figure 14.4 along with 90% credible intervals. The posteriors are relatively bell-shaped, and although there are discernible shifts between the models, there are overlaps in these distributions.\nWe can also use the posteriors to estimate how much of the error in the data was associated with the resample-to-resample effect (i.e., the previously described nuisance parameter). The percentage of total error attributable to the resample-to-resample effects is 82.5%. This illustrates how important it is to account for all of the systematic effects, wanted or unwanted, in the model.\n\n\n\n\n\n\n\n\nFigure 14.4: Posterior distributions of accuracy for each of the models. The horizontal bars represent the 90% credible intervals about the posterior accuracy values for each model.\n\n\n\n\n\nSince we are interested in contrasting models, we can get a sample of the posterior and take the difference between the parameters that correspond to the model terms (i.e., \\(\\beta\\) parameters). We can create a large number of these samples to form posterior distributions of the pairwise contrasts and compute credible intervals. Bayesian modeling does not use the null hypothesis testing paradigm as Frequentist methods, so it is unclear whether post hoc adjustments are needed or how that would be computed. See Gelman, Hill, and Yajima (2012) and Ogle et al. (2019) for discussions. Figure 14.5 illustrates posteriors for the pairwise contrasts.\n\n\n\n\n\n\n\n\nFigure 14.5: Posterior distributions of pairwise differences in accuracy for each pair of models. The vertical dashed lines represent the 90% credible intervals for each contrast.\n\n\n\n\n\nBased on these results, we would reach the same conclusion as the Frequentist results; the boosting and logistic regression models are the only pair to appear to have comparable accuracies. We can compute probability statements such as “There is a 85.4% probability that the boosting ensemble exhibits a higher accuracy than logistic regression.”\nIn addition to utilizing the posterior distribution to compute probabilities of difference, we can also use this distribution to calculate probabilities of similarity or equivalence. The region of practical equivalence (ROPE) defines a range of metric values that we believe contains practically insignificant results (Kruschke 2014). When using the equivalence testing from Section 14.1.2.2, we considered models equivalent if the accuracy was within 3%. Using this region, the posterior distribution of the parameter of interest is used to generate a probability that the parameter is within this region. If a large proportion of the posterior distribution falls within the ROPE, we can conclude that the difference is minimal.\nThe primary difference between the Frequentist and Bayesian equivalence testing lies in interpretation and flexibility. Frequentist equivalence testing is based on pre-specified significance levels and long-run error rates. Bayesian ROPE, on the other hand, provides a probability-based assessment of practical equivalence. This is particularly useful in predictive modeling, where slight differences in performance may not be operationally relevant.\nFigure 14.6 illustrates the ROPE for the forestation data, defined as \\(\\pm\\) 3% accuracy. The probability mass within this region, shown in blue, represents the likelihood that the models have equivalent performance based on this criterion. Table 14.2 summarizes the probabilities for each model comparison.\n\n\n\n\n\n\n\n\nFigure 14.6: Posterior distributions of pairwise differences in accuracy for each pair of models. The vertical dashed lines represent the region of practical equivalence. The probability mass within this region represents the probability that the models are practically equivalent within +/- 3% accuracy.\n\n\n\n\n\nFor instance, there is an overwhelming probability that boosting and logistic regression are practically equivalent. The other two comparisons (with naive Bayes) show scant evidence of the same.\n\n\n\n\n\n\n\n\n\n\n\nComparison\nProbability of Practical Equivalence\n\n\n\n\nBoosting vs Logistic\n0.997\n\n\nBoosting vs Naive Bayes\n0.081\n\n\nLogistic vs Naive Bayes\n0.362\n\n\n\n\n\n\n\n\nTable 14.2: Probability of practical equivalence between each pair of models’ performance.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-holdout",
    "href": "chapters/comparing-models.html#sec-compare-holdout",
    "title": "14  Comparing Models",
    "section": "14.2 Single Holdout Data Sets",
    "text": "14.2 Single Holdout Data Sets\nUnlike resampled data sets, validation sets and test sets offer a single “look” at how well the model operates. In the case of the validation set, we would repeatedly use it to guide our model development process, much like how we would rely on resamples. For the test set, it should only be evaluated once, hopefully on a single model, but perhaps to make a final choice between a select few. It is not intended for repeat use. In the discussions below, we’ll use the term “holdout set” to describe the validation or test set, without loss of generality. In this scenario, the focus will be on Frequentist methods by way of bootstrap sampling (Efron 1979, 2003), which we have seen previously.\nYou were first introduced to the bootstrap in Section 10.6, where it was applied to estimate model performance during tuning. In essence, the bootstrap involves resampling with replacement from the available data to estimate the sampling variability of a statistic. This variability can then be used to construct confidence intervals of the performance metrics of interest, offering a range of plausible values for model performance on similar validation sets.\nFigure 10.7 illustrates the bootstrap process during the model-building tuning phase. Recall that resampling with replacement from the training data generates multiple bootstrap samples of the same size as the original. Because sampling is done with replacement, some observations may appear multiple times within a sample, while others may be omitted. The omitted samples are used as holdout sets to evaluate model performance.\nTo adapt this idea to make predictions on the single holdout set, then repeatedly draw bootstrap samples from it. Each bootstrap sample is the same size as the holdout set and is created via random sampling with replacement (as illustrated in Figure 14.7). We compute our metric(s) of interest on each analysis set, which are just the bootstrap samples. The corresponding assessment sets are not used in this process.\n\n\n\n\n\n\n\n\nFigure 14.7: A schematic of bootstraps resamples created from a validation set.\n\n\n\n\n\nThis process yields a distribution of performance metrics, from which bootstrap confidence intervals can be derived. These intervals offer an estimate of the uncertainty around the holdout set’s performance and give us a sound way to express the expected range of performance across other holdout sets drawn from the same population.\nThere are several methods for computing bootstrap confidence intervals. See Davison and Hinkley (1997) and Chapter 11 of Efron and Hastie (2016). We’ll describe one that is straightforward and often used: percentile intervals. The idea is simple: for an interval with \\(100 (1 - \\alpha)\\)% confidence, we compute the \\(\\alpha/2\\) quantiles on both ends of the bootstrap distribution. For our 90% intervals, we determine the 5% and 95% quantiles. Since the method relies on estimating very small areas in the distributional tails, it is important to create many bootstrap samples (i.e., thousands) to guarantee dependable results.\nLet’s suppose that we considered a wide variety of models and feature sets, and still found our boosted tree and logistic regression to be the best. We’ve previously seen that, while the ensemble appears to perform best, the results are fairly similar, and the logistic model is simpler and perhaps more explainable. We might take both of these pipelines to the test set to make one final comparison before making a choice.\nWe fit the final models on the entire training set and then predict the test set of 1,371 locations. The accuracies for the boosting and logistic models were very close; they were 87.7% and 87.3%, respectively. Is this difference within the experimental noise or could there be a (statistical) difference?\nWe arrange the data so that there are separate columns for the boosted tree and logistic regression predictions (i.e., they are merged by the row number of the test set). Using this data, 5,000 bootstrap samples were created. From each of these, we can compute intervals for the accuracy of each and the difference in accuracy between them.\nFigure 14.8 shows the bootstrap distributions of the individual model results. They are fairly similar, with a small shift between them. Above each histogram are two 90% confidence intervals. One is the bootstrap percentile interval, and the other is a theoretically based asymmetric interval that assumes that the correct/incorrect data for each model follows a Bernoulli distribution. For each model, these intervals are almost identical. We computed each to demonstrate that the bootstrap has very good statistical performance (compared to its theoretical counterpart).\n\n\n\n\n\n\n\n\nFigure 14.8: For both models, the bootstrap distributions of the accuracy statistics are shown as histograms. The intervals at the top of each panel show the 90% confidence intervals computed using the bootstrap and via the traditional analytical method.\n\n\n\n\n\n\nThe confidence intervals between models substantially overlap. Ordinarily, we should take that as reliable information that there is no statistical difference between the two models. However, this is not true for this particular situation.\n\nThe issue is that the two sets of predictions are highly correlated. On the test set, they agree 93.1% of the time. Unlike our previous mixed model approach from Equation 14.2, the individual confidence intervals have no way to correct for this issue (since they evaluate one model at a time).\nWhen computing the difference between two things (A and B), the variance of their difference is:\n\\[\nVar[A-B] = Var[A] + Var[B] - 2Cov[A, B]\n\\tag{14.5}\\]\nWhen A and B are related, their covariance will be very high and, if we ignore it, the power of any statistical test on that difference can be severely underpowered. This is the same issue as shown in Section 14.1.2. When we didn’t account for the resampling effect, we accidentally sabotaged the test’s ability to find a difference (if there was one).\nFor the test set, we compute the difference in accuracy by resampling the matched pairs of predictions. From this, the bootstrap estimate of the difference was 0.4% with 90% bootstrap interval (-0.8%, 1.5%). This does indicate that the models have statistically indistinguishable accuracies, but the lower bound is very close to zero. With slightly different models or a different random split of the data, they might very well be considered different7. There is an overwhelmingly strong case to make that they are practically equivalent, though.\nAlso, as the data set size increases, the variation we expect to see in the summary statistic will decrease. This concept is illustrated in Figure 14.9. In this figure, we have randomly selected subsets of varying size from the data set and then computed a 90% confidence interval using these smaller sets. Notice that as the sample size decreases, the width of the confidence interval increases. Therefore, the smaller the data set, the higher the variability will be in the estimate of model quality, and the more important it is to understand the range of potential uncertainty. This pattern is true of any interval procedure.\n\n\n\n\n\n\n\n\nFigure 14.9: 90% confidence interval widths of accuracy for data sets of varying size, assuming the same performance as the boosted ensemble.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-conclusion",
    "href": "chapters/comparing-models.html#sec-compare-conclusion",
    "title": "14  Comparing Models",
    "section": "14.3 Conclusion",
    "text": "14.3 Conclusion\nSelecting the best predictive model involves assessing performance metrics and accounting for practical considerations. Thus far, this chapter has outlined both Frequentist and Bayesian approaches to comparing model performance, emphasizing the importance of appropriate statistical techniques when evaluating differences. The Frequentist approach provides a formal hypothesis testing framework, while the Bayesian approach offers a more direct probability-based interpretation of model differences. When there is just one data set for evaluation, like a validation set or test set, then the bootstrap technique can be used to create an interval about the performance metric (or differences between models).\nUltimately, while statistical significance can indicate meaningful differences between models, practical significance should also guide decision-making. Equivalence testing and Bayesian ROPE analysis help determine whether models perform similarly within a defined threshold of practical relevance. In our forestation classification example, these methods suggested that while the boosting model demonstrated superior predictive performance overall, differences between some models may not always be large enough to drive a clear choice.\nIn practice, model selection is rarely based on performance alone. Factors such as interpretability, computational cost, and deployment feasibility must also be considered. By combining quantitative performance assessment with practical domain knowledge, we can make informed decisions that balance accuracy with real-world constraints.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#chapter-references",
    "href": "chapters/comparing-models.html#chapter-references",
    "title": "14  Comparing Models",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300.\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nDudoit, S, and M Van Der Laan. 2008. Multiple Testing Procedures with Applications to Genomics. Springer.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nEfron, B. 2003. “Second Thoughts on the Bootstrap.” Statistical Science, 135–40.\n\n\nEfron, B, and T Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.\n\n\nGelman, A, J Hill, and M Yajima. 2012. “Why We (Usually) Don’t Have to Worry about Multiple Comparisons.” Journal of Research on Educational Effectiveness 5 (2): 189–211.\n\n\nHsu, J. 1996. Multiple Comparisons: Theory and Methods. Chapman; Hall/CRC.\n\n\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press.\n\n\nKutner, M, C Nachtsheim, and J Neter. 2004. Applied Linear Regression Models. McGraw-Hill/Irwin.\n\n\nOgle, K, D Peltier, M Fell, J Guo, H Kropp, and J Barber. 2019. “Should We Be Concerned about Multiple Comparisons in Hierarchical Bayesian Models?” Methods in Ecology and Evolution 10 (4): 553–64.\n\n\nSchuirmann, Donald J. 1987. “A Comparison of the Two One-Sided Tests Procedure and the Power Approach for Assessing the Equivalence of Average Bioavailability.” Journal of Pharmacokinetics and Biopharmaceutics 15: 657–80.\n\n\nTukey, John W. 1949. “Comparing Individual Means in the Analysis of Variance.” Biometrics, 99–114.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#footnotes",
    "href": "chapters/comparing-models.html#footnotes",
    "title": "14  Comparing Models",
    "section": "",
    "text": "This isn’t normally our first choice. However, the outcome classes are fairly balanced. Also, accuracy is intuitively understandable, and the broader set of classification metrics has yet to be described in detail.↩︎\nThis is another example of an analysis of variance (ANOVA) model.↩︎\nIt may seem odd that we would assume a Gaussian distribution for a variable with values between zero and one. Almost all performance statistics are means; accuracy is the average of 0/1 data. The Central Limit Theorem tells us that, despite the original data distribution, the mean tends to normality as the sample size increases. Assessment and test sets often have enough data points to trust this assumption.↩︎\nThis is common notation for reporting the F-statistic, where the first number represents the ratio of the mean square of the model to the mean square of the error. The second and third numbers represent the degrees of freedom of the numerator and denominator, respectively.↩︎\nThis again echoes the discussion of racing from Section 11.3.3.↩︎\nWhen performing an equivalence test, the margin of practical indifference should be specified before the test is performed.↩︎\nOne reason to avoid using metrics based on qualitative predictions (e.g., “yes” or “no”) is that they have a diminished capability to detect differences. They tend to contain far less information than their corresponding class probabilities. A metric such as the Brier score would have additional precision and might be able to tell the two models apart.↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html",
    "href": "chapters/cls-metrics.html",
    "title": "15  Characterizing Classification Models",
    "section": "",
    "text": "15.1 Class Distributions\nClassification models are used to predict outcomes that are categorical in nature. There are two types of predictions:\nHard class predictions are almost always created by finding the category corresponding to the largest value of the soft predictions.\nThis chapter discussed important characteristics of qualitative outcomes and appropriate metrics for quantifying how well the model functions. There is an abundance of metrics; we describe many of them here. Additionally, we’ll highlight different aspects of “performance” and when one might be more important than others.\nAn important characteristic of classification outcomes is the prevalence of the classes. This is the frequency at which each class level occurs in the wild. For example, in late 2024, the prevalence of COVID-19 was believed to be between 5% and 7%. The prevalence is not a universal, static constant; it can change by geography and time. However, as we’ll see below, the prevalence can affect the results of many metrics.\nFor classification models, the prevalence is directly related to Bayes’ Rule. It is the prior distribution for the outcome variable (i.e., \\(Pr[y]\\) in Equation 12.10). While the prevalence can be estimated from our training set, we should also realize that it should reflect our a priori beliefs (if any) about the outcome data.\nFor example, Figure 15.1 (left panel) shows the prior for Wordle1 scores as defined by one of the authors. Wordle is a game where the player has six attempts to guess a five-letter word. There are seven levels, the last (an X) representing the case where the player failed to guess the word. The prior for guessing the word on a single try should not be zero but incredibly small. There are thousands of possible words, although many players choose a starter word that they believe optimal2. Guessing the word on two tries is considerably fortuitous, and the author placed a 4% probability of this occurring. There is also the opinion that correctly guessing in three or four turns should consume the bulk of the probability, and values of 40% and 35% were assigned to these. Not guessing the word was also thought to be about 5%. The final prior was (in order) 0.1%, 4%, 40%, 35%, 10%, 5.9%, and 5%.\nFigure 15.1: Two potential prior distributions for Wordle scores, one based on the prior belief of a single person and another extracted from a social media platform.\nThe figure also shows the distribution of 296,534 self-reported scores across 805 words on the Bluesky social media platform. While more bell-shaped, these data have some potential issues. The first two probabilities, 0.75% and 7.14%, are abnormally high. It is exceedingly unlikely that more than one half of a percent of guesses were correct on the first try, given that there are 805 unique words in this data set3. Also, the second guess probability is only achievable if a highly optimized first word is used rigorously and the player is smiled upon by fortune.\nTheoretically, these two distributions should be different since one is the belief of a single person and may not generalize well to the general population of players. However, the prior is about belief and only has a significant effect when the size of the data is small (as previously discussed in Sections 6.4.3 and 12.5.1).\nThis chapter describes myriad approaches for quantifying how well a classification model functions via performance metrics.\nThe next section discusses important characteristics of performance metrics. After this, two sections discuss specific metrics for both types of predictions. Another section discussed the special case where the class levels are ordered (such as with the Wordle scores). There is also a section specifically on model calibration and another describing the use of multiple metrics.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-cls-distributions",
    "href": "chapters/cls-metrics.html#sec-cls-distributions",
    "title": "15  Characterizing Classification Models",
    "section": "",
    "text": "It is very important to carefully choose which metric (or metrics) to optimize a model. The metric defines what your model should consider important, and it will almost always believe that your chosen metric is the only important criterion. This can often lead to unintended consequences.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-metric-information",
    "href": "chapters/cls-metrics.html#sec-metric-information",
    "title": "15  Characterizing Classification Models",
    "section": "15.2 Choosing Appropriate Metrics",
    "text": "15.2 Choosing Appropriate Metrics\nLet’s consider a few critical aspects of classification metrics. We’ll focus on specific metrics computed with hard or soft class predictions to further this discussion. We’ve seen the Brier score used repeatedly in the previous chapters for class probability predictions. As a reminder, this statistic is similar to the sum of squared errors in regression: for each class, the estimated class probabilities are contrasted with their corresponding binary indicators. These differences are squared and averaged. Let’s juxtapose the Brier score with the simplest metric for hard predictions: the model’s overall accuracy (i.e., the proportion of correctly predicted samples).\nTo start, should our metric be based on hard or soft predictions? This, of course, depends on how the model will be applied to data and how users will consume its predictions. However, a strong argument can be made that performance metrics that use soft predictions should be favored over those using hard predictions. Information theory introduces the idea of self-information: if the value of a variable has some probability \\(\\pi\\) of occurring, we can measure the amount of information using \\(I = -log_2(\\pi)\\) where the units are called bits4.\nFor binary classification, there are two possible hard prediction values, so the information is \\(-log_2(1/2)\\) = 1.0 bit. For simplicity, suppose the probability estimates are measured to the second decimal place. In this case, there are 101 possible probability estimates, so the data contain \\(-log_2(1/101)\\) bits (about 6.7).\nThis simple analysis implies that metrics that use probabilities will contain much more information (literally and figuratively) than class predictions. As a result, probability-based metrics will be more capable of discriminating between models.\nLet’s also consider two examples of how metrics can lead to unintended consequences.\nFirst, accuracy is very sensitive to class imbalances (where one or more classes do not occur very often). Suppose that we have two classes, and one only occurs 1% of the time (called the “minority class” in this context). When accuracy is used for these data, an inferior model can easily achieve 99% accuracy simply by always predicting the majority class. If accuracy is all that we care about, then we can satisfy the literal goal, but our model will probably fail to fulfill the intention of the user5.\nThe second example contrasts the concepts of discrimination and calibration.\n\nA model can discriminate the classes well when the probability estimates of the true class are larger than the probabilities for the others. In other words, being good at discrimination means that you can, to some degree, successfully discern the true classes\nA classification model is well-calibrated when the estimated probability predictions are close to their real theoretical counterparts. In other words, if we estimate a probability to be 55%, the event should occur at this same rate if realized multiple times.\n\nSome metrics favor one of these characteristics over the other. For example, neural networks for classification use binary indicators for their outcome data and try to produce models so that their predictions are near these values. To ensure that the results are “probability like,” meaning that they add up to one and are on [0, 1], the softmax transformation (Bridle 1990) is applied to the raw predictions:\n\\[\n\\hat{p}_k = \\frac{e^{\\hat{y}_k}}{{\\displaystyle  \\sum_{l=1}^Ce^{\\hat{y}_l}}}\n\\tag{15.1}\\]\nwhere \\(C\\) is the number of classes, and \\(\\hat{y}_k\\) is a predicted score for class \\(k\\).\nWhile the softmax method ensures they have the correction range, it does not guarantee that a model with good discrimination is well-calibrated. Figure 15.2 shows a hypothetical example that illustrates what can occur when the softmax method is used. The distributions of the two classes have a good degree of separation, indicating that the model can discriminate the true classes. The observed accuracy is large (98.2%) but the predicted probabilities only range from 38.2% to 63%. It is improbable that the true probability values have such a narrow range, and we can assume that the calibration is poor here. The observed Brier score is large (0.203), supporting this assumption.\n\n\n\n\n\n\n\n\nFigure 15.2: An example of a model that has good discrimination but poor calibration.\n\n\n\n\n\nIn some cases, the pattern shown in Figure 15.2 may be acceptable. For example, if the goal of the model is to identify “events” (such as the most profitable consumers in a database), the model’s calibration may not be important; your main interest is in ranking the rows in a data set. However, we suggest that probability predictions are far more informative than class predictions, and thus, good calibration is a key feature of most classification models.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-mushrooms",
    "href": "chapters/cls-metrics.html#sec-mushrooms",
    "title": "15  Characterizing Classification Models",
    "section": "15.3 Example Data: Poisonous Mushrooms",
    "text": "15.3 Example Data: Poisonous Mushrooms\nTo demonstrate classification metrics, we’ll primarily use a data set where mushroom characteristics are used to predict whether specific mushrooms are edible or poisonous (Wagner, Heider, and Hattab 2021). These data were obtained from the UCI Machine Learning Archive and we’ll use them in conjunction with a naive Bayes classification model6. This model uses a simplified version of Bayes’ Rule (see Section 17.1.1).\nThe initial data pool contained 61,069 data points. The main partition of the data was into training (\\(n_{tr}\\) = 42,748), validation (\\(n_{val}\\) = 6,107), and test (\\(n_{te}\\) = 6,107). Additionally, a calibration set of \\(n_{cal}\\) = 6,107 mushrooms were allocated for the procedures described in Section 15.8.\nThe predictors of the outcome were:\n\n\n\ndoes bruise or bleed (yes or no)\ncap color (categorical, 12 values)\ncap diameter (numeric)\ncap shape (categorical, 7 values)\ncap surface (categorical, 8 values)\ngill attachment (categorical, 8 values, contains unknowns)\ngill color (categorical, 12 values)\ngill spacing (close, distant, or none)\nhabitat (categorical, 8 values)\nhas ring (yes or no)\n\n\n\nring type (categorical, 9 values, contains unknowns)\nseason (categorical, 4 values)\nspore print color (categorical, 7 values)\nstem color (categorical, 13 values)\nstem height (numeric)\nstem root (categorical, 4 values)\nstem surface (categorical, 8 values)\nstem width (numeric)\nveil color (categorical, 7 values)\npartial veil (partial or unknown)\n\n\n\nThe outcome data has relatively balanced classes with 55.5% of the mushrooms being poisonous. It is unclear whether this rate is consistent with mushrooms in the wild.\nNo preprocessing was applied to the data for this model. Naive Bayes models do not require the categorical predictors to be decomposed into binary indicator columns or transformations of the numeric predictors. As shown above, some predictors have missing data. In these cases, “missing” was added as an additional category.\nWe’ll fit the model to the training set and, for the most part, use the validation set to demonstrate the various metrics. Figure 15.3 shows the distribution of the probability of the event (poisonous) for the validation set.\n\n\n\n\n\n\n\n\nFigure 15.3: The distribution of the validation set class probability estimates for a naive Bayes model, split by the true class of the mushroom.\n\n\n\n\n\nThese distributions demonstrate a few attractive characteristics. First, the probability estimates span the entire range of possible values, and each distribution peaks at appropriately extreme values (i.e., zero or one). Conversely, the model rarely produces “confidently incorrect” predictions (i.e., erroneously predicting with a large probability estimate). We’ll see this model’s calibration and separation properties in later sections.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-cls-hard-metrics",
    "href": "chapters/cls-metrics.html#sec-cls-hard-metrics",
    "title": "15  Characterizing Classification Models",
    "section": "15.4 Assessing Hard Class Predictions",
    "text": "15.4 Assessing Hard Class Predictions\nWe’ll start by focusing on the qualitative predictions: \"poisonous\" or \"edible\". Unless otherwise stated, hard class predictions for two-class problems are assigned in this chapter using a 50% cutoff. Alternate cutoffs are investigated in Section 15.7.3.\nOne of the most fundamental ways to inspect and understand how well hard class prediction performs is the confusion matrix, simply a cross-tabulation of the observed and predicted classes. From this, we can conceptualize where the model underperforms. An excellent model has the largest counts along the diagonal where the observed and predicted classes are the same. For the naive Bayes model, Table 15.1 shows the confusion matrix for the mushroom data’s validation set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTruth\n\n\n\nPrediction\n\npoisonous\nedible\n\n\n\n\npoisonous\n\n2,613\n564\n\n\nedible\n\n750\n2,180\n\n\n\n\n\n\n\nTable 15.1: Confusion matrix for validation set predictions from the naive Bayes model.\n\n\n\n\n\n\n\nIt may be helpful to present the confusion matrix in terms of percentages. For example, we might think that the model is worse at predicting the poisonous than edible mushrooms since the lower left cell has a larger count (750) than the upper right cell (564). However, this difference is partially explained by the presence of more poisonous mushrooms in the left column than in the left column of edible samples. It can also be helpful to visualize the matrix as a heatmap where darker colors represent larger counts, especially when there are many possible classes.\nWe can also view the confusion matrix as a “sufficient statistic” since it has all the information required to compute the metrics based on hard class predictions. For example, the accuracy computed from our validation set was 78.5%, which is\n\\[\naccuracy = \\frac{1}{n}\\sum_{k=1}^Cn_{kk}\n\\tag{15.2}\\]\nwhere \\(n_{ij}\\) are the number of counts in the i th row and the j th column of the confusion matrix, \\(n\\) is the total number of data points.\nMany of the metrics in this section are in the form of a proportion. For these, confidence intervals can be computed using typical methods for binomial proportions. We suggest using asymmetric intervals, such as the “exact” technique in Clopper and Pearson (1934). See Newcombe (1998) and Meeker, Hahn, and Escobar (2017) for discussions of different methods for computing intervals on proportions. For the validation set results, the 90% confidence interval for accuracy was (77.6%, 79.3%). For other types of metrics, we can use the bootstrap methods described in Section 14.2 to obtain confidence limits.\nAs previously mentioned, accuracy is misleading when there is a class imbalance. There are a few ways to correct for imbalances, most notably the measure of agreement called Kappa (Cohen 1960). This normalizes the accuracy by a measure of expected accuracy based on chance agreement (\\(E\\)):\n\\[\nKappa = \\frac{accuracy - E}{1 - E}\n\\tag{15.3}\\]\nwhere \\(E\\) is the expected accuracy based on the marginal totals of the confusion matrix. For two classes, this is:\n\\[\nE =\\frac{1}{n^{2}}\\sum_{k=1}^2n_{k1}n_{k2}\n\\tag{15.4}\\]\nMuch like correlation statistics, the range of Kappa is \\(\\pm\\) 1, with values of one corresponding to perfect agreement and zero meaning negative agreement. While negative values are possible, they are uncommon. Interpreting Kappa can be difficult since no well-defined values indicate the results are good enough. This is compounded by the fact that the values can change with different prevalences of the classes (Thompson and Walter 1988; Guggenmoos-Holzmann 1996). For this reason, it isn’t easy to compare values between data sets or to have a single standard. There are also differences in range when the outcome data has different numbers of classes; a Kappa value of 0.5 has a different meaning when \\(C = 2\\) than with some other number of possible classes.\nThe Kappa statistic can also incorporate different weights for different types of errors (Cohen 1968). This is discussed in more detail in Section 15.9 below.\nUsing the data from Table 15.1, the Kappa value was 0.568. While subjective, this value probably indicates satisfactory agreement between the true classes and the predictions.\nDespite the qualitative nature of the classes, we can also compute a correlation statistic between the observed and predicted classes. Initially developed in the early twentieth century (Ekstrom 2011), the Phi-coefficient was rediscovered by Matthews (1975) and, more recently, is referred to as the Matthews correlation coefficient (MCC). For \\(C = 2\\), the statistic is:\n\\[\nMCC = \\frac {n_{11}n_{22}-n_{12}n_{21}}{\\sqrt {n_{1\\boldsymbol{\\cdot} }n_{2\\boldsymbol{\\cdot} }n_{\\boldsymbol{\\cdot} 2}n_{\\boldsymbol{\\cdot} 1}}}\n\\tag{15.5}\\]\nwhere the dot notation indicates the sum of the row or column. More generally, for \\(C\\) classes:\n\\[\n{\\displaystyle {\\text{MCC}}={\\frac {\\displaystyle{\\sum_{k=1}^C\\sum_{l=1}^C\\sum _{m=1}^C(n_{kk}n_{lm}-n_{kl}n_{mk})}}{{\\sqrt {{\\displaystyle \\sum_{k=1}^C\\left(\\sum _{l=1}^Cn_{kl}\\right)\\left(\\sum _{\\substack{k'=1\\\\k'\\neq k}}^C\\sum _{l'=1}^Cn_{k'l'}\\right)}}}{\\sqrt {{\\displaystyle\\sum_{k=1}^C\\left(\\sum _{l=1}^Cn_{lk}\\right)\\left(\\sum _{\\substack{k'=1\\\\k'\\neq k}}^C\\sum _{l'=1}^Cn_{l'k'}\\right)}}}}}}\n\\tag{15.6}\\]\nLike Kappa, the statistic mirrors the correlation coefficient for numeric data in that it ranges from -1 to +1 and a similar interpretation to ordinary correlation coefficients. For the mushroom data, the MCC estimate, 0.569, is very close to the Kappa estimate shown above. This is often the case, and the similarity was investigated more in Delgado and Tibau (2019).\nAccuracy, Kappa, and MCC can be computed regardless of the number of classes. There are numerous specialized metrics for \\(C = 2\\), i.e., “binary classification.”. The next section reviews these, and Section 15.6 describes a few methods for using these binary metrics when \\(C &gt; 2\\).",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-cls-two-classes",
    "href": "chapters/cls-metrics.html#sec-cls-two-classes",
    "title": "15  Characterizing Classification Models",
    "section": "15.5 Metrics for Two Classes",
    "text": "15.5 Metrics for Two Classes\nIt helps to conceptualize metrics for two classes when one class can be defined as the event of interest (i.e., which of the two is most important). The choice mainly affects the interpretation of metrics. For the mushroom data, either class would be appropriate, but we will choose \"poisonous\" as the event for our descriptions. We’ll also use “positives” to refer to the event of interest and “negatives” for the other class value. From this, we can name those positives correctly predicted as “true positives” (TP) and the accurately predicted negatives as “true negatives” (TN). If a positive sample was incorrectly predicted as negative, we have “false negatives” (FN) and, conversely, “false positives” (FP) for poorly predicted negative samples. Table 15.2 arranges these cases in a 2 \\(\\times\\) 2 confusion matrix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTruth\n\n\n\nPrediction\nevent\nnon-event\n\n\n\n\nevent\nTP\nFP\n\n\nnon-event\nFN\nTN\n\n\n\n\n\n\n\nTable 15.2: A general confusion matrix for outcomes with two classes.\n\n\n\n\n\n\n\nThere are particular metrics associated with the accuracy of each class. Sensitivity is the probably of correctly predicting the event of interest:\n\\[\nSensitivity = \\frac{\\text{correctly predicted events}}{\\text{total events}} = \\frac{TP}{TP + FN}\n\\tag{15.7}\\]\nThe accuracy of non-events is measured by specificity:\n\\[\nSpecificity = \\frac{\\text{correctly predicted non-events}}{\\text{total non-events}} = \\frac{TN}{TN + FP}\n\\tag{15.8}\\]\nWe often refer to the false positive rate which is simply one minus the specificity.\nSensitivity and specificity are bedrock metrics for two classes since they represent the two possible ways models can go wrong. In the Statistics literature, they correspond to “Type I” and “Type II” errors, respectively. We’ll examine their relationship below when receiver operating characteristic (ROC) curves are discussed.\nWith the mushroom data, the sensitivity was 77.7% and the specificity was nearly the same: 79.4%. Since these are simple ratios, confidence intervals can be computed using the same technique for overall accuracy.\nThese two metrics are commonly reported when a two-class model is evaluated. However, there is a subtle issue in their usage. If we want to know how well the model works, they don’t really answer a question that anyone would ask. For example, sensitivity is the answer to:\n\nQ1: If I know my mushroom is poisonous, what percentage of the time do I correctly predict it to be poisonous?\n\nThis is because sensitivity conditions on the sample being positive. If we knew that, we would not require a model to make a prediction. What we would really like to know:\n\nQ2: If I predict this mushroom to be poisonous, what is the probability that it is actually poisonous?\n\nThis is a different question that requires more data than just the sensitivity estimate.\nTo explain, we must return to Bayes’ Rule (Equation 12.10). In this context, the sensitivity is the likelihood term \\(Pr[x = poison | y = poison]\\) (where \\(x\\) represents the predicted class). To answer question Q2, what we need is:\n\\[\nPr[y = poison| x = poison] = \\frac{Pr[y = poison] Pr[x = poison | y = poison]}{Pr[x = poison]}\n\\tag{15.9}\\]\nThis equation reflects the idea that we need to know how often the event really occurs (independent of our model). This is the Bayesian prior probability (i.e., the prevalence). If the event rarely happens anyway, that should affect the probability of us predicting it to happen.\nFor the event class, Q2 is measured by a statistic called the positive predictive value (PPV), which requires sensitivity, specificity, and prevalence:\n\\[\nPPV= \\frac{Prev \\times Sens}{(Sens \\times Prev) + \\left((1 - Spec) \\times (1 - Prev)\\right)}\n\\tag{15.10}\\]\nWithout knowing anything about how often the average mushroom is poisonous, we could choose a prevalence of 50% as an uninformative prior. Our estimate from the training set (55.5%) is close to uninformative so that we will use that in our computations.\nWith our estimates in hand, the answer to Q2 is\n\nA2: 82.2% of the time, a mushroom is poisonous when we predict it to be.”\n\nThe Bayesian analog to specificity is called the negative predictive value (NPV) and has a similar formula:\n\\[\nNPV= \\frac{(1 - Prev) \\times Spec}{\\left(Prev \\times (1 - Sens)\\right) + \\left(Spec \\times (1 - Prev)\\right)}\n\\tag{15.11}\\]\nUsing the prevalence from our training set and the metrics computed on the validation set, the NPV estimate is 74.4%.\nThe PPV and NPV are functions of three quantities, and it might be challenging to understand how the prevalence can affect the statistics. Figure 15.4 can help us understand these relationships. For example, if the prevalence is near zero, achieving an acceptable PPV value may be difficult unless the specificity is perfect (in which case it is quite easy to attain a good NPV).\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: fig-prevalence\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(bslib)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\n\n# ------------------------------------------------------------------------------\n\nui &lt;- fluidPage(\n#  theme = grid_theme,\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-3, 6, -3)),\n    column(\n      width = 10,\n      sliderInput(\n        inputId = \"prev\",\n        label = \"Prevelance\",\n        min = 0,\n        max = 1,\n        value = 1/2,\n        width = \"100%\",\n        step = 1/20\n      )\n    )\n  ),\n  as_fill_carrier(plotOutput('results'))\n)\n\nserver &lt;- function(input, output) {\n  \n  ppv_fn &lt;- function(sens, spec, prev = 1 / 2) {\n    ( sens * prev ) / ( (sens * prev) + ( (1 - spec) * (1 - prev) ) )\n  }\n  npv_fn &lt;- function(sens, spec, prev = 1 / 2) {\n    ( spec * (1 - prev) ) / ( ( (1 - sens) * prev) + ( (spec) * (1 - prev) ) )\n  }\n  \n  output$results &lt;-\n    renderPlot({\n      \n      x &lt;- seq(0, 1, length.out = 50)\n      n &lt;- length(x)\n      cond_x &lt;- (0:4) / 4\n      rng &lt;- extendrange(0:1, f = 0.02)\n      \n      ###\n      \n      pos_df &lt;- expand.grid(spec = cond_x, sens = x)\n      pos_df$prev &lt;- input$prev\n      pos_df$PPV &lt;- ppv_fn(pos_df$sens, pos_df$spec, prev = pos_df$prev)\n      pos_df$Specificity &lt;- format(pos_df$spec, digits = 3)\n      \n      pos_p &lt;- \n        pos_df %&gt;% \n        ggplot(aes(x = sens, y = PPV, col = Specificity)) +\n        geom_line(linewidth = 1) + \n        lims(x = 0:1, y = rng) + \n        theme_bw() +\n        theme(\n          legend.position = \"top\",\n          legend.text = element_text(size = 9)\n        ) +\n        scale_color_brewer(palette = \"Reds\") +\n        labs(x = \"Sensitivity\")\n      \n      ###\n      \n      neg_df &lt;- expand.grid(sens = cond_x, spec = x)\n      neg_df$prev &lt;- input$prev\n      neg_df$NPV &lt;- npv_fn(neg_df$sens, neg_df$spec, prev = neg_df$prev)\n      neg_df$Sensitivity &lt;- format(neg_df$sens, digits = 3)\n      \n      neg_p &lt;- \n        neg_df %&gt;% \n        ggplot(aes(x = spec, y = NPV, col = Sensitivity)) +\n        geom_line(linewidth = 1) + \n        lims(x = 0:1, y = rng) + \n        theme_bw() +\n        theme(\n          legend.position = \"top\",\n          legend.text = element_text(size = 9)\n        ) +\n        scale_color_brewer(palette = \"Blues\") +\n        labs(x = \"Specificity\")\n      \n      print(pos_p + plot_spacer() + neg_p + plot_layout(widths = c(6, 1 , 6)))\n      \n    }, res = 100)\n}\n\napp &lt;- shinyApp(ui, server)\n\napp\n\n\n\nFigure 15.4: A visualization of how the ingredients to the positive and negative predictive values affect the statistics.\n\n\n\nThe predictive value metrics are rarely quoted since they are more complex and rely on good prevalence estimates. They also tend to give worse impressions of model efficacy than sensitivity and specificity.\nSensitivity and specificity (and PPV/NPV) measure opposite types of errors. When paired, they are most appropriate when the overall model quality is important. In contrast, we’ll now consider a few other metrics for two-class problems, focusing primarily on the event class.\nThe field of Information Retrieval (IR) focuses on finding relevant documents from a large collection. Once a training set of documents is labeled (as relevant or not), IR systems consume the document’s contents (mostly text) and use those data to predict whether a new document is relevant. In our previous discussion, the event of interest is when the true class of the outcome is “relevant.” The term “retrieved” is also used to define the identified documents, meaning the documents that have been predicted to be relevant (i.e., TP + FP).\nThe primary two metrics used in information retrieval are precision and recall. Precision is the proportion of relevant documents in the data set that have been identified:\n\\[\nPrecision = \\frac{\\text{relevant and retrieved}}{\\text{total retrieved}}=\\frac{TP}{TP + FP}\n\\tag{15.12}\\]\nFor example, in the validation set, our naive Bayes model has predicted that 3,177 mushrooms were predicted/identified to be poisonous using a 50% cutoff. Of these, 2,613 were truely poisonous. The ratio of these values produced a precision value of 74.4%.\nIt is sometimes said that precision is the same as the positive predicted value. However, this is only the case when the prevalence is 50% (i.e., balanced classes). For information retrieval, it is doubtful that the event rate would be so large (otherwise, documents would be very easily found).\nThe recall is the rate that identified documents are actually relevant:\n\\[\nRecall = \\frac{\\text{relevant and retrieved}}{\\text{total relevant}} = \\frac{TP}{TP + FN}\n\\tag{15.13}\\]\nThis is the same as sensitivity.\nThese two metrics are sometimes qualified by how many of the top K documents are found (e.g., “precision@K”).\nAlso related are F-measures, which are combinations of precision and recall. For example, the \\(F_1\\) score is the harmonic mean of precision and recall:\n\\[\nF_{1}=2\\left(\\frac{precision \\times recall}{precision + recall}\\right)\n\\tag{15.14}\\]\nMore generally, the F-score can be written as\n\\[\nF_{\\beta}=(1+\\beta_2)\\left(\\frac{precision \\times recall}{(\\beta^2 \\times precision) + recall}\\right)\n\\tag{15.15}\\]\nStatistics called gain (or sometimes lift) quantify the number of relevant data points found relative to chance. These depend on a probability cutoff defining which samples are retrieved. For a set of \\(n\\) samples, suppose that \\(n_{prev}\\) is the number expected to be relevant by chance. For some threshold \\(t\\), let \\(n_t\\) be the number of retrieved documents that are truly relevant (such that \\(Pr[event]\\ge t\\)). The gain at threshold \\(t\\) is then\n\\[\nGain_t = n_t / n_{prev}\n\\tag{15.16}\\]\nso that \\(Gain_0 = 1\\) and \\(Gain_1 = 0\\). We’ll consider this metric again in Section 15.7.6.\nIt is worth reiterating that precision, recall, \\(F_{\\beta}\\), and Gain serve a completely different goal than sensitivity and specificity. They are focused on finding events in the data, while sensitivity and specificity are concerned with overall model quality by balancing the two possible error types.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-cls-metrics-wts",
    "href": "chapters/cls-metrics.html#sec-cls-metrics-wts",
    "title": "15  Characterizing Classification Models",
    "section": "15.6 Weighted Performance Metrics",
    "text": "15.6 Weighted Performance Metrics\nTwo class performance metrics are convenient summaries and are generally interpretable. As we saw earlier in this chapter, the metrics of accuracy and Kappa naturally handle more than two classes. Because of the desirable characteristics of other two-class performance metrics, conventions have been constructed to extend these metrics (Lewis 1991). These conventions can be applied to any two-class metric. We will use sensitivity, S, to illustrate the conventions.\nThe first approach, macro averaging, computes \\(C\\) versions of the statistic using a “one-versus-all” approach. For example, when \\(C = 3\\), \\(S_1\\) is the sensitivity when the first class is considered the event and the second and third classes are pooled into the non-event category. \\(S_2\\) is the sensitivity when the second class is considered the event, and so on. The macro estimate of sensitivity is computed as their average:\n\\[\nS_{macro} = \\frac{1}{C}\\sum_{k=1}^C S_k\n\\tag{15.17}\\]\nwhere the \\(S_k\\) are the one-versus-all metrics.\nA naive average of sensitivities is appropriate when the classes are balanced. When the classes are unbalanced, an unweighted average will bias this metric towards the sensitivity of the largest class. To account for class imbalance, weighted macro averaging uses the class frequencies \\(n_k\\) as weights\n\\[\nS_{macro, wts} = \\sum_{k=1}^C \\frac{S_k}{n_k}\n\\tag{15.18}\\]\nFinally, micro averaging is more complex because it aggregates the ingredients that go into the specific calculations. Sensitivity, for example, requires the number of true-positive and false negatives. These are summed and used in the regular computation of sensitivity:\n\\[\nS_{micro} = \\frac{{\\displaystyle \\sum_{k=1}^C TP_k}}{\\left( {\\displaystyle \\sum_{k=1}^C TP_k} \\right) + \\left( {\\displaystyle \\sum_{k=1}^C FN_k} \\right) }\n\\tag{15.19}\\]\nNote that many performance metrics naturally work with three or more classes. There is little point in using these averaging tools for: accuracy, Kappa, MCC, ROC AUC, and others.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-cls-metrics-soft",
    "href": "chapters/cls-metrics.html#sec-cls-metrics-soft",
    "title": "15  Characterizing Classification Models",
    "section": "15.7 Evaluating Probabilistic Predictions",
    "text": "15.7 Evaluating Probabilistic Predictions\nAs previously stated, class probabilities have far more information than categorical class predictions. We’ll describe common metrics that use these quantitative estimates. A few of these utilize indicators for different classes. We’ll denote binary indicators for class \\(k = 1, \\ldots, C\\) and sample \\(i = 1, \\ldots, n\\) as \\(y_{ik}\\). For binary data, we use \\(k=1\\) to define the event of interest and \\(k=2\\) as the non-event so that \\(y_{i1} = 1\\) and \\(y_{i2} = 0\\).\n\n15.7.1 Cross-Entropy\nCross-entropy is one of the most commonly used metrics for evaluating class probability estimates. It evaluates the difference between two probability distributions. For classification models, it is the distance between the predicted probabilities (\\(\\hat{p}_{ik}\\)) and their true values. The latter terms are represented by their binary indicators (\\(y_{ik}\\)). The general equation is:\n\\[\nCE = -\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^Cy_{ik}\\log(\\hat{p}_{ik})\n\\tag{15.20}\\]\nwhere we assume that none of the estimated probabilities are zero. For the special case of \\(C=2\\) classes, it simplifies to:\n\\[\nCE = -\\frac{1}{n}\\sum_{i=1}^n \\left[y_{i1}\\log(\\hat{p}_{i1}) + y_{i2}\\log(1 - \\hat{p}_{i1})\\right]\n\\tag{15.21}\\]\nsince \\(\\hat{p}_{i2} = 1 - \\hat{p}_{i1}\\). A good model minimizes cross-entropy.\nThere is an alternative way to view this statistic. It turns out that if we assume the outcome data independently arise from a Bernoulli distribution with an event probability of \\(\\pi\\), the corresponding likelihood statistic is\n\\[\n\\ell(y_{ik}; \\pi) = \\prod_{i=1}^n \\pi^{y_{i1}} (1 - \\pi)^{y_{i2}}\n\\tag{15.22}\\]\nWe’ve described that, by maximizing the likelihood, we find values(s) of the parameter(s) that explain the data best. To improve numerical stability, we typically try to minimize the negative log-likelihood. Logging \\(\\ell(y_{ik}; \\pi)\\) and substituting our estimate \\(\\hat{p}_{i1}\\) for \\(\\pi\\), we also arrive at:\n\\[\n-\\log \\ell(y_{ik}; \\hat{p}_{i1}) = -\\sum_{i=1}^n \\left[y_{i1}\\log(\\hat{p}_{i1}) + y_{i2}\\log(1 - \\hat{p}_{i1})\\right]\n\\tag{15.23}\\]\nwhich is the same as cross-entropy. When there are more than two classes, the same equivalence can be shown to the multinomial log-likelihood. Often, with classification models, this statistic will be referred to as “log loss7”.\nCross-entropy can be an effective objective function for classification. On the theoretical side, it also has the characteristic of being a proper scoring rule; it is optimized at the true probability distribution. We’ll see a little more about this below when Brier scores are discussed.\nOne issue is that the scale of the value is not interpretable. For example, knowing that the cross-entropy value for our naive Bayes model is 0.455 does not obviously indicate whether that value is good or bad. We can determine what a poor value of cross-entropy is for this data set by repeatedly permuting the true class column of the validation set, recomputing the metrics, and taking the average. This gives us an estimate of the metrics when there is no connection between the observed classes and their predictions. Following this approach with 100 permutations, the baseline cross-entropy of a poor model was 0.551.\nAlso, there can be numerical issues when some probability estimates are at or near zero; \\(\\log(\\hat{p}_{ik})\\) can diverge quickly to a large negative number and is undefined as zero. The common correction for these issues is to cap the probability estimates to a very small number. This helps avoid \\(\\log(0)\\), but there is still the issue that extremes of the probability scale have more dramatic emphasis (and influence) on the metric.\n\n\n15.7.2 Brier Scores\nWe want our probability estimates to be as close as possible to their true values. For classification, we don’t know the true probability values; we only have the observed class and the corresponding indicators for each class level. Similar to regression, we can use a metric that measures the squared distance between the observed and predicted probabilities, \\(y_{ik}\\) and \\(\\hat{p}_{ik}\\), respectively. For a binary outcome, that metric is called the Brier score8 and has the form:\n\\[\nBrier = \\frac{1}{n}\\sum_{i=1}^n(y_{i1} - \\hat{p}_{i1})^2\n\\tag{15.24}\\]\nSee Brier (1950) and, more recently, Bella et al. (2013). A perfect model would estimate probabilities to be either 1.0 or 0.0, depending on the class, and would minimize this statistic. What would an ineffective model produce? While there is only one way to be perfect, there are many ways in which a model could produce poor estimates. Let’s say the model is completely equivocal and every prediction for this binary outcome is 0.5. In this case, we are summing repeated values of 0.52 so that the bad models would have scores \\(\\ge\\) 0.25. This serves as a good rule of thumb, but a better (but more complex) approach is via the permutation method previously described.\nLooking at our mushroom model, the Brier score was estimated as 0.1502 with 90% bootstrap confidence limits of (0.1452, 0.1553). The permutation estimate shows that a poor model would have an average score of 0.5202. While not perfect, the model can achieve a fairly small average squared error.\nIn some cases, Equation 15.24 is written with the components for both classes. In this case, the possible range of values and the rule of thumb value would be doubled. While Stanski, Wilson, and Burrows (1989) recommends dividing the score by the number of classes to maintain a range of [0, 1], there does not appear to be an established convention for this aspect of the Brier score. The more general equation for any number of classes is:\n\\[\nBrier = \\frac{1}{nC}\\sum_{i=1}^n\\sum_{k=1}^C (y_{ik} - \\hat{p}_{ik})^2\n\\tag{15.25}\\]\nBased on this equation, a more general version of our rule of thumb would be \\((1 - (1/C))^2\\) for the worst-case Brier score. For example, with ten class levels, an unsatisfactory model would have a score around 0.81. We’ll use this version of the Brier score so that values are always between zero and one.\nThis metric has some interesting qualities that make it attractive. As previously mentioned, a perfect model has a score of zero. If a performance metric achieves its smallest value when the probability estimate is perfect, it is called a proper scoring rule. There are theoretical benefits to using such a metric which Kruppa et al. (2014) describes succinctly:\n\nThe Brier score is a strictly proper score (Gneiting and Raftery 2007), which means that it takes its minimal value only when the true probabilities are inserted as predictions. Phrased differently, it is not possible to improve the Brier score by systematically predicting probability values other than the best estimate.\n\nAdditionally, the Brier score can be partitioned into more specific components that measure different aspects of the model fit (similar to the variance-bias decomposition from Section 8.4). Sanders (1963) split the Brier score into two components: “reliability” and “resolution.” This requires the probability estimates to have replicate values, i.e., the estimated probability values are not unique.\nLet’s demonstrate with a binary outcome (\\(C = 2\\)). Suppose out of \\(n\\) data points there are only \\(n_p\\) unique values of \\(\\hat{p}_{i1}\\). We’ll denoted these unique values as \\(\\hat{p}_{j}\\) (\\(j=1, \\ldots, n_p\\)) each occuring with frequency \\(n_j\\).\nEarlier, we remarked with dissatisfaction that the only representations we have of the true probability estimates are the binary indicators \\(y_{i1}\\). Now, we are in a more interesting situation: for each unique probability estimate, we have \\(n_j\\) values, and the mean of the \\(y_{i1}\\) is a higher resolution representation of the true values. This lets us get a little closer to a situation where we can contrast our estimates with the truth (to some degree). We’ll denote the mean of the \\(y_{i1}\\) indicators corresponding to probability \\(j\\) as \\(\\bar{y}_j\\).\nWe can then partition the Brier score into:\n\\[\n\\begin{align}\nBrier &= \\frac {1}{n_p} \\sum_{j=1}^{n_p}n_{j}(\\hat{p}_{j} - \\bar{y}_j)^2 + \\frac {1}{n_p} \\sum_{j=1}^{n_p}n_{j} \\bar{y}_j (1 - \\bar{y}_j) \\label{eq-brier-cal} \\\\\n&= \\text{reliability} + \\text{refinement} \\notag \\\\\n\\end{align}\n\\]\nReliability compares our more refined values of “truth” with our model’s estimates; this is as close as we can get to the previously stated goal of calibration. We should minimize this value.\nRefinement is interesting because it indirectly uses the model probability estimates (by contributing the \\(j\\) data groupings). The pattern \\(\\bar{y}_j (1 - \\bar{y}_j)\\) also drives the binomial variance and the Gini Index9. This pattern measures impurity. The best-case scenario is that the true outcome values for every unique model probability estimate are the same as possible. We want to minimize this term too. Murphy (1973) shows how to further divide refinement, and Yates (1982) compares the two decompositions.\nThis partition could be very helpful, except for the problem that most models produce nearly unique probability estimates. For example, there are 6,107 mushrooms in our validation set and 6,100 unique predictions. To show how the decomposition could be useful, we’ll artificially round our probabilities to the second decimal place, yielding 101 unique predictions. Figure 15.5 shows the \\(n_p\\) = 101 values for the reliability and refinement terms. On the left, we can see a strong linear relationship between the estimates and the \\(\\bar{y}_j\\). The calibration is more variable in the middle of the probability range, and there is some deviation from the diagonal line when the estimates are in the range of [0.1, 0.5] (a smoothed line is shown in blue).\n\n\n\n\n\n\n\n\nFigure 15.5: The data making up the two-component partition of the Brier score (reliability/calibration and refinement). The left panel plots \\(\\hat{p}_{j}\\) versus \\(\\bar{y}_j\\) and the right panels plots \\(\\hat{p}_{j}\\) versus \\(n_{j} \\bar{y}_j (1 - \\bar{y}_j)\\).\n\n\n\n\n\nThe right-hand panel shows the refinement data. The extreme ranges of the probability estimates are very pure. Just outside of the extremes, the values spike around 0.20 and 0.85.\nIf we can construct them, these diagnostic plots can help us determine where the model may lack correctness. We’ll see related plots in Section 15.8.\nFinally, compared to cross-entropy, the Brier score has a slight advantage in how extreme probabilities influence the overall statistic. Recall that the logarithm of the probabilities might be problematic when estimates are close to zero. The Brier score is less affected due to its use of squared terms. At most, an extreme probability estimate has a definitive limit to how much it can impact the overall score.\n\n\n15.7.3 Reciever Operating Characteristic Curves\nEarlier, we found that our naive Bayes model had a sensitivity of 77.7% and a specificity of 79.4%. Since a false negative might end up poisoning someone, we might want to increase our sensitivity to identify every poisonous mushroom we can. One way to do this is to lower the weight of evidence required to make a class prediction of “poisonous.” The sensitivity and specificity quoted above correspond to a rule where a 50% probability or more indicates poison. What if we were to loosen that threshold and make it 10% so that we classify as an event if \\(Pr[poisonous] \\ge 0.10\\)?\nIn doing this, we will designate many more mushrooms as poisonous and, as a result, the sensitivity increases to 97.4%. Now, we’re identifying almost all of the poisonous mushrooms. The consequence is that many of these newly classified mushrooms are actually edible. The specificity falls to 35.2%. That might be acceptable; we would probably want to discard perfectly edible mushrooms to avoid being poisoned.\nIf our model’s application was more focused on specificity, we could raise the threshold about 50%. In this case, we increase specificity but do harm to sensitivity (by designating fewer samples to be events).\n\nThe point of this exercise is to demonstrate that the default 50% cutoff for binary classification might not be satisfactory for our model’s application.\n\nThis presents some difficulty, though. We probably want to know the sensitivity and specificity to determine whether the model is good enough or compare it to others. Should we optimize the probability cutoff for each model as it is being developed? How would we pick an appropriate threshold?\nThe receiver operating characteristic (ROC) curve was designed to do just that. The points on the curve are created by looking at many cutoff points10 and track how the sensitivity and specificity change. The curve is created by plotting the true positive rate (i.e., sensitivity) on the y-axis and the false positive rate (one minus specificity) on the x-axis. Figure 15.6 shows this curve for our naive Bayes predictions where three cutoffs are highlighted: 10%, 50%, and 90%.\n\n\n\n\n\n\n\n\nFigure 15.6: Validation set ROC curves for the naive Bayes model. The symbols represent different probability thresholds.\n\n\n\n\n\nThe points along the curve help us make tradeoffs between sensitivity and specificity. For example, we might want to increase the sensitivity further. However, there is not much more vertical space above the point that identifies the 10% cutoff. In moving along the curve to the right, we gain a little more sensitivity, but the loss of specificity would result in far more false positives than we gain in finding true positives.\nThe ROC curve has a few more features that help with model development. The best-case model is one where the distributions of the predicted class probabilities are separated by the true classes. In this case, the ROC curve would start in the lower left corner and go directly to the upper left corner, where the sensitivity and specificity are both at 1.0. The curve would then directly move to the upper right corner. Since the ranges of the two axes are between zero and one, the area under the curve for this perfect model is 1.0.\nWhat if our model was incapable of learning anything from the data? Choosing a threshold for this ineffective model would not change the sensitivity and specificity, and, as a result, the curve would move along the dotted diagonal line shown in the figure11. In this situation, the area under the curve would be about 0.5. For Figure 15.6, the ROC AUC was 0.871.\nBased on these patterns, we can use the area under the ROC curve as an optimization metric to differentiate or rank different models (or tuning parameter candidates). This single metric value encapsulates all possible thresholds. This allows us to avoid finding an optimal cutoff for each model during development; we can wait until we have narrowed our choice of the model down to a few and then examine their ROC curves to determine the best sensitivity and specificity for the situation. We could show the different ROC curves in the same plot, and the curve that moves closer to the upper left corner is better than the others.\nOf course, there are disadvantages to summarizing an entire curve with a single number. Some models might have portions of their curves that show superiority to others, but not uniformly. If a particular section of the curve is specifically interesting, a partial area under the ROC curve can be computed (McClish 1989) and might offer a more selective metric.\nIt can also be difficult to conceptualize what the area under the curve means, how important the difference in AUCs is, and so on.\nThe ROC curve is defined for data sets where the outcome has two classes. However, Hand and Till (2001b) derived a method to calculate a single AUC when there are three or more classes. Hanley and McNeil (1982) had previously recognized that the ROC AUC has a form equivalent to the Wilcoxon statistic (and the Gini Index). Given this, Hand and Till derived a method to combine the AUC values of all pairwise ROC curves to find an overall metric. While it is possible to derive a standard error (and confidence intervals) of a single ROC curve, the authors suggest using bootstrap methods to estimate these statistics for multiclass ROC AUCs.\nFinally, the most critical information about ROC curves is that they measure how well we can separate the classes. It is very possible to have a high ROC AUC and a very poor Brier score; see Figure 15.2 for a previous example.\n\n\n15.7.4 Precision-Recall Curves\nLike the ROC curve, the PR curve plots the two statistics against one another, with recall being on the x-axis for all possible probability thresholds. Unlike the ROC curve, the threshold determines how many of the most likely items should be retrieved. Figure 15.7 shows the results for our example data set. We start the curve from the left-hand side where the threshold is zero, meaning that samples where the probability of the event is greater than zero (i.e., all of them) are selected. The recall is perfect since we have selected all of the relevant items. The precision is equal to the event rate since we have all the true and false positives.\nAs we increase the threshold, we select fewer items, and the recall decreases and eventually becomes zero at the left-hand side of the curve since no documents are selected. In contrast, the precision increases since the ratio of the selected true positives increases.\n\n\n\n\n\n\n\n\nFigure 15.7: A precision-recall curve, estimated with the validation set, for the naive Bayes model.\n\n\n\n\n\nNote that neither the precision or recall (Equations 15.12 and 15.13) directly use the true negative component of the confusion matrix. This reflects the focus of these two statistics on finding positives.\nWhile that the ROC curve has its optimal point in the top left, the optimal setting for PR curves is in the plot area’s top right. We can also summarize the PR curve using the under the curve, a value of one being the best. Based on Figure 15.7, the PR AUC was 0.899.\nHowever, note that the horizontal baseline value for the curve corresponds to the prevalence. The PR curve baseline, unlike ROC, moves with outcome distribution. This complicates the understanding of the AUC since the baseline section of the curve moves from data set to data set, and it may not be obvious just by looking at the AUC value.\n\n\n15.7.5 Comparing ROC and PR Curves\nMany resources both in social media, blog posts, and the literature (Ozenne, Subtil, and Maucort-Boulch 2015; Saito and Rehmsmeier 2015) make the claim that:\n\nPR curves should always be used for models with class imbalances and\nROC curves are ineffectual with rare events.\n\nMany of the arguments made about these effects are not correct. One assumption is that when there is a class imbalance, the goal is always to identify events in a population (i.e., information retrieval). Indeed, a curve based on metrics important to information retrieval will always be better at information retrieval than alternatives. However, there are many instances where we are interested in knowing that there is good class separation even though one class occurs infrequently.\nLet’s look at a real example. Pharmaceutical companies use machine learning to derisk drug candidates by predicting the probability that the potential drug has different types of toxicity. For example, Maglich et al. (2014) considered models using laboratory tests to predict whether some molecules can cause steroidal toxicity. Kuhn (2016) provides a summary of these types of predictive models.\nSuppose we have a sufficiently sized training set of relevant drug candidates and have used laboratory tests and other data to label them as “toxic” (the event of interest) or “non-toxic.” Let’s assume that the toxicity prevalence is around 1% in the population of molecules of interest.\nOnce deployed, a model would be used as follows: a medicinal chemist designs a potential drug in-silico (i.e., via a computer). They pass the chemical structure to a machine learning model that can predict the probability of being toxic.\nIn this instance, the key goal of the machine learning model is that it should be able to accurately distinguish toxic from non-toxic and that the predicted probabilities are well-calibrated. If there is any indication of toxicity, the chemist will be keenly interested in knowing how well the model can differentiate events from non-events12. The estimate of the true negative rate is just as important to the chemist as the false positive rate.\nThe context here is that the model is being used to screen new items for the event of interest. The model’s users want to be confident in its quality when events or non-events are predicted. The focus is not solely on finding events (individually or within a collection of items).\nIn this scenario, the quality of the model should be assessed using multiple metrics: the ROC and PR curves, as well as the Brier score. We’ll show a technique for blending metrics in ?sec-multi-objectives.\nLet’s also look at a counterexample where a PR curve has more utility than other metrics. Suppose we work for a retailer trying to find customers in their database who will most likely make a large purchase within a year. We can create a machine learning model to predict this event based on their demographics, previous purchases, and other predictors. We can apply this model to new customers. Still, the initial goal is to find the target customers from the existing data, so the company might send a promotional offer to incentivize them to purchase.\nWe are looking for customers in an existing database to find positives. There is less emphasis on accuracy on the truly negative customers; there isn’t much downside to offering a promotion to someone unlikely to act on it. Good discrimination isn’t very important, and neither is calibration if we will be choosing the top X% of most likely customers. In this instance, the PR curve is clearly more appropriate for the job.\nSome technical characteristics that relate the two curves to one another, gathered from Davis and Goadrich (2006):\n\nFor a specific data set, the ROC and PR curves use the same data points but highlight different aspects of them.\nWhen comparing two models, one ROC curve is uniformly better than the other only if this is also true with the PR curves.\nIf one model has the best ROC AUC, there is no guarantee that the same is true for the PR AUC.\n\nWhen choosing between these two curves, we should consider how aligned our goals are with the goal of the modeling problem.\n\n\n15.7.6 Gain Charts\nGain charts (a.k.a. lift charts, a.k.a. cumulative accuracy profiles) create a curve that modulates the probability threshold \\(t\\) described in Equation 15.16. If we are looking for relevant documents, we can determine how many (or what percentage of) our unknown samples we should retrieve to achieve our goals.\nFigure 15.8 shows the results where the x-axis is the percentage of data points retrieved, and the y-axis is the percentage of relevant retrieved samples. The grey background reflects the feasible region that contains possible curve values13.\nIn this example, the curve is close to the best possible results until roughly 25% of the samples have been retrieved. The dotted lines show that we would need to retrieve about 30% of the data set to acquire half of the truly relevant samples (on average)\n\n\n\n\n\n\n\n\nFigure 15.8: A gain curve based on the validation set. The dotted linear indicates the percentage of samples to query/acquire in order to capture 30% of the relevant samples.\n\n\n\n\n\nOne could summarize this curve by computing how much of the feasible region is covered by the curve.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-cls-calibration",
    "href": "chapters/cls-metrics.html#sec-cls-calibration",
    "title": "15  Characterizing Classification Models",
    "section": "15.8 Measuring and Improving Calibration",
    "text": "15.8 Measuring and Improving Calibration\nWe often talk about a model having good calibration, but, as noted by Lichtenstein, Fischhoff, and Phillips (1977), calibration is really a characteristic of a single prediction. A prediction is well-calibrated if the probability estimate is close to the true value. We have no way of knowing or proving this. We can conceptualize that, if we could repeatedly collect the data point, the rate at which the event occurs should match our prediction.\nAs seen in a previous section, the only numeric estimates we have for the true value are the binary indicators for each class (\\(y_{ik}\\)). Unfortunately, these values do not facilitate diagnostics to assess how well our model works for an individual data point14.\nThe primary diagnostic for classification models is the calibration plot of the entire data set (a.k.a. the reliability diagram). For two-class outcomes, this plot is usually created by ordering the data by the estimated probability of the event and binning these into groups (usually ten). From these groups, we can compute the event rate from the outcome data and plot that rate against the midpoint of the bins. The points should fall on a 45-degree diagonal line if the model is well-calibrated. Here, we are assessing statistics involving multiple data points and inferring whether the model is calibrated in general (i.e., not for individual points).\nFigure 15.9 shows this type of curve for two models. Panel (a) shows the validation results for our naive Bayes model. The points fall on a fairly straight line, demonstrating good overall calibration. Panel (b) has the same visualization for a boosted tree model limited to only three trees in the ensemble. The result is a curve that is not close to the diagonal. The points on the lower left indicate that the predicted probabilities are much larger than they should be. The opposite occurs in the upper right of panel (b); the probabilities are generally underestimated when compared to the rate in the groups.\n\n\n\n\n\n\n\n\nFigure 15.9: The validation set calibration curves for a naive Bayes model (a) and a sabotaged boosted tree (b). The grey marks on the bottom and top frames of the plots are “rugs” and show where the actual values are; the rug on the top corresponds to the poisonous samples, and the marks on the bottom are for the edible mushrooms.\n\n\n\n\n\nThe downside to this binning approach is that, for small data sets, the results can have very large variability, or there may not be enough data within each bin to make accurate judgments regarding calibration. One alternative is to use moving windows. For example, for a fixed bin width of 0.20, we compute the required statistics and then move the bin by a small amount (say 0.02). This would result overlapping groups that would slide across the range of probability values. Figure 15.10(a) shows the results for the naive Bayes model. This visualization is more capable of demonstrating more nuance in the results.\n\n\n\n\n\n\n\n\nFigure 15.10: Other types of calibration curves demonstrated using the naive Bayes model results.\n\n\n\n\n\nAnother alternative is to fit a model to the data. Logistic regression is a relatively simple classification model for binary outcomes and is discussed in Section 16.2. We can fit this model using our observed outcomes and use the estimated class probabilities (\\(\\hat{p}_i\\)) as the predictor. If the probabilities are well-calibrated, the model should show a linear relationship. Another feature that can be used in this diagnostic is to model the predictor with a smoothing spline. This enables the logistic model to reflect where in the data range the calibration is very poor. Figure 15.10(b) illustrates the same pattern as the moving window but uses all the data simultaneously.\nWhat can be done with poorly calibrated predictions? There are approaches for postprocessing the predicted probabilities to have higher fidelity to the observed data. The customary method is to use a logistic regression fit, as seen in Figure 15.10(b), to recalibrate the predictions. First a suitable logistic regression model is created in the same manner as the one used for diagnosis. Recall that the original estimates are used as predictors of the model. To recalibrate a set of predictions, the original values are pushed through the model, and the resulting logistic regression values are used to create new probability predictions.\nPlatt (1999) proposed using a nonstandard logistic regression to convert any quantitative score into a probability-like value. Platt scaling also adjusts the class indicators from binary 0/1 values to values slightly different from those values.\nThis raises questions about which data should be used to fit the calibration model and which to evaluate its efficacy. We’ve previously mentioned that using simple repredictions of the training set is a very bad idea, and that is especially true here. If the model was resampled, the out-of-sample predictions can be used to fit the calibration model. In our example, a single validation set was used.\nWe were able to set aside 6,107 samples for the calibration set, and we will use these to fit the logistic model. Figure 15.11 shows the results where panel (a) uses a sliding window to assess the smaller data set using a window size of 20% that moves in 5% increments. From these figures, the class probabilities have been improved.\n\n\n\n\n\n\n\n\nFigure 15.11: The validation set calibration curves for naive Bayes model after recalibration using a logistic regression fit to the calibration set. Different data were used to recalibrate the data and to assess how well the procedure worked.\n\n\n\n\n\nUsing this approach to calibration, the new probability estimates have a Brier score of 0.1465. As shown in Table 15.3, this is slightly less than the uncalibrated score. However, since the uncalibrated and calibrated predictions are so highly correlated, the 90% intervals do not overlap with zero; the difference is real but not large enough to be seen as important.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrier Score\nDifference\n90% Interval\n\n\n\n\nUncalibrated\n0.1502\n—\n—\n\n\nLogistic Calibration\n0.1465\n0.00234\n(0.00234, 0.00493)\n\n\nIsotonic Boot. Calibration\n0.1474\n0.00170\n(0.0017, 0.00391)\n\n\n\n\n\n\n\n\n\nTable 15.3: Validation set Brier scores for the original model predictions and calibrated versions. The differences are the uncalibrated values minus the calibrated values.\n\n\n\n\n\n\n\nThere are other approaches for improving calibrating. One is isotonic regression (Zadrozny and Elkan 2002). This tool fits a model that will generate monotonic, non-decreasing, predictions: \\(\\hat{y}_i \\le \\hat{y}_{i+ 1}\\) when \\(x_i \\le x_{i+ 1}\\). Multiple methods exist to create such models but the most straightforward approach is the pooled-adjacent violators algorithm (PAVA) of Miriam et al. (1955). This method adjusts a sequence of data points that are not monotonically increasing to be so. For example, Table 15.4 shows an example where the rows are ordered by their model’s predicted probabilities. One column shows a binary indicator for class A. The first three rows have the same outcome (B) but the fourth row is actually class A. The PAVA algorithm then takes the averages of the binary indicators that bracket the discordant value, and all of these probability estimates are given the average value (1/3). From the table, it is clear that there are three possible probability estimates after isotonic regression is used.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated Probability of Class A\n\n\n\nTruth\nBinary\nModel\nIsotonic\n\n\n\n\nB\n0\n0.00\n0.00\n\n\nB\n0\n0.01\n0.00\n\n\nB\n0\n0.10\n0.00\n\n\nA\n1\n0.20\n0.33\n\n\nB\n0\n0.40\n0.33\n\n\nB\n0\n0.50\n0.33\n\n\nA\n1\n0.60\n1.00\n\n\nA\n1\n0.70\n1.00\n\n\nA\n1\n0.80\n1.00\n\n\nA\n1\n0.90\n1.00\n\n\n\n\n\n\n\nTable 15.4: An example of calculations for calibration via isotonic regression.\n\n\n\n\n\n\n\nThe nice feature of this tool is that it is nonparametric and makes almost no distributional assumptions. However, one downside is that there are few unique recalibrated probability estimates. For this reason, isotonic regression is advised for large data sets. For example, for the naive Bayes data used for calibration, there were originally 6,100 unique probability estimates while, after isotonic regression is applied, there are only 34 possible values. Figure 15.12(a) shows the calibration curve after isotonic regression is applied.\nWe can improve the isotonic regression results using bagging. As discussed later in Section 19.1, bagging creates an ensemble of models by taking \\(B\\) bootstrap samples to build \\(B\\) discinct models. The final model prediction is the average of the \\(B\\) model predictions. For isotonic regression, this reduces the variability in the predicted (i.e., recalibrated) probabilities and also increases the number of unique probability values. Using 1,000 bootstrap samples, we have 940 unique probability estimates.\n\n\n\n\n\n\n\n\nFigure 15.12: Validation set calibration curves for naive Bayes model after recalibration using isotonic regression from a calibration set. Both panels show moving 20% windows.\n\n\n\n\n\nFigure 15.12(b) shows the ensemble results and Table 15.3 contains the Brier scores. The curve is slightly closer to the line, and there is far less “wiggle” around the values. The bootstrapping method reduces the Brier score from 0.15 to 0.1474. Like logistic calibration, the different is statistically significant but trival.\nThere are many other tools in the calibration toolbox, such as beta calibration (Kull, Filho, and Flach 2017). There are also methods for calibrating classification models with three or more classes (Zadrozny 2001; Zhao et al. 2021; Johansson, Lofstrom, and Bostrom 2021).\nIt is important to recompute all metrics using recalibrated predictions. Also note that since the class probabilities have been altered, any hard predictions should be recomputed since some values may have moved across the chosen probability threshold. Using the set of 6,107 data points that have been recalibrated, the reestimated sensitivity and specificity were 77.7% and 79.4%, respectively and the area under the ROC curve was 0.87.\n\nThe user should not set expectations very high; recalibration methods often have limited ability to correct predictions. In our example, a large amount of data was set aside for calibration, and we were able to fix a minor issue with the original model’s predictions. In most cases, recalibration might only be able to fix gross miscalibration trends.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-ordered-categories",
    "href": "chapters/cls-metrics.html#sec-ordered-categories",
    "title": "15  Characterizing Classification Models",
    "section": "15.9 Ordered Categories",
    "text": "15.9 Ordered Categories\nAs previously mentioned, our outcome classes can have an inherent ordering. Since there is an ordering, it might be reasonable to think that misclassification severity relates to how “far” a predicted class is from its true class. For example, risk analysis systems, such as Failure Mode, Effects, and Criticality Analysis (US Department of Defense 1980), involve enumerating different ways a thing can fail and the corresponding consequences. Suppose that we have an ordinal scale for risk consisting of “low,” “moderate,” “severe,” and “catastrophic.” It is reasonable to think that predicting the risk is moderate when it is truly catastrophic is very bad. Predicting a moderate risk to be low may not be as dire.\nTo demonstrate, let’s return to the Wordle data. We can treat the outcome as ordinal since the number of tries has a natural ordering. Using the data associated with Figure 15.1, we assembled data from a single user for 467 days. These data were modeled with different sets of predictors computed from the letters: counts of each letter (in the word, first letter, last letter, etc), indicators for double letters, and common bi-grams. A set of nine popular starter words was also used to compute a similarity score for each daily word. They are used to predict how uncommon the word might be and would probably influence the number of tries. The distribution of the outcome was: two tries (\\(n\\) = 34), three tries (\\(n\\) = 137), four tries (\\(n\\) = 181), five tries (\\(n\\) = 85), six tries (\\(n\\) = 25), and unsuccessful (\\(n\\) = 5). A 3:1 training/testing split was used to partition these data, and different models were optimized for predicting the number of tries. The left-hand side of Table 15.5 shows a test set confusion matrix for a Wordle model for a RuleFit model (described in Section 19.5.2). Note that no unsolved words (i.e., class “X”) were in the test set partition.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTruth\n\n\n\nLinear Weight 'Errors'\n\n\n\nPrediction\n\n2\n3\n4\n5\n6\nX\n\n2\n3\n4\n5\n6\nX\n\n\n\n\n2\n\n10\n2\n0\n0\n0\n0\n\n0\n1\n2\n3\n4\n5\n\n\n3\n\n0\n28\n0\n4\n0\n0\n\n1\n0\n1\n2\n3\n4\n\n\n4\n\n0\n7\n43\n2\n0\n0\n\n2\n1\n0\n1\n2\n3\n\n\n5\n\n0\n0\n2\n13\n3\n0\n\n3\n2\n1\n0\n1\n2\n\n\n6\n\n0\n0\n3\n0\n1\n0\n\n4\n3\n2\n1\n0\n1\n\n\nX\n\n0\n0\n0\n0\n0\n0\n\n5\n4\n3\n2\n1\n0\n\n\n\n\n\n\n\nTable 15.5: A six-class ordinal outcome with a confusion matrix (left) and a linear set of weights for each potential error (right).\n\n\n\n\n\n\n\nAbout 81% of the class predictions mostly fall along the diagonal line of the confusion matrix. The classes corresponding to three, four, and five tries had appreciable errors.\nThere are metrics that use hard classification predictions from ordinal models. We previously mentioned the Kappa statistic. To penalize misclassification errors based on their distance to the true classification, Cohen (1968) developed the weighted Kappa statistic, where the errors’ weights depend on their positional differences.\nFirst, let’s rewrite the Kappa statistic definition from Equation 15.3 in terms of classification error (i.e., the opposite of accuracy). We can compute the error as the number of off-diagonal counts divided by the total count. For expected errors, a similar approach to Equation 15.4 can be used to find the amount of chance error. With these two types of error in hand, Kappa is:\n\\[\nKappa = 1 - \\frac{error}{expected\\:error}\n\\tag{15.26}\\]\nWeighted Kappa assigns larger costs (i.e., errors) to cells in the confusion matrix that are “farther away” from the true category. A linear weighting for the Kappa statistic is shown in Table 15.5. There are bands of diagonal integer weights that are 0, 1, 2, \\(\\ldots\\), 5. A quadratic weighting scheme would square each of these weights (e.g., 0, 1, 22, 32, etc.).\nFor this test set, the estimate of the unweighted Kappa statistic was 0.722, which is outstanding. If we increase the penalty for more disparate errors, the performance values for linear and quadratic weights were 0.75 and 0.791, respectively. Notice that the Kappa values increase as the weights become more severe. In these data, the “cost” of the errors increases faster in the expected agreement than in the observed agreement. This is not often the case.\nNakov et al. (2019) suggested ordinal versions of common regression metrics, such as the mean squared error (MSE). These use the positional “errors” between the true and predicted outcomes. For example, if a word requires four tries, but we predict the puzzle will be solved with two tries, the error is -2. These metrics should be fairly understandable since they mimic common statistics used to evaluate regression models. However, there has been scant evidence that they offer a benefit over existing metrics that have been studied and evaluated for decades. The mean squared error for these data was 0.611. How bad is this? If we repeatedly permute the outcome classes and measure the RMSE we find that the worst-case scenario is about 1.34.\nFor probability predictions, there is an extension to the Brier score called the ranked probability score (RPS). See Epstein (1969) and Constantinou and Fenton (2012). The computations are very similar to Equation 15.25 but use different data:\n\\[\nRPS = \\frac{1}{n(C-1)}\\sum_{i=1}^n\\sum_{k=1}^C (\\tilde{y}_{ik} - \\tilde{p}_{ik})^2\n\\tag{15.27}\\]\nInstead of using a binary value \\(y_{ik}\\) corresponding to class \\(k\\) only, the RPS assigns a value of 1 to \\(\\tilde{y}_{ik}\\) if the position of the true class is \\(\\le k\\). Similarly, the value of \\(\\tilde{p}_{ik}\\) is the cumulative sum of the estimated probabilities corresponding to classes \\(\\le k\\). For example, with the Wordle data, \\(k = 3\\) corresponds to four tries and \\(\\tilde{y}_{i3}\\) would have a value of 1.0 if the ith sample was solved in two, three, or four tries (and is zero otherwise). The corresponding cumulative probability is the sum of the individual probabilities of solving the puzzle in two, three, or four tries. The RPS inherits the same benefits as the Brier score since they are both proper scoring rules. See Stael von Holstein (1970) and Gneiting and Raftery (2007).\nThe denominator is normalized by dividing by \\(C-1\\). The minus one compensates for the fact that all predictions are perfect for the largest class level (since they are cumulative).\nThe ranked probability score measures the correctness (and calibration) of the question: “How well am I predicting that the event occurs in \\(k\\) or fewer classes?” RPS can be more forgiving of inaccuracies in the individual probability estimates as long as their sum is accurate. See Murphy (1970) for a comparison of these two flavors of the Brier score.\nThe test set probabilities associated with the test set were used to compute the Brier score estimate and the ranked probability score value. The RPS estimate was 0.0458 and its corresponding permuted “no information” estimate was 0.149. This indicates that the model functions effectively when evaluating the cumulative probability estimates. For comparison, the unordered Brier score for these data was 0.162 (with permuted value 0.476).",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#sec-cls-multi-objectives",
    "href": "chapters/cls-metrics.html#sec-cls-multi-objectives",
    "title": "15  Characterizing Classification Models",
    "section": "15.10 Multi-Objective Assessments",
    "text": "15.10 Multi-Objective Assessments\nChoosing a single metric that captures all of the model’s goals may be challenging. When hard class predictions are important, the choice of the probability cutoff should be driven by measures such as sensitivity, specificity, or precision. We might want to optimize a function of multiple metrics (e.g., the ratio of sensitivity and specificity, etc.). Additionally, these metrics don’t consider model calibration and other aspects of the model.\nSometimes, a joint metric like the F-score can be used if it aligns with the project goals. Similarly, the J-index (Youden 1950) might also suffice:\n\\[\nJ = Sensitivity + Specificity - 1\n\\]\nAdditionally, we might want to factor in the characteristics of our trained model. For example, we’ve previously seen that the number of active features in the model can help us better choose which candidate to select. As another example, if we choose between different tuning parameters in a neural network, we might be interested in seeing how the total number of model parameters affects the primary performance metric. There might be an advantageous trade-off between model complexity and performance.\nA more advanced multiparameter optimization may be required to differentiate models. Many different methods for accomplishing this are well summarized in Karl et al. (2023). If we have \\(s\\) candidate models, some approaches require the full set of metrics and characteristics to decide (e.g., Pareto optimality). Alternatively, we might be more interested in developing a single objective function that combines metrics in a meaningful way that can measure a single candidate. Iterative optimization (or racing methods) would require a combined performance metric in real time.\nWe’ll focus on one method to blend multiple criteria into a single composite value called “desirability functions.” Originally conceived by Harrington (1965), the versions of desirability functions defined by Derringer and Suich (1980) are more commonly used. They are extensively used in statistical experimental design but also in a wide range of applications, including designing cancer combination therapy (Kuhn, Carter, and Myers 2000), medicinal chemistry (Wager et al. 2010), wastewater processing (Bobadilla et al. 2019), and others. These tools are knowledge-based; users must state specific values of their criteria that are desirable or unacceptable.\nTo optimize ML models, desirability functions can be defined for any important facet of the model result. For example, we could articulate that we should minimize the Brier score such that\n\nFewer than 20 active predictors are used.\nSpecificity is at least 80%\nSensitivity is maximized\n\nIn each of these cases, we can define ranges of acceptable results (e.g., Brier \\(\\in [0, .1)\\), sensitivity \\(\\in (0.8, 1.0]\\), etc.)\nDesirability functions map values to a unit range where a value of 1.0 is the most desirable situation while a value of zero is a dealbreaker. Figure 15.13 shows an example mapping for each of the four hypothetical characteristics given above. Three of the four mappings are linear between the best and worst cases, and the other is a “box” constraint that indicates that any value in the range is acceptable.\n\n\n\n\n\n\n\n\nFigure 15.13: Examples of desirability functions for four characteristics.\n\n\n\n\n\nThe Brier score and active features use a minimization strategy that is encoded as\n\\[\nd_i=\n\\begin{cases}\n    0       &\\text{if $x&gt; B$}   \\\\\n    \\left(\\frac{x-B}{A - B}\\right)^{\\gamma}     \n            &\\text{if $A \\leq x \\leq B$}    \\\\\n    1       &\\text{if $x&lt;A$}\n\\end{cases}\n\\tag{15.28}\\]\nwhere \\(A\\) and \\(B\\) are the low and high values, respectively. The \\(\\gamma\\) parameter is a scaling factor that modulates of easy (or difficult) it should be to satisfy the goal. The curves in Figure 15.13 use the default value of \\(\\gamma = 1\\), producing straight lines between the limits. Values greater than one make desirability more easily satisfied, and smaller values will result in individual desirability having more impact on the overall optimization.\nThe sensitivity curve in Figure 15.13 is meant to maximize the characteristic using the equations:\n\\[\nd_i=\n\\begin{cases}\n    0       &\\text{if $x &lt; A$}  \\\\\n    \\left(\\frac{x-A}{B-A}\\right)^{\\gamma}       \n            &\\text{if $A \\leq x \\leq B$}    \\\\\n    1       &\\text{if $x&gt;B$}\n\\end{cases}\n\\tag{15.29}\\]\nDifferent types of desirability functions exist for different goals (box constraints, qualitative values, etc.).\nOnce we have defined a function for each of the \\(q\\) important characteristics, the overall desirability is a combination of each using a geometric mean:\n\\[\nD = \\left(\\prod_{i=1}^q d_j\\right)^{1/q}\n\\tag{15.30}\\]\nAfter these computations, the candidates can be ranked by \\(D\\). Note that the geometric mean has the effect that if any single characteristic has an unacceptable value (i.e., zero), the overall combination is unacceptable.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#chapter-references",
    "href": "chapters/cls-metrics.html#chapter-references",
    "title": "15  Characterizing Classification Models",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nBella, A, C Ferri, J Hernandez-Orallo, and Maria J Ramirez-Quintana. 2013. “On the Effect of Calibration in Classifier Combination.” Applied Intelligence 38 (4): 566–85.\n\n\nBobadilla, C, R Lorza, R García, F Gómez, and E González. 2019. “Coagulation: Determination of Key Operating Parameters by Multi-Response Surface Methodology Using Desirability Functions.” Water 11 (2): 398.\n\n\nBridle, J. 1990. “Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition.” In Neurocomputing: Algorithms, Architectures and Applications, 227–36. Springer-Verlag.\n\n\nBrier, G. 1950. “Verification of Forecasts Expressed in Terms of Probability.” Monthly Weather Review 78 (1): 1–3.\n\n\nClopper, C, and E Pearson. 1934. “The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial.” Biometrika 26 (4): 404–13.\n\n\nCohen, J. 1960. “A Coefficient of Agreement for Nominal Data.” Educational and Psychological Measurement 20: 37–46.\n\n\nCohen, J. 1968. “Weighted Kappa: Nominal Scale Agreement Provision for Scaled Disagreement or Partial Credit.” Psychological Bulletin 70 (4): 213.\n\n\nConstantinou, A, and N Fenton. 2012. “Solving the Problem of Inadequate Scoring Rules for Assessing Probabilistic Football Forecast Models.” Journal of Quantitative Analysis in Sports 8 (1).\n\n\nDavis, J, and M Goadrich. 2006. “The Relationship Between Precision-Recall and ROC Curves.” In Proceedings of the 23rd International Conference on Machine Learning, 233–40.\n\n\nDelgado, R, and XA Tibau. 2019. “Why Cohen’s Kappa Should Be Avoided as Performance Measure in Classification.” PloS One 14 (9): e0222916.\n\n\nDerringer, G, and R Suich. 1980. “Simultaneous Optimization of Several Response Variables.” Journal of Quality Technology 12 (4): 214–19.\n\n\nEkstrom, J. 2011. “The Phi-Coefficient, the Tetrachoric Correlation Coefficient, and the Pearson-Yule Debate.” UCLA Department of Statistics. https://escholarship.org/uc/item/7qp4604r.\n\n\nEpstein, E. 1969. “A Scoring System for Probability Forecasts of Ranked Categories.” Journal of Applied Meteorology (1962-1982) 8 (6): 985–87.\n\n\nGneiting, T, and A Raftery. 2007. “Strictly Proper Scoring Rules, Prediction, and Estimation.” Journal of the American Statistical Association 102 (477): 359–78.\n\n\nGuggenmoos-Holzmann, I. 1996. “The Meaning of Kappa: Probabilistic Concepts of Reliability and Validity Revisited.” Journal of Clinical Epidemiology 49 (7): 775–82.\n\n\nHand, D, and R Till. 2001b. “A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.” Machine Learning 45 (2): 171–86.\n\n\nHand, D, and R Till. 2001a. “A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.” Machine Learning 45: 171–86.\n\n\nHanley, J, and B McNeil. 1982. “The Meaning and Use of the Area Under a Receiver Operating (ROC) Curvel Characteristic.” Radiology 143 (1): 29–36.\n\n\nHarrington, E. 1965. “The Desirability Function.” Industrial Quality Control 21 (10): 494–98.\n\n\nJohansson, U, T Lofstrom, and H Bostrom. 2021. “Calibrating Multi-Class Models.” In Proceedings of the Tenth Symposium on Conformal and Probabilistic Prediction and Applications, edited by L Carlsson, Z Luo, G Cherubin, and K An Nguyen, 152:111–30. Proceedings of Machine Learning Research.\n\n\nKarl, F, T Pielok, J Moosbauer, F Pfisterer, S Coors, M Binder, L Schneider, et al. 2023. “Multi-Objective Hyperparameter Optimization in Machine Learning—an Overview.” ACM Transactions on Evolutionary Learning and Optimization 3 (4): 1–50.\n\n\nKruppa, J, Y Liu, HC Diener, T Holste, R Weimar, I Konig, and A Ziegler. 2014. “Probability Estimation with Machine Learning Methods for Dichotomous and Multicategory Outcome: Applications.” Biometrical Journal 56 (4): 564–83.\n\n\nKuhn, M. 2016. “Nonclinical Statistics for Pharmaceutical and Biotechnology Industries.” In, edited by L Zhang, 141–55. Springer International Publishing.\n\n\nKuhn, M, WH Carter, and R Myers. 2000. “Incorporating Noise Factors into Experiments with Censored Data.” Technometrics 42 (4): 376–83.\n\n\nKull, M, T Filho, and P Flach. 2017. “Beta Calibration: A Well-Founded and Easily Implemented Improvement on Logistic Calibration for Binary Classifiers.” In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, edited by A Singh and J Zhu, 54:623–31. Proceedings of Machine Learning Research.\n\n\nLewis, D. 1991. “Evaluating Text Categorization.” In Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California.\n\n\nLichtenstein, S, B Fischhoff, and L Phillips. 1977. “Calibration of Probabilities: The State of the Art.” In Decision Making and Change in Human Affairs: Proceedings of the Fifth Research Conference on Subjective Probability, Utility, and Decision Making, 275–324. Springer.\n\n\nMaglich, J, M Kuhn, R Chapin, and M Pletcher. 2014. “More Than Just Hormones: H295R Cells as Predictors of Reproductive Toxicity.” Reproductive Toxicology 45: 77–86.\n\n\nMatthews, B. W. 1975. “Comparison of the Predicted and Observed Secondary Structure of T4 Phage Lysozyme.” Biochimica Et Biophysica Acta (BBA) - Protein Structure 405 (2): 442–51.\n\n\nMcClish, D. 1989. “Analyzing a Portion of the ROC Curve.” Medical Decision Making 9: 190–95.\n\n\nMeeker, W, G Hahn, and L Escobar. 2017. Statistical Intervals: A Guide for Practitioners and Researchers. Vol. 541. John Wiley & Sons.\n\n\nMiriam, A, H Brunk, G Ewing, W Reid, and E Silverman. 1955. “An Empirical Distribution Function for Sampling with Incomplete Information.” The Annals of Mathematical Statistics 26 (4): 641–47.\n\n\nMurphy, A. 1970. “The Ranked Probability Score and the Probability Score: A Comparison.” Monthly Weather Review 98 (12): 917–24.\n\n\nMurphy, A. 1973. “A New Vector Partition of the Probability Score.” Journal of Applied Meteorology and Climatology 12 (4): 595–600.\n\n\nNakov, P, A Ritter, S Rosenthal, F Sebastiani, and V Stoyanov. 2019. “SemEval-2016 Task 4: Sentiment Analysis in Twitter.” arXiv.\n\n\nNewcombe, R. 1998. “Two-Sided Confidence Intervals for the Single Proportion: Comparison of Seven Methods.” Statistics in Medicine 17 (8): 857–72.\n\n\nOzenne, B, F Subtil, and D Maucort-Boulch. 2015. “The Precision–Recall Curve Overcame the Optimism of the Receiver Operating Characteristic Curve in Rare Diseases.” Journal of Clinical Epidemiology 68 (8): 855–59.\n\n\nPlatt, J. 1999. “Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.” Advances in Large Margin Classifiers 10 (3): 61–74.\n\n\nSaito, T, and M Rehmsmeier. 2015. “The Precision-Recall Plot Is More Informative Than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets.” PloS One 10 (3): e0118432.\n\n\nSanders, F. 1963. “On Subjective Probability Forecasting.” Journal of Applied Meteorology and Climatology 2 (2): 191–201.\n\n\nStael von Holstein, CA. 1970. “A Family of Strictly Proper Scoring Rules Which Are Sensitive to Distance.” Journal of Applied Meteorology and Climatology 9 (3): 360–64.\n\n\nStanski, H, L Wilson, and W Burrows. 1989. “Survey of Common Verification Methods in Meteorology.” World Meteorological Organization.\n\n\nThompson, D, and S Walter. 1988. “A Reappraisal of the Kappa Coefficient.” Journal of Clinical Epidemiology 41 (10): 949–58.\n\n\nUS Department of Defense. 1980. Military Standard: Procedures for Performing a Failure Mode, Effects, and Criticality Analysis (MIL-STD-1629A).\n\n\nWager, T, X Hou, P Verhoest, and A Villalobos. 2010. “Moving Beyond Rules: The Development of a Central Nervous System Multiparameter Optimization (CNS MPO) Approach to Enable Alignment of Druglike Properties.” ACS Chemical Neuroscience 1 (6): 435–49.\n\n\nWagner, D, D Heider, and G Hattab. 2021. “Mushroom Data Creation, Curation, and Simulation to Support Classification Tasks.” Scientific Reports 11 (1): 8134.\n\n\nYates, F. 1982. “External Correspondence: Decompositions of the Mean Probability Score.” Organizational Behavior and Human Performance 30 (1): 132–56.\n\n\nYouden, W. 1950. “Index for Rating Diagnostic Tests.” Cancer 3 (1): 32–35.\n\n\nZadrozny, B. 2001. “Reducing Multiclass to Binary by Coupling Probability Estimates.” In Advances in Neural Information Processing Systems, edited by T Dietterich, S Becker, and Z Ghahramani. Vol. 14. MIT Press.\n\n\nZadrozny, B, and C Elkan. 2002. “Transforming Classifier Scores into Accurate Multiclass Probability Estimates.” In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 694–99. New York, NY, USA: Association for Computing Machinery.\n\n\nZhao, S, M Kim, R Sahoo, T Ma, and S Ermon. 2021. “Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration.” In Advances in Neural Information Processing Systems, edited by M Ranzato, A Beygelzimer, Y Dauphin, P Liang, and J Wortman Vaughan, 34:22313–24. Curran Associates, Inc.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-metrics.html#footnotes",
    "href": "chapters/cls-metrics.html#footnotes",
    "title": "15  Characterizing Classification Models",
    "section": "",
    "text": "https://www.nytimes.com/games/wordle/index.html↩︎\nThis author’s starter word is caret - it contains two common vowels and three important consonants.↩︎\nIf first guesses were random, this rate is 6-fold higher than is expected by chance. Even though initial guesses are not random, the initial guess rate is implausibly high.↩︎\nThis is the same as Shannon entropy and “information content”.↩︎\nClass imbalances can be difficult to overcome. ?sec-imbalances will discuss different approaches to alleviating their effects.↩︎\nThis data set is very easy to predict. Other models, even simple decision trees, can produce nearly perfect results. We’ll use it here with a less powerful model for illustration purposes. The gravity of incorrectly predicting a poisonous mushroom to be edible makes discussing certain errors more compelling.↩︎\nThis does lead to confusion since the loss function in question, the likelihood, is different for different data types (e.g., real numbers, counts, etc.). In linear regression, ”log loss” also corresponds to the sum of squared errors.↩︎\nAlso called “mean probability score.”↩︎\nThere is also a direct connection between refinement and the area under the ROC curve. See Hand and Till (2001a).↩︎\nUsually all possible cutoffs.↩︎\nDue to randomness, it might have some area under the diagonal line.↩︎\nIt is not ideal to be in the position of having to justify a model that just gave someone bad news about their favorite thing. This situation makes one appreciate the crucial importance of very good documentation about the model.↩︎\nThe curve can fall slightly lower than the lower diagonal boundary by chance.↩︎\nThis will not be the case when assessing calibration for regression models because our outcome is in the same numerical format as the prediction.↩︎",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Characterizing Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/cls-linear.html",
    "href": "chapters/cls-linear.html",
    "title": "16  Generalized Linear and Additive Classifiers",
    "section": "",
    "text": "16.1 Exploring Forestation Data\nWe often conceptualize classification models by the type of class boundary they produce. For example, in Figure 16.1, two predictors are visualized, and the colors and shapes of the data indicated their class memberships. The class boundaries are visualized as black lines, dividing the data into two (or more) regions where the model predicted a specific class. When there are more than two predictors, visualizing this boundary becomes impossible, but we still use the idea of a line that demarcates different regions that have the same (hard) class prediction.\nThe low-complexity model attempts to bisect the two classes using a simple straight line. Points to the northwestern side of the line are classified as rose-colored triangles, and those on the opposite side are green circles. The background colors in the plot represent the estimated probabilities. This model underperforms since there are a high number of triangles on the wrong side of the line. The medium complexity model divides the data using a boundary that has three straight line segments. These linear segments result in a nonlinear boundary function. It also does not do a perfect job of predicting the data, but it has adapted to the data more than the simple linear boundary. The high complexity boundary uses the same underlying model as the medium complexity fit, but a tuning parameter was increased to allow the model to adapt more to the training data (perhaps a little too much). As a result, there are four distinct regions that are produced by these boundaries. Unfortunately, the model is overfits to the training set. To address this, the tuning parameter in question should be better optimized to balance complexity and overfitting.\nThis chapter focuses on models that, at first appearance, produce strictly linear class boundaries. Apart from feature engineering, they tend to be relatively fast to train, and are more likely to be interpretable due to their simplicity. We’ll start by discussing the most used classification model: logistic regression. This model has many important aspects to explore, as well as numerous ways to estimate model parameters. An extension of this model for more than two classes, a.k.a. multinomial regression, is also described. Finally, we review generalized additive models (GAMs) that feature spline basis functions.\nHowever, before diving into modeling techniques, let’s take a deep look at the Washington State forestation data originally introduced in Section 3.9. These data will be used to demonstrate the nuances of different classifications from this chapter through Chapter 19.\nThese data have been discussed in Sections 3.9, 10.8, and 13.4. As a refresher, locations in Washington state were surveyed, and specific criteria were applied to determine whether they were sufficiently forested. Using predictors on the climate, terrain, and location, we want to accurately predict the probability of forestation at other sites within the state.\nAs previously mentioned, the data exhibits spatial autocorrelation, where objects close to each other tend to have similar attributes. Although this book does not focus on spatial analysis; we apply ordinary machine learning tools to analyze these data, which may be, to some degree, suboptimal for the task. Fortunately, severals aspects of the study reduce the impact of spatial autocorrelation. First, Bechtold and Patterson (2015) describes the sampling methodology, in which the on-site inspection locations are sampled from within a collection of 6,000-acre hexagonal regions. Thus, the spatial autocorrelation is less severe than it might otherwise be, reducing the risk of using spatially ignorant modeling methodologies. Second, our data spending methodologies are spatially aware, which helps mitigate problems that ordinary ML models have with spatially autocorrelated data. Specifically, when splitting the data for training and evaluation, we used a buffer to add some space between the data to separate fit and assessment sets (e.g., Figures 3.5 and 10.10). This practice helps to reduce the risk of ignoring the autocorrelation when estimating model parameters.\nTo learn more about spatial machine learning and data analysis, Kopczewska (2022) is a nice overview. We also recommend Nikparvar and Thill (2021), Kanevski, Timonin, and Pozdnukhov (2009), and Cressie (2015).\nAs with any ML project, we conduct preliminary exploratory data analysis to determine whether any data characteristics might affect how we model them. Table 16.1 has statistical and visual summaries of the 15 numeric predictors using the training data. Several of the predictors exhibit pronounced skew (right or left leaning). By coercing the distributions of some predictors to be more symmetric, we might gain robustness and perhaps an incremental improvement in performance (for some models).\nAlso, the annual minimum temperature, dew temperature, January minimum temperature, and maximum vapor show bimodality in the distributions. The year of inspection is also interesting; the data collection was sparse before 2011, and subsequent years contain a few hundred data points per year before beginning to drop off in 2021. These characteristics are not indicative of problems with data quality, but it can be important to know that they exist when debugging why a model is underperforming or showing odd results.\nRecall that Figure 13.2 previously described the correlation structure of these predictors. There were several clusters of predictors with strong magnitudes of correlation. This implies that there is some redundancy of information in these features. However, machine learning is often “a game of inches,” where even redundant predictors can contribute incremental improvements in performance. In any case, the analyses in this chapter will be profoundly affected by this characteristic; it will be investigated in in more detail below.\nIndividually, how does each of these features appear to relate to the outcome? To assess this, we binned each numeric predictor into roughly 20 groups based on percentiles and used these groups (each containing about 230 locations) to compute the rate of forestation and 90% confidence intervals1. As an exploratory tool, we can use these binned versions of the data to see potential relationships with the outcome. Figure 16.2 shows the profiles. Note that there is enough data in each bin to make the confidence intervals very close to the estimated rates.\nFigure 16.2: Binned rates of forestation over percentiles of the numeric predictors. The shaded regions are 90% confidence intervals.\nQuite a few predictors show considerable nonlinear trends, and a few are not monotonic (i.e., the sign of the slope changes over the range of values). The Eastness and Northness features, which capture the landscape orientation at the location, show flat trends. This means that these predictors are less likely to be important. However, once in a model with other predictors, the model may be able to extract some utility from them, perhaps via interaction terms. The primary takeaway from this visualization is that models that are able to express nonlinear trends will probably do better than those restricted to linear classification boundaries.\nIn addition to longitude and latitude, the data contains a qualitative location-based predictor: the county in Washington. There are data on 39 counties. The number of locations within each county can vary with San Juan county having the least training set samples (6) and Okanogan having the most (365). Figure 16.3 shows how the rate of forestation changes and the uncertainty in these estimates. Several counties in the training set have no forested locations. Given the number of counties and their varying frequencies of data, an effect encoding strategy might be appropriate for this predictor.\nFigure 16.3: Outcome rates for different counties in Washington State.\nFinally, it might be a good idea to assess potential interaction effects prior to modeling. Since almost all of our features are numeric, it can be difficult to assess interactions visually, so the H-statistics for two-way interactions were calculated using a boosted tree as the base model using the numeric features. Since there are only 105 possible interactions, the H-statistics were recomputed 25 times using different random number generators so that we can compute a mean H-statistic and its associated standard error.\nFigure 16.4: Results for the top 25 H-statistic interactions. The error bars are 90% intervals based on replicate computations.\nThe vast majority of the 105 H-statistics are less than 0.215 and there are a handful of interactions that are greater than that value; we’ll take the top five interactions and use them in a logistic regression shown below.\nThe next section will describe a mainstay of machine learning models for two classes: logistic regression.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear and Additive Classifiers</span>"
    ]
  },
  {
    "objectID": "chapters/cls-linear.html#sec-forestation-eda",
    "href": "chapters/cls-linear.html#sec-forestation-eda",
    "title": "16  Generalized Linear and Additive Classifiers",
    "section": "",
    "text": "Predictor\nMinimum\nMean\nMax\nStd. Dev\nSkewness\nDistribution\n\n\n\n\nAnnual Maximum Temperature\n−2.98\n13.9\n19.0\n2.79\n−1.00\n\n\n\n   \n\n\n\nAnnual Mean Temperature\n−7.89\n8.51\n12.5\n2.37\n−1.12\n\n\n\n   \n\n\n\nAnnual Minimum Temperature\n−15.4\n−3.13\n4.50\n3.16\n0.200\n\n\n\n   \n\n\n\nAnnual Precipitation\n171\n1,200\n5,900\n1,040\n1.15\n\n\n\n   \n\n\n\nDew Temperature\n−14.4\n2.26\n8.56\n2.89\n−0.0831\n\n\n\n   \n\n\n\nEastness\n−100\n−3.21\n100\n69.4\n0.0649\n\n\n\n   \n\n\n\nElevation\n0\n674\n3,820\n484\n0.915\n\n\n\n   \n\n\n\nJanuary Minimum Temperature\n−12.8\n3.12\n8.14\n2.24\n−0.751\n\n\n\n   \n\n\n\nLatitude\n45.6\n47.4\n49.0\n0.877\n0.0592\n\n\n\n   \n\n\n\nLongitude\n−125\n−120\n−117\n2.04\n−0.0843\n\n\n\n   \n\n\n\nMaximum Vapor\n306\n1,100\n2,020\n351\n0.165\n\n\n\n   \n\n\n\nMinimum Vapor\n12.0\n132\n336\n75.1\n0.271\n\n\n\n   \n\n\n\nNorthness\n−100\n−2.08\n100\n70.0\n0.0432\n\n\n\n   \n\n\n\nRoughness\n0\n48.8\n376\n47.7\n1.44\n\n\n\n   \n\n\n\nYear\n2,000\n2,020\n2,020\n3.24\n−0.402\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\nTable 16.1: Histograms and statistical summaries of the numeric predictors in the Washington State training set.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear and Additive Classifiers</span>"
    ]
  },
  {
    "objectID": "chapters/cls-linear.html#sec-logistic-reg",
    "href": "chapters/cls-linear.html#sec-logistic-reg",
    "title": "16  Generalized Linear and Additive Classifiers",
    "section": "16.2 Logistic Regression",
    "text": "16.2 Logistic Regression\nLogistic regression is a classification model that can be used when there are \\(C = 2\\) classes. Like all binary classification models, its goal is to accurately estimate the probability of an event occurring2. As discussed in the previous chapter, the probability parameter \\(\\pi\\) must be between zero and one, and we often frame the problem via the Bernoulli distribution3 (i.e., \\(y \\sim Bernoulli(\\pi)\\)). The likelihood function4 is:\n\\[\n\\ell(\\pi_i; y_{ij}) = \\prod_{i=1}^{n_{tr}} \\pi_i^{y_{i1}} (1-\\pi_i)^{y_{i2}}\n\\tag{16.1}\\]\nwhere \\(\\pi_i\\) is the theoretical probability of an event for sample \\(i\\) (\\(i = 1, \\ldots, n_{tr}\\)) and \\(y_{ic}\\) is the binary indicator for class \\(c\\). As written above, each row of the data has its own probability estimate. However, the likelihood function alone is ignorant of relationships with features that may be associated with the resulting probability of a sample within a particular class. Let’s now see how we can relate this likelihood to information to a set of features.\nIn many modeling tasks, we start with a tabular dataset of predictors and seek to understand how they jointly affect the response (\\(\\pi_i\\), in this context) in a clear and understandable way. A familiar equation that we have used in previous chapters is a linear combination parameters of the \\(p\\) predictors:\n\\[\n\\boldsymbol{x}_i'\\boldsymbol{\\beta} = \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}\n\\tag{16.2}\\]\nThis quantity is often referred to as the “linear predictor” (denoted as \\(\\eta_i\\)), and it can take any value on the real line (i.e. from -\\(\\infty\\) to \\(\\infty\\)). Because probabilities must lie between 0 and 1, the linear predictor on its own cannot be used directly to model a probability. To resolve this, we apply a nonlinear transformation that constrains the outputs to [0, 1]. In logistic regression, we use the logistic model which takes the form:\n\\[\n\\pi_i = \\frac{1}{1 + \\exp(-\\boldsymbol{x}_i'\\boldsymbol{\\beta})}\n\\tag{16.3}\\]\nThis keeps the probability values between zero and one no matter how large or small the linear predictor becomes. This nonlinear function can be derived using differential equations for growth curves. Although the model has been rediscovered several times, it was first derived in the 1800s and was not commonly known as the “logistic” model until 1925. See Cramer (2004) for a history.\nThere are several established techniques for estimating parameters in logistic regression; we will examine the most important ones in the subsections that follow. The most common approach is maximum likelihood estimation (MLE), where we use a numerical optimization method to identify parameter estimates to minimize the negative log-likelihood loss function:\n\\[\n-\\log \\ell(\\pi_i; y_{ij}) = \\sum_{i=1}^{n_{tr}} \\left[ y_{i1}\\log(\\pi_i) + (1- y_{i1}) \\log(1-\\pi_i)\\right]\n\\tag{16.4}\\]\nWorking with the log-likelihood is more numerically stable than attempting to maximize the likelihood itself for finding MLEs of the \\(\\boldsymbol{\\beta}\\) parameters. McCullagh and Nelder (1989) provides a detailed description of the gradient-based estimation procedure know as iteratively reweighted least squares (IRLS). IRLS works by repeatedly fitting weighted least squares, where each observation’s weight is a function of the current estimate of its variance. This algorithm tends to converge very quickly for logistic regression5 and has theoretical guarantees.\nOverall, for logistic regression, maximum likelihood estimation generates a relatively stable estimator. While it can be sensitive to outliers and high influence points (Pregibon 1981), a limited amount of data jittering or omissions will not widely affect the parameter estimates unless the training set is fairly small. We’ll see an exception below in Section 16.2.3, where specific characteristics of the training data make the estimators unstable.\nFrom a statistical perspective, MLEs yield parameter estimates that maximize the probability of observing the given data under the logistic model. They are also unbiased estimates of the true parameters. However, they may suffer from high variance.6.\nTo illustrate logistic regression, let’s use the training data to model the probability of forestation as a function of the longitude. In this simple logistic model, the logit function has just the slope and intercept parameters. Using the entire training set, we solved the log-likelihood equations (based on Equation 16.4) to estimate the parameters. The optimization converges after 4 iterations, producing the linear predictor function:\n\\[\n\\boldsymbol{x}_i'\\boldsymbol{\\beta} = -49.276   -0.411\\: longitude_i\n\\tag{16.5}\\]\nFigure 16.5 shows this pattern using the longitude predictor in the forestation data in green. The data points shown are the binned data from Figure 16.2; the model is actually fit on the individual locations in the training set (i.e., the binned version of the data were not used for training).\nWhy does the rate swing up and down? Looking back at Figure 3.3, we see that most of the initial western portion of the state is forested. However, Seattle is adjacent to the water of Puget Sound; this accounts for the area of nonforested locations around longitudes of about -122.3°. Further east of Seattle is mostly forested. East of this lies a large swath of unforested land (especially in the mid to south east). This is reflected in the drop in the rates from about -120° to -118.5°. In the easternmost section of the state, there is a region of forested land to the north, accounting for the rate increase at the right-hand side of Figure 16.5.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: fig-longitude\n#| out-width: \"80%\"\n#| viewerHeight: 600\n#| standalone: true\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(parsnip)\nlibrary(mgcv)\nlibrary(splines2)\nlibrary(scales)\n\nlight_bg &lt;- \"#fcfefe\" # from aml4td.scss\ngrid_theme &lt;- bs_theme(\n  bg = light_bg,\n  fg = \"#595959\"\n)\n\ntheme_light_bl &lt;- function(...) {\n  ret &lt;- ggplot2::theme_bw(...)\n\n  col_rect &lt;- ggplot2::element_rect(fill = light_bg, colour = light_bg)\n  ret$panel.background &lt;- col_rect\n  ret$plot.background &lt;- col_rect\n  ret$legend.background &lt;- col_rect\n  ret$legend.key &lt;- col_rect\n\n  ret$legend.position &lt;- \"top\"\n\n  ret\n}\n\n# ------------------------------------------------------------------------------\n\nui &lt;- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 5, 5, -1)),\n    column(\n      width = 4,\n      checkboxGroupInput(\n        inputId = \"include\",\n        label = \"Include\",\n        choices = list(\n          \"Binned Data\" = \"Data\",\n          \"Untransformed\" = \"fit_linear\",\n          \"Splines\" = \"fit_spline\",\n          GAM = \"fit_gam\"\n        ),\n        selected = c(\"fit_linear\", \"Data\"),\n        inline = TRUE\n      )\n    ),\n    column(\n      width = 4,\n      radioButtons(\n        inputId = \"yaxis\",\n        label = \"y-axis\",\n        choices = c(\"Event Rate\" = \"rate\", \"Logit\" = \"logit\"),\n        inline = TRUE\n      )\n    )\n  ),\n  as_fill_carrier(plotOutput(\"plot\"))\n)\n\nserver &lt;- function(input, output) {\n  load(url(\n    \"https://raw.githubusercontent.com/aml4td/website/main/RData/forested_data.RData\"\n  ))\n\n  lng_bin &lt;-\n    forested_train |&gt;\n    dplyr::select(class, longitude) |&gt;\n    mutate(lng_bin = ntile(longitude, 21)) |&gt;\n    dplyr::summarize(\n      rate = mean(class == \"Yes\"),\n      lng = median(longitude),\n      n = length(longitude),\n      .by = c(lng_bin)\n    ) |&gt;\n    mutate(logit = binomial()$linkfun(rate))\n\n  lng_rng &lt;- extendrange(lng_bin$lng)\n  # lng_rng[1] &lt;- 0.0\n\n  lng_grid &lt;- tibble(longitude = seq(lng_rng[1], lng_rng[2], length.out = 100))\n\n  linear_fit &lt;-\n    logistic_reg() |&gt;\n    fit(class ~ longitude, data = forested_train)\n\n  linear_pred &lt;-\n    augment(linear_fit, new_data = lng_grid) |&gt;\n    mutate(Model = \"Linear Term\", group = \"fit_linear\")\n\n  num_spline &lt;- 10\n  spline_fit &lt;-\n    logistic_reg() |&gt;\n    fit(\n      class ~ naturalSpline(longitude, df = num_spline),\n      data = forested_train\n    )\n\n  spline_lab &lt;- paste0(\"Natural Splines (\", num_spline, \" df)\")\n\n  spline_pred &lt;-\n    augment(spline_fit, new_data = lng_grid) |&gt;\n    mutate(Model = spline_lab, group = \"fit_spline\")\n\n  gam_fit &lt;-\n    gen_additive_mod() |&gt;\n    set_mode(\"classification\") |&gt;\n    fit(class ~ s(longitude), data = forested_train)\n  gam_lab &lt;- paste0(\"GAM (\", round(sum(gam_fit$fit$edf[-1]), 1), \" df)\")\n\n  gam_pred &lt;-\n    augment(gam_fit, new_data = lng_grid) |&gt;\n    mutate(Model = gam_lab, group = \"fit_gam\")\n\n  predictions &lt;-\n    bind_rows(linear_pred, spline_pred, gam_pred) |&gt;\n    mutate(\n      Model = factor(Model, levels = c(\"Linear Term\", spline_lab, gam_lab))\n    ) |&gt;\n    mutate(logit = binomial()$linkfun(.pred_Yes))\n\n  output$plot &lt;-\n    renderPlot(\n      {\n        if (input$yaxis == \"rate\") {\n          p &lt;-\n            lng_bin |&gt;\n            ggplot(aes(lng)) +\n            labs(x = \"Longitude\", y = \"Probability of Forestation\") +\n            lims(y = 0:1)\n        } else {\n          p &lt;-\n            lng_bin |&gt;\n            ggplot(aes(lng)) +\n            labs(x = \"Longitude\", y = \"Logit\")\n        }\n\n        if (any(input$include == \"Data\")) {\n          if (input$yaxis == \"rate\") {\n            p &lt;- p + geom_point(aes(y = rate), alpha = 1 / 3, cex = 3)\n          } else {\n            p &lt;- p + geom_point(aes(y = logit), alpha = 1 / 3, cex = 3)\n          }\n        }\n\n        if (any(grepl(\"fit_\", input$include))) {\n          curve_data &lt;- dplyr::filter(predictions, group %in% input$include)\n\n          if (input$yaxis == \"rate\") {\n            p &lt;- p +\n              geom_line(\n                data = curve_data,\n                aes(x = longitude, y = .pred_Yes, color = Model),\n                linewidth = 1\n              )\n          } else {\n            p &lt;- p +\n              geom_line(\n                data = curve_data,\n                aes(x = longitude, y = logit, color = Model),\n                linewidth = 1\n              )\n          }\n\n          p &lt;- p +\n            theme(legend.position = \"top\") +\n            scale_color_brewer(drop = FALSE, palette = \"Dark2\")\n        }\n\n        p &lt;- p  + theme_light_bl()\n        print(p)\n      },\n      res = 100\n    )\n}\n\napp &lt;- shinyApp(ui = ui, server = server)\napp\n\n\n\nFigure 16.5: An example of a logistic regression model with a single predictor (longitude). The binned data points are the same as those shown in Figure 16.2 and are for visualization purposes only; the models are trained on the individual rows in the training set.\n\n\n\nThe model fit initially underestimates the rate of forestation for the most western locations, and the predicted probability slowly decreases in a fairly flat sigmoid. Given the ups and downs of the observed event rates over longitude, this model fit misses almost all of the nuance in the data. The data trend is not monotonically decreasing, so our simple linear predictor is doing its best but is not flexible enough to emulate what is actually occurring.\n\nGeneralized Linear Models\nLogistic regression is a special case of the class of generalized linear models (GLMs) (McCullagh and Nelder 1989). GLMs allow us to model different types of responses, such as real-valued continuous data, counts, or binary outcomes, by relating a transformation of the expected value of the response to a linear predictor. In the case of logistic regression, the response is binary and we use the logit link function (the inverse of Equation 16.3) to connect the probability of \\(\\pi_i\\) to the linear predictor:\n\\[\nlog\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\eta_i = \\boldsymbol{x}_i'\\boldsymbol{\\beta}\n\\tag{16.6}\\]\nHere, \\(\\boldsymbol{x}_i'\\boldsymbol{\\beta}\\) is the linear predictor and \\(log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right)\\) is the link function. The link function must be chosen so that the transformed expected values can lie anywhere on the real line, even though \\(\\pi_i\\) is constrained to the interval [0, 1]. Logistic regression specifically uses the logit link, but several other link functions are often used in binary regression, such as the probit and complementary log-log links, providing alternative ways to map \\(\\pi_i\\) into the linear predictor space. See Morgan (1992) for more specialized “link functions.” From a broader view, we might refer to our model as “binary regression.”\nLooking back to Figure 16.5, we can change the y-axis data and show how the logit changes for our fitted model. The line visualizes the estimated linear predictor for these data. Sadly, when the representation of our data is similarly changed, the data pattern is not linear; our linear model isn’t effectively emulating the nonlinear pattern in the data.\nEmbedding logistic regression within the GLM framework offers important advantages. First, it insures certain theoretical properties, such as the concavity of the log-likelihood, which means that we can rely on well-established gradient-based optimization methods to find the maximum likelihood estimates. Second, the GLM framework gives us tools for inference where we can assess the importance of each predictor through hypothesis testing and confidence intervals around the parameter estimates.\nIt’s easy to take these benefits for granted. Suppose we were to find parameter estimates using some other objective function, such as the Brier score or the area under the ROC curve. In that case, it might be difficult to solve the required equations, let alone derive the methods to make inferences regarding parameters.\n\nBefore moving on, it’s important to clarify what “linear predictor” means in the context of GLMs. The term “linear” refers to the linearity in the parameters \\(\\boldsymbol{\\beta}\\), not linearity in the relationships between inputs and outcomes. In other words, class boundaries (or decision boundaries) can be nonlinear so long as the model remains a linear combination of model terms. These terms, \\(x_{ij}\\) can represent any function of one or more parameters. We can achieve this nonlinearity through feature engineering as discussed in Chapters 5 through 8 to create a better model.\n\nFor example, in Figure 16.5, we saw that using the longitude values as our single predictor \\(x\\) didn’t work well. When plotted on the logit scale, there is a clear nonlinear function in the binned rates. To model this more flexibly, we incorporate spline-based features. Specifically, Figure 16.5 displays the results when ten natural spline features are used in the model. Visually, we can see that this expansion improves the model’s accuracy at predicting the outcome.\nLogistic regression models can be drastically improved by including better representations of our predictors. Since it is an interpretable and stable model, we might be motivated to spend more time developing this model via exploratory data analysis and feature engineering. In many teaching materials and websites, we see analyses that quickly label the results of a logistic model as ineffective (due to the use of simple model terms) before moving on to more black-box machine-learning methods. The virtue of learning about your data and how the predictors related to the outcome can not only improve the logistic model but enable a more complete understanding and description of why it works.\n\n\n\nForestation Model Development\nWe can assess each of these feature engineering steps by fitting logistic regression pipelines that sequentially add preprocessing operations. Our five models are:\n\nSimple model: Convert each of the 39 counties to binary indicators and drop any predictors with zero-variance.\nNormalization model: Begin with the simple model and add a normalization step that applies the ORD transformation to all numeric predictors.\nEncoding model: Build on the normalization model by replacing the county dummy indicators with effect encoding.\nInteraction model: extend the encoding by including interaction terms.\nSpline model: Enhance the interaction model further with ten natural spline basis functions for a set of predictors.\n\nFor each of the five preprocessing/model configurations, we’ll resample the training set and estimate several performance metrics (but will focus on the Brier score).\nFigure 16.6 displays the results from resampling each of our five logistic regression models. Most preprocessing steps yield incremental reductions in the Brier score. The most substantial improvement comes from replacing many county-level indicator variables with effect encoding. Normalization also appears to reduce the Brier score, although its error bars overlap with those of previous models—suggesting that this benefit may not be statistically significant. Overall, the best pipeline achieves approximately one-third lower Brier score compared to the simple model by improving how we represent the predictors.7.\n\n\n\n\n\n\n\n\nFigure 16.6: Cross-validated Brier scores for several stages of feature engineering of the forestation data using basic logistic regression. The red dashed line represents the permutation estimate of the no-information rate. The error bars are 90% confidence intervals based on the naive standard error estimates produced during resampling.\n\n\n\n\n\nAlong with the Brier score values of 0.0836, the spline pipeline (i.e., Model #5) had the following resampling estimates: ROC AUC = 0.943, a PR AUC = 0.941, and a cross-entropy = 0.293. During resampling, there were 10 resamples, leading to 10 ROC curves and corresponding performance metrics. By pooling predictions from all 10 assessment sets, we constructed approximate ROC and calibration curves (Figure 16.7). The calibration curve indicates that the model reliably predicts samples with accuracy The ROC curve shows good class separation, although it’s shape is otherwise unremarkable.\n\n\n\n\n\n\n\n\nFigure 16.7: Calibration and ROC curves for the pooled assessment sets for the logistic regression model and the full preprocessing pipeline (e.g. up to spline terms). The calibration plot used a moving window, where windows that span 10% of the data are computed with moving increments of 2%.\n\n\n\n\n\nFrom here, we should investigate which predictors had the greatest influence on the model. For example, were the 10 interactions that we included statistically significant and practically important? Did we include too many or too few of them? We can also visualize the predicted probabilities across geographic locations to identify areas where calibration is poor. Additionally we might improve the model by applying a supervised feature selection method to filter out less useful predictors.\n\n\n\n16.2.1 Examining the Model\nThere are many ways to evaluate a model’s performance. One common, but often misplaced, focus is on the statistical significance (p-values) of individual model terms. If the model’s primary goal is interpretation, this may make sense. However, for ML applications, p-values are usually not very informative, because they often do not correlate with how well the model will make predictions. This is especially true in our logistic regression model, which includes many spline basis functions: knowing that the fourth spline term has a small p-value tells us little about overall predictive power. That said, we will revisit inference later as a diagnostic tool.\nInstead, for assessing model quality, we recommend using the held-out predictions to conduct residual analysis, looking for systematic deviations in prediction errors. The most basic residual is the simple difference between the “observed” outcome and the predicted probability: \\(e_{ic} = y_{ic} - \\widehat{p}_{ic}\\). However, these residuals have heteroscedastic variance–the variance of the \\(\\widehat{p}_{ic}\\) changes over their [0,1] range. A residual of 0.1 has a different context when \\(\\widehat{p}_i = 0.01\\) compared to \\(\\widehat{p}_i = 0.5\\). To normalize these values, we can use Pearson residuals, which are \\(e_{ic}\\) divided by the standard deviation:\n\\[\ne^p_{ic} = \\frac{y_{ic} - \\widehat{p}_{ic}}{\\widehat{p}_{ic} (1 - \\widehat{p}_{ic})}.\n\\]\nAlternatively, we can look at deviance residuals. The deviance is a likelihood-based comparison between two models: our current model and one that is saturated where the model perfectly fits the training data. For logistic regression, these take the form:\n\\[\ne^d_{ic} = \\text{sign}(e_{ic}) \\left[-2(y _{ic}\\, \\text{log}(\\widehat{p} _{ic}) + (1 - y _{ic})\\, \\text{log}(1 - \\widehat{p}_{ic}))\\right]^{1/2}\n\\]\nSee Pregibon (1981) and Agresti (2015) for discussions.\nIn practice, we should plot residuals against each predictor’s values to check for systematic patterns. If the residuals show non-random structure (e.g. curved or clustered patterns), it suggests that the predictor may not be represented properly in the model. For example, in Figure 16.8 we plot deviance residuals versus longitude in the top panel, using predictions from each resampling fold on the held-out data.\nThe top left-hand panel shows residuals from the model that uses only simple linear predictor terms. For each class, we can see large spikes in residuals at specific longitude ranges–these spikes correspond to to the previous ups and downs shown in Figure 16.5. It is interesting that this visualization shows different patterns for each class; they are not mirrored reflections of one another. For example, for the region between longitudes of approximately -120.5° and -118°, the residuals for unforested locations are very small, while the forested locations exhibit a spike.\nIn contrast, the right-hand panel has many more residuals near zero and significantly attenuated spikes/patterns. This is indicative of an improvement of model fit. However, some samples in the spline model have absolute residuals larger than the simpler model. Overall, though, the spline model exhibits fewer systematic deviations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.8: Top: Deviance residuals plotted against longitude for models with and without spline terms for predictors (including longitude). Bottom: The largest residuals (\\(\\left|e^d_{ic}\\right| \\ge 1.25\\)) from the spline model mapped geographically. Hovering over the ponts shows the true class, the estimated probability of being forested, and the residual value.\n\n\n\nGiven the spatial structure of our data, mapping residuals geographically is also useful. the bottom panel of Figure 16.8 shows an interactive map of the the largest residuals for the model containing spline terms. We can see locations with the absence of large residuals and specific areas where \\(e^d_{ic}\\) tend to be large such as the border with Canada and south of the Strait of Juan de Fuca.\nLet’s move on to consider how the logistic model can facilitate interpretation.\n\n\n\n16.2.2 Interpreting the Logistic Model\nThe logit function has a meaningful interpretation since it contains the odds of an event occurring. Let’s consider a simple case where we are modeling the change by county. If we use Yakima county as the reference cell (recall Section 6.2), the intercept of a logistic model is -0.423. This corresponds to a probability of 39.6%. The odds of some event that occurs with probability \\(\\pi\\) is\n\\[\nodds = \\pi / (1-\\pi)\n\\]\nor “one in \\(1/\\pi\\).” For Yakima, this relates to the logistic regression model since\n\\[\nlog\\left(\\frac{Pr[\\text{Yakima forested}]}{Pr[\\text{Yakima unforested}]}\\right) = log\\left(\\text{Yakima odds}\\right) = \\beta_0\n\\]\nTherefore, the odds of a tree being forested in this county are exp(-0.423) = 0.655 or, one in about 2.5 locations.\nAlternatively, consider Skamania County, which contains Gifford Pinchot National Forest. It’s model coefficient is 3.44 However, recall that with a reference cell encoding, this is the change in the logit relative to the reference value (Yakima County). If we are interested in the probability of forestation for Skamania, we have to look at the sum of both coefficients (3.02 logit units) which means that the probability of forestation estimated to be about 95.3%.\nOne way of assessing how much more likely an event is to occur is the odds ratio. For example, if we wanted to compute the odds ratio of forestation between Yakima and Skamania counties, we can use different sets of coefficients from the linear predictor. Since the forestation rate is higher in Skamania than Yakima, the odds ratio of interest is\n\\[\n\\frac{\\text{Skamania odds}}{\\text{Yakima odds}}\n\\]\nWe know that\n\\[\nlog\\left(\\text{Skamania odds}\\right) = log\\left(\\frac{Pr[\\text{Skamania forested}]}{Pr[\\text{Skamania unforested}]}\\right) = \\beta_0 +\\beta_1\n\\]\nso that\n\\[\n\\frac{\\text{Skamania odds}}{\\text{Yakima odds}} = \\frac{exp(\\beta_0 +\\beta_1)}{exp(\\beta_0)} = exp(\\beta_0 + \\beta_1 - \\beta_0) = exp(\\beta_1)\n\\]\nFrom our estimated coefficient, the odds of being forested in Skamania is 31.1 times larger than in Yakima. For each categorical predictor, we can use its model coefficient to compute odds ratios relative to the reference cell8. Since we are using maximum likelihood estimation, we can also compute confidence intervals for this odds ratio; the 90% interval is (15.3, 74.5), the lower bound being very far from a equivocal value of 1.0.\nFor numeric predictors, such as eastness or elevation, exponentiating the coefficient gives us a per unit odds ratio. Since the predictors might be in different units, the interpretation can be more complex depending on the situation9.\nIn either case, the logistic regression model does have a structure that lends itself to natural probabilistic interpretation. See Hosmer, Lemeshow, and Sturdivant (2013) for a broader discussion.\nIn addition to fitting models, there are methods to explain how a model or a specific prediction, works, regardless of the model type. These tools are part of the field of explainable machine learning. Some, like the H-statistic for detecting interactions (see Section 8.1.2) help quantify effects of predictors. These will be discussed in detail in ?sec-explanations, but here is a preview.\nWe already saw that longitude exhibits a complex, nonlinear relationship with forestation classification when considered alone. When other predictors are included in the model, though, it becomes important to understand how longitude contributes to predictions in that broader context. How does longitude affect model predictions when placed in a model with other predictors?\nPartial dependence plots (PDP) are visualization tools designed for this task. Here is how they work: suppose we were to sample a single location \\(x_i\\) from the training set (or any other set of data). Hold all predictors except longitude constant, and vary longitude over a grid of values. For each grid value, compute the model’s predicted probability. That gives one profile curve showing how the prediction changes as longitude changes. Since this profile depends on the particular point’s other predictor values, different data points generate different profiles. To summarize the effect of longitude overall, sample many data points, compute all their profiles, and average them. The result is a PDP, which shows the average effect of longitude across its observed range, accounting for the rest of the model.\nFigure 16.9 shows a PDP for longitude from our most complex model (including interactions and splines). The 1,000 grey lines show the individual profiles, and the red curve shows the average effect of this predictor, in the presence of the other model terms.\n\n\n\n\n\n\n\n\nFigure 16.9: A partial dependency plot visualizing how longitude affects the probability when averaged over 1,000 random draws of the data. The red line shows the average trend while the grey lines show the trends for the 1,000 location samples. This is an interesting contrast to Figure 16.5.\n\n\n\n\n\nThe pattern is different from Figure 16.5. For example, there is no decreasing trend in the eastern part of the state; probabilities for locations greater than longitudes of -121° are fairly constant. The difference is due to some other predictor having additional influence on locations in that area of the state (perhaps the county).\nThis model appears to be adequate, but it is not. There is a major issue related to our parameter estimates that, while not obviously affecting the predictive ability, reflects a flaw that could make the fit unreliable.\n\n\n\n16.2.3 A Potential Problem: Multicollinearity\nExamining which model terms demonstrate statistical significance offers a valuable framework for assessing their relative importance within the model. Basic logistic regression via MLE provides a foundation for standard hypothesis tests. Table 16.2 shows the five most statistically significant terms (excluding the intercept) based on their p-values. Notably, the county effect encoding appears among the top predictors, which should be unsurprising since it captures the shrunken mean logit for each county. The remaining significant terms all correspond to spline basis expansion terms. Using an \\(\\alpha\\) value that corresponds to a 10% false positive rate, 41 terms out of 130 achieve statistical “significance”. When interpreting these results, it is important to note that the original predictors were normalized to approximate a standard normal distribution, meaning that most of the predictor values should be between \\(\\pm 3\\), prior to making spline and/or interaction terms.\n\n\n\n\n\n\n\n\n\n\n\nModel Term\nEstimate\nStd. Err.\nt-statistic\np-value\n\n\n\n\nSmallest p-values\n\n\ncounty\n−0.706\n0.0782\n−9.02\n&lt; 2e-16\n\n\nroughness (spline #6)\n−2.91\n0.628\n−4.63\n3.6e-06\n\n\nroughness (spline #8)\n−2.72\n0.590\n−4.60\n4.1e-06\n\n\nroughness (spline #7)\n−2.53\n0.623\n−4.06\n5.0e-05\n\n\nlatitude (spline #1)\n−15.0\n3.93\n−3.82\n0.00014\n\n\nLargest Standard Errors\n\n\nannual preciptation (spline #10)\n−423\n422\n−1.00\n0.31646\n\n\nannual preciptation (spline #1)\n−217\n213\n−1.02\n0.30728\n\n\nannual preciptation (spline #3)\n−146\n141\n−1.03\n0.30165\n\n\nannual preciptation (spline #5)\n−148\n141\n−1.05\n0.29425\n\n\nannual preciptation (spline #7)\n−148\n141\n−1.05\n0.29405\n\n\n\n\n\n\n\n\nTable 16.2: The top five most signficant model terms (top) and the five terms with the highest standard errors (bottom).\n\n\n\n\nFor further insight, it is instructive to examine which predictors show no evidence of being different from zero. The bottom of the table shows five terms with large p-values, all of which are spline terms derived from annual precipitation. However, notice that the estimated coefficients and their standard error terms are of magnitude in the hundreds. Given that the predictor values for these spline terms range between zero and 0.617 (on average), it is difficult to imagine that the coefficients and their values are reasonable.\nThe reason for these excessively large becomes apparent when examining Figure 13.2, which revealed several predictors that had large (absolute) correlations with one another. High correlation between multiple predictors, known as multicollinearity10, is a well-known challenge in regression analysis and occurs frequently in many data sets. As illustrated in the correlation matrix heatmap (Figure 13.2), numerous climate-related predictors exhibit strong correlations with one another. This issue intensifies after creating spline and interaction terms: there are 62 pairwise correlations between model terms that have correlations greater than 0.75 and 23 term pairs with correlations greater than 0.90.\nWhile this model’s predictive performance is good, the model coefficients are extremely unstable. Multicollinearity not only inflates the variance of the parameter estimates but can also reverse the coefficient’s sign, leading to an incorrect interpretation (see below).\nThe variance inflation factor (VIF) quantifies the extent to which the variance of a coefficient estimate is inflated due to correlations among predictors (Belsley, Kuh, and Welsch 2013). Specifically, VIF measures the ratio of the variance for a set of potentially correlated predictors relative to a set of completely uncorrelated predictors. For linear regression, the VIF is calculated as:\n\\[ \\text{VIF}_j = \\frac{1}{1 - R_j^2} \\]\nwhere \\(R_j^2\\) represents the coefficient of determination from a linear regression in which predictor \\(X_j\\) serves as the response variable and the remaining \\(j-1\\) predictors function as explanatory variables. While originally developed for linear regression contexts, Weissfeld and Sereika (1991) extended this diagnostic framework to generalized linear models. A general rule of thumb suggest that VIF values exceeding 5.0 or 10.0 indicate substantial multicollinearity, reflecting overlapping information among predictors. For our model, the median VIF across the parameters was 294 with the largest being 371,153, which corresponds to an annual precipitation spline term.\nTable 16.3 demonstrates the multicollinearity problem using a simplified model with two untransformed predictors: elevation and mean temperature. The correlation between these two predictors is -0.94, indicating substantial collinearity. The first two rows present single-predictor models, while the third row displays the results when both predictors are included simultaneously. The parameter estimates undergo dramatic changes in the combined model: the elevation coefficient reverses sign, and the mean temperature coefficient increases by several orders of magnitude. Notably, the variance inflation factor reaches sufficiently high levels to flag this multicollinearity issue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\n\nIntercept\n\n\nElevation\n\n\nAnnual Mean Temperature\n\n\n\nEstimate\nStd. Error\nVIF\nEstimate\nStd. Error\nVIF\nEstimate\nStd. Error\nVIF\n\n\n\n\nelevation only\n0.445\n0.0519\n—\n−0.00106\n0.0000678\n—\n—\n—\n—\n\n\nmean temperature only\n−3.49\n0.151\n—\n—\n—\n—\n0.371\n0.0165\n—\n\n\nboth\n−14.3\n0.572\n—\n0.00449\n0.000223\n7.91\n1.28\n0.0498\n7.91\n\n\n\n\n\n\n\n\n\nTable 16.3: An example of how highly colinear predictors can affect parameter estimates and standard errors in a logistic regression model.\n\n\n\n\n\n\n\nWhat can we do about this problem? Several strategies exist for mitigating multicollinearity. First, transforming the original predictors using principal component analysis (PCA) or partial least squares (PLS) can substantially reduce correlations among predictors. However, this approach complicates the incorporation of interaction terms and nonlinear relationships, as each derived predictor represents a composite of all original variables. Second, a correlation filter can be employed to remove the minimal set of predictors necessary to reduce multicollinearity’s overall impact. While this represents a practical approach, it carries a risk of predictive performance degradation. Given that machine learning performance improvements often derive from incremental gains, the removal of nearly redundant predictors may diminish predictive ability.\nFor logistic regression specifically, the most effective solution is to retain the original predictor set while employing regularization—an alternative parameter estimation technique that naturally addresses multicollinearity.\n\n\n\n16.2.4 Regularization Through Penalization\nRegularization is a technique that coerces a potentially complex solution into one that is less extreme. We’ve already seen regularization a few times:\n\nEarly stopping when fitting neural networks (Section 2.4, footnote) prevented model training to become worse.\nEffect encodings (Section 6.4.3) moved naive statistical estimates toward more moderate values.\nBayes’ Rule for estimating a proportion (Section 12.5.1) shrank the naive probability estimate towards the prior.\nTree complexity tuning parameters (e.g. \\(n_{min}\\), see Section 9.3) restrict tree depth by preventing splits on small subsets.\n\nThis section examines regularization techniques commonly applied to regression models, primarily through the penalization of model coefficients. The next section introduces Bayesian estimation as a framework that naturally produces regularization effects.\nWhy would we regularize a logistic regression model? There are several reasons. First, regularization prevents coefficient estimates from exploding to magnitudes larger than appropriate. As demonstrated in the preceding section, highly correlated predictors can produce abnormally large coefficients—potentially with incorrect signs—and inflated standard errors. Second, regularization enables feature selection: when a predictor has no relationship with the outcome, any non-zero coefficient should be reduced toward zero.\nIn logistic regression, parameter estimation proceeds through maximization of the Bernoulli log-likelihood function (Equation 16.1). The most common approach is to incorporating a penalty term to the log-likelihood (denoted as \\(\\mathcal{P}(\\boldsymbol{\\beta})\\)) that discourages large parameter values. This penalty function may also include tuning parameters.\n\nAll of the techniques discussed in this section make the implicit assumption that the model features have been standardized to be in the same units (including indicator features).\n\nRidge regression represents one of the earliest penalization methods developed for regression models (A. Hoerl and Kennard 1970a, 1970b; R. Hoerl 2020). The ridge penalty function reduces the log-likelihood by subtracting the sum of squared regression coefficients11:\n\\[\n\\mathcal{P}_{rr}(\\boldsymbol{\\beta};\\lambda) =  \\lambda\\sum_{j=1}^{p} \\beta_j^2\n\\tag{16.7}\\]\nThe summation index starts at one, reflecting the convention that \\(\\beta_0\\) denotes the intercept parameter. Intercept terms are universally excluded from regularization penalties for several compelling reasons. First, the intercept lacks any meaningful null value toward which shrinkage would be appropriate. A value of \\(\\beta_0 = 0\\) corresponds to a probability estimate of 0.5, which may differ substantially from the true population intercept determined by outcome prevalence. Forcing shrinkage toward this value risks substantial model degradation by directing the intercept toward a value that is inconsistent with the overall event rate. Second, for the summation in Equation Equation 16.7 to function appropriately, predictors need to be in the same units. When data are standardized to common mean and variance, the intercept’s mean and variance become fixed at one and zero, respectively. Including the intercept in this summation would be like comparing apples to horses; it doesn’t make sense and lacks statistical justification. Third, the intercept may naturally have a large magnitude when outcome prevalence is extremely low or high. Squaring such a large value and incorporating it into the penalty sum causes the penalty to be dominated by this single term, thereby overwhelming the regularization effect on the slope coefficients—the parameters of primary inferential and predictive interest.\nFor a generalized linear model and a fixed value of the \\(\\lambda\\) penalty, we estimate parameters that maximize the penalized log-likelihood:\n\\[\n\\underset{\\boldsymbol{\\beta}}{\\text{minimize}}:  -\\log \\ell(x| \\boldsymbol{\\beta}) + \\mathcal{P}(\\boldsymbol{\\beta};\\lambda)\n\\]\nRidge regression is particularly effective when the feature matrix is ill-conditioned—that is, when we cannot compute the inverse of critical matrices (notably \\(X'X\\)). This condition typically arises in two scenarios: when the number of features exceeds the number of training set points, or when severe multicollinearity exists among predictors. The ridge penalty has the effect of shrinking the model coefficients, driving them toward zero at varying rates across predictors. This shrinkage prevents parameter estimates from becoming too large due to multicollinearity, stabilizes the standard errors of estimates, and can mitigate coefficient sign instability, as signs may fluctuate with penalty magnitude changes.\nThe amount of regularization is a tuning parameter, with effective values typically falling between zero and 0.1 (although larger values may be appropriate in certain contexts). When the penalty becomes excessively large, all coefficients shrink toward zero and the model effectively reduces to an intercept-only specification, yielding constant predictions across all observations. The penalty parameter is commonly evaluated on a \\(\\log_{10}\\) scale to facilitate tuning across orders of magnitude.\nFigure 16.10 illustrates the behavior of parameter estimates for the forestry model containing elevation and mean annual precipitation. To ensure comparability, the orderNorm transformation was applied to both predictors prior to model fitting. This standardization procedure eliminates precipitation skewness and places both predictors have the same units. The dashed horizontal lines represent parameter estimates obtained through ordinary maximum likelihood estimation without regularization. As the ridge penalty increases, each estimate smoothly decreases toward zero. At very large penalty levels, the elevation coefficient reverses sign to negative values before approaching zero.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-two-feature-penalty\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-penalty.R\")\n\napp\n\n\n\nFigure 16.10: The path of coefficients for a logistic regression with two features when using different penalization methods. The horizontal dashed lines indicate the unpenalized coefficients.\n\n\n\nWe choose the amount of regularization just like any other parameter: using a performance metric to decide an appropriate value. As we’ll see in subsequent analyses, the penalty-response curve typically exhibits a plateau region where model performance remains relatively stable across a broad range of penalty values. This characteristic makes it virtually impossible to select a penalty magnitude that induces overfitting. Suboptimal tuning parameter choices instead manifest as over-regularization, wherein coefficients shrink excessively toward zero (i.e. underfitting).\nAs previously mentioned, standard maximum likelihood estimation yields unbiased estimates; the estimates \\(\\hat{\\beta}\\) have expected values that equal the true, unknown parameters \\(\\beta\\). We can conceptualize this as “MLE is aiming for the right target.” In contrast, the penalization methods discussed herein introduce bias into the estimates, with the magnitude of bias increasing monotonically with penalty strength. The advantage of ridge regression lies in its substantial reduction of parameter estimate variance as bias increases incrementally. This phenomenon exemplifies the fundamental bias-variance tradeoff: when the variance reduction outweighs the increase in squared bias, the mean squared error improves. However, when the predictor set lacks substantial multicollinearity, introducing bias provides no benefit, as variance reduction opportunities are minimal and the effects on estimation accuracy are adverse.\nAlternatively, we can also penalize the regression using sum of the absolute value of the parameters:\n\\[\n\\mathcal{P}_{lasso}(\\boldsymbol{\\beta};\\lambda) =  \\lambda\\sum_{j=1}^{p} |\\beta_j|\n\\tag{16.8}\\]\nThere are various methods that can be used to optimize this version of the penalized log-likelihood (Breiman 1995; Efron et al. 2004), but the Lasso framework introduced by Tibshirani (1996) has emerged as the predominant approach12. The Lasso penalty has an interesting theoretical property: at sufficient penalty magnitudes, individual parameter estimates can take a value of exactly zero, meaning that they are functionally eliminated from the model. This characteristic makes the Lasso one of the most effective tools for automated feature selection. However, there are numerical issues that require careful attention. Standard gradient-based optimization algorithms cannot produce exact zero values due to numerical limitations, instead yielding parameter estimates arbitrarily close to zero without reaching exactly aero. Consequently, specialized optimization tools are required to achieve genuine feature elimination.\nThere are a few ways that Lasso penalization regularizes the parameter estimates that are different than ridge regression. First, while ridge penalization simultaneously moves groups of correlated features towards zero, the Lasso will often choose a single feature out of a group of correlated features. Friedman, Hastie, and Tibshirani (2010) summarized this nicely:\n\n“Ridge regression is known to shrink the coefficients of correlated predictors towards each other, allowing them to borrow strength from each other. In the extreme case of \\(k\\) identical predictors, they each get identical coefficients with \\(1/k^{th}\\) the size that any single one would get if fit alone.[…]\nLasso, on the other hand, is somewhat indifferent to very correlated predictors, and will tend to pick one and ignore the rest.”\n\nAnother phenomenon occurs when one or more features are set to zero: the rate of shrinkage to zero can change from the remaining parameters, and, in some rare cases, a parameter may become non-zero again.\nFigure 16.10 illustrates the behavior of parameter estimates under varying Lasso penalty magnitudes for the two-feature model. Specifically, the elevation coefficient reaches zero and is eliminated from the model before the precipitation coefficient reaches zero. Once elevation has been removed, the remaining coefficient exhibits a slower rate of shrinkage toward zero compared to its trajectory when both predictors were present in the model.\nAs the comments above indicate, the Lasso model is more strongly associated with feature selection than mitigating the effects of multicollinearity (where the ridge penalty is favored). Why not use a combination to accomplish both goals? Zou and Hastie (2005) describe the elastic net model that includes a separate penalty for both criteria:\n\\[\n\\mathcal{P}_{en}(\\boldsymbol{\\beta};\\lambda_1 \\lambda_2) =  \\lambda_1\\sum_{j=1}^{p} \\beta_j^2 + \\lambda_2\\sum_{j=1}^{p} |\\beta_j|\n\\tag{16.9}\\]\nAlternatively, Friedman, Hastie, and Tibshirani (2010) described what is referred to as the glmnet penalty that uses the form:\n\\[\n\\mathcal{P}_{gn}(\\boldsymbol{\\beta};\\lambda, \\alpha) =  \\lambda \\left[\\frac{1}{2}(1 - \\alpha) \\sum_{j=1}^{p} \\beta_j^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j|  \\right]\n\\tag{16.10}\\]\nwhere \\(\\lambda\\) is the total amount of regularization and \\(\\alpha\\) controls the mixture of the two methods so that \\(\\alpha = 1\\) is a Lasso model and \\(\\alpha = 0\\) is ridge regression. The glmnet penalty formulation tends to be favored by users.\nThe combination strategy can help the model move between the goals of dealing with multicollinearity (and similar issues) and feature selection. By tuning the penalty and the mixing parameters, we can let the data decide which penalty it needs most (and how much). As we’ll see below, a typical pattern is a fairly flat plain where there isn’t much change in our preferred metric for a wide range of penalty values. Also, the relationship between the penalty and performance can be similar for different mixtures but shows a lagging pattern over values of \\(\\alpha\\). As such, we can achieve virtually the same performance but with different numbers of non-zero coefficients.\nThe lagging trend might be related to the half contribution of the ridge penalty in Equation 16.10. If we consider a pure ridge model (\\(\\alpha = 0\\)) versus a pure lasso model (\\(\\alpha = 1\\)), the total amount of regularization for the former is half of the latter (for a fixed \\(\\lambda\\)). This will be offset by the magnitudes of the summations; a summation of squared parameters will be larger than the sum of absolute parameter estimates. However, we have observed that pure ridge solutions can require larger \\(\\lambda\\) values than pure lasso models to have the same effect on performance metrics.\nWhen choosing the best parameters for glmnet, we might want to factor in the number of “active features” along with performance. For example, we could use desirability functions to blend the two outcomes or create a more direct rule, such as “choose \\(\\lambda\\) and \\(\\alpha\\) with the fewest number of predictors as long as the area under the ROC curve is greater than 0.85.”\nFigure 16.10 shows the glmnet results for the two-feature model. Starting with \\(\\alpha = 1.0\\), we have the same solution as the Lasso results. Using the slider to decrease the proportion of Lasso in the penalty slowly shifts the curves to the right, where more penalization is needed to eliminate both parameters (since more ridge regression is used). Also, as \\(\\alpha\\) moves closer to the minimum of 10%, there becomes less difference in the slopes for the mean annual temperature before and after elevation is removed. In any case, elevation is removed before mean annual temperature for each setting.\nThe nature of penalized models, such as Lasso and glmnet, makes them likely to be able to exploit submodels (Section 11.3.1) for very efficient model tuning. For a specific mixture percentage, we can fit one glmnet model and evaluate a large number of penalty values almost for free13. This is only the case when the tuning grid has many \\(\\lambda\\) values for each \\(\\alpha\\). Space-filling designs and iterative search would not be good choices here; regular grids for these model parameters can be more computationally efficient.\nThe glmnet framework represents the most widely adopted approach to model regularization, yet it exhibits limitations when one or more parameter estimates are excessively large. Under Lasso or ridge penalization, a parameter estimate outlier can exert linear or quadratic influence on the penalty term, respectively. This disproportionate effect can draw other estimates toward the outlier, thereby granting it undue influence on the penalization. Fan and Li (2001) and Zhang (2010) introduced alternative approaches—the smoothly clipped absolute deviation (SCAD) and minimax concave penalty (MCP) models, respectively—which address this vulnerability through their respective penalty functions:\n\\[\n\\mathcal{P}_{scad}(\\boldsymbol{\\beta};\\lambda, \\gamma) =     \\begin{cases}\n     \\sum_{j=1}^{p}\\lambda|\\beta_j| & |\\beta_j| \\le \\lambda \\\\\n     \\sum_{j=1}^{p} \\lambda|\\beta_j| \\frac{2\\gamma\\lambda|\\beta_j|-\\beta_j^2-\\lambda^2}{2(\\gamma - 1)} & \\lambda &lt; |\\beta_j| \\le \\gamma\\lambda \\\\\n     \\frac{\\lambda^2(\\gamma + 1)}{2} & |\\beta_j| \\ge \\gamma\\lambda\n    \\end{cases}\n\\tag{16.11}\\]\nand\n\\[\n\\mathcal{P}_{mcp}(\\boldsymbol{\\beta};\\lambda, \\gamma) =   \\begin{cases}\n     \\sum_{j=1}^{p}\\frac{\\lambda|\\beta_j| - \\beta_j^2}{2\\gamma} & |\\beta_j| \\le \\gamma\\lambda \\\\\n     \\sum_{j=1}^{p}\\frac{\\lambda^2\\gamma}{2} & |\\beta_j| &gt; \\gamma\\lambda\n    \\end{cases}\n\\tag{16.12}\\]\nBoth SCAD and MCP incorporate an additional tuning parameter \\(\\gamma\\) that provides a “clipping” effect. When coefficient magnitudes become sufficiently large (i.e., exceeding the threshold \\(\\lambda\\gamma\\)), the penalty function becomes constant. This mechanism constrains the influence that abnormally large parameters have on the regularization procedure. Consequently, these methods offer particular advantages in scenarios where the ratio of features to training samples is exceeding large, as they prevent individual outlier parameters from dominating the penalization process.\nThese two penalties are also shown in Figure 16.10 where there is an extra control for the clipping parameter (\\(\\gamma\\)). For clipping values \\(\\gamma &lt; 10\\), there is a sharp change in the parameter paths where they abruptly jump to their unpenalized values. This is related to the limit of \\(\\lambda\\gamma\\); values above this combination are clipped at a value that does not involve the \\(\\beta\\) parameters. As \\(\\gamma\\) becomes relatively large for these data, the static penalty has a less abrupt effect on the parameter estimates, and the change is not as instantaneous as it is for smaller clipping values. For this example, there is very little difference between the SCAD and MCP approaches.\nPenalization of generalized linear models has been a very fertile area of research, and there are many more variations of these models.\n\nGrouped penalization: Predictors can be organized into groups that share a common penalty term, ensuring they enter or exit the model simultaneously (Bien, Taylor, and Tibshirani 2013; Simon et al. 2013). This approach facilitates targeted regularization for natural groupings, such as spline terms derived from a single parameter or sets of dummy variables representing high-cardinality categorical predictors.\nHierarchical interaction constraints: Models containing extensive interaction terms can be regularized such that interaction effects remain eligible for removal only when their corresponding main effects are also eliminated from the model, thereby enforcing the hierarchy principle for interactions (Bien, Taylor, and Tibshirani 2013; Lim and Hastie 2015).\nAdaptive and two-stage methods: Initial model fits inform subsequent estimation and penalization strategies (Zou 2006; Meinshausen 2007). For instance, the UniLasso method developed by Chatterjee, Hastie, and Tibshirani (2025) employs slope coefficients from unpenalized univariate models to enhance Lasso penalty performance.\n\nHastie, Tibshirani, and Wainwright (2015) provides a comprehensive overview of penalized regression frameworks, with particular emphasis on Lasso and similar penalties that achieve sparse solutions through predictor elimination.\nA significant limitation of most penalization methods involves their incompatibility with traditional statistical inference techniques14. Quantifying parameter estimate uncertainty becomes particularly challenging when feature selection occurs through Lasso penalties. Even bootstrap resampling encounters substantial difficulties: the bootstrap distributions for many parameters are bimodal, reflecting a mixture of coefficient estimates from models where the parameter was retained and a concentrated mass at zero when it was excluded. While bootstrap confidence intervals for individual parameters may be out of reach, Section 6.2 of Hastie, Tibshirani, and Wainwright (2015) presents bootstrap analyses that quantify selection stability by estimating the probability of non-zero coefficients for each predictor across bootstrap samples.\n\n\nRefitting the Forestation Model\nLet’s take our previous logistic regression Model #5 and create a penalized fit via the glmnet framework. The tuning process involves simultaneous optimization over the amount of penalization and Lasso mixing proportion. We evaluated 50 penalization values from 10-6 to 10-1. In addition, we evaluated five mixing parameter values: pure ridge (0% Lasso), 25% Lasso, 50% Lasso, 75% Lasso, and pure Lasso (100%). All combinations of these parameters form a complete regular grid for evaluation.\nFigure 16.11 (top panel) presents the tuning results, with the Brier score serving as the performance metric. The curves demonstrate that excessive penalty strength leads to elimination of important features and consequent underfitting. Conversely, at minimal penalty levels, Brier scores remain relatively stable across the entire range of mixing proportions. The numerically optimal combination corresponds to 25% Lasso with a penalty value of 10-4.4. However, there are many different combinations with near equal Brier scores.\nThe bottom panel of Figure 16.11 reveals that despite the relative stability of Brier scores across the parameter grid, the \\(\\hat{\\beta}\\) estimates exhibit substantial variation. The labeled predictors are those with the largest absolute coefficient values at the best penalty/mixture combination. These values are spline basis expansion terms for several different predictors.\n\n\n\n\n\n\n\n\nFigure 16.11: Top: Results of tuning the complex logistic model using the glmnet penalty. The dotted vertical line designates the numerically optimal result while the dashed line is the selected penalty value. Bottom: Parameter estimates over penalty values for the best model. The features with the largest values at the selected penalty are highlighted.\n\n\n\n\n\nHowever, when we use the numerically optimal parameter settings, there is substantial evidence of multicollinearity, as evidenced by the continued presence of large parameter values. Consequently, we decided to reduce the penalty value to about 10-3 (designated as the vertical dashed line). This adjustment yields more substantially attenuated coefficients without incurring any meaningful increase in the Brier score, which is estimated to be 0.0835 with 90% confidence interval (0.078, 0.0889). This is almost identical to the value from the unpenalized model. However, with these tuning parameter values, 17 parameters were were set to zero.\n\n\n\n\n16.2.5 Bayesian Estimation\nHaving examined parameter estimation for logistic regression through maximum likelihood methods—both penalized and unpenalized—we now turn to Bayesian estimation approaches. It is important to note that this explanation presents a simplified conceptual framework; the mechanisms for serious Bayesian analysis are highly optimized and sophisticated. Our example will demonstrate fundamental principles but would be inadequate for for solving most real problems.\nRecall that the Bayesian posterior distribution is the endpoint of a Bayesian analysis. It combines our prior belief with the observed data to make a complete probabilistic description of the parameters, given the data. For a small-scale illustration, we will employ a single-predictor model:\n\\[\nlog\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\beta_0 + \\beta_1 {x}_i\n\\]\nwhere \\(i=1, \\ldots, 100\\). We’ll simulate the 100 training set data points using \\(\\beta_0 = 1.0\\) and \\(\\beta_1 = -1.5\\), with predictor values drawn from a uniform distribution on \\(\\pm 1\\). Under these specifications, the simulated dataset exhibits an event rate of 69%.\nLet’s assume that the data are independent Bernoulli random variables. Instead of maximizing the likelihood, the goal is to optimize the posterior probability:\n\\[\nPr[\\boldsymbol{\\beta}| x] = \\frac{Pr[\\boldsymbol{\\beta}]  \\ell(x| \\boldsymbol{\\beta})}{Pr[x]}\n\\]\nAs mentioned in Section 12.5, the objective of Bayesian estimation is to optimize the posterior distribution of the parameters rather than merely identifying point estimates. This requires specifying prior distributions for all unknown parameters. If we know a lot about the data, then we could make an informed decision about the prior distributions. For instance, if we were extremely sure that the slope parameter \\(\\beta_1\\) should be non-negative, then a right-skewed distribution with no negative values would be appropriate. However, keep in mind that our priors define the feasible parameter space. For example, if we used a uniform prior with limits of \\(\\pm 2.0\\), the posterior distribution will never have values outside those limits, since they have zero prior probability of occurring.\nIn the absence of strong prior information, the conventional approach employs broad, diffuse distributions that exert minimal influence on the posterior distribution. Since logistic regression parameters may take values across the entire real line, independent Gaussian priors \\(N(0, 10)\\) represent a natural choice for both \\(\\beta_0\\) and \\(\\beta_1\\). Given that approximately 99.7% of probability mass for these distributions lies within \\(\\pm 3\\) standard deviations, these priors are sufficiently diffuse to provide minimal prior information about likely parameter values. While Gelman et al. (2008) makes a recommendation for a weakly informative default prior, we’ll discuss more complex methods later in this section.\nAt this point, we can calculate the likelihood and the prior probabilities for proposed parameter values. Note that Bayes Rule above also includes \\(Pr[x]\\). This is a normalizing constant and is not a function of the unknown parameters. It is not generally ignorable, but, in this case, the methodology used here does not require its calculation15; we’ll multiply the prior and likelihood to compute an un-normalized posterior probability.\nA practical computational challenge arises in Bayesian estimation: multiplying numerous small probability values can produce products that are numerically indistinguishable from zero. The standard solution involves working in logarithmic space, summing log-probabilities rather than multiplying probabilities directly. Maximizing the sum of log-posterior values yields equivalent results to maximizing the posterior in its natural units, but the logarithmic transformation provides both computational stability and analytical insights. This may seem trivial, but it also helps us understand the effect of the prior. In our example, the prior contribution consists of two probability values, with the product of \\(Pr[\\boldsymbol{\\beta}]\\). The likelihood for this example is the product of \\(n_{tr} = 100\\) probability density values. When computing the log of the posterior, we sum 102 individual log-probabilities, with precisely two originating from the prior distribution. Although these two values come from different probability scales, they contribute to less than 2% of the total sum. Conversely, with a substantially smaller training set of \\(n_{tr} = 8\\) observations, the prior would account for 20% of the posterior. This numerical relationship demonstrates that, absent other factors, the influence of prior information is inherently constrained by sample sizeunless it is extraordinarily opinionated and/or narrow).\nThe Metropolis-Hastings (MH) algorithm is used to demonstrate the process of estimating parameters (Chib and Greenberg 1995; Robert and Casella 2004; Van Ravenzwaaij, Cassey, and Brown 2018). Current technology employs substantially more sophisticated and efficient methods for posterior estimation; however, MH is valuable for illustrating the fundamental principles of Markov Chain Monte Carlo (MCMC) methods. The algorithmic structure resembles simulated annealing, which represents a specific application of the MH framework.\nThe procedure begins with initial draws from the two prior distributions, denoted as \\(\\boldsymbol{\\beta}^{(0)}\\). Using these initial values in conjunction with the observed data, we compute the unnormalized posterior probability. Similar to simulated annealing, we generate proposals within a local neighborhood of the current parameter values. For this demonstration, we’ll take a simple approach and simulate proposed values, \\(\\boldsymbol{\\beta}^{(1)}\\), that are within \\(\\pm 2.0\\) of our current initial values. Upon computing the posterior probability for the proposed \\(\\boldsymbol{\\beta}^{(1)}\\) values, the acceptance decision as follows:\n\nIf the proposed posterior probability exceeds the current value, accept \\(\\boldsymbol{\\beta}^{(1)}\\) as the new best estimate\nOtherwise, accept \\(\\boldsymbol{\\beta}^{(1)}\\) if a random uniform number is larger than the ratio of the first posterior value to the second. If it is not accepted, start the process anew, where the next proposal is again based on \\(\\boldsymbol{\\beta}^{(0)}\\).\n\nIf \\(\\boldsymbol{\\beta}^{(1)}\\) is close (in posterior probability) to \\(\\boldsymbol{\\beta}^{(0)}\\), we have a higher probability of accepting it (as with simulated annealing).\nTable 16.4 shows some of the initial iterations of the MH algorithm. Following the initial parameter value, another proposal is generated in the nearby parameter space. The log-posterior for this proposal is smaller than the initial value, so we do not automatically accept it. The acceptance probability is effectively zero, since the difference in the log-posterior values is -24, and the exponentiated value is very close to zero. The random uniform value for this decision (0.593) formally rejects it. The subsequent proposal, also based on the initial value, is likewise rejected. On the third iteration, the proposed parameter value achieves a larger log-posterior probability and is therefore accepted as the current state.\n\n\n\n\n\n\n\n\n\n\n\nIteration\nParent\nIntercept\nSlope\n(Log) Posterior\nPr[Accept]\nRandom\nDecision\n\n\n\n\n\n\n\n0\n—\n−15.7\n−7.05\n−1,842\n—\n—\ninitial\n\n\n1\n0\n−16.0\n−8.46\n−1,866\n0.000\n0.593\nrejected\n\n\n2\n0\n−17.4\n−5.13\n−1,985\n0.000\n0.587\nrejected\n\n\n3\n0\n−13.9\n−8.01\n−1,694\n—\n—\nimprovement\n\n\n4\n3\n−15.6\n−8.40\n−1,835\n0.000\n0.229\nrejected\n\n\n5\n3\n−14.2\n−6.87\n−1,716\n0.000\n0.043\nrejected\n\n\n6\n3\n−15.0\n−6.97\n−1,783\n0.000\n0.400\nrejected\n\n\n7\n3\n−13.1\n−8.02\n−1,624\n—\n—\nimprovement\n\n\n8\n7\n−14.6\n−6.11\n−1,746\n0.000\n0.479\nrejected\n\n\n\n\n\n19,992\n19,981\n0.761\n−0.461\n−703.8\n0.657\n0.218\nacceptable\n\n\n19,993\n19,992\n2.62\n0.634\n−742.0\n0.000\n0.374\nrejected\n\n\n19,994\n19,992\n0.512\n0.738\n−714.8\n0.000\n0.665\nrejected\n\n\n19,995\n19,992\n1.14\n0.931\n−718.1\n0.000\n0.781\nrejected\n\n\n19,996\n19,992\n1.57\n−0.0206\n−712.9\n0.000\n0.634\nrejected\n\n\n19,997\n19,992\n−0.0426\n−2.20\n−717.2\n0.000\n0.871\nrejected\n\n\n19,998\n19,992\n2.66\n−1.66\n−726.9\n0.000\n0.186\nrejected\n\n\n19,999\n19,992\n1.29\n−2.37\n−707.0\n0.041\n0.381\nrejected\n\n\n20,000\n19,992\n−0.0841\n0.271\n−717.5\n0.000\n0.729\nrejected\n\n\n\n\n\n\n\n\nTable 16.4: A set of iterations of the Metropolis-Hastings algorithm for Bayesian logistic regression at the beginning and ending phases of the optimization.\n\n\n\n\nThis process, called the “warm up phase,” proceeds a large number of times (as defined by the user). We can monitor the proposed parameter values and assess if they appear to converge to a stable parameter space. Figure 16.12 shows the progress of MH for the first 250 iterations. The process starts far away from the true parameters (represented by the dotted lines), and the next 25 iterations happen to sample slope values (\\(\\beta_1\\)) in a narrow, negative range. There are some improved values until the slope meanders to larger values, where new proposals are consistently accepted.\n\n\n\n\n\n\n\n\nFigure 16.12: The coefficient path during the warmup period for Metropolis-Hastings optimization.\n\n\n\n\n\nAround iteration 55, the MH process has settled near the region surrounding the true parameter values. Throughout the remainder of the warm up phase of 250 iterations, proposals continue to be generated within this high-probability region. The acceptance distribution during this stable period consists of 93.4% being outright rejected, 3.6% are provisionally accepted, and 3.1% are improvements (proposals that increase posterior probability and are automatically accepted).\nAt this point, it is reasonable to believe the MH has converged to the location of the optimal values. Since we may not be able to analytically determine the true distribution of the posterior, we spend a good deal of time running new iterations. The non-rejected proposals form a sample of the posterior, and we continue to sample until there are enough data points to make precise numerical summaries. After a total of 19,750 additional iterations, there were 1,463 usable samples. In the last 500 iterations of MH, approximately 9% of proposals were not rejected.\nFigure 16.13 shows the univariate distributions of our parameters. The slope’s posterior mean was -1.18, with 90% of the samples within (-1.99, -0.352). This interval is credible and can come from a more rational probability statement about this parameter (compared to a confidence interval).\n\n\n\n\n\n\n\n\nFigure 16.13: Samples of the posterior distribution (post-warm) for the two logistic regression parameters.\n\n\n\n\n\nThe previous section showed how penalizing the likelihood can mitigate issues with our model related to multicollinearity or irrelevant predictions. Penalization produces biased estimates for those models since they are shrunken towards zero. From a Frequentist perspective, using priors in Bayesian estimation also produces biased estimates since the posterior is shrunken to the prior (to some degree).\nWe mentioned that there are more specialized priors that can be used. For example, we can choose priors to encourage sparsity in our regression parameters (as we did with the lasso model). It turns out that the regularization of the Lasso model is equivalent to using a Laplace distribution (a.k.a the double exponential distribution) as a prior (Tibshirani 1996). Figure 16.14 (left) show an example of the Laplace distribution. Compared to a similar Gaussian distribution, the Laplacian has larger probabilities near zero and in the distribution’s tails. Bayesian models using this prior are discussed by Park and Casella (2008).\n\n\n\n\n\n\n\n\nFigure 16.14: Examples of priors that can be used to encourage sparsity in the parameter values. The spike distribution has been scaled to be smaller for illustration.\n\n\n\n\n\nAnother idea is to use a composite distribution. The “spike and slab” prior contains three distributions (Mitchell and Beauchamp 1988; Malsiner-Walli and Wagner 2011). The first is a slab: a very wide distribution such as our earlier N(0, 10). This is designed to represent our (weak) belief for parameters that should have a non-sparse/non-zero value. The spike element is an incredibly tight distribution centered around zero (such as an N(0, 0.1)) and typifies parameters that should have little or no effect on the model. These two choices are shown in Figure 16.14 (right). Finally, the mixture distribution has values on [0, 1] that indicate how to blend the spike and slab elements. For example, with the values given above, the spike-slab prior could be:\n\\[\nPr[\\beta|\\alpha] = (1 - \\alpha) N(0, 10) + \\alpha N(0, 0.1)\n\\]\nWhat should we use for the mixture prior \\(Pr[\\alpha]\\)? With a Beta distribution, we could have the prior take values of the entire range. This would include a uniform distribution if we lack an opinion on the sparsity rate or we could encode the prior with more informative beliefs. Alternatively, we could choose a Bernoulli distribution that produces values in \\(\\alpha \\in \\{0, 1\\}\\) with some probability. We would choose a value for \\(Pr[\\alpha = 1]\\) or put yet another prior on that parameter.\nWhile these approaches do regularize parameters toward zero, it is important to remember that Bayesian models do not produce a single point estimate for each parameter, but rather a posterior distribution of values. In contrast, methods such as the Lasso and glmnet can shrink coefficients exactly to zero, thereby removing predictors entirely from the model. This behavior does not generally occur in Bayesian models. As illustrated by the priors in Figure 16.14, there is theoretically only an infinitesimal probability that a continuous posterior distribution will place substantial mass at exactly zero. In the case of spike-and-slab priors, the posterior distribution may be bimodal, with one mode concentrated at zero. Nevertheless, even in this setting, predictors are not deterministically excluded from the model. In summary, Bayesian regularization can strongly shrink coefficients toward zero but does not, in practice, eliminate predictors from the prediction equation.\nThe posterior predictive distribution is used to generate predictions for new data. When predicting a new observation at \\(x_0\\), this distribution incorporates both uncertainty in the model parameters (as reflected in the posterior) and uncertainty in the new observation itself, represented by \\(Pr[X = x_0]\\), where \\(X\\) denotes the random predictor variable. Samples from the posterior predictive distribution are used to estimate class probabilities. Although it is common to report a single predictive summary—such as the posterior mean or mode—we can also report credible intervals that quantify uncertainty in these predictions. This ability to directly characterize predictive uncertainty is a key advantage of Bayesian estimation for these models.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear and Additive Classifiers</span>"
    ]
  },
  {
    "objectID": "chapters/cls-linear.html#sec-multinomial-reg",
    "href": "chapters/cls-linear.html#sec-multinomial-reg",
    "title": "16  Generalized Linear and Additive Classifiers",
    "section": "16.3 Multinomial Regression",
    "text": "16.3 Multinomial Regression",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear and Additive Classifiers</span>"
    ]
  },
  {
    "objectID": "chapters/cls-linear.html#sec-cls-gam",
    "href": "chapters/cls-linear.html#sec-cls-gam",
    "title": "16  Generalized Linear and Additive Classifiers",
    "section": "16.4 Generalized Additive Models",
    "text": "16.4 Generalized Additive Models",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear and Additive Classifiers</span>"
    ]
  },
  {
    "objectID": "chapters/cls-linear.html#chapter-references",
    "href": "chapters/cls-linear.html#chapter-references",
    "title": "16  Generalized Linear and Additive Classifiers",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAgresti, A. 2015. Foundations of Linear and Generalized Linear Models. John Wiley & Sons.\n\n\nBechtold, W, and P Patterson. 2015. The Enhanced Forest Inventory and Analysis Program - National Sampling Design and Estimation Procedures. U.S. Department of Agriculture, Forest Service, Southern Research Station.\n\n\nBelsley, D, E Kuh, and R Welsch. 2013. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. John Wiley & Sons.\n\n\nBien, J, J Taylor, and R Tibshirani. 2013. “A Lasso for Hierarchical Interactions.” Annals of Statistics 41 (3): 1111.\n\n\nBreiman, L. 1995. “Better Subset Regression Using the Nonnegative Garrote.” Technometrics 37 (4): 373–84.\n\n\nChatterjee, S, T Hastie, and R Tibshirani. 2025. “Univariate-Guided Sparse Regression.” arXiv.\n\n\nChib, S, and E Greenberg. 1995. “Understanding the Metropolis-Hastings Algorithm.” The American Statistician 49 (4): 327–35.\n\n\nCramer, J. 2004. “The Early Origins of the Logit Model.” Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences 35 (4): 613–26.\n\n\nCressie, N. 2015. Statistics for Spatial Data. John Wiley & Sons.\n\n\nEfron, B, T Hastie, I Johnstone, and Robert R. 2004. “Least Angle Regression.” The Annals of Statistics 32 (2): 407–99.\n\n\nFan, J, and R Li. 2001. “Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties.” Journal of the American Statistical Association 96 (456): 1348–60.\n\n\nFriedman, J, T Hastie, and R Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software 33: 1–22.\n\n\nGelman, A, A Jakulin, M Pittau, and YS Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” The Annals of Applied Statistics, 1360–83.\n\n\nHastie, T, R Tibshirani, and M Wainwright. 2015. Statistical Learning with Sparsity. Chapman; Hall/CRC.\n\n\nHoerl, A, and R Kennard. 1970a. “Ridge Regression: Applications to Nonorthogonal Problems.” Technometrics 12 (1): 69–82.\n\n\nHoerl, A, and R Kennard. 1970b. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics 12 (1): 55–67.\n\n\nHoerl, R. 2020. “Ridge Regression: A Historical Context.” Technometrics 62 (4): 420–25.\n\n\nHosmer, D, S Lemeshow, and R Sturdivant. 2013. Applied Logistic Regression. John Wiley & Sons.\n\n\nKanevski, M, V Timonin, and A Pozdnukhov. 2009. Machine Learning for Spatial Environmental Data: Theory, Applications, and Software. EPFL press.\n\n\nKopczewska, K. 2022. “Spatial Machine Learning: New Opportunities for Regional Science.” The Annals of Regional Science 68 (3): 713–55.\n\n\nLim, M, and T Hastie. 2015. “Learning Interactions via Hierarchical Group-Lasso Regularization.” Journal of Computational and Graphical Statistics 24 (3): 627–54.\n\n\nMalsiner-Walli, G, and H Wagner. 2011. “Comparing Spike and Slab Priors for Bayesian Variable Selection.” Austrian Journal of Statistics 40 (4): 241–64.\n\n\nMcCullagh, P, and J. Nelder. 1989. Generalized Linear Models. 2nd ed. London: Chapman & Hall.\n\n\nMeinshausen, N. 2007. “Relaxed Lasso.” Computational Statistics & Data Analysis 52 (1): 374–93.\n\n\nMitchell, T, and J Beauchamp. 1988. “Bayesian Variable Selection in Linear Regression.” Journal of the American Statistical Association 83 (404): 1023–32.\n\n\nMorgan, B. 1992. Analysis of Quantal Response Data. Chapman; Hall/CRC.\n\n\nNikparvar, B, and JC Thill. 2021. “Machine Learning of Spatial Data.” ISPRS International Journal of Geo-Information 10 (9): 600.\n\n\nPark, T, and G Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86.\n\n\nPregibon, D. 1981. “Logistic Regression Diagnostics.” The Annals of Statistics 9 (4): 705–24.\n\n\nRobert, C, and G Casella. 2004. “Monte Carlo Statistical Methods.” In, 267–320. Springer New York.\n\n\nSimon, N, J Friedman, T Hastie, and R Tibshirani. 2013. “A Sparse-Group Lasso.” Journal of Computational and Graphical Statistics 22 (2): 231–45.\n\n\nTibshirani, R. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 58 (1): 267–88.\n\n\nVan Ravenzwaaij, D, P Cassey, and S Brown. 2018. “A Simple Introduction to Markov Chain Monte-Carlo Sampling.” Psychonomic Bulletin & Review 25 (1): 143–54.\n\n\nWeissfeld, L, and S Sereika. 1991. “A Multicollinearity Diagnostic for Generalized Linear Models.” Communications in Statistics-Theory and Methods 20 (4): 1183–98.\n\n\nZhang, CH. 2010. “Nearly Unbiased Variable Selection Under Minimax Concave Penalty.” The Annals of Statistics 38 (2): 894–942.\n\n\nZou, H. 2006. “The Adaptive Lasso and Its Oracle Properties.” Journal of the American Statistical Association 101 (476): 1418–29.\n\n\nZou, H, and T Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (2): 301–20.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear and Additive Classifiers</span>"
    ]
  },
  {
    "objectID": "chapters/cls-linear.html#footnotes",
    "href": "chapters/cls-linear.html#footnotes",
    "title": "16  Generalized Linear and Additive Classifiers",
    "section": "",
    "text": "Binning is used here as a visualization tool; we re-emphasize that converting numeric predictors into categorical features is problematic.↩︎\nWe’ll assume that class \\(j = 1\\) is the class that is considered the event, for \\(j = 1, \\ldots, C\\) classes.↩︎\nWe often talk about logistic regression as being driven by a Binomial distribution. Either distribution can be used for this model, but the Binomial is used for grouped data where each row of the data set has \\(n_i\\) “trials” and \\(r_i\\) “successes”. In ML, we are not usually in this situation and have a single outcome result per row, hence why we use the Bernoulli (where \\(n_i = 1\\)).↩︎\nThis and other likelihood equations that are simple products make the additional assumption that the rows in the training set are statistically independent.↩︎\nIn fact, as we’ll see shortly, their theory is not limited to logistic regression; it encompasses linear regression, Poisson regression, and other models.↩︎\nRecall from Section 8.4 that unbiased estimates might have high variance. For the forestation data, we’ll see that some of our logistic regression parameter estimates have irrationally large variances. Later in this chapter, we will introduce biased estimation methods (via regularization) to reduce the variance and improve predictive performance.↩︎\nWe’ll discuss regularized models shortly, but in practice, we might start with a complex model (such as Model #5) and use a penalized model to determine which model terms to keep. We’ll do exactly that in the next section.↩︎\nRecall that if there are multiple categorical predictors, the reference cell is multidimensional.↩︎\nThis might be one factor that leads some people to discretize numeric predictors; they can compute odds ratios for “low” and “high” predictor ranges.↩︎\nMeaning that multiple predictors are “co-linear” or have a strong linear relationship with one another↩︎\nThis approach is commonly referred to as an \\(L_2\\) penalty, as it employs the quadratic norm of the coefficients. Also, in neural network literature, this penalty is often termed “weight decay.”↩︎\nThe term “Lasso” is an acronym for “least absolute shrinkage and selection operator.” Also, a penalty based on the absolute parameter values is commonly referred to as an \\(L_1\\) penalty.↩︎\nThis depends on the implementation, though.↩︎\nThe main exception is standard ridge regression, where covariance matrices can be computed once the penalty value is finalized.↩︎\nIt is not required because it does not contain any of the \\(\\beta\\) parameters and will not change the optimization.↩︎",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear and Additive Classifiers</span>"
    ]
  },
  {
    "objectID": "chapters/cls-nonlinear.html",
    "href": "chapters/cls-nonlinear.html",
    "title": "17  Complex Nonlinear Boundaries",
    "section": "",
    "text": "17.1 Nonlinear Discriminants\nThis chapter focuses on several classes of models that naturally produce nonlinear boundaries, such as neural networks, K-nearest neighbors, and support vector machines. A separate category of models, which use tree structures, also has inherently nonlinear classification boundaries. We’ll discuss those models in Chapter 18 and Chapter 19.\nIn this chapter, several recurring motifs are present across different types of models. First, this involves the use of an operation that utilizes the positive part of a function, known as “hinge” or “rectifier” functions. We’ll see these in Section 17.1.2 and Section 17.4.1. Another element that will be encountered across different models is the kernel function, previously seen in Section 12.6.2. These functions are used to quantify the distance or similarity between data points (or, more specifically, vectors). Both the K-nearest neighbor and the support vector machine (SVM) models rely on these functions. Finally, the dot product between vectors plays a critical role in the important attention mechanism in neural networks (?sec-attention) as well as in SVM models (Section 17.6).\nThis chapter contains two sections on neural networks. The first describes the most basic version of these models, called the multilayer perceptron. The fundamentals of these models are discussed in this section. The second section explores various methodologies to enhance the performance of neural network models for tabular data, similar to their effectiveness in predicting images and text. At the time of this writing, substantial research on this topic is underway in the field. This section will also discuss one of the most effective tools for most deep learning models: attention-based transformers.\nTo get started, the first chapter summarizes an old, traditional model for classification, linear discriminant analysis (LDA), and shows how it can be effectively modified to produce nonlinear class boundaries.\nDiscriminant analysis is a classical statistical method for classification. While its prominence in the statistical learning literature has diminished in recent decades, understanding its foundations provides valuable context for several modern and highly effective classification methods. This section focuses on linear discriminant analysis (LDA).\nLinear discriminant analysis (McLachlan 2005; Murphy 2012) produces linear classification boundaries and can be motivated via Bayes’ Rule. Let’s say that our training set predictors for class \\(k\\) are contained in the random variable vector \\(X_k\\) with \\(p\\) elements. LDA assumes that the predictor distributions within each class follow a multivariate Gaussian distribution with class-specific mean vectors but a common covariance matrix across all classes. Formally, for class \\(k\\), the conditional distribution of the predictors given class membership is \\(X_k \\sim N(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma})\\).\nTo classify a new observation with predictor vector \\(\\boldsymbol{x}\\), LDA applies Bayes’ theorem to compute the posterior probability of class membership. Given a prior probability \\(Pr[Y = k]\\) for class \\(k\\), the posterior probability is:\n\\[\nPr[Y = k |\\boldsymbol{x}] =\\frac{Pr[Y = k]Pr[\\boldsymbol{x}|Y = k]}{\\sum\\limits_{l=1}^C Pr[Y = l]Pr[\\boldsymbol{x}|Y = l]}\n\\tag{17.1}\\]\nThis equation is straightforward to calculate. The parameters \\(\\boldsymbol{\\mu}_k\\) and \\(\\boldsymbol{\\Sigma}\\) are estimated from the training data using their maximum likelihood estimates: the class-specific sample mean vectors \\(\\bar{\\boldsymbol{x}}_k\\) and the pooled sample covariance matrix \\(\\boldsymbol{S}\\), respectively. The class-conditional densities (\\(Pr[\\boldsymbol{x}|Y]\\)) are then evaluated using the multivariate normal probability density function, which modern statistical software computes efficiently. In practice, computational efficiency can be improved by calculating only the numerator of Equation 17.1 for each class, then normalizing these values to ensure the posterior probabilities sum to 1. For a new observation \\(\\boldsymbol{x}\\), the class with the maximum posterior probability is assigned as the predicted class.\nAlternatively, we can frame the prediction in terms of the discriminant function:\n\\[\nD_k(\\boldsymbol{x}) = \\boldsymbol{x}'\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_k + \\frac{1}{2}\\boldsymbol{\\mu}_k'\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_k + \\log\\left(Pr[Y = k]\\right)\n\\tag{17.2}\\]\nThe discriminant values are conceptually similar to the linear predictor in logistic regression, taking values on the real line, with larger values corresponding to higher posterior probabilities. The predicted class is determined by identifying which discriminant function yields the largest value.\nA notable advantage of LDA is its explicit modeling of the predictor covariance structure through \\(\\boldsymbol{\\Sigma}\\). This characteristic renders the method robust to moderate levels of multicollinearity among predictors. Furthermore, the framework naturally extends to multi-class problems.\nHowever, LDA has important limitations. The method requires that \\(\\boldsymbol{\\Sigma}\\) be non-singular (invertible). When the training set contains fewer observations than predictors (\\(n &lt; p\\)) or when perfect linear dependencies exist among predictors, the sample covariance matrix becomes singular and the discriminant functions cannot be computed. Additionally, the Gaussian distributional assumption is fundamental to the method’s theoretical justification. While binary indicator variables can technically be included in the model (as they are numeric), their discrete, non-Gaussian nature violates the model assumptions and undermines the probabilistic interpretation of the results. For datasets with categorical predictors or high-dimensional settings where \\(n &lt; p\\), alternative classification methods may be more appropriate as we will see later.\nAs presented, LDA is limited as a classification tool. Its assumption of a common covariance matrix across classes restricts it to producing linear decision boundaries. While nonlinear boundaries could be induced through feature engineering (e.g., adding polynomial or interaction terms), such an approach would be better done using logistic regression.\nNevertheless, the discriminant analysis framework has led to extensions that address the various limitations of LDA. Quadratic discriminant analysis (QDA) (Ghojogh and Crowley 2019) relaxes the assumption of a common covariance matrix, allowing each class to have its own covariance structure: \\(X_k \\sim N(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\). This modification results in quadratic decision boundaries, providing greater flexibility at the cost of estimating additional parameters. Robust variants of both LDA and QDA have been developed to mitigate the influence of outliers on parameter estimation (Hubert, Raymaekers, and Rousseeuw 2024), addressing the well-known sensitivity of maximum likelihood estimates to extreme observations. Additionally, regularized discriminant analysis methods (Friedman 1989; Pang, Tong, and Zhao 2009) introduce penalty terms that shrink the covariance matrix estimates, improving stability and predictive performance, particularly when sample sizes are modest relative to the number of predictors.\nTwo extensions of discriminant analysis are practically useful. The first, naive Bayes classification, represents a simplification that assumes conditional independence among predictors within each class. The second, flexible discriminant analysis (FDA), is a generalization of the LDA framework. FDA can accommodate nonlinear decision boundaries, perform implicit feature selection, and incorporate various modeling strategies, making it a considerably more versatile tool for classification problems. These methods are discussed in the following sections.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Complex Nonlinear Boundaries</span>"
    ]
  },
  {
    "objectID": "chapters/cls-nonlinear.html#sec-nonlinear-da",
    "href": "chapters/cls-nonlinear.html#sec-nonlinear-da",
    "title": "17  Complex Nonlinear Boundaries",
    "section": "",
    "text": "17.1.1 Naive Bayes\nWhile LDA estimates the class-conditional density \\(Pr[\\boldsymbol{x}|Y = k]\\) using a multivariate Gaussian distribution, naive Bayes (Hand and Yu 2001) adopts an alternative approach by decomposing this joint density into a product of univariate marginal distributions:\n\\[\nPr[\\boldsymbol{x}|Y = k] = \\prod_{j=1}^p Pr[x_j|Y = k]\n\\tag{17.3}\\]\nThis formulation assumes conditional independence among all predictors within each class; i.e., the predictors are all statistically independent of one another. This assumption is often violated in practice. Despite this seemingly restrictive assumption, naive Bayes classifiers can occasionally perform surprisingly well in practice.\nThe primary advantage of the conditional independence assumption is that it relaxes the distributional requirements for individual predictors. Unlike LDA, which requires joint multivariate normality, naive Bayes allows each predictor to follow its own distribution, which need not be Gaussian. Moreover, these distributions do not even need to be fully parametric.\nFor continuous predictors, kernel density estimation (Silverman 1986; Chen 2017) provides a flexible, nonparametric approach to estimating \\(Pr[x_j|Y = k]\\). Kernel density estimators are smoothing methods that adaptively characterize the empirical distribution of the data. A kernel function \\(K(\\cdot)\\) is a non-negative, symmetric function that integrates to one and serves as a weighting scheme1. Common kernel functions are:\n\nThe uniform kernel, which assigns equal weight to all observations within a fixed range\nThe Gaussian kernel, \\(K(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-u^2/2)\\), which assigns weights that decrease exponentially with distance from the point of interest\n\nThe kernel density estimate at a point \\(u\\) is constructed as:\n\\[\n\\hat{f}(u) = \\frac{1}{nh}\\sum_{i=1}^n K\\left(\\frac{u - x_i}{h}\\right)\n\\]\nwhere \\(n\\) is the number of observations, \\(x_i\\) are the observed training set data points, and \\(h\\) is the bandwidth parameter that controls the smoothness of the estimate. We’ll come back to the bandwidth parameter shortly.\nFor example, Figure 17.1 displays estimated densities for the minimum vapor pressure variable across three subsets of the data. Kernel density estimates are shown in blue, while fully parametric Gaussian density estimates (assuming \\(N(\\mu, \\sigma)\\)) are shown in orange. The left-hand panel presents the entire training set, which exhibits modest right skewness. Both estimation approaches provide reasonable approximations for vapor pressure values2 above 200. However, the Gaussian estimate lacks the flexibility to adequately capture the density in the middle of the range, and both methods perform poorly for values below 200, where the data are sparse.\nThe middle panel shows the density for non-forested locations. The histogram suggests these data are approximately trimodal with left skewness, and the kernel density estimate successfully captures these multiple modes. In contrast, the Gaussian does not capture the multimodal structure, and relying on this parametric assumption would likely result in underfitting and poor probability estimates for classification.\nThe right-hand panel displays the density for forested locations, which exhibits right skewness. The parametric Gaussian approach provides a particularly poor fit for the lower half of this distribution, where the bulk of the data reside. The kernel density estimate adapts to the asymmetry and provides a better representation of the empirical distribution.\nThese examples illustrate the potential advantage of kernel density estimation within the naive Bayes framework: the ability to accommodate non-Gaussian, multimodal, and skewed distributions without requiring distributional transformations or parametric assumptions that may be violated in practice.\n\n\n\n\n\n\n\n\nFigure 17.1: Parametric and nonparametric density estimates for the minimum vapor predictor, shown separately for the entire training set and then for each class. The blue line is the nonparametric estimate, and the orange curve is the traditional Gaussian estimate.\n\n\n\n\n\nThe kernel density estimator contains a tuning parameter: the bandwidth \\(h\\), which controls the degree of smoothing applied to the density estimate. The bandwidth determines the effective width of the kernel’s influence around each observation. Small bandwidth values produce density estimates that closely track the observed data, potentially capturing fine-scale features but risking variability and overfitting. Conversely, large bandwidth values distribute the kernel weight over a broader range, yielding smoother density estimates that may obscure multimodality or other important distributional characteristics.\nSeveral data-driven methods exist for selecting the bandwidth that optimizes specific criteria. Common approaches include minimizing an estimate of the mean integrated squared error or using cross-validation (Park and Marron 1990). These default bandwidth selection methods are designed to produce generally accurate density estimates.\nHowever, the bandwidth that yields the most accurate density estimate may not necessarily produce the best classification performance. The objective in density estimation—minimizing the mean squared error—differs from the classification objective of minimizing prediction error. To address this potential misalignment, the bandwidth can be treated as a tuning parameter in the naive Bayes model. Rather than directly tuning \\(h\\), it is often more convenient to tune a bandwidth adjustment factor that is a multiplier of the bandwidth. A value greater than 1.0 encourages more smoothing of the data, while a smaller values make very bumpy densities.\nSince naive Bayes does not assume much about \\(Pr[x_j|Y = k]\\), categorical predictors can be readily incorporated into the model. For a categorical predictor, the conditional probabilities are estimated directly from the observed frequencies within each class. Figure 16.3 showed the raw probabilities associated with the forested class (i.e., \\(Pr[y = forested]\\)).\nHowever, sparse data can create computational difficulties. Some counties contain no forested locations, while others have very few locations at all. When a particular predictor-class combination is unobserved in the training data, the estimated probability is zero. Due to the multiplicative structure in Equation 17.3, a single zero probability causes the entire class-conditional probability to equal zero, regardless of the values of other predictors. This results in the class receiving zero posterior probability, which is clearly problematic, especially when it occurs with all classes.\nSeveral smoothing methods address this issue by adjusting probability estimates away from the boundaries of zero and one. One approach employs a Beta-Binomial model, as described in Equation 12.4, which shrinks extreme estimates toward more moderate values. For example, consider a county with ten locations, none of which are forested. The Beta-Binomial method with \\(\\alpha = 1\\) and \\(\\beta = 3\\) would pull the estimate from 0.0% to 7.1%.\nAlternatively, Laplace smoothing (Manning, Raghavan, and Schutze 2009, sec. 11.3.2) provides another approach:\n\\[\n\\hat{p}_{k} = \\frac {x_k+\\alpha }{n_k+\\alpha C_{x}}\n\\tag{17.4}\\]\nwhere \\(C_{x}\\) is the number of levels of the qualitative predictor and \\(x_k\\) and \\(n_k\\) are the number of events and locations, respectively. Here, with \\(C_x = 2\\) and \\(\\alpha = 1\\), the new estimate is 8.3%.\nDespite the potentially unrealistic assumption of conditional independence among predictors, naive Bayes classifiers can demonstrate competitive predictive performance with other methods. In addition, naive Bayes models can be computationally efficient since many of the calculations can be done in parallel. Depending on how the conditional densities are computed, they can be easily added to database tables, and the final prediction can be executed using highly efficient SQL.\nOne potential downside for this model is that there is a tendency for the class probability estimates to be seriously miscalibrated. Since we treat the predictors as independent entities, we end up multiplying many probabilities together. When the predictors are related, this means including redundant terms. When any set of probabilities is multiplied together, there is a tendency for the values to become polar; the products tend to migrate towards zero or one. This may not effect the ability of the model to separate classes, but it will compromise the accuracy of the probability estimates. Many predictions, when incorrect, are confidently incorrect.\nFigure 17.2 illustrates the probability distribution and calibration performance for one model candidate applied to the forestation data. The left panel demonstrates that the predicted probabilities exhibit a bimodal distribution, with the majority of observations concentrated near the boundaries of zero and one, while relatively few observations fall in the intermediate probability range. The right panel presents the corresponding calibration curve, which reveals severe miscalibration. Specifically, observations assigned predicted probabilities near zero exhibit observed event rates of approximately 10%, while those with predicted probabilities approaching one show similarly discordant observed rates. The intermediate probabilities are no better, with observed event rates clustering near 0.5 across multiple bins regardless of predicted probability. The Brier score for these data is poor (0.128) but the area under the ROC curve (0.922) shows good separation of the classes by the model.\n\n\n\n\n\n\n\n\nFigure 17.2: An example of how the probability estimates produced by naive Bayes can have poor calibration properties.\n\n\n\n\n\nTo address these calibration issues, an initial feature filter could be helpful. Reducing the number of predictors may improve probability estimates by mitigating potential overfitting or reducing noise from uninformative predictors. For the forestation data, a random forest model was computed (within each resample), and predictors were ranked according to their permutation importance scores. We can choose a proportion of parameters to retain and then fit our naive Bayes models. Proportions of 20%, 40%, …, 100% were used during the tuning process. Figure 17.3 displays the resulting importance scores, with error bars representing 90% confidence intervals estimated using the 10 replicate values produced during cross-validation.\n\n\n\n\n\n\n\n\nFigure 17.3: Aggregate feature rankings using a random forest importance score on the forestation data. The bands reflect 90% confidence intervals computed using replicates over cross-validation.\n\n\n\n\n\nThe results show that, according to random forest, annual precipitation has, by far, the largest effect on the prediction function. The second tier of importance includes maximum vapor pressure and longitude predictors, which exhibit nearly equivalent importance scores. A moderate cluster of predictors shows intermediate importance values, while several predictors—including county, year, eastness, and westness—yield low importance scores.\nIt is important to note that variable importance measures are model-specific and should not be interpreted as universal or absolute assessments of predictor relevance3. In this context, the random forest importance scores serve as a filtering mechanism to identify and remove potentially irrelevant predictors prior to fitting the classification model, rather than as definitive decisions regarding predictor-response relationships.\nAdditionally, we have summarized the results over resamples for visualization. In practice, we execute the random forest within each resample and then fit the model on that resample’s listing of which predictors are “important”. We do not compute the importance once and use the same ranking across resamples.\nFor the forestation data, the naive Bayes kernel adjustment parameter was tuned over values ranging from 0.5 to 1.5, while the Laplace smoothing parameter was held constant at \\(\\alpha = 1\\). Beyond feature selection, no additional preprocessing was applied; the categorical counties remained as-is , and no transformations were applied to the data.\nThe tuning results for Brier score and ROC AUC are presented in Figure 17.4. Both metrics achieved optimal performance when the full predictor set was retained, with substantial performance degradation observed when only 20% of predictors were included. Across all feature selection thresholds, the results favor smaller kernel bandwidths (more wiggly). However, the magnitude of improvement attributable to bandwidth adjustment is modest relative to the effect of feature selection proportion.\nThe predictions for the best candidate are those shown in Figure 17.2. As noted there, the Brier score was mediocre while the ROC curve results indicate that the model can do a good job at qualitative separating the classes.\n\n\n\n\n\n\n\n\nFigure 17.4: Naive Bayes tuning results for the forestation data.\n\n\n\n\n\nIt is also worth noting that, for this model, the Brier and ROC AUC values point to similar findings. Across all of the candidates, the correlation between these metrics was -0.95; they tend to agree very often. The correlation between these metrics and cross-entropy was slightly lower: -0.72 and 0.85 for the Brier score and the ROC AUC, respectively.\nNow, let’s turn our attention to the other notable variation of LDA: flexible discriminant analysis.\n\n\n\n17.1.2 Flexible Discriminants\nFlexible discriminant analysis (FDA) by Hastie, Tibshirani, and Buja (1994) extends LDA to accommodate more complex discriminant functions. The derivation involves substantial linear algebra; you can skip to the “Important” box below for the key concepts.\nHand (1981) and Breiman and Ihaka (1984) demonstrated that the LDA model can be estimated through a series of basic linear regressions. To do this, we first construct an \\(n_{tr}\\times C\\) indicator matrix \\(\\boldsymbol{Y}\\) where each column contains binary indicators for the corresponding class. Next, create a \\(C\\times (C-1)\\) matrix of initial “scores” \\(\\boldsymbol{\\Omega}\\) using a contrast function (see Section 6.2). Hastie and Tibshirani (2024) employ a Helmert-like contrast matrix. For \\(C=3\\) classes, the Helmert contrast is\n\\[\nHelmert =\n\\begin{pmatrix}\n-1 & -1 \\\\\n\\phantom{-}  1 & -1 \\\\\n\\phantom{-}  0 &\\phantom{-}   2 \\\\\n\\end{pmatrix}\n\\]\nwhich is then normalized using a function of training set class proportions. For example, with \\(C = 3\\) equally frequent classes, the initial score matrix is:\n\\[\n\\boldsymbol{\\Omega} =\n\\begin{pmatrix}\n\\phantom{-} 1.22 & -0.71 \\\\\n-1.22 & -0.71 \\\\\n\\phantom{-} 0.00 & \\phantom{-} 1.41 \\\\\n\\end{pmatrix}\n\\]\nBeing a contrast matrix, the column means are zero and have covariances of zero.\nThe modified response matrix for linear regression is computed as \\(\\boldsymbol{\\Omega}^* = \\boldsymbol{Y}\\boldsymbol{\\Omega}\\). Using the predictor matrix \\(\\boldsymbol{X}\\) and the columns of \\(\\boldsymbol{\\Omega}^*\\) as outcomes, multivariate linear regression yields coefficient matrix \\(\\widehat{\\boldsymbol{B}}\\) and fitted values \\(\\hat{\\boldsymbol{\\Omega}} = \\boldsymbol{X}\\hat{\\boldsymbol{B}}\\).\nAn eigendecomposition of \\(\\boldsymbol{\\Omega}'\\hat{\\boldsymbol{\\Omega}}\\) produces eigenvalues and a matrix of eigenvectors \\(\\boldsymbol{\\Phi}\\).\nThe matrix \\(\\boldsymbol{\\Phi}\\) scales the linear regression predictions to approximate the discriminant functions in Equation 17.2. For a new sample \\(\\boldsymbol{x}\\), the vector of FDA discriminant functions is:\n\\[\n\\boldsymbol{D}(\\boldsymbol{x}) \\approx \\hat{\\boldsymbol{\\Omega}}\\Phi = \\boldsymbol{x}'\\hat{\\boldsymbol{B}}\\Phi\n\\tag{17.5}\\]\nThis \\((C-1) \\times 1\\) vector contains entries for all but the reference class4. Class probabilities are computed from the discriminant functions (typically using the softmax transformation). We compute last probability estimate using \\(\\hat{\\pi}_C = 1 - \\sum_k^{C-1}\\hat{\\pi}_k\\).\n\nThe magic in Hastie, Tibshirani, and Buja (1994) is that linear regression can be replaced with any regression method while maintaining the discriminant analysis framework. For example, ridge or lasso regression can produce improved predictions that are then converted to class probabilities by rescaling with \\(\\boldsymbol{\\Phi}\\) as in Equation 17.5.\n\nHastie, Tibshirani, and Buja (1994) originally implemented FDA using multivariate adaptive regression splines (MARS) and a penalized spline method called Bruto. In practice, we have found that the best application of FDA uses MARS, which will be discussed further in ?sec-mars.\nAs we will see in ?sec-reg-nonlinear, MARS is a forward stepwise procedure that sequentially adds pairs of features for each predictor. These features are derived via a hinge function: \\(h(x) = xI(x &gt; 0)\\). At each iteration, MARS identifies the optimal predictor and split point \\(a\\), then adds two terms: \\(h(x - a)\\) and \\(h(a - x)\\). The first term equals zero for \\(x &lt; a\\), while the second equals zero for \\(x &gt; a\\). Figure 17.5 illustrates these “left” and “right” hinge functions.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: shiny-fda-hinge\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: Examples of a single set of MARS features. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nui &lt;- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(xs = c(-2, 8, -2), sm = 4),\n    sliderInput(\n      \"beta_1\",\n      label = \"Left slope\",\n      min = -4.0,\n      max = 4.0,\n      step = 0.5,\n      value = 1\n    ),\n    sliderInput(\n      \"beta_2\",\n      label = \"Right slope\",\n      min = -4.0,\n      max = 4.0,\n      step = 0.5,\n      value = 1\n    ),\n    sliderInput(\n      \"split\",\n      label = \"Split\",\n      min = 0,\n      max = 10,\n      step = 0.1,\n      value = 5\n    )\n  ),\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(xs = c(-2, 8, -2)),\n    as_fill_carrier(plotOutput(\"plots\"))\n  )\n)\n\nserver &lt;- function(input, output) {\n  theme_set(theme_bw())\n  output$plots &lt;-\n    renderPlot(\n      {\n        dat &lt;- tibble(x = seq(0, 10, by = 0.05))\n\n        h_right &lt;- function(x, a) ifelse(x &gt; a, x - a, 0)\n        h_left &lt;- function(x, a) ifelse(x &lt; a, a - x, 0)\n\n        split &lt;- input$split\n        beta_0 &lt;- 0\n        beta_1 &lt;- input$beta_1\n        beta_2 &lt;- input$beta_2\n\n        feat &lt;-\n          dat |&gt;\n          mutate(\n            left = h_left(x, split),\n            right = h_right(x, split),\n            pred = beta_0 + beta_1 * left + beta_2 * right\n          )\n\n        p &lt;-\n          feat |&gt;\n          pivot_longer(\n            cols = c(left, right, pred),\n            names_to = \"side\",\n            values_to = \"value\"\n          ) |&gt;\n          mutate(\n            group = case_when(\n              side == \"left\" ~ \"Left Hinge: h(x - a)\",\n              side == \"right\" ~ \"Right Hinge: h(a - x)\",\n              TRUE ~ \"Regression Equation\"\n            ),\n            group = factor(\n              group,\n              levels = c(\n                \"Left Hinge: h(x - a)\",\n                \"Right Hinge: h(a - x)\",\n                \"Regression Equation\"\n              )\n            )\n          ) |&gt;\n          ggplot(aes(x, value)) +\n          geom_line() +\n          geom_vline(xintercept = split, lty = 3) +\n          facet_wrap(~group, ncol = 1, scale = \"free_y\") +\n          labs(x = \"Original Predictor\", y = \"Feature Value\")\n\n        print(p)\n      },\n      res = 100\n    )\n}\n\napp &lt;- shinyApp(ui, server)\n\napp\n\n\n\nFigure 17.5: An example of how MARS partitions the predictor space into two distinct regions, each with its own regression line that connects at the split point. For the bottom panel, we assume an intercept of \\(\\beta_0 = 0\\) for Figure 17.5.\n\n\n\nAs seen in the figure, the hinge feature \\(h(x − a)\\) isolates the effect of the predictor to values greater than \\(a\\). Likewise, its companion feature \\(h(a − x)\\) restricts the effect of the predictor only when \\(x &lt; a\\). If \\(x\\) is the first predictor selected by MARS, these functions create a flexible segmented regression:\n\\[\ny_i = \\beta_0 + \\beta_1 h(x_{i1} - a) + \\beta_2 h(a - x_{i1}) + \\epsilon_i\n\\]\nwhich is estimated using ordinary linear regression. The figure illustrates the impact of different values of \\(\\beta_1\\) and \\(\\beta_2\\).\nThe “growing” phase of MARS creates new pairs of features by evaluating all prospective pairs, retaining the pair that most improves model fit, and repeating until a user-specified maximum number of terms is reached.\nDuring the training process, MARS may split the same predictor multiple times when the predictor-response relationship exhibits substantial nonlinearity. For categorical predictors with \\(C\\) levels, MARS automatically generates \\(C\\) binary indicators that can then be split. The algorithm can also include a predictor in its original form (without hinge functions) if that representation best improves the model at a given iteration. The maximum number of model terms is an important tuning parameter that controls model complexity.\nThus far, we have described MARS when it is constrained to produce an _additive_model (i.e., where each feature depends on a single predictor). MARS can also generate interaction terms. After selecting an initial pair of hinge functions on one predictor, the algorithm conducts a secondary search over all other predictors (and their candidate split points) to find the combination that most improves the model. This produces four new features, each representing a distinct two-dimensional region of the predictor space. The degree of interaction (typically one or two) is a tuning parameter; higher-order interactions are possible but computationally prohibitive.\nFollowing the growing phase, MARS performs a backward pass that sequentially removes terms using generalized cross-validation (GCV), an efficient approximation to leave-one-out cross-validation that is available when parameters are estimated by ordinary least squares (Golub, Heath, and Wahba 1979). GCV quantifies the increase in prediction error when a given term is removed. During pruning, one or both members of a hinge pair may be removed. The final model consists of the retained features, their estimated coefficients, and the \\(\\Phi\\) matrix, which together define the prediction function.\nFDA-MARS models can produce class boundaries that are segmented or trapezoidal. For example, the “Medium Complexity” and “High Complexity” panels in Figure 16.1 were generated using FDA-MARS, with interactions and differing numbers of retained features.\nRegarding preprocessing, MARS internally converts categorical predictors to binary indicators, so this transformation should not prior to model fitting. Since MARS estimates feature coefficients using ordinary least squares, standard preprocessing techniques for linear models apply. For instance, transforming predictors to improve symmetry may enhance performance. Missing values must also be imputed prior to modeling.\nMARS performs automatic feature selection: if a predictor is never used in a basis function or if all terms containing that predictor are removed during pruning, the final model is independent of that predictor. This property extends to FDA-MARS. As illustrated in Figure 13.1, irrelevant predictors can be included during training without adverse effects, eliminating the need for a separate feature selection step. However, including irrelevant features will increase computation time. This may or may not have a practical impact, depending on the data set size and number of irrelevant features.\nFor using MARS with FDA with our forestation data, we initially generated a total of 200 features (apart from the intercept) during the growing phase. To tune the model, we considered additive and interaction models as well as using forward and backward removal strategies for the pruning phase. The number of retained terms was optimized to be between 2 and 50 final model terms.\nWe can see how this worked out in Figure 17.6. Performance leveled off around 25-35 terms for each pruning method, with forward selection showing a tiny advantage for these data. Models with interaction effects were uniformly more successful than additive models. An interaction model using forward selection and 30 terms appears to be a reasonable choice. This configuration had an estimated Brier score of 0.083 and an area under the ROC curve of 0.95.\n\n\n\n\n\n\n\n\nFigure 17.6: FDA-MDA tuning results for the forestation data.\n\n\n\n\n\nOnce the FDA-MARS model was fit to the training set, the results was 25 total coefficients. The implementation of MARS checks several stopping conditions for the growing phase based on the coefficient of determination (a.k.a. R2), including:\n\nReached a R2 of 0.999 or more.\nNo new term increases R2\nAdding a term changes R2 by less than 0.001.\n\nThe last condition triggered the training procedure to stop at 25 model terms instead of the 30 that were requested. Of these terms, 6 used a single predictor and 18 were interaction terms. The categorical predictor (county) was used 3 times in both main effects and interactions. To give the reader a sense of what the linear regression model looks like, Table 17.1 shows the set of coefficients, the model terms, and split points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Term\nvalue\n\n\n\n\n(Intercept)\n−1.04\n\n\nh(834 - annual precipitation) x h(annual maximum temperature - 10.3)\n0.0000138\n\n\nh(43 - roughness)\n0.0173\n\n\nh(annual precipitation - 650) x h(8.65 - annual maximum temperature)\n0.000135\n\n\nh(annual mean temperature - 7.31) x h(longitude - - 118.905)\n0.0876\n\n\nh(650 - annual precipitation)\n0.00478\n\n\nh(1920 - annual precipitation) x h(annual mean temperature - 7.31)\n0.000180\n\n\nh(650 - annual precipitation) x h(annual mean temperature - 7.95)\n−0.00122\n\n\nh(650 - annual precipitation) x county: ferry\n−0.00431\n\n\nh(650 - annual precipitation) x county: stevens\n−0.00409\n\n\nh(1348 - maximum vapor) x county: whitman\n0.00978\n\n\nh(141 - elevation) x h(annual maximum temperature - 10.3)\n0.000645\n\n\nh(650 - annual precipitation) x h(7.95 - annual mean temperature)\n−0.000898\n\n\nh(annual mean temperature - 7.31) x h( - 118.905 - longitude)\n−0.0332\n\n\nh(year - 2020) x h(43 - roughness)\n−0.0158\n\n\nh(0.3 - dew temperature) x h(annual mean temperature - 7.31)\n−0.708\n\n\nh(43 - roughness) x h(1018 - annual precipitation)\n−0.0000238\n\n\nh(43 - roughness) x h(annual precipitation - 1018)\n−0.00000822\n\n\nh(roughness - 43)\n0.00164\n\n\nh(annual mean temperature - 7.31)\n0.151\n\n\nh(10.3 - annual maximum temperature)\n0.0876\n\n\nh(roughness - 7) x h(annual mean temperature - 7.31)\n−0.000739\n\n\nh(annual precipitation - 650)\n0.0000552\n\n\nh(elevation - 141) x h(annual maximum temperature - 10.3)\n0.0000548\n\n\nh(7 - roughness) x h(annual mean temperature - 7.31)\n0.00595\n\n\n\n\n\n\n\n\n\nTable 17.1: Model terms contained used the final FDA-MARS fit from the training set. There were 25 total coefficients, 6 of which used a single predictor, and 18 were interaction terms. The categorical predictor (county) was used in 3 terms.\n\n\n\n\n\n\n\nTo better understand what is driving the model, MARS has an internal variable importance metric. The method estimates variable importance based on the improvement in model fit as terms are added. During the forward pass, MARS quantifies the reduction in GCV associated with each new basis function and attributes this improvement to the corresponding predictor(s). These GCV reductions are summed across all terms involving each predictor and scaled to range from 0 to 100, where 0 indicates the predictor does not appear in the final model. For our model, Figure 17.7 displays the results. Annual precipitation and annual maximum temperature were the most influential predictors, each receiving an importance score of 100. Terrain roughness, annual mean temperature, and longitude also contributed substantially to the classifier. There were 6 predictors that were not used at all: annual minimum temperature, eastness, january minimum temperature, latitude, minimum vapor, and northness.\n\n\n\n\n\n\n\n\nFigure 17.7: MARS feature importance for the final forested model.\n\n\n\n\n\nFDA via MARS occupies a middle ground between fully interpretable models and black-box methods. While it may not achieve optimal predictive performance, it often approaches it while still being interpretable through visualization, particularly when the model is additive.\nFor example, suppose we chose an additive model based on the tuning results in Figure 17.6. In an additive model, the functional form relating each predictor to the outcome is independent of the other predictors. Although the values of other predictors shift the range of predicted probabilities, the shape of each predictor’s effect remains constant. This property enables us to construct partial dependence plots showing how predicted class probabilities vary with each predictor while holding others fixed. Figure 17.8 illustrates such profiles from an additive fit, enabling an interpretation of the model’s behavior.\n\n\n\n\n\n\n\n\nFigure 17.8: Profile plots showing how each predictor related to the probability of forestation when an additive FDA-MDA is used. The y-axis has no numbers since the probabilities are relative.\n\n\n\n\n\nFor example, we can say that:\n\nThe probability of forestation has notable jump after the year 2020.\nThe probability increases sharply when annual precipitation exceeds approximately 500 mm and remains elevated thereafter (holding other predictors constant).\nThe downward spike in the longitude corresponds to the unforested region east of Seattle.\n\nand so on. These patterns illustrate how individual predictors influence the classification boundary in an interpretable manner.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Complex Nonlinear Boundaries</span>"
    ]
  },
  {
    "objectID": "chapters/cls-nonlinear.html#sec-cls-knn",
    "href": "chapters/cls-nonlinear.html#sec-cls-knn",
    "title": "17  Complex Nonlinear Boundaries",
    "section": "17.2 K-Nearest Neighbors",
    "text": "17.2 K-Nearest Neighbors\nK-nearest neighbors (KNN) is predicated on the idea that data points that are close to one another in the predictor space should have outcomes that are also similar. If we are predicting a new sample, we find the K training set points that are “closest” to the new data point and use their outcome values to estimate the prediction. There is essentially no training here; everything happens when the sample is being predicted. KNN falls into a class of techniques called instance-based learning since it stores the training set instances and directly uses these data during prediction.\nWhat does “similar” and/or “close” mean? The operational definition of “proximity” or “closeness” requires specifying either a distance or similarity metric. While Euclidean distance is a common metric, the Minkowski distance provides a more flexible framework. For a new observation \\(\\boldsymbol{u}\\) with \\(p\\) predictors, the Minkowski distance is defined as:\n\\[\nd(\\boldsymbol{u}, \\boldsymbol{x}) ={\\biggl (}\\sum _{j=1}^{p}|u_j-x_{j}|^{degree}{\\biggr )}^{\\frac {1}{degree}},\n\\]\nwhere \\(degree\\) is the distance parameter. This formulation yields Euclidean distance when \\(degree = 2\\), Manhattan distance (also called taxicab or city-block distance) when \\(degree = 1\\), and intermediate distance metrics for other values of \\(degree\\).\nThe choice of distance metric can substantially influence model performance and can be considered a tuning parameter in practice (via the Minkowski degree).\nBeyond the Minkowski family, several specialized distance metrics may be appropriate depending on the structure and nature of the predictor space:\n\nGeodesic distance (discussed in Section 7.3.1) is based on a general distance matrix to construct a graph-based representation of inter-point relationships.\nHaversine distance (introduced in Section 7.4) accounts for the curvature of the Earth’s surface and is the appropriate metric when predictors represent geographic coordinates (latitude and longitude).\nMahalanobis distance (De Maesschalck, Jouan-Rimbaud, and Massart 2000; Johnson and Wichern 2007) incorporates the covariance structure among predictors, effectively scaling predictor differences by their variances and correlations. This metric is particularly valuable when predictors exhibit substantial correlation and sufficient data are available to reliably estimate the covariance matrix. The Mahalanobis distance has a direct connection to the mathematics of discriminant analysis.\n\nThese distance-based metrics require numeric predictors. For the Minkowski distance, predictors must be expressed in the same units to prevent variables with larger scales from dominating the distance calculation. This is typically accomplished through centering and scaling (standardization) or other normalization techniques. Additionally, addressing skewness through appropriate transformations (e.g., Box-Cox, Yeo-Johnson, orderNorm, etc.) prior to distance calculation is recommended, as highly skewed distributions can distort relationships and degrade model performance.\nFor qualitative predictors, one approach is to encode them as indicator (dummy) variables, which should then be preprocessed in the same way as continuous predictors. However, when the predictor set contains a mixture of data types, Gower distance (Gower 1971) provides a unified framework that accommodates different variable types without requiring uniform encoding. Gower distance computes a dissimilarity measure separately for each predictor according to its data type, then aggregates these component distances:\n\\[\nd(\\boldsymbol{u}, \\boldsymbol{x}) =\\frac {1}{p}\\sum _{j=1}^{p}d_j(u_j, x_{j}),\n\\]\nwhere \\(d_j\\) denotes the distance function for predictor \\(j\\). For continuous predictors, the component distance is range-normalized:\n\\[\nd_j(u_j, x_{j})=1-{\\frac {|u_{j}-x_{j}|}{R_{j}}}\n\\]\nwhere \\(R_j\\) represents the range of predictor \\(j\\) in the training set. This normalization ensures all continuous predictors contribute equally regardless of their original scales.\nFor categorical predictors, the component distance is a simple mismatch indicator, \\(d_j(u_j, x_{j}) = I(u_j = x_j)\\). This binary metric assigns zero distance for matching categories and unit distance otherwise.\nFor a comprehensive treatment of distance metrics in the context of nearest neighbor methods, see Cunningham and Delany (2021).\nFor classification problems, KNN identifies the K nearest neighbors and estimates class probabilities as the proportion of neighbors belonging to each class:\n\\[\n\\widehat{\\pi}_{c} = \\frac{1}{K}\\sum_{k=1}^K y_{kc}\n\\]\nwhere \\(y_{kc}\\) is a binary indicator equal to 1 if neighbor \\(k\\) belongs to class \\(c\\), and 0 otherwise. For small values of K, these empirical probability estimates can be unreliable, particularly when certain classes are underrepresented among the neighbors. The Laplace correction (previously discussed in Section 17.1.1) can be used to regularize the estimate:\n\\[\n\\widehat{\\pi}_{c} = \\frac{1}{K}\\sum_{k=1}^K \\frac{y_{kc} +\\alpha}{K + \\alpha C}\n\\]\nwhere \\(C\\) denotes the number of classes and \\(\\alpha \\in [0, 2]\\) is a smoothing parameter. Setting \\(\\alpha = 0\\) yields the unregularized estimate, while \\(\\alpha &gt; 0\\) shrinks extreme probabilities toward a uniform distribution.\nIt is common to fix K and collect all neighbors, regardless of their distances to the query point. Alternatively, one can put a cap on the maximum distance or use a more informative approach of using weighted distances:\n\\[\n\\widehat{p}_{c} = \\frac{1}{W}\\sum_{k=1}^K g(d_k)\\;y_{kc}\n\\]\nwhere \\(g()\\) is a kernel function that weights the distances and \\(W\\) is the sum of the \\(g(d_k)\\) (Dudani 1976; Zuo, Zhang, and Wang 2008; Samworth 2012). Figure 17.9 shows some commonly used functions (Hechenbichler and Schliep 2004).\n\n\n\n\n\n\n\n\nFigure 17.9: Different kernel functions to downweight far-away neighbors. Values have been rescaled to have the same upper value.\n\n\n\n\n\nOutside of preprocessing methods, the primary tuning parameters are the number of neighbors, the weighting function, and the distance degree (if using a Minkowski distance). Recall from Figure 13.1 that KNN models can be harmed when used with irrelevant predictors. As will be seen with these data, tuning an initial feature selection filter can improve model quality.\nHow well does this technique work for the forestation data? To start, we used preprocessing that included the same random forest importance filter as Section 17.1.1 (and tune the proportion fo predictors retained). After that, an effect encoding was used for the county column, and then applied an orderNorm transformation to all predictors to standardize them to the same units and potentially remove skewness. The model was tuned over the proportion of features retained (5% to 100%), the number of neighbors (two through fifty), the Minkowski degree parameter (between 0.1 and 2.0), and the kernel functions listed in Figure 17.9. A total of fifty configurations were evaluated using resampling, and the Brier scores are shown in Figure 17.10.\n\n\n\n\n\n\n\n\nFigure 17.10: K-nearest neighbors tuning results for the forestation data.\n\n\n\n\n\nPoor results occurred when there were fewer than twenty neighbors, and subsequently, the Brier scores remained consistently low. The distance parameter appeared to favor small values (around 0.5). There are no strong trends in terms of which kernel functions; Gaussian kernels appear to do well overall, but there are configurations that do well using any kernel. Numerically, the best results retained 86.4% of the predictors, used 28 neighbors, a Minkowski degree of 1.65, and a cos kernel function. In the figure above, we can see that there is a small increase in error when the entire predictor set is used. Otherwise, smaller feature sets are underfitting the data. The corresponding Brier score was 0.0778, which is very the smallest Brier score seen so far. The area under the ROC curve was also good, with a resampling estimate of 0.953.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Complex Nonlinear Boundaries</span>"
    ]
  },
  {
    "objectID": "chapters/cls-nonlinear.html#sidebar-dot-products",
    "href": "chapters/cls-nonlinear.html#sidebar-dot-products",
    "title": "17  Complex Nonlinear Boundaries",
    "section": "17.3 Sidebar: Dot Products",
    "text": "17.3 Sidebar: Dot Products\nDot products will become important in several subsequent sections of this chapter. While distance metrics (such as those used in KNN) provide one framework for comparing vectors, dot products offer another perspective that captures different geometric properties. Understanding their relationship requires some basic linear algebra and geometry.\nVectors are characterized by two fundamental properties:\n\nMagnitude (or length) is denoted \\(||\\boldsymbol{a}||\\) for a vector \\(\\boldsymbol{a}\\) with \\(p\\) elements. The standard measure is the Euclidean norm, computed as the square root of the sum of squared elements: \\(||\\boldsymbol{a}|| = \\sqrt{\\sum_{i=1}^p a_i^2}\\).\nDirection is defined relative to another reference (say another vector \\(\\boldsymbol{y}\\) with the same dimension (\\(p\\))), typically quantified by the angle \\(\\vartheta\\) between two vectors of equal dimension.\n\nDistance-based methods like KNN depend solely on the magnitude of the difference between vectors and do not consider the angle between them.\nThe dot product (or inner product) of two \\(p\\)-dimensional vectors is defined as:\n\\[\n\\boldsymbol{a}'\\boldsymbol{b} = \\sum_{i=1}^p a_ib_i\n\\tag{17.6}\\]\nNote that the dot product does not satisfy the properties of a distance metric since the distance between itself could never be zero unless all of its elements are zero.\nDot products measure both the magnitude of each vector as well as the angle between them. We can rewrite Equation 17.6 as\n\\[\n\\boldsymbol{a}'\\boldsymbol{b} = ||\\boldsymbol{a}||\\,||\\boldsymbol{b}|| \\cos{\\vartheta}\n\\tag{17.7}\\]\nThis formulation reveals that: The dot product is maximized when vectors are parallel (\\(\\vartheta = 0^{\\circ}\\), \\(\\cos\\vartheta = 1\\)), it equals zero when vectors are orthogonal (\\(\\vartheta = 90^{\\circ}\\), \\(\\cos\\vartheta = 0\\)), and it is negative when vectors point in generally opposite directions (\\(\\vartheta &gt; 90^{\\circ}\\)). This dual sensitivity to both magnitude and direction distinguishes dot products from distance-based comparisons and makes them particularly useful in certain modeling contexts.\nTo illustrate, Figure 17.11 shows the dot products of a vector \\(\\boldsymbol{x} = (2.0, 0.5)\\) to a grid of vectors with elements ranging between \\(\\pm 3\\). We can see a diagonal region of light colored values that have dot products close to or at zero. This represents vectors that are orthogonal to \\(\\boldsymbol{x}\\). There is also a dotted line extending to the origin (0.0, 0.0) that coincides with this line.\n\n\n\n\n\n\n\n\nFigure 17.11: Dot product values of a grid of vectors relative to \\(\\boldsymbol{x} = (2.0, 0.5)\\) (the black point). The dashed black line shows the contour of constant dot products and the white dotted line is points to the origin.\n\n\n\n\n\nWhat is the dot product of \\(\\boldsymbol{x}\\) with itself? That value is \\(\\boldsymbol{x}'\\boldsymbol{x} =\\) 4.25. The dashed black diagonal line shows which vectors whose dot product is 4.25. These are points that point in the same direction (i.e., \\(\\vartheta = 0^{\\circ}\\)) and magnitude as our example vector. Note that this line is parallel to the region of zero values, reemphasizing that the zero region is orthogonal to vectors like \\(\\boldsymbol{x}\\).\nThe vector associated with the largest dot product would move in the same direction as \\(\\boldsymbol{x}\\). Since \\(||\\boldsymbol{x}||\\) is fixed and \\(\\cos(\\vartheta) = 0^{\\circ}\\), the magnitude of the vector would be as large as possible. Conversely, the vector with the smallest dot product will move in the opposite direction and as far as possible. For our \\(\\boldsymbol{x}\\), the dot product is maximized in the upper right corner of Figure 17.11 and minimized in the lower left corner.\nWe’ll see the dot product again in ?sec-attention where it is intended to measure the similarity of two embedding vectors within a neural network, and again in Section 17.6 where it is the computational backbone of support vector machines.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Complex Nonlinear Boundaries</span>"
    ]
  },
  {
    "objectID": "chapters/cls-nonlinear.html#sec-cls-nnet",
    "href": "chapters/cls-nonlinear.html#sec-cls-nnet",
    "title": "17  Complex Nonlinear Boundaries",
    "section": "17.4 Neural Networks via Multilayer Perceptrons",
    "text": "17.4 Neural Networks via Multilayer Perceptrons\nNeural networks are complex models comprised of multiple hierarchies of latent variables designed to represent different motifs in the data. The model’s architecture consists of a series of layers, each containing a set of units. Neural networks begin with the input layer that contains units representing our predictors. After this are one or more hidden layers, each containing a preset number of hidden units. These are artificial features that are influenced by the layer before. For example, if there were four predictors and the first hidden layer had two hidden units, each hidden unit is defined by an equation that includes each of the predictors. In the broadest sense, it is similar to how, in PCA, each principal component is a linear combination of the predictors. However, the similarities between neural networks and PCA end there.\nThese connections between units on different layers continue as layers are added. Eventually, the last layer is the output layer. For classification, this usually has as many units as there are class levels in the outcome. Figure 17.12 shows a general architectural diagram for a basic model with a single hidden layer, where the lines indicate that a model parameter exists to connect the nodes.\n\n\n\n\n\n\n\n\nFigure 17.12: Architecture of a simple neural network with a single layer of hidden units.\n\n\n\n\n\nTraditionally, the connections that come into a hidden unit are combined using a linear predictor: each incoming unit has a corresponding slope, called a “weight” in the neural network literature5. The linear combinations commonly include an intercept term (called a “bias”). These linear predictors are embedded within a nonlinear “activation” function (\\(\\sigma\\)) to enable complex patterns. An example of an activation function would be a sigmoidal function, much like the logistic function used in Equation 16.3.\n\nIn this section we will primarily be concerned with multilayer perceptrons (MLPs), a subset of the broader class of neural networks. MLPs are feed-forward models, meaning each layer only connects to the subsequent layer; there are no feedback loops where future units connect to units in previous layers.\nAs a counterexample, residual networks (He et al. 2016) are connected to previous layers (a.k.a. skip layers); therefore, they are not multilayer perceptrons. Another popular example is the long-term short memory model for sequential data such as time series (Hochreiter and Schmidhuber 1997).\n\n\nLet’s walk through the network structure, starting at the input layer. We have predictors \\(x_1, \\ldots x_p\\). These will be mathematically connected to the next layer containing \\(m\\) hidden units \\(H_\\ell\\) (\\(\\ell=1, \\ldots m\\)). First, we create a separate linear predictor for hidden unit \\(H_l\\) denoted as \\(\\beta_{\\ell 0} + \\beta_{\\ell 1} x_{1} + \\ldots + \\beta_{\\ell p} x_{p}\\). We then embed this inside the activation function \\(\\sigma()\\) used for this layer so that there is a nonlinear relationship between the predictors and their new representation. Visually6:\n\n\n\n\n\n\n\n\n\nAs we add more layers, each of the first set of hidden units is similarly connected to the hidden units in the next layer, and so on. The layers do not have to use the same activation function. Also, the layers can be specialized, such as the image processing layers shown in Figure 1.3. For each hidden layer, we have to specify the number of units and the activation function. These, along with the number of layers, are tuning parameters.\nNote that the units in the subsequent hidden layer are nested inside linear combinations of the previous hidden layers (plus the input units). These models have sets of nested nonlinear functions and can be very complex, but are mathematically tractable.\nFor the final layer, the previous set of hidden units is often connected to the output units using a linear activation. For classification, there are often \\(C\\) output units (i.e., one for each class):\n\n\n\n\n\n\n\n\n\nSince linear activation is used for the final layer, the values of the output units (\\(Y\\)) are unlikely to be in the proper units for classification. To remedy this, the raw outputs are normalized to be probability-like via the softmax function:\n\\[\n\\pi_{k} = \\frac{\\exp(Y_{k})}{\\sum\\limits_{k=1}^C \\exp(Y_{k})}\n\\]\nIn this section, we’ll describe the important elements of multilayer perceptrons, such as the different types of activation functions. We’ll walk through a small but specific model.\nIn later sections, we’ll also spend a considerable amount of time discussing the various methods for training MLPs. There are also discussions on modern deep learning models that are specifically designed for tabular data. These are discussed below.\n\n17.4.1 Activation Functions\nHistorically, the most commonly used nonlinear activation functions were sigmoidal. This means that the linear combination of the elements in the previous layer was converted into a “squashed” format, typically bounded (e.g., logistic activation values range between 0 and 1). Figure 17.13 shows several such functions. The logistic (Equation 16.3) function and hyperbolic tangent are fairly similar in shape, and were typical defaults.\n\n\n\n\n\n\n\n\nFigure 17.13: Different sigmoidal activation functions.\n\n\n\n\n\nOne issue with bounded sigmoidal shapes, particularly the logistic function, is that they can become problematic when used in conjunction with many hidden layers. Since the logistic function is on [0, 1], repeated multiplication of such values can cause their values to converge to be very close to zero (as seen in Section 17.1.1). This is also true of their gradients, leading to the vanishing gradient problem. This is likely not an issue with shallow networks that have a handful of hidden layers.\nThe tanshrink function diminishes the impact of inputs that are in a neighborhood around zero, where it is sigmoidal. For larger input values, the output is linear in the output (with an offset of \\(\\pm 1\\)).\nIn modern neural networks, the rectified linear unit (ReLU) is the default activation function. As shown in Figure 17.14, the function resembles the hinge functions used by MARS; to the left of zero, the output is zero, and otherwise, it is linear. Much like MARS, this has the effect of isolating some region of the predictor space. However, there are a few key differences. MARS hinge functions affect a single predictor at a time, while neural networks’ activation functions act on the linear combination of parameters. Also, neural networks with multiple layers recursively activate linear combinations inside the prior layers (and so on).\n\n\n\n\n\n\n\n\nFigure 17.14: Different ReLU-based activation functions.\n\n\n\n\n\nThere are various modified versions of ReLU that emulate the same form but have continuous derivatives. These are also shown in Figure 17.14, including softplus (Dugas et al. 2000; Glorot, Bordes, and Bengio 2011), gelu (Hendrycks and Gimpel 2016), silu (Elfwing, Uchibe, and Doya 2018), elu (Clevert, Unterthiner, and Hochreiter 2015), and others. ReLU, and its offspring activation functions, appear to solve the issue of vanishing gradients.\nLet’s use an example data set with a simple MLP to demonstrate how ReLU activation works. Figure 17.15 displays the results of a model with a single layer comprised of three hidden units, utilizing ReLU activation. The model converged in 9 epochs and does a good job discriminating between the two classes.\n\n\n\n\n\n\n\n\nFigure 17.15: An example data set with two predictors and two classes. The black line is the class boundary from an MLP model using three hidden units with ReLU activation.\n\n\n\n\n\nWith two predictors and three hidden units, the first set of coefficients for the model is three sets of linear predictors (i.e., an intercept and two slopes) for each of the three hidden units. The predictors were normalized to have the same mean and variance (zero and one, respectively). The three equations produced during training were:\n\n\\[\\begin{align}H_{1}&= 2.298+1.6\\:A-0.3\\:B \\notag \\\\\nH_{2}&=-0.131-1.2\\:A-0.3\\:B \\notag \\\\\nH_{3}&=-0.064+1.0\\:A+2.0\\:B \\notag\\end{align}\\]\n\nThe intercept estimate for the first hidden unit is significantly larger than those of the other two. That unit also has a large positive slope for predictor A. The second hidden unit emphasizes predictor B, while \\(H_{i3}\\) has strong positive effects for both predictors.\nThe top of Figure 17.16 displays a heatmap illustrating how these three artificial features vary across the two-dimensional predictor space (before activation). The black line segments show the contour corresponding to zero values. The first hidden unit has larger values on the right-hand side of the predictor space, with a slightly diagonal contour for zero. The pattern shown for the second hidden unit is nearly the opposite of the first, although the zero contour cuts more through the center of the space. Finally, the third unit is driven by both predictors with larger values in the upper right region.\n\n\n\n\n\n\n\n\nFigure 17.16: Heatmaps of the three linear predictors and their ReLU values across the predictor space. The black line emphasizes the location where the linear predictor is zero.\n\n\n\n\n\nThe figure’s bottom panels illustrate the effect of the ReLU function, where values less than zero are set to zero. At this point, \\(H_{i1}\\) and \\(H_{i3}\\) have the most substantial effect on the model, but in somewhat overlapping regions.\nHowever, the model estimates additional parameters that will merge these regions using weights that are estimated to be most helpful for predicting the data. Those equations were estimated to be:\n\n\\[\\begin{align}Y_{i1}&= 1.10-1.3H_{i1}+1.2H_{i2}+1.3H_{i3} \\notag \\\\\nY_{i2}&=-0.11+1.1H_{i1}-0.9H_{i2}-2.1H_{i3} \\notag\\end{align}\\]\n\nNote that the coefficients’ signs are opposite for each outcome unit. Figure 17.17 illustrates the transition from the hidden layer to the outcome layer. The weighted sum of the three variables at the top produces the regions of strong positive (or negative) activation for the two classes. Once again, the black curve indicates the location of the zero contour.\n\n\n\n\n\n\n\n\nFigure 17.17: Heatmaps of the three ReLU units and their output units across the predictor space.\n\n\n\n\n\nThe two output panels at the bottom of the figure show nearly opposite patterns, which is not coincidental. The model converges to this structure, allowing the output units to favor one class or the other. The legend on the bottom right clearly shows inappropriate units; we’d like them to resemble probabilities. As previously mentioned, the softmax function coerces these values to be on [0, 1] and sum to one. This produces the class boundary seen in Figure 17.15.\nAs previously stated in Section 17.1.2, ReLU is not a smooth function, so this demonstration will somewhat differ from the other activation functions; they generally don’t completely isolate regions of the predictor space. However, the overall flow of the network remains the same.\nNow that we’ve discussed the architecture of these models and how the layers relate to one another, let’s examine how to estimate model parameters effectively.\n\n\n17.4.2 Training and Optimization\nTraining neural networks can be challenging. Their model structure is composed of multiple nested nonlinear functions that may not be smooth or continuous. The loss function for classification, commonly cross-entropy, is unlikely to be convex as model complexity increases with the addition of more layers. Consequently, we have no guarantees that a gradient-based optimizer will converge. There is also the possibility of local minima that might trap the training process, saddle points that could lead the search in the wrong direction, and regions of vanishing gradients (Hochreiter 1998).\nFirst is the challenge of local minima. In some cases, ML models can provide some guarantees regarding the existence of a global minimum loss value7. This may not be the case for neural networks. Their loss surface may contain multiple valleys where the derivatives of some of the parameters are zero, but a better solution exists in another region of the parameter space. In other words, the model can appear to converge but has suboptimal performance. Baldi and Hornik (1989) show that, under some conditions, single-layer feedforward networks can have a unique global minimum. It is expected that as more layers and units are added, the probability of finding a unique optimal solution decreases. When “caught” in a local valley, we need our optimization algorithm to be able to escape.\nA second related issue is saddle points. These are locations where all of the gradients are zero (i.e., a stationary point), but some directions have increasing loss, and others will produce a decrease. Dauphin et al. (2014) postulate that these will exist, and their propensity to occur increases with the number of model parameters. The concern here is that, depending on the type of multidimensional curvature encountered, the optimization algorithm may move in the direction of increasing loss or become stuck.\nFigure 17.18 has an example with two parameters and a highly nonlinear loss surface. There are four stationary points, the best of which minimizes the loss function at \\(\\boldsymbol{\\theta} = [6.8, 0.2]\\). The lower right region has a point associated with maximum loss. The other two locations are saddle points. The point at \\(\\boldsymbol{\\theta} = [6.9, -0.3]\\) is a good example; moving to the left or right of the point will increase the loss. Moving up or down will decrease the loss (with the upper value being the correct choice in this parameter range). Our optimizer should be able to move in the proper direction or, like simulated annealing, recover and move back to progressively better results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.18: The loss surface for \\(\\psi(\\boldsymbol{\\theta}) = \\theta_1 cos(0.5\\,\\theta_1) - 2 \\exp(-6(\\theta_2 - 0.3)^2) + 0.2\\, \\theta_1 \\theta_2\\). Four stationary points are shown.\n\n\n\nAnother possible factor is that many deep learning models are trained on vast amounts of data. This in itself presents constraints on how much data can be held in memory at once, as well as other logistical issues. This may not be an issue for the average application that uses tabular data; however, the problem still persists.\nBefore proceeding, recall Section 12.2 where gradient descent was introduced. There are two commonly used classes of gradient-based optimization:\n\nFirst-order techniques: \\(\\quad\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\alpha\\,g(\\boldsymbol{\\theta}_i)\\),\nSecond-order methods: \\(\\quad\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\alpha H(\\boldsymbol{\\theta_i})^{-1}\\,g(\\boldsymbol{\\theta}_i)\\),\n\nwhere \\(\\boldsymbol{\\theta}\\) are the unknown parameters, \\(\\alpha\\) is the learning rate, \\(g(\\boldsymbol{\\theta}_i)\\) is the gradient vector of first derivatives, and \\(H(\\boldsymbol{\\theta_i})\\) is the Hessian matrix of second derivatives. The Hessian measures the curvature of the loss function at the current iteration. Pre-multiplying by its inverse rescales the gradient vector, generally making it more effective. We’ll discuss them both in Section 17.4.9 and Section 17.4.10. Generally speaking, second-order methods are preferred since they are more accurate when they are computationally feasible. However, first-order search is more commonly used because it is faster to compute and can be significantly more memory-efficient.\nTo demonstrate, we can use an MLP model for the forestation data that contains one layer of 30 ReLU units and an additional layer of 5 ReLU units, yielding 677 model parameters. An L2 penalty of 0.15 was set to regularize the cross-entropy loss function. A maximum of 100 epochs was specified, but training was instructed to stop after five consecutive epochs with a loss increase (measured using held-out data). An initial learning rate of \\(\\alpha = 0.1\\) was used, but a variable, step-wise rate schedule was implemented to modulate the rate across epochs, as described below.\nThe model was first trained using basic gradient descent. For illustration, training was executed five times using different initial parameter values. The leftmost panel in Figure 17.19 shows the results. In each case, the loss initially decreased in the first set of epochs. However, in each case, the model used all 100 epochs. This approach was not very successful. There was little reduction in the loss function, and the terminal phase had a minuscule decrease with no real improvement. This could be due to the search approaching the saddle point. In this case, the gradient becomes very close to zero, and first-order gradient searches have tiny, ineffective updates. We can measure the efficiency of optimizers by computing the time it takes to train a complete iteration. For gradient descent, the time was 0.013s per iteration.\n\n\n\n\n\n\n\n\nFigure 17.19: The loss function, using held out data, for the same MLP model for four optimization strategies. The lines on each panel represent runs initialized with different random numbers. The profiles are normalized to start from the same initial loss value.\n\n\n\n\n\nWhen training neural networks, stochastic gradient descent (SGD) is the standard approach. SGD uses randomly allocated batches training set samples. The gradient search is updated by processing each of these batches until the entire trained set has been used, which signifies the end of that epoch. In other words, a potentially large number of parameter updates are computed based on a small number of training point samples. The batch size used here was 64 data points. As a result, for this training set size, about 76 gradient updates are executed before the model has seen all of the training data.\nFor stochastic gradient descent, we track the process by epochs. An epoch is finished when the model has been updated using every data point in the training set. For this analysis, this means that there are about 76 parameter updates within an epoch. For regular gradient descent and second-order search methods, we’ll count their progress as iterations since there are no batches.\nAs shown in Figure 17.19, SGD does far better than plain GD and although none stop early, there is a relatively flat trend after about 20 epochs, where the search does not substantially reduce the loss. In terms of time per epoch, SGD takes much longer than GD at 0.163s.\nAs we’ll see in the section below, many extensions to SGD have made it more effective. One improvement is to use Nesterov momentum (Sutskever et al. 2013). This initially computes a standard update to the parameters, then approximates the gradient at this position to compute a second prospective step. The actual update that is used is the lookahead position. This technique yields better results than SGD, with several iterations stopping early. However, the training time was longest, using 0.24s per epoch.\n\nThe intended takeaways from this exercise are that training these models can be challenging and that the type of parameter optimization can significantly impact model quality.\n\nLet’s discuss a few operational aspects of gradient-based methods before discussing the first- and second-order search techniques.\n\n\n17.4.3 Initialization\nNetwork parameter initialization can significantly impact the success of training. Given the size and complexity of these models, it is nearly impossible to analytically derive optimal methodologies for setting initial values. As such, parameters are initialized using random numbers. This means that different initial values will results in different model parameter estimates.\nThere are a few approaches to do simulate initial values. LeCun et al. (2002) initialization uses a uniform distribution centered around zero with limits defined by the number of connections feeding into the current node (known as the “fan in” and is denoted as \\(m_{in}\\)). They used \\(U(\\pm \\sqrt{2/m_{in}})\\) as the distribution. There have been a few variations of this:\n\nGlorot/Xavier initialization (Glorot and Bengio 2010): \\(U(\\pm \\sqrt{6/(m_{in}+m_{out})})\\)\nHe/Kaiming initialization (He et al. 2015): \\(N(0, \\sqrt{2 / m_{in}})\\).\n\nwhere \\(m_{out}\\) is the number of parameters leaving the node.\nOne reason that initialization is important is to help mitigate numerical overflow errors. For example, the softmax transformation shown above exponentiates the raw output units (and sums them). If the output units have relatively large values, exponentiating and summing them can exceed allowable tolerances. For example, the output units shown in Figure 17.17 have a wide enough range that can be as large as 12.\n\n\n17.4.4 Preprocessing\nNeural networks require complete, numeric features for computations.\nFor non-numeric data, the tools described in Chapter 6 can be very effective. Additionally, there are pretrained neural network models that can translate the category string to a word embedding (Boykis 2023), such as BERT (Devlin et al. 2019) or GPT-4 (Achiam et al. 2023). The potential upside to doing this is that the resulting numeric features may measure some relative context between categorical values. For example, suppose we have a predictor with levels red, darkred, green, chartreuse, and white. Basic encoders, such as binary indicators or hashing features, would not recognize that there are three clusters in these values and that sets such as {red, darkred} and {green, chartreuse} should have values that are “near” one another. LLMs can measure this semantic similarity and potentially generate more meaningful features.\nIf the non-numeric predictors are not complex, a layer could be added early in the network, enabling the model to derive encodings based on the data when training. Guo and Berkhahn (2016) describes a relatively simple architecture for doing this. While LLMs are better at creating embeddings overall, they are unaware of the context in your data. Estimating the parameters associated with a simple embedding layer may be more effective, as it is optimized for your dataset.\nFor numeric predictors, we suggest avoiding irrelevant predictors and highly collinear feature sets. Figure 13.1 in Chapter 13 demonstrated the harm of including predictors unrelated to the outcome in a neural network. Multicollinearity can also affect these models, even if we do not invert the feature matrix or use the Hessian during optimization. Highly redundant predictors can result in the loss function having difficult characteristics and a large number of unnecessary parameters. Using an L2 penalty or a feature extraction method can be very helpful.\nAlso, predictors with highly skewed distributions might benefit from a transformation to make their distribution more symmetric. If not, outlying values can exert significant influence in computations, potentially preventing the generation of high-quality parameter estimates.\nFinally, since the networks use random initialization, we should ensure that our predictors are standardized to have the same units. If there are only numeric predictors, centering/scaling or orderNorm operations can be very effective8. Using this approach, we must also apply the same standardization to binary indicators created from categorical predictors. Alternatively, we could leave 0/1 indicators as-is, but range scale our predictors to also be between zero and one. This may have a disadvantage; the range will be common across predictors, but the mean and variance can be quite different.\n\n\n17.4.5 Learning Rates\nThe learning rate (\\(\\alpha\\)) can profoundly affect model quality, perhaps more than the structural tuning parameters that define complexity (e.g., number of hidden units). A high learning range (\\(\\approx\\) 0.1) will help converge towards effective parameter estimates, but once there, it can cause the model to consistently overshoot the best location. One approach to modulating the rate is to use a scheduler (Lu 2022,chapt 4). These are functions/algorithms that will change the rate (often to decrease it) over epochs. For example, to monotonically decrease the rate, two options are:\n\ninverse decay: \\(f(\\alpha_0, \\delta, i) = \\alpha_0/(1 + \\delta i)\\)\nexponential decay: \\(f(\\alpha_0, \\delta, i) = \\alpha_0\\exp(-\\delta i)\\)\n\nwhere \\(i\\) is the epoch number and \\(\\delta\\) is a decay value. These two functions gradually decrease the rate so that, when near an optimum, it is slow enough not to exceed the optimal settings. Another approach is a step function that decreases the rate but maintains it at a constant level for a range of epochs. Those ranges can be predefined or set dynamically to decrease when the loss function is reduced. The function for the former is:\n\nstep decay: \\(f(\\alpha_0, \\zeta, r, i) = \\alpha_0 \\zeta^{\\lfloor i/r\\rfloor}\\)\n\nwhere \\(\\zeta\\) is the reduction value, \\(r\\) is the step length, and \\(\\lfloor x\\rfloor\\) is the floor function.\nFinally, some schedulers modulate the rate up and down. A cyclic schedule (Smith 2017) starts at a maximum value, decreases to a minimum value, and then increases cyclically. For some range \\(r\\) and minimum rate \\(\\alpha_0\\), an epoch’s location within the series is \\(cycle = \\lfloor 1 + (i / (2r) )\\rfloor\\) and the rate is\n\ncyclic: \\(f(\\alpha_0, \\alpha, r, i) = \\alpha_0 + ( \\alpha - \\alpha_0 ) \\max(0, 1 - x)\\)\n\nwhere \\(x = |(i/r) - (2\\,cycle) + 1|\\). This is a good option when we are unsure how fast or slow the convergence will be. If the rate is too high near an optimum value, it will soon decrease to an appropriate value. Additionally, if the search is stuck in a local optimum (or a saddle point), increasing the learning rate may help it escape.\nFigure 17.20 uses the loss function example from Figure 17.18 to demonstrate the effect of rate schedulers. Two starting points are used. One (in orange) is very close to a saddle point, while the other starts from the upper right corner of the parameter space (in green).\n\n\n\n\n\n\n\n\nFigure 17.20: The effect of different schedulers on the rate and success of convergence.\n\n\n\n\n\nUsing a constant learning rate of \\(\\alpha = 0.1\\), the search marches to the optimum value, but oscillates wildly on approach. Ultimately, it falls short of the best estimates, as it is constrained to make large jumps on each iteration. The cyclic schedule allowed rates to move between \\(\\alpha_0 = 0.001\\) to \\(\\alpha = 0.1\\) with a cycle range of \\(r = 10\\) epochs. This appears to work well, or at least, it had good timing. The decay panel represents inverse decay with \\(\\delta = 0.025\\). For the corner starting point, this is a very effective strategy. From the saddle point, we can observe that decreasing the learning rate results in fewer oscillations as the search progresses and eventually leads to the optimum. Finally, the panel for stepwise reduction used an initial learning rate of \\(\\alpha_0 = 0.1\\), \\(\\eta = 0.5\\), and a step length of 20 epochs. This approach worked well when the search began from the saddle location, but performed poorly when starting at the corner. The success depended on when the search was near the optimum; if not timed right, the oscillations were too wide to converge on the best point. A reduction in the step length might help.\nThe schedule type and its additional parameter(s) are added to the list of possible values that could be optimized during tuning.\n\n\n17.4.6 Regularization\nAs seen in a previous chapter, we can add a penalty to the loss function to discourage particularly large model coefficients. This is especially beneficial for neural network models.\nWhile Hanson and Pratt (1988) and Weigend, Rumelhart, and Huberman (1990) made some early proposals for penalizing the size of the estimated parameters, Krogh and Hertz (1991) proposed to use L2 regularization to improve generalization. This is now a standard approach for training neural networks, and “weight decay” is now synonymous with squared penalties.\nWhile either L1, L2, or a mixture of the two can be used, there is some reason to use just L2 penalization. While either type of penalty might be able to combat multicollinearity, the quadratic penalty has the advantage of maintaining a smooth loss surface. With an L1 penalty, we have at least one point on the loss surface that is not differentiable. Also note that using the L1 penalty will not produce exact zeros in the parameter estimates when using stochastic gradient descen.\nAs we will describe later in this chapter, Loshchilov and Hutter (2017) note that there are cases when penalizing the loss function does not precisely match the definition of weight decay (in the neural network context of Hanson and Pratt (1988)). Their adjustment for these issues is called the “AdamW” method.\nAnother regularization technique is called dropout. This method randomly removes some units (hidden or not) from the network. This drops incoming and outgoing connections to other units, which propagates through all of the layers. These temporary coercions to zero recur with each evaluation of the gradient. In the final training step, a full set of parameters is used. Dropout has an effect similar to averaging many smaller networks. The amount of dropout should be tuned.\n\n\n17.4.7 Early Stopping\nWhen should training stop? We can estimate the number of epochs that might be needed, but if we make an incorrect guess, we risk under- or overfitting the model. It makes sense to include the number of epochs as a tuning parameter, but this could be costly9.\nIf we have some data to spare, an efficient and effective approach is to use early stopping. If we set aside a small proportion of the training set to measure the search’s progress, we can set the maximum number of epochs and stop training when the loss worsens (based on our holdout set). Often, we can tune the number of “stopping epochs.”. If this value were five, we would stop training after five consecutive epochs where the loss function worsens (and use the parameter estimates from our last best epoch). Values greater than one protect against overreacting when the loss profile is noisy.\n\n\n17.4.8 Batch Learning\nOperationally, within each epoch, a random batch is selected to evaluate the gradient and corresponding update the parameters. This occurs for each batch until the entire set is evaluated. This is the end of the epoch. If we are monitoring a holdout set for early stopping, this is the point at which we evaluate the loss function at the current parameter estimate using the holdout set. After this, the process begins again by iterating through the batches.\nSGD is thought to work well because it injects a considerable amount of variation into the gradients and updates. This would generally be considered a bad thing, but, as with simulated annealing, it makes the optimization less greedy. The somewhat noisy directions the optimizer takes can help the optimization escape from local optimums and be more resilient when the search veers near a saddle point or other regions where the gradient may be flat or consistently zero. Hoffer, Hubara, and Soudry (2017) referred to SGD as a “high-dimensional random walk on a random potential” process.\nAs the search proceeds, lowering the learning rate can mitigate the extra noise in the gradients so that, hopefully, the search does not move away from the best parameter estimates.\nWe can tune the batch size if there is no intuition for what a good value might be. Smallish batch sizes could range from 16 to 512 training set points10, depending on the training set size. Keskar et al. (2016) remarks that smaller batches tend to explore the parameter space more thoroughly and that, as the batch size becomes larger, SGD will find a solution that is closer to the initial values.\nLoshchilov and Hutter (2017) make a few interesting observations related to L2 penalization with stochastic gradient descent. Their results show that as the number of batches increases, the optimal penalization becomes smaller. In other words, as the batch size decreases, the amount of penalization should also decrease.\n\n\n17.4.9 Gradient Descent Algorithms\nGradient descent methods for training neural networks have become a popular research topic over the last 15 years. We’ll discuss a few major techniques here, but there are myriad other tools for training these models. In this section, we’ll temporarily forgo discussing changes from epoch to epoch. Since we are focused on SGD, we’ll use the term update to refer to the point at which the parameters move to a different location due to a batch step.\n\nMomentum\nTo start, let’s consider a more common form of momentum in gradient search11 Momentum is a useful tool that can help in situations such as the results in Figure 17.20, where the updates make the search path make large, ineffective jumps. When the learning rate is consistently large, the search route bounces back and forth since the gradients are substantial in magnitude but moving in different consecutive directions.\nMomentum is one of several tools to adjust the gradient vector based on previous gradients. For iteration \\(i\\), a vector is computed that is a combination of the current and previous gradients12:\n\\[\nv_i = \\phi v_{i-1} + \\alpha_i g(\\boldsymbol{\\theta}_i)\n\\]\nand this is used to make the update \\(\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - v_i\\). This weighting system emphasizes recent gradients while also taking into account previous gradients. This can accelerate the search and diminish oscillations.\nValues of \\(\\phi\\) are typically between 0.8 and 0.99, representing a weighted average of previous gradient directions, with a greater emphasis on recent iterations as \\(\\phi\\) increases. If the directions have been fairly stable, there is little change; however, if they are oscillating, the direction takes a more consistent central route with fewer directional jumps. It also helps if the search is in a region where gradients are close to zero; historical gradients will have some non-zero values that are unlikely to become stuck with updates near zero.\nFigure 17.21 shows the results when momentum is used for our 2D toy example13. The cyclic rate settings were used for this example, and the results appear very similar to the previous path when initialized near the saddle point. The corner point starting value shows that momentum can lead to a location near the optimal value, but takes a more circuitous route.\n\n\n\n\n\n\n\n\nFigure 17.21: Examples of four different optimizers for the loss surface previously shown in Figure 17.20.\n\n\n\n\n\nMomentum should be used in conjunction with a rate scheduler, or at the very least, without a consistently large learning rate. Making very large jumps in a central direction might lead the search to points far away from regions of improvement. Penalization also helps.\n\n\nAdaptive Learning Rates\nThere is also a class of gradient descent methods that try to avoid using a single learning rate for all of the elements of \\(\\boldsymbol{\\theta}\\). Instead, they use a constant learning rate and, at each update, adjust the rate using functions of the gradient vector so that the rates are different from each \\(\\theta_j\\).\nOne approach is the adaptive gradient algorithm, a.k.a. AdaGrad (Duchi, Hazan, and Singer 2011), that scales the constant learning rate \\(\\alpha\\) by the accumulated square of the gradients:\n\\[\n\\boldsymbol{v}_i =\\sum_{b=1}^t g(\\boldsymbol{\\theta}_b)^2\n\\] so that\n\\[\n\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\frac{\\alpha}{\\sqrt{\\boldsymbol{v}^2_i + \\epsilon}}\\, g(\\boldsymbol{\\theta}_i)\n\\]\nwhere \\(\\epsilon\\) is a small constant to avoid division by zero. The square root in the denominator is required to keep the denominator units the same as those in the numerator. AdaGrad has the advantage of keeping the adjusted learning rate steady for elements of \\(g(\\boldsymbol{\\theta})\\) that do not change substantially from update to update. At update \\(i\\), the contribution to the learning rate is inversely proportional to the squared gradient value.\nThe problem is the unweighted accumulation of squared values. For a reasonable number of updates, elements of \\(\\boldsymbol{v}\\) can explode to very large values, and this prevents almost any movement of \\(\\boldsymbol{\\theta}\\) from update to update. For the example shown in Figure 17.21, some of these values were in the thousands, and progress clearly halted before reaching the optimal parameters.\nOne method of fixing this problem is RMSprop (Hinton 2012). Here, the denominator uses an exponentially weighted moving average of the squared gradients:\n\\[\n\\boldsymbol{v}_i = \\rho\\, \\boldsymbol{v}_{i-1} + (1 - \\rho)\\, g(\\boldsymbol{\\theta}_i)^2\n\\]\nThis formulation places more weight on the current and recent gradients than the statistic used by AdaGrad. The RMSprop update becomes:\n\\[\n\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\frac{\\alpha}{\\sqrt{\\boldsymbol{v}_i + \\epsilon}}\\, g(\\boldsymbol{\\theta}_i).\n\\]\nThe “RMS” in the name is due to taking the square root of the weighted sum of squared gradients (i.e., similar to a traditional root mean square).\nGoodfellow, Bengio, and Courville (2016) suggested using RMSprop with Nesterov momentum. This leads to a more complicated procedure. First, we compute our initial update: \\(\\quad\\tilde{\\boldsymbol{\\theta}}_{i} = \\boldsymbol{\\theta}_{i} - \\phi\\,\\boldsymbol{\\delta}_{i-1}\\) where \\(\\boldsymbol{\\delta}_0\\) is initialized with a vector of zeros. We compute the gradient at this temporary update and use this to compute \\(\\boldsymbol{v}_i\\). From here:\n\\[\n\\boldsymbol{\\delta}_{i} = \\phi\\,\\boldsymbol{\\delta}_{i-1} - \\frac{\\alpha}{\\sqrt{\\boldsymbol{v}_i + \\epsilon}}\\, g(\\tilde{\\boldsymbol{\\theta}}_i)\n\\]\nThe results are in Figure 17.21. The curves are similar to those for AdaGrad except that they do reach the location of minimum loss. The path for the starting value near the saddle point exhibits several minor oscillations en route to the optimal result.\nThe adaptive moment estimation (Adam) optimizer from Diederik and Ba (2015) extends the moving average approach to the gradient term as well:\n\\[\n\\begin{align}\n\\boldsymbol{m}_i &= \\phi\\, \\boldsymbol{m}_{i-1} + (1 - \\phi)\\, g(\\boldsymbol{\\theta}_i) \\notag \\\\\n\\boldsymbol{v}_i &= \\rho\\, \\boldsymbol{v}_{i-1} + (1 - \\rho)\\, g(\\boldsymbol{\\theta}_i)^2 \\notag\n\\end{align}\n\\]\nAdam also tries to normalize these quantities so that the first few updates have larger values:\n\\[\n\\begin{align}\n\\boldsymbol{m}^*_i &= \\frac{\\boldsymbol{m}_i}{1 - \\phi^2}\\notag \\\\\n\\boldsymbol{v}^*_i &= \\frac{\\boldsymbol{v}_i}{1 - \\rho^2} \\notag\n\\end{align}\n\\]\nThis helps offset the initialization of \\(\\boldsymbol{m}_0 = \\boldsymbol{v}_0 = \\boldsymbol{0}\\). At update \\(i\\), Adam’s proposal is\n\\[\n\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\frac{\\alpha}{\\sqrt{\\boldsymbol{v}^*_i + \\epsilon}}\\,\\boldsymbol{m}^*_i.\n\\]\nThe results above show that Adam’s path towards the optimal parameters is slightly wobbly compared to RMSprop but otherwise effective.\nBoth \\(\\phi\\) and \\(\\rho\\) are tunable in these models and typically range within [0.8, 0.99]. The constant learning rate \\(\\alpha\\) is also tunable. In Figure 17.21, \\(\\phi=\\rho=0.9\\). AdaGrad, Adam, and RMSprop used \\(\\alpha = 0.1\\). For RMSprop, decreasing the rate to \\(\\alpha = 0.08\\) removed the oscillation effects shown in Figure 17.21.\nAs previously alluded to, Loshchilov and Hutter (2017) recognized that, for Adam, penalizing the loss function does not align with the definition of weight decay. Without a remedy, they found that Adam is not as effective as it could be. Their method, called AdamW, includes the penalty in the gradient definition. The gradient used to compute \\(\\boldsymbol{m}_i\\) and \\(\\boldsymbol{v}_i\\) is a combination of the previous “raw” gradient and the penalty (\\(\\lambda\\)):\n\\[\ng(\\boldsymbol{\\theta}_i) = \\nabla \\psi(\\boldsymbol{\\theta}_{i}) - \\lambda \\boldsymbol{\\theta}_{i}\n\\]\nThe updating equation also uses an extra term containing the penalty:\n\\[\n\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\alpha_i\\left[\\frac{\\boldsymbol{m}^*_i}{\\sqrt{\\boldsymbol{v}^*_i + \\epsilon}} + \\lambda \\boldsymbol{\\theta}_{i}\\right].\n\\]\nNote that the learning rate \\(\\alpha\\) now has a subscript. They also recommend trying a cyclic learning rate scheduler with AdamW.\n\n\nOther Methods\nAs previously mentioned, there are numerous other variations of SGD that will not be discussed here, including AdaMax (Diederik and Ba 2015), Nadam (Dozat 2016), MadGrad (Defazio and Jelassi 2021), and others.\n\n\n\n17.4.10 Newton’s Method\nAs previously shown, Newton’s method (a.k.a., the Newton–Raphson method) uses both gradients and the Hessian matrix to update parameters. This approach can be very effective when the local loss surface can be approximated by a second-degree polynomial. When the local search includes a stationary point, the signs of the eigenvalues of the Hessian can characterize it: all non-negative imply a maximum, all non-positive imply a minimum, and mixed signs indicate a saddle point.\nOne issue with the method is related to the Hessian. If there are \\(p\\) parameters, the matrix needs to save \\(p(p+1)/2\\) parameters. For neural networks, \\(p\\) can be in the millions, so the basic application of Newton’s method can be slow and might require infeasible amounts of memory (depending on the model size). We also need to invert the Hessian, making it even more costly.\nWe can approximate Newton’s method using the Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm (L-BFGS) search (Lu 2022,chapt 6). It uses past gradients to approximate the Hessian as the search proceeds. L-BFGS computes gradients at different values of \\(\\boldsymbol{\\theta}\\) for more than just updating the parameters (i.e., Hessian approximation and the line search), so it is not a good solution when loss function or gradient evaluations are expensive.\nHowever, if we have a small dataset, a low-to-medium complexity architecture, or both, L-BFGS can be a very effective tool for estimating parameters.\n\n\n17.4.11 Forestation Modeling\nFor the forested data, there are a few options for preprocessing:\n\nUse binary indicators to represent the 39 counties or a supervised effect encoding strategy.\nSeveral predictors are highly skewed. We can leave them as-is before centering/scaling, or use an orderNorm transformation to coerce each predictor to a standard normal distribution.\nDeal with the high correlation for some predictors via PCA extraction or leave as-is in the data.\n\nSeveral combinations of these operations were used:\n\nAn effect encoding with and without PCA feature extraction.\nStandard binary indicators with and without PCA feature extraction and with and without a symmetry transformation (i.e., orderNorm versus centering/scaling).\n\nFor the PCA extraction, all possible components were used.\nFor the model, there are a fair number of tuning parameters to combine:\n\nOne or two layers.\nThe number of hidden units. For each layer, these ranged from two to fifty units.\nThe activation type: ReLU, elu, tanh, and tanhshrink.\nL2 regularization values between 10-10 and 10-1.\nBatch sizes between 23 and 28.\nLearning rates between 10-4 and 10-1.\nLearning rate schedules: constant, cyclic, and time decay.\nMomentum values between [0.8, 0.99].\n\nAdamW was used to train the models, with a maximum of 25 epochs allowed, but early stopping could occur after five consecutive epochs with poor performance.\nA space-filling design with 25 candidates was evaluated. The six preprocessing schemes listed above were each run with the same model grid, resulting in 300 candidates to be evaluated.\nFigure 17.22 shows the results for the 300 candidates. They are ranked by their resampling estimate of the Brier scores, and the figure presents these rankings from best to worst, breaking out the combinations of preprocessing scenarios. While each panel contains models with sufficiently low Brier scores, several trends are evident. The “Indicators” panel represents models using dummy variable indicators with no symmetry transformation, and this did not do well (with or without PCA feature extraction).\n\n\n\n\n\n\n\n\nFigure 17.22: Rankings of multilayer perceptron models for the forestation models using the Brier score. The panels represent different types of preprocessing, and the vertical lines are 90% confidence intervals.\n\n\n\n\n\nIn fact, PCA has suboptimal results in each of the three bottom panels. This could be due to the components being unsupervised; they are not optimized for prediction. Alternatively, the last few components might be measuring noise and have less relevance to the model. Adding irrelevant predictors to neural networks can diminish their effectiveness.\nThe two cases that did the best were the “Indicators + Symmetry” and “Effect Encoding + Symmetry” panels. While both used the orderNorm transformation, the former panel employed indicators, whereas the latter used a supervised effect encoding, where a single column was used to represent the county effect. This suggests that eliminating distributional skew in the predictors may yield a modest improvement in predictive performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing\nArchitecture\n# Parameters\nPenalty\nBatch Size\nLearning Rate\nMomentum\n\nPerformance\n\n\n\nBrier\nROC\n\n\n\n\nEffect Encoding, Symmetry, No PCA\n46 elu , 8 relu\n1176\n10-7.38\n16\n10-3.25 (none)\n0.871\n0.0806\n0.9515\n\n\nIndicators, Symmetry, No PCA\n46 elu , 8 relu\n2878\n10-7.38\n16\n10-3.25 (none)\n0.871\n0.0809\n0.9496\n\n\nEffect Encoding, Symmetry, No PCA\n32 tanh\n610\n10-9.62\n29\n10-3.00 (none)\n0.950\n0.0828\n0.9472\n\n\nIndicators, Symmetry, No PCA\n32 tanh\n1794\n10-9.62\n29\n10-3.00 (none)\n0.950\n0.0829\n0.9452\n\n\nEffect Encoding, Symmetry, No PCA\n10 elu\n192\n10-6.62\n52\n10-3.12 (inverse decay)\n0.982\n0.0842\n0.9486\n\n\n\n\n\n\n\n\nTable 17.2: The best models as ranked by the Brier score.\n\n\n\n\nDigging in further, Table 17.2 displays the top five candidates from a pool of 300. They are a mixture of one- and two-layer networks, but note that we have to go to the third decimal place to distinguish their Brier scores. Constant learning rates did well as long as their rates were fairly low and batch sizes were fairly small. The number of parameters varied as well; increasing the model’s complexity did not appear to help for this dataset. In fact, the rank correlation between the Brier score and the number of parameters was -0.08, implying that there is little evidence that adding more layers and/or units to the MLP improves the model for these data.\nThe Brier scores are low and the areas under the ROC curves are all at least 0.94, indicating that the predictions effectively separate the classes and that the probability estimates are accurate. Across the candidates, the rank correlation between the Brier score and the ROC AUC was -0.95; in this case, their optimization results are well-aligned.\nHow do these results compare to the previous KNN model? The Brier scores are very close: 0.0828 for the neural network and 0.0778 for KNN The difference is very small, and a 90% confidence interval for the difference is (-0.0071, -0.0031), suggesting the FDA model has statistically better performance although the difference is very small. The difference in the area under the ROC curve is -0.0057 with interval (-0.0086, -0.00341), leading us to a similar conclusion. Given that the KNN model is simpler, we might favor that model over our neural network.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Complex Nonlinear Boundaries</span>"
    ]
  },
  {
    "objectID": "chapters/cls-nonlinear.html#sec-cls-tab-net",
    "href": "chapters/cls-nonlinear.html#sec-cls-tab-net",
    "title": "17  Complex Nonlinear Boundaries",
    "section": "17.5 Special Tabular Network Models",
    "text": "17.5 Special Tabular Network Models",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Complex Nonlinear Boundaries</span>"
    ]
  },
  {
    "objectID": "chapters/cls-nonlinear.html#sec-cls-svm",
    "href": "chapters/cls-nonlinear.html#sec-cls-svm",
    "title": "17  Complex Nonlinear Boundaries",
    "section": "17.6 Support Vector Machines",
    "text": "17.6 Support Vector Machines",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Complex Nonlinear Boundaries</span>"
    ]
  },
  {
    "objectID": "chapters/cls-nonlinear.html#chapter-references",
    "href": "chapters/cls-nonlinear.html#chapter-references",
    "title": "17  Complex Nonlinear Boundaries",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAchiam, J, S Adler, S Agarwal, L Ahmad, I Akkaya, F Leoni Aleman, D Almeida, et al. 2023. “GPT-4 Technical Report.” arXiv.\n\n\nBaldi, P, and K Hornik. 1989. “Neural Networks and Principal Component Analysis: Learning from Examples Without Local Minima.” Neural Networks 2 (1): 53–58.\n\n\nBoykis, V. 2023. “What are embeddings?” https://doi.org/10.5281/zenodo.8015028.\n\n\nBreiman, L, and R Ihaka. 1984. “Nonlinear Discriminant Analysis via Scaling and ACE.” University of. California, Berkeley.\n\n\nChen, YC. 2017. “A Tutorial on Kernel Density Estimation and Recent Advances.” Biostatistics & Epidemiology 1 (1): 161–87.\n\n\nClevert, DA, T Unterthiner, and S Hochreiter. 2015. “Fast and Accurate Deep Network Learning by Exponential Linear Units (Elus).” arXiv 4 (5): 11.\n\n\nCunningham, P, and SJ Delany. 2021. “K-Nearest Neighbour Classifiers-a Tutorial.” ACM Computing Surveys (CSUR) 54 (6): 1–25.\n\n\nDauphin, Y, R Pascanu, C Gulcehre, K Cho, S Ganguli, and Y Bengio. 2014. “Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization.” Advances in Neural Information Processing Systems 27.\n\n\nDe Maesschalck, R., D. Jouan-Rimbaud, and D Massart. 2000. “The Mahalanobis Distance.” Chemometrics and Intelligent Laboratory Systems 50 (1): 1–18.\n\n\nDefazio, A, and S Jelassi. 2021. “Adaptivity Without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization.” arXiv.\n\n\nDevlin, J, MW Chang, K Lee, and K Toutanova. 2019. “Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1, 4171–86.\n\n\nDiederik, P, and J Ba. 2015. “A Method for Stochastic Optimization.” In International Conference on Learning Representations (ICLR). Vol. 5. 6. California;\n\n\nDozat, T. 2016. “Incorporating Nesterov Momentum into Adam.” In Proceedings of the 4th International Conference on Learning Representations, 1–4.\n\n\nDuchi, J, E Hazan, and Y Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” Journal of Machine Learning Research 12 (7).\n\n\nDudani, S. 1976. “The Distance-Weighted k-Nearest-Neighbor Rule.” IEEE Transactions on Systems, Man, and Cybernetics, no. 4: 325–27.\n\n\nDugas, C, E Bengio, F Bélisle, C Nadeau, and R Garcia. 2000. “Incorporating Second-Order Functional Knowledge for Better Option Pricing.” Advances in Neural Information Processing Systems 13.\n\n\nElfwing, S, E Uchibe, and K Doya. 2018. “Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning.” Neural Networks 107: 3–11.\n\n\nFriedman, J. 1989. “Regularized Discriminant Analysis.” Journal of the American Statistical Association 84 (405): 165–75.\n\n\nGhojogh, B, and M Crowley. 2019. “Linear and Quadratic Discriminant Analysis: Tutorial.” arXiv.\n\n\nGlorot, X, and Y Bengio. 2010. “Understanding the Difficulty of Training Deep Feedforward Neural Networks.” In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 249–56. JMLR Workshop; Conference Proceedings.\n\n\nGlorot, X, A Bordes, and Y Bengio. 2011. “Deep Sparse Rectifier Neural Networks.” In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, 315–23. JMLR Workshop; Conference Proceedings.\n\n\nGolub, G, M Heath, and G Wahba. 1979. “Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter.” Technometrics 21 (2): 215–23.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nGower, J. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71.\n\n\nGuo, C, and F Berkhahn. 2016. “Entity Embeddings of Categorical Variables.” arXiv.\n\n\nHand, D. 1981. Discrimination and Classification. Wiley.\n\n\nHand, D, and K Yu. 2001. “Idiot’s Bayes — Not so Stupid After All?” International Statistical Review 69 (3): 385–98.\n\n\nHanson, S, and L Pratt. 1988. “Comparing Biases for Minimal Network Construction with Back-Propagation.” Advances in Neural Information Processing Systems 1.\n\n\nHastie, T, R Tibshirani, and A Buja. 1994. “Flexible Discriminant Analysis by Optimal Scoring.” Journal of the American Statistical Association 89 (428): 1255–70.\n\n\nHastie, T, and T Tibshirani. 2024. “mda: Mixture and Flexible Discriminant Analysis.” https://doi.org/10.32614/CRAN.package.mda.\n\n\nHe, K, X Zhang, S Ren, and J Sun. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on Imagenet Classification.” In Proceedings of the IEEE International Conference on Computer Vision, 1026–34.\n\n\nHe, K, X Zhang, S Ren, and J Sun. 2016. “Deep Residual Learning for Image Recognition.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–78.\n\n\nHechenbichler, K., and K. Schliep. 2004. “Weighted k-Nearest-Neighbor Techniques and Ordinal Classification.” Sfb386. http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-1769-9.\n\n\nHendrycks, D, and K Gimpel. 2016. “Gaussian Error Linear Units (Gelus).” arXiv.\n\n\nHinton, G. 2012. “Neural Networks for Machine Learning, Lecture 6e.” https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.\n\n\nHochreiter, S. 1998. “The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions.” International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 6 (02): 107–16.\n\n\nHochreiter, S, and J Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80.\n\n\nHoffer, E, I Hubara, and D Soudry. 2017. “Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks.” Advances in Neural Information Processing Systems 30.\n\n\nHubert, M, J Raymaekers, and P Rousseeuw. 2024. “Robust Discriminant Analysis.” Wiley Interdisciplinary Reviews: Computational Statistics 16 (5): e70003.\n\n\nJohnson, R, and D Wichern. 2007. Applied Multivariate Statistical Analysis. 6th ed. Upper Saddle River, NJ: Prentice Hall.\n\n\nKeskar, NS, D Mudigere, J Nocedal, M Smelyanskiy, and PTP Tang. 2016. “On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.” arXiv.\n\n\nKrogh, A, and J Hertz. 1991. “A Simple Weight Decay Can Improve Generalization.” Advances in Neural Information Processing Systems 4.\n\n\nLeCun, Y, L Bottou, G B Orr, and KR Müller. 2002. “Efficient Backprop.” In Neural Networks: Tricks of the Trade, 9–50. Springer.\n\n\nLoshchilov, I, and F Hutter. 2017. “Decoupled Weight Decay Regularization.” arXiv.\n\n\nLu, J. 2022. Gradient Descent, Stochastic Optimization, and Other Tales. Eliva Press.\n\n\nManning, D, P Raghavan, and H Schutze. 2009. Introduction to Information Retrieval. Cambridge University Press.\n\n\nMcLachlan, G. 2005. Discriminant Analysis and Statistical Pattern Recognition. John Wiley & Sons.\n\n\nMurphy, K. 2012. Machine Learning: A Probabilistic Perspective. MIT press.\n\n\nPang, H, T Tong, and H Zhao. 2009. “Shrinkage-Based Diagonal Discriminant Analysis and Its Applications in High-Dimensional Data.” Biometrics 65 (4): 1021–29.\n\n\nPark, B, and J Marron. 1990. “Comparison of Data-Driven Bandwidth Selectors.” Journal of the American Statistical Association 85 (409): 66–72.\n\n\nSamworth, R. 2012. “Optimal Weighted Nearest Neighbour Classifiers.” The Annals of Statistics 40 (5): 2733–63.\n\n\nSilverman, B. 1986. Density Estimation for Statistics and Data Analysis. Chapman & Hall.\n\n\nSmith, L. 2017. “Cyclical Learning Rates for Training Neural Networks.” In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), 464–72. IEEE.\n\n\nSutskever, I, J Martens, G Dahl, and G Hinton. 2013. “On the Importance of Initialization and Momentum in Deep Learning.” In International Conference on Machine Learning, 1139–47. pmlr.\n\n\nWeigend, A, D Rumelhart, and B Huberman. 1990. “Generalization by Weight-Elimination with Application to Forecasting.” Advances in Neural Information Processing Systems 3.\n\n\nZuo, W, D Zhang, and K Wang. 2008. “On Kernel Difference-Weighted k-Nearest Neighbor Classification.” Pattern Analysis and Applications 11 (3): 247–57.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Complex Nonlinear Boundaries</span>"
    ]
  },
  {
    "objectID": "chapters/cls-nonlinear.html#footnotes",
    "href": "chapters/cls-nonlinear.html#footnotes",
    "title": "17  Complex Nonlinear Boundaries",
    "section": "",
    "text": "We’ll see kernel functions two more times in this chapter.↩︎\nIn units of Pa × 100.↩︎\nA different view of predictor importance will be seem later in Figure 17.7.↩︎\nThe approximation involves scaling factors derived from the eigenvalues of \\(\\boldsymbol{\\Phi}\\).↩︎\nFor consistency, we’ll use “slopes” and “intercepts” here.↩︎\n This representation and the one below were adapted from Izaak Neutelings’s excellent work, which can be found at https://tikz.net/neural_networks/ and is licensed under a CC BY-SA 4.0.↩︎\nGeneralized linear models are a good example of this↩︎\norderNorm is perhaps more attractive since it standardizes the predictors and coerces them to a symmetric distribution.↩︎\nAlthough the submodel trick described in Section 11.3.1 could greatly improve the effectiveness of this approach.↩︎\nTypically on log2 units↩︎\nIn Section 17.4.2, we previously described Nesterov momentum. This is unrelated to the current tool bearing the same name.↩︎\nFor the first update, the value is simply \\(v_1 = \\alpha g(\\boldsymbol{\\theta}_i)\\).↩︎\nSince the loss function for this example is based on a single, deterministic value, the momentum value was set very low (\\(\\phi = 2/3\\)). However, the overall trend is consistent with results from a typical performance metric like cross-entropy.↩︎",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Complex Nonlinear Boundaries</span>"
    ]
  },
  {
    "objectID": "chapters/cls-trees.html",
    "href": "chapters/cls-trees.html",
    "title": "18  Classification using Trees and Rules",
    "section": "",
    "text": "18.1 Elements of Trees",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Classification using Trees and Rules</span>"
    ]
  },
  {
    "objectID": "chapters/cls-trees.html#sec-cls-tree-elements",
    "href": "chapters/cls-trees.html#sec-cls-tree-elements",
    "title": "18  Classification using Trees and Rules",
    "section": "",
    "text": "18.1.1 Splitting\n\n\n18.1.2 Growing\n\n\n18.1.3 Pruning\n\n\n18.1.4 Missing Data Handling",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Classification using Trees and Rules</span>"
    ]
  },
  {
    "objectID": "chapters/cls-trees.html#sec-cls-single-tree",
    "href": "chapters/cls-trees.html#sec-cls-single-tree",
    "title": "18  Classification using Trees and Rules",
    "section": "18.2 Single Trees",
    "text": "18.2 Single Trees\n\n18.2.1 CART\n\n\n18.2.2 C5.0\n\n\n18.2.3 Conditional Inference Trees\n\n\n18.2.4 Oblique Trees\n\n\n18.2.5 Bayesian Trees",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Classification using Trees and Rules</span>"
    ]
  },
  {
    "objectID": "chapters/cls-trees.html#sec-cls-tree-unstable",
    "href": "chapters/cls-trees.html#sec-cls-tree-unstable",
    "title": "18  Classification using Trees and Rules",
    "section": "18.3 The Instability of Trees",
    "text": "18.3 The Instability of Trees",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Classification using Trees and Rules</span>"
    ]
  },
  {
    "objectID": "chapters/cls-trees.html#sec-cls-rules",
    "href": "chapters/cls-trees.html#sec-cls-rules",
    "title": "18  Classification using Trees and Rules",
    "section": "18.4 From Trees to Rules",
    "text": "18.4 From Trees to Rules",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Classification using Trees and Rules</span>"
    ]
  },
  {
    "objectID": "chapters/cls-trees.html#chapter-references",
    "href": "chapters/cls-trees.html#chapter-references",
    "title": "18  Classification using Trees and Rules",
    "section": "Chapter References",
    "text": "Chapter References",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Classification using Trees and Rules</span>"
    ]
  },
  {
    "objectID": "chapters/cls-ensembles.html",
    "href": "chapters/cls-ensembles.html",
    "title": "19  Classification Ensembles",
    "section": "",
    "text": "19.1 Bagging",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification Ensembles</span>"
    ]
  },
  {
    "objectID": "chapters/cls-ensembles.html#sec-cls-rand-forest",
    "href": "chapters/cls-ensembles.html#sec-cls-rand-forest",
    "title": "19  Classification Ensembles",
    "section": "19.2 Random Forest",
    "text": "19.2 Random Forest",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification Ensembles</span>"
    ]
  },
  {
    "objectID": "chapters/cls-ensembles.html#sec-cls-bart",
    "href": "chapters/cls-ensembles.html#sec-cls-bart",
    "title": "19  Classification Ensembles",
    "section": "19.3 Bayesian Additive Regression Trees",
    "text": "19.3 Bayesian Additive Regression Trees",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification Ensembles</span>"
    ]
  },
  {
    "objectID": "chapters/cls-ensembles.html#sec-cls-boost",
    "href": "chapters/cls-ensembles.html#sec-cls-boost",
    "title": "19  Classification Ensembles",
    "section": "19.4 Boosting",
    "text": "19.4 Boosting",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification Ensembles</span>"
    ]
  },
  {
    "objectID": "chapters/cls-ensembles.html#sec-cls-rule-ensembles",
    "href": "chapters/cls-ensembles.html#sec-cls-rule-ensembles",
    "title": "19  Classification Ensembles",
    "section": "19.5 Rule-Based Ensembles",
    "text": "19.5 Rule-Based Ensembles\n\n19.5.1 C5.0 Rules\n\n\n19.5.2 RuleFit",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification Ensembles</span>"
    ]
  },
  {
    "objectID": "chapters/cls-ensembles.html#chapter-references",
    "href": "chapters/cls-ensembles.html#chapter-references",
    "title": "19  Classification Ensembles",
    "section": "Chapter References",
    "text": "Chapter References",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification Ensembles</span>"
    ]
  },
  {
    "objectID": "chapters/cls-summary.html",
    "href": "chapters/cls-summary.html",
    "title": "22  Classification Summary",
    "section": "",
    "text": "22.1 Final Forestation Results",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Classification Summary</span>"
    ]
  },
  {
    "objectID": "chapters/cls-summary.html#sec-cls-other",
    "href": "chapters/cls-summary.html#sec-cls-other",
    "title": "22  Classification Summary",
    "section": "22.2 Other Topics",
    "text": "22.2 Other Topics\n\n22.2.1 Ordered Categories\n\n\n22.2.2 Multilabel Outcomes",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Classification Summary</span>"
    ]
  },
  {
    "objectID": "chapters/cls-summary.html#chapter-references",
    "href": "chapters/cls-summary.html#chapter-references",
    "title": "22  Classification Summary",
    "section": "Chapter References",
    "text": "Chapter References",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Classification Summary</span>"
    ]
  },
  {
    "objectID": "chapters/cls-imbalance.html",
    "href": "chapters/cls-imbalance.html",
    "title": "20  Class Imbalances",
    "section": "",
    "text": "20.1 Example: Predicting Myopia\nExamples: churn, click through, diseases (trip test, Parkinson’s), blood specimen mixups\nFocus on two class problems, but an issue for all classes\nPrior versus likelihood, posterior\nnote on time slices to make events\nFor demonstration, we’ll use data from the myopia study as described in Section 1.6.6 of Hosmer, Lemeshow, and Sturdivant (2013). Myopia (i.e., nearsightedness) can be caused by the shape of one’s eye, genetics, and environmental conditions, including the heavy use of computer or television screens. The data are from a medical study running from 1990 to 1995 and contained 302 girls and 316 boys ranging from 5 to 9 years old. These data are from the initial exam and include demographic factors, several physiological measurements of the eye from an ocular examination, and whether either parent is also myopic. The outcome is determined after a five-year follow-up visit. In this study, 81 (13.1%) of children were nearsighted.\nTo begin, a 3:1 stratified split was used to partition the data into training and testing sets. Stratification was based on the outcome. This results in a training set of 462 children, only 60 of whom are myopic. For the test set, 21 children out of 156 were myopic. Since the training set and the number of events are small, five repeats of 10-fold cross-validation were used with the same stratification strategy. The repeats will help increase the precision of our resampling statistics.\nWe’ll conduct some quick exploratory data analysis, then proceed to creating models based on traditional modeling methods, i.e., without making adjustments for class imbalance during model optimization.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Class Imbalances</span>"
    ]
  },
  {
    "objectID": "chapters/cls-imbalance.html#sec-yyopia",
    "href": "chapters/cls-imbalance.html#sec-yyopia",
    "title": "20  Class Imbalances",
    "section": "",
    "text": "It is critical that the test set reflects the true event rate in the population of interest. While we may change the event rate in the training set during model development, the test set data should be as consistent as possible with reality.\n\n\n\n20.1.1 Exploring the Data\nFirst, let’s examine the age and sex of the participants. Figure 20.1 shows the class percentages for 10 subgroups of the data. The size of the point is based on the number of children in the subgroup, indicating that many of them are 6 years old, while very few are 9 years old. There were no nine-year-old myopic girls in the training set.\n\n\n\n\n\n\n\n\nFigure 20.1: Training set rates of myopia by age and sex of the children. The error bars are based on 90% confidence intervals, and the size of the point indicates the number of children in each subgroup.\n\n\n\n\n\nThe points suggest a slight increase in myopia rates among girls, but this is well within the error bars defined by the 90% confidence intervals. Due to the concentration of points in a single age group, it is difficult to determine whether there is an age trend. However, since the slight rate increase for girls is constant from ages six to eight, there is little indication of an interaction between these factors.\nSecond, do genetics matter? If neither parent is myopic, the rates for their children are low: 1.8% with 90% confidence interval (0.3%, 5.7%). When both parents have the condition, the rate is 21.6% (CI: 15.4%, 28.8%), indicating a strong signal. The effects of single paternal and maternal genetics are in between, with rates 15.3% and 12.6%, respectively, with similar confidence intervals.\nThe ocular exam yielded five different measurements of the eye. Figure 20.2 shows a scatterplot matrix of pairwise relationships, colored by the outcome class. With one exception, there is little to no correlation between predictors. However, the axial length and the vitreous chamber depth have a high positive correlation. Additionally, most predictors have fairly symmetric distributions, although the spherical equivalent refraction data exhibit a moderately strong right skew. This predictor also shows some raw separation between the classes.\n\n\n\n\n\n\n\n\nFigure 20.2: Scatterplot matrix of eye measurements.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Class Imbalances</span>"
    ]
  },
  {
    "objectID": "chapters/cls-imbalance.html#sec-imbalance-math",
    "href": "chapters/cls-imbalance.html#sec-imbalance-math",
    "title": "20  Class Imbalances",
    "section": "20.2 Underlying Mathematical Issue",
    "text": "20.2 Underlying Mathematical Issue\ntalk about specific literature on logistic models.\nWe’ll use two predictors in the myopia data to visualize the effects of different techniques. In the upper-left panel of the lower diagonal of Figure 20.2 we see the spherical equivalent refraction and vitreous chamber depth predictor data from the training set, colored by the outcome class. A flexible discriminant analysis model using MARS basis functions was used to model these data. Interactions between hinge functions were allowed, and generalized cross-validation was used to automatically determine the optimal level of model complexity. The model was fit to the training set, and Figure 20.3 shows the class boundary, defined using the 50% probability cutoff. The events (in red) occupy the space to the left of the main data stream. The class boundary does its best to find the optimal partition of the data, but we can see that less than half of the training set events are not to the left of the boundary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.3: A flexible discriminant fit to the myopia training set. Using the default probability cutoff of 50%, shown as the black contour, 24 of the 60 events are correctly predicted as events.\n\n\n\nTrees provide another example that demonstrates how models can overly focus on the majority class. Since most trees employ a greedy search, the first split is often one that captures the majority of the majority class. As the tree-growing process continues, the minority class becomes less likely to be predicted in the nodes, since \\(n_{min}\\) limits will restrict fine-grained splits. Also, many pruning methods will remove nodes with higher uncertainty, which is often a function of sample size; smaller nodes are more likely to be pruned. When a standard CART model was fit to the data, the rules to each terminal node were:\n\nif spherical equivalent refraction \\(\\ge 0.28\\) then rate = 6% (\\(n\\) = 382)\nif spherical equivalent refraction \\(\\in [0.23, 0.28)\\) then rate = 60% (\\(n\\) = 15)\nif spherical equivalent refraction \\(\\in [-0.05, 0.23)\\) then rate = 33% (\\(n\\) = 46)\nif spherical equivalent refraction \\(\\le  -0.05\\) then rate = 74% (\\(n\\) = 19)\n\nSplit 1 was the initial split, and we can see that it captures most of the non-myopic children. The second and third rules attempt to identify spaces where myopic children can be identified, but these two equations are likely the result of overfitting. Regardless, the splits demonstrate how the majority class can dominate the objective function, making the majority class data more likely to be correctly predicted.\nThis also brings up a pattern that can occur when dealing with class imbalances that is somewhat counterintuitive: models that are less complex tend to work better than highly complex models. In essence, the more models can adapt to the data, the more likely they are to overfit to the majority class. The rule set shown above is a good example. Tree-based models employ a greedy optimization approach to train the model and can carve out small subspaces of predictors to make local predictions. It would be very difficult for a logistic regression to do the same, at least without enormously complex feature engineering. As a result, these simpler models can have sufficient predictive power to be useful, but not so much as to discern minute patterns in the data, thereby correctly predicting the few events and risking overfitting. We’ll see this pattern in the example data, which is admittedly small, but it has also repeated in much larger datasets with class imbalances.\nBefore moving on to specific techniques, let’s discuss a potential fallacy encountered when there are rare events: correctly predicting the events to the detriment of the overall model goals is often inappropriate.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Class Imbalances</span>"
    ]
  },
  {
    "objectID": "chapters/cls-imbalance.html#sec-imbalance-goals",
    "href": "chapters/cls-imbalance.html#sec-imbalance-goals",
    "title": "20  Class Imbalances",
    "section": "20.3 Imbalances Make Us Focus on Our Goals",
    "text": "20.3 Imbalances Make Us Focus on Our Goals\nBefore starting the model development process, it is essential to consider how class balance impacts the project goals. It is often the case that the fact that events rarely occur means that the goal of the model is to identify the precious events in a large dataset (such as in information retrieval), even if it is at the cost of more false positives than we would ordinarily tolerate. In other words, finding the events is of paramount importance, and other considerations, such as interpretability, inference, and explanation of predictions, are not terribly important. This is often the case where the hard class prediction is used in an automated system that only takes the qualitative prediction as input. For example, Kuhn and Johnson (2013) discusses a model used for a high-performance computing grid to route computational jobs to the correct queue based on their execution time. The system only needs to know where to send the job, and measures of uncertainty, such as estimated probabilities for how long the job will take, play no role in the process.\nHowever, it can also be the case that we need to understand why the model works or why a data point was predicted to belong to a specific class. In ?sec-drug-interactions, we’ll discuss a project from drug development where a model is used to estimate the risk of a potential drug causing patient harm. We definitely want to find the events (e.g., problematic drugs), but we also need a good understanding of how certain we are that there is an issue.\n\nIn other words, we should not confuse what we will do to deal with extreme frequencies with the idea that we should just care about finding important events.\n\nThese issues are important to consider because, for class imbalances, several approaches can be taken to mitigate the disparity in class frequencies. Some of them can subvert the accuracy of the predicted class probabilities (i.e., result in poor calibration). If we require a model that predicts useful probabilistic outputs, we would avoid some modeling techniques.\nAdditionally, we should settle on what performance metrics are most important. We may need multiple metrics, including some to measure calibration (Brier scores), overall class separation (ROC curves), and/or the ability to detect events (PR curves). Additionally, knowing how well the hard class predictions work may require attention to sensitivity, specificity, recall, and other metrics.\nThese are all issues that should be thought about for every modeling project. We emphasize it here because imbalances, especially extreme ones, substantially raise the level of difficulty.\nThe next two sections will outline two general philosophies when modeling imbalanced data. The first is to accept that the imbalance and model it as-is. We can utilize all the tools and methods previously discussed to create a model driven by performance statistics based on probabilistic predictions, such as log-loss or the area under the receiver operator characteristic curve. Once we have an appropriate model for these criteria, we can focus on how to postprocess the predictions to produce the optimal hard class predictions.\nThe second philosophy is to circumvent the issue of the imbalance by either removing it or by making the performance metric aware that some training set points (e.g., the events) are more important than others. Both approaches bias the model to be less focused on the majority class.\nBoth philosophies have advantages and disadvantages. Generally, if finding the events is the overriding goal, the second philosophy will likely be the best choice.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Class Imbalances</span>"
    ]
  },
  {
    "objectID": "chapters/cls-imbalance.html#sec-imbalance-acceptance",
    "href": "chapters/cls-imbalance.html#sec-imbalance-acceptance",
    "title": "20  Class Imbalances",
    "section": "20.4 Strategy 1: Accepting the State of Nature",
    "text": "20.4 Strategy 1: Accepting the State of Nature\nIn this case, we’ll try to build ML models while generally ignoring the imbalance. The idea is that we should initially focus on optimizing metrics based on class probability estimates, such as the Brier score, cross-entropy, or the areas under the ROC or PR curves. Once we have a model with good calibration and/or discrimination, we can then focus on making the best hard class predictions. We’ll show the advantages and disadvantages of using this strategy.\nTo get started, we’ll fit a subset of models to these data and generally follow the process outlined in the previous chapters. There is little to no preprocessing required for these models. There is a single pair of highly correlated predictors, but that will be handled by the model in situations where multicorrelation is a concern.\nA set of five models was tested. First, a regularized logistic regression was used with glmnet penalties. The predictors included main effects and all two-factor interactions. Penalty values ranging from 10-4 to 10-0.6 were evaluated with four mixtures that ranged from 0% to 100% Lasso. A regular grid of 72 candidates was used to optimize these parameters1.\nSingle-layer neural networks were evaluated and were tuned over:\n\n\n\nThe number of hidden units (2 to 50).\nThe L2 penalty (10-10 to 10-1).\nThe learning rate (10-4 to 10-1).\n\n\n\nThe batch size (8 to 256 samples).\nAdamW momentum (0.8 to 0.99).\n\n\n\nSGD via AdamW was used to optimize the model for up to 100 epochs, with early stopping after 5 consecutive poor results. ReLU activation was used.\nBoosted trees (via light GBM) were also trained for up to 1,000 iterations with early stopping after 5 bad iterations. The model was tuned over:\n\n\n\nTree depth (1 to 15 splits).\nAmount of data needed for further splitting (2 to 40 samples).\n\n\n\nThe learning rate (10-3 to 10-0.5).\n\\(m_{try}\\) (1 to 16 predictors).\n\n\n\nAdditionally, random forests with 2,000 classification trees were tuned over:\n\nAmount of data needed for further splitting (2 to 40 samples).\n\\(m_{try}\\) (1 to 16 predictors)..\n\nFinally, a RuleFit model was optimized over:\n\n\n\nThe number of boosting iterations (5 to 100)\nTree depth (2 to 4 splits).\nAmount of data needed for further splitting (2 to 10 samples).\n\n\n\nThe learning rate (10-10 to 100).\nProportional \\(m_{try}\\) (10% to 100%).\nThe L1 penalty (10-10 to 100).\n\n\n\nApart from the logistic regression, a space-filling design of 25 candidates was used for optimization.\nFigure 20.4 visualizes the results across and within models. Each point represents a candidate within a model. These are ranked by their resampled Brier score and are ordered accordingly. The vertical lines help highlight where the best candidate was located in the ranking. Interestingly, the logistic regression and RuleFit models had the best results and were very similar (see discussion below). The other three models had both good and bad performance across their candidates, but, in terms of model calibration, did not do as well. This may be due to the fact that this data set is better served by simpler models; the added complexity generated by large tree ensembles and neural networks is likely overfitting to some degree.\n\n\n\n\n\n\n\n\nFigure 20.4: Cross-validated Brier scores for different modeling approaches without adjustments for class imbalance. The dashed vertical lines indicate the highest-ranked candidate for each model.\n\n\n\n\n\nWhile the Rulefit and logistic regressions had the smallest Brier scores, their respective best candidates have issues. For the RuleFit model, the maximum estimated probability of a patient being myopic is 71.5% across all of the assessment sets. While the values are well-calibrated within the observed data range, this upper limit of probability is probably too small to be the final model. We would expect at least one of the held-out predictions to be closer to one when the patient is truly myopic. We also do not have a clear sense of how well-calibrated the predictions are when values are closer to one.\nTODO: problem of never predicting the minor class with poor probabilities\nFor the regularized logistic model, Figure 20.5 illustrates the relationship between the tuning parameters and the Brier score. We can see that while the numerically best results are to use 100% Lasso regularization with a penalty of 10-2, there are several other combinations with nearly equivalent metrics. The model fitted on the entire training set could have a maximum of 136 model coefficients (apart from the intercept) since there are 16 main effects and 120 possible two-way interactions. However, the Lasso model can remove predictors, and the fitted model contains only 1 main effect and 16 interactions. For a model used purely for prediction, this is not a terrible situation. However, the lack of main effects is troubling since there are interaction terms in the model with no corresponding main effects. This is inconsistent with the heredity principle discussed in Section 8.1.1 and suggests that these interactions are unlikely to be real.\nTo sidestep this issue, we can choose a different tuning parameter candidate. If we use no Lasso regularization, the model will contain all parameters, but regularizes some of them to be close to zero. This would ensure that all of the interaction terms have corresponding main effects in the model. Taking this approach, the penalty associated with the smallest Brier score is 10-0.8. For this model, the resampling estimate of the Brier score is 0.0864%, which is 2.8 worse than the original choice, but still very good when compared to the other models. We’ll use this candidate going forward.\nAdditionally, Figure 20.5 shows some diagnostics for the logistic regression. These plots are constructed by obtaining the assessment set predictions across the 50 resamples. Note that since repeated cross-validation was used, each training set point has five replicates in the held-out data, totaling 23,100 rows across all resamples. To create the calibration and ROC curves, the five predictions for each training set point are averaged so that we have a set of 462 rows of predictions. Please note that these are approximate and may differ from the performance statistics generated by resampling. For example, Figure 20.5 shows a single curve while the resampled ROC AUC is the average of 50 ROC curves created with 10% of the training set. Despite this, the visualizations can help us characterize the quality of the fit and detect any significant issues in the predictions.\n\n\n\n\n\n\n\n\nFigure 20.5: Results of the logistic regression model optimization (without adjustments for the class imbalance). The top plot shows the performance profile for the two tuning parameters. Underneath are the calibration and ROC curves. The solid blue point on the ROC curve corresponds to the default cutoff. The pooled averaged holdout predictions were used to compute these visualizations.\n\n\n\n\n\nThe calibration curve is generally good, except for the upper end, where it indicates that the model underpredicts the probability of myopia. There is also a region around an x-axis point where events occur at a rate of 60%, and the probabilities are slightly overpredicted. All in all, though, the calibration curve is impressive.\nThe ROC curve is symmetric and fairly unremarkable. The visualization shows that the default 50% threshold for converting the probabilities to hard “yes/no” predictions is somewhat problematic. The sensitivity for this cutpoint is very poor (16.4%) while the specificity is exquisite (98.2%). This is fairly common for class imbalances. The model performs significantly better where the data are more abundant, and since most participants in the study are not myopic, high specificity is more easily achieved.\nWe quantified performance for these models using a variety of methods. Table 20.1 shows several statistics for both hard and soft prediction types. When assessing class probability estimates, the Brier scores for each model are all acceptably low, and most areas under the ROC curves have objectively large values. The areas under the precision-recall curves are mediocre, with the largest possible value being 1.0 and the worst value is 0.13.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\n\nHard Class Prediction Metrics\n\n\nProbability Prediction Metrics\n\n\n\nAccuracy\nSensitivity\nSpecificity\nKappa\nROC AUC\nPR AUC\nBrier Score\n\n\n\n\nLogistic (glmnet)\n87.7%\n16.4%\n98.2%\n0.197\n0.849\n0.536\n0.0864\n\n\nNeural Network\n86.3%\n12.4%\n97.3%\n0.125\n0.800\n0.401\n0.0980\n\n\nRandom Forest\n87.2%\n21.4%\n96.9%\n0.236\n0.852\n0.501\n0.0889\n\n\nBoosted Tree\n87.2%\n17.3%\n97.6%\n0.200\n0.864\n0.508\n0.0884\n\n\nRuleFit\n87.7%\n11.1%\n99.0%\n0.141\n0.869\n0.541\n0.0875\n\n\n\n\n\n\n\n\n\nTable 20.1: Resampled performance metrics for the best candidates across different models. The models were optimized based on the Brier score.\n\n\n\n\n\n\n\nThe hard class predictions in Table 20.1 use the default 50% probability cutoff. A few of the models have accuracies that are larger than the rate that the majority class occurs in the training set (87%). The Kappa statistics indicate a significant signal in the models for predicting qualitative class membership. However, with this cutoff, the model is effectively useless at predicting which people are myopic, as the best possible sensitivity is 21.4%.\nIf we are satisfied with the model’s ability to discriminate between classes (as evidenced by the ROC curve) and that the probabilities are sufficiently calibrated, we can address the sensitivity problem by selecting an appropriate probability threshold. As with most optimization problems, we require an objective function to make the best decision regarding this tuning parameter. There are a few ways we can approach the problem:\n\nGiven what we know about how the model will be used, we could define specific costs for false negatives and false positives (with correctly predicted classes having zero cost). Using these values, we can choose a threshold that minimizes the cost of the decision. This is similar to a weighted Kappa approach, except the weights would not be symmetric.\nSeek a constrained optimization. For example, we can try to maximize the sensitivity under the constraint that the specificity is at an acceptable value. The ROC curve provides the substrate for these calculations, as it includes sensitivity and specificity for various thresholds in the data.\nOptimize an existing composite score that captures the right balance of sensitivity and specificity. The Matthews correlation coefficient and F-statistics are examples of such metrics.\nUse a multiparameter optimization method, such as desirability functions from Section 15.10, to define a jointly acceptable result (i.e., overall desirability). This would enable us to consider multiple performance metrics, as well as other variables, in the decision-making process.\n\nWe’ll focus on the first two options.\nTo choose the threshold using a custom cost metric (the first option in the list above), we would have to determine an appropriate cost structure for false positives and false negatives, usually based on the consequences of either error. For our myopia example, this is a somewhat subjective choice that depends on some context-specific rationale. In other cases, the costs might be analytically calculated as a function of monetary costs. Figure 20.6 shows examples of cost curves if the false positive cost is fixed at 1.0 and the cost of a false negative varies in severity. The y-axis is the mean of the cost values averaged across the 50 assessment sets.\n\n\n\n\n\n\n\n\nFigure 20.6: Cost curves when the cost of a correct prediction is zero, the cost of a false positive is 1.0, and the cost of a false negative varies.\n\n\n\n\n\nThe figure shows that increasing the cost of incorrectly predicting the events yields increasingly smaller thresholds. This is due to classifying more points as events, regardless of their actual class. The cost metric is punished less by incorrectly predicted non-events, so the overall cost statistic decreases substantially in the curves with higher false negative costs.\n\nIt is important to realize that this strategy only optimizes the probability threshold. The model fit (e.g., slope and intercept parameter estimates) has not been affected by the cost function. In the next section, we will consider costs to the objective function that the model uses during training.\n\nMoving on to constrained optimization, when choosing a threshold using an ROC curve, an immediate question arises: “Which data should be used to compute the ROC curve?” Preferably, the cutoff selection should use different data than the data used to fit the model. For example, diagnostic tests that require approval from a government agency (e.g., the Food and Drug Administration in the US) require a different data set to set the threshold. However, this may not be feasible if there is not an abundance of data or the resources to acquire such data. If we can’t hold out another data set, we can use the assessment sets generated during resampling2. There are at least two different approaches.\n\nOption 2A - post hoc pooling: After we have resampled our model configurations, we can pool the assessment set predictions for the best candidate. This ROC curve is computed based on the overall existing class probability estimate values. This approach generated the visualizations in Figure 20.5.\nOption 2B - threshold tuning: During model tuning, we treat the probability threshold as a tuning parameter and include it in the grid search. For each threshold in the grid, conditional on any other tuning parameters, we have resampling estimates for sensitivity and specificity. These points are then used to create an ROC curve.\n\nThe primary difference between these approaches lies in data usage: do we compute one large ROC curve from pooled data or an ROC curve from resampled statistics? Statistically, the latter is more appropriate, but either might produce acceptable results.\nNote that there are a few options when we treat the threshold as a tuning parameter. A one-stage approach would include the threshold along with any other tuning parameters and use a single grid for all3. However, recall that the choice of this parameter does not affect any probability-based metrics, such as the Brier score, log-loss, or the area under the ROC curve. Instead, we could use a two-stage approach where the non-threshold parameters are optimized to find their best results, and then we can use a one-dimensional grid to find the best threshold (based on metrics appropriate for hard class predictions). We took this approach here: 20 threshold values ranging from 2% to 40% defined the second-stage grid.\n\n\n\n\n\n\n\n\nFigure 20.7: Two approaches for optimizing the probability threshold via the ROC curve. Both curves use the regularized logistic regression’s resampling results.\n\n\n\n\n\nFor the logistic regression model, Figure 20.7 shows the ROC curves for both schemes. The curves significantly overlap for these data. Note that the curve associated with a second-stage grid contains 20 unique points since it is based on the values used in our second-stage grid. The curve for the post hoc pooling method has 462 points, which represents how many unique predicted probabilities are contained in the post hoc data pool.\nUsing either method for composing the ROC tool provides us with a tool to make trade-offs between sensitivity and specificity. For example, if we want to maximize sensitivity on the condition that specificity is at least 80%, the curve produced by threshold tuning indicates that the cutoff should be 16%. The estimated statistics associated with this value are a sensitivity of 75.7% and a specificity of 81.6%.\nThis overall approach involves finding a suitable ML model that does not allow class imbalance to alter our methodology, while deferring cutoff selection until the end of the process. This approach can lead to a well-calibrated model that appropriately classifies new samples. The downside becomes apparent when trying to explain individual results to the model’s consumers.\nFor our myopia example, suppose we make a prediction for a patient where the probability of myopia is estimated to be 20%. The conversation with the patient might sound something like\n\n“We estimate that your child has a 20% chance of being severely nearsighted and, based on this, we would give them a diagnosis of myopia.”\n\nIt is natural for someone to find the diagnosis counterintuitive since they are probably comparing the probability to the natural 50% threshold. It might lead the parent to ask:\n\n“That probability seems pretty low to make that diagnosis. Is that a mistake?”\n\nThe non-technical response could be:\n\n“We think that the diagnosis is appropriate because the overall chances of being myopic are low, and we have determined that our cutoff for making the diagnosis (16%) gives us the most effective diagnoses.”\n\nOf course, this is paraphrasing, but the hypothetical conversation does illustrate the potential confusion.\nThis seeming disconnect can become worse as the prevalence becomes smaller. For very rare events (i.e., &lt; 5%), the cutoff associated with the sensitivity-specificity trade-off can be very small, perhaps less than 1%. This is the price paid for having probability estimates that are consistent with the rate at which the event occurs in the wild.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Class Imbalances</span>"
    ]
  },
  {
    "objectID": "chapters/cls-imbalance.html#sec-imblance-correction",
    "href": "chapters/cls-imbalance.html#sec-imblance-correction",
    "title": "20  Class Imbalances",
    "section": "20.5 Strategy 2: Correcting the Imbalance",
    "text": "20.5 Strategy 2: Correcting the Imbalance\nIn the previous section, the only operation used to account for the small minority class was to optimize the probability threshold after the model had been chosen. In this section, we’ll consider several methods for actively addressing the imbalance during model optimization. The two primary classes of techniques are:\n\nChanging the composition of the training set data.\nUpweighting the minority class’s effect on the objective function.\n\nAs we’ll see, these can have a profound effect on the class probability estimates (both good and bad). These techniques are also most effective when the primary goal of the model is to identify events in new data.\n\n20.5.1 Sampling and Synthesis Techniques\nOne strategy for addressing a class imbalance is to eliminate it in the training set by adding or removing rows, depending on the class. A “sampling” method would remove existing rows from the data (likely from the majority class) to even the class frequencies. The removal may be random or based on characteristics of the existing data. Other sampling methods can oversample existing data. “Synthesis” methods invent new rows of the data in a way that makes them similar to or consistent with existing examples from the same class.\nRegardless of the method used, note that rebalancing occurs only on the data being used to train the model. If we are within a resampling loop, the analysis sets are affected, but the assessment sets are not. If we are going to fit a model to the entire data set, the training set is rebalanced, but the test set4 are unaffected. The reason for this is that the data used to measure how well the models work should be drawn from the same population as the data the model will eventually be used to predict. Rebalancing tools deliberately bias the sample to be unlike the population being predicted.\nNote that all the techniques mentioned here should be considered as yet another preprocessing method. As such, it is in the interest of the modeler to resample the models and include these steps within the resampling process. It would be inappropriate to apply a technique such as downsampling and then resample the data. Since these methods involve randomness, we need to account for it in our performance statistics.\nWe’ll describe each technique in the context of its use to modify the training set (or the analysis set when resampling). However, for models that resample the data, such as stochastic gradient boosting or random forests, these tools could also be applied within each resample. For example, our first technique described below, downsamples the majority class to reduce its size. A random forest model could do the same by using a stratified bootstrap sample that produces the sample number of data points for each class (chen2004using?). (chawla2003smoteboost?) and (galar2011review?) provide similar examples. This is highly dependent on the implementation, but it might be advantageous because the sampling/synthesis methods would be applied repeatedly, which can reduce model variance by exposing it to more unique data points.\n\n20.5.1.1 Downsampling\nThis approach is straightforward: every class with more data than the minority class is randomly sampled, ensuring that all classes have the same number of training set points. The minority class stays the same. For our myopia data, the majority class is randomly reduced from 462 to 81 patients. Figure 20.8 show examples for the two predictor subset including how the FDA fit changes with the reduced data set. We can see that the class boundary is slightly more simplistic than Figure 20.3 but has shifted to the right, capturing more events with the default 50% cutoff (at the expense of increased false positives).\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| label: fig-subsampling\n#| out-width: \"80%\"\n#| viewerHeight: 600\n#| standalone: true\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(munsell)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\n# source(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-sa.R\")\n\nui &lt;- page_fillable(\n  theme = bs_theme(bg = \"#fcfefe\", fg = \"#595959\"),\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 10, -1)),\n    column(\n      width = 10,\n      selectInput(\n        inputId = \"method\",\n        label = \" \",\n        choices = c(\n          \"Downsampled\",\n          \"SMOTE\",\n          \"ROSE\",\n          \"Near Miss\",\n          \"Adaptive Synthetic Algorithm\",\n          \"Tomek\"\n        ),\n        selected = c(\"Downsampled\")\n      )\n    )\n  ),\n  as_fill_carrier(plotOutput(\"plot\")),\n  br(),\n  textOutput(\"text\")\n)\n\nserver &lt;- function(input, output) {\n  load(url(\n    \"https://raw.githubusercontent.com/aml4td/website/main/RData/imbalanced_sampled.RData\"\n  ))\n\n  xrng &lt;- range(imbalanced_sampled$spherical_equivalent_refraction)\n  yrng &lt;- range(imbalanced_sampled$vitreous_chamber_depth)\n\n  orig &lt;- imbalanced_sampled %&gt;%\n    dplyr::filter(Data == \"Original\")\n\n  output$plot &lt;-\n    renderPlot(\n      {\n        dat &lt;- imbalanced_sampled %&gt;%\n          dplyr::filter(Data == input$method) |&gt;\n          dplyr::bind_rows(orig) |&gt;\n          dplyr::mutate(\n            Data = factor(Data, levels = c(\"Original\", input$method))\n          )\n\n        grid &lt;- imbalanced_grid %&gt;%\n          dplyr::filter(Data == input$method) |&gt;\n          dplyr::select(-Data) |&gt;\n          filter(!is.na(.pred_yes))\n        p &lt;-\n          dat |&gt;\n          ggplot(\n            aes(\n              spherical_equivalent_refraction,\n              vitreous_chamber_depth\n            )\n          ) +\n          geom_point(\n            aes(\n              col = class,\n              pch = class\n            ),\n            cex = 1.6,\n            alpha = 2 / 3\n          ) +\n          facet_wrap(~Data) +\n          coord_fixed(ratio = 1.4, xlim = xrng, ylim = yrng)\n\n        p &lt;- p +\n          theme(legend.position = \"top\") +\n          scale_color_brewer(drop = FALSE, palette = \"Set1\", direction = 01) +\n          theme_light_bl()\n\n        p &lt;-\n          p +\n          geom_tile(data = grid, aes(fill = .pred_yes), alpha = .1) +\n          geom_contour(\n            data = grid,\n            aes(z = .pred_yes),\n            breaks = 1 / 2,\n            col = \"black\"\n          ) +\n          scale_fill_gradient2(\n            low = \"#377EB8\",\n            mid = \"white\",\n            high = \"#E41A1C\",\n            midpoint = 0.5\n          ) +\n          labs(\n            x = \"spherical equivalent refraction\",\n            y = \"vitreous chamber depth\",\n            fill = \"Probability\"\n          )\n        print(p)\n      },\n      res = 100\n    )\n  \n  output$text &lt;-\n    renderText(\n      paste(\n        \"Using the default 50% threshold, this method correctly identified\", \n        imbalanced_hits[input$method], \n        \"truly myopic subjects (out of 60).\") \n    )\n}\n\napp &lt;- shinyApp(ui = ui, server = server)\napp\n\n\n\nFigure 20.8: The effect of a few sampling and synthesis techniques on two of the predictors in the myopia training set. The class boundary is based on a 50% probability cutoff.\n\n\n\nBesides rebalancing, the advantage of downsampling is that models can be trained faster. The downside is that important information might be lost in the majority class(es) due to random sampling. We are diminishing the influence of the non-minority classes and creating a smaller training set. Another effect of this is that our models will tend to be more simplistic, as the data cannot support excessive complexity.\nCan also be used inside of SGB and RF\n\n\n20.5.1.2 Upsamping\nUpsampling will create random replicates of the majority class(es) so that all classes have the same frequency in the training set. This increases the training set, sometimes to orders of magnitude larger, so training time can become excessive for some models. There is also the issue of having a potentially large number of replicates for each minority5 class value in the original training set. For example, for models such as random forest that use bootstrap samples to fit models, we could have even more replicates of these data points. Another complication occurs when a model, such as K-nearest neighbors, is used; all of the nearest neighbors might be the same data point.\nUpsampling is probably the least used of these techniques do to these issues.\n\n\n20.5.1.3 SMOTE\nThe Synthetic Minority Oversampling Technique (SMOTE) is a method that combines random sampling with the creation of novel data points, which are fusions and/or perturbations of existing points. There are a few flavors of this method, but we’ll focus on the original technique by REF where all predictors are required to be numeric.\nFor each data point in the minority class, compute its K-nearest neighbors. The create a new artificial data point, we select a minority class point at random (\\(\\boldsymbol{x}_i\\)) as well as one of its neighbors. For each predictor, we compute the difference between the original data point and its chosen neighbor (call this \\(\\delta_j\\) for predictor \\(j\\)). For each predictor, we compute a uniform random number \\(u_j \\sim U(0, 1)\\) and modify the original point as \\(x^*_{ij} = x_{ij} + u_j\\delta_j\\). Essentially, SMOTE generates a hyperrectangle between the selected point and its chosen neighbor, and then randomly places the new sample within the hyperrectangle.\nFigure 20.9 illustrates the process using a selected point (large red circle) in the north-west portion of the previously shown predictor space. A neighbor was selected below and to the right of the original point (large red square). The dotted lines show the area where newly synthesized points can be placed, and twenty examples are shown (open circles).\n\n\n\n\n\n\n\n\nFigure 20.9: Demonstrations of how SMOTE and ROSE synthesize new samples. The chosen point is the large red circle, and SMOTE’s selected nearest neighbor is the large red square. The ellipses in panel (b) represent Gaussian probability contours based on the kernel density estimate. Twenty points are synthesized for each panel.\n\n\n\n\n\nNote that SMOTE does not center the new samples around the original point; they can only be adjacent to it, since newly created data points must lie between it and the neighbor. This has the effect of clumping or clustering points to some degree. This can be seen in Figure 20.11 where there are gaps in the minority class distribution where no minority class data reside. This can be resolved by increasing the number of neighbors, but adding too many would lead to overlap in predictor distributions for each class. One could tune the number of neighbors, but it may not produce a clear winner for this parameter.\nnote overfitting in Figure 20.11\ngeneralizations\n\n\n20.5.1.4 ROSE\n\n\n20.5.1.5 Neighbor-Based Sampling\n(mani2003knn?) describe tools for undersampling the majority class using nearest neighbors. Their idea is to find points to remove based on how close (or far) they are from the minority class data. They outlined three procedures that use the name “NearMiss”. We’ll briefly describe each in turn and use a small “toy” data set to outline the details. These data are shown in Figure 20.10. There are 10 data points in the simulated training. The first 6 samples are from the majority class, and the next 4 samples labeled are from the minority data. Each of the three NearMiss methods uses a \\(4 \\times 6\\) distance matrix that captures the distances between the majority and minority class data (there is no need to find the neighbors within these two groups).\n\n\n\n\n\n\n\n\nFigure 20.10: Data to demonstrate Near Miss and Tomek Link methods.\n\n\n\n\n\nThe technique called “NearMiss-1” retains the majority-class samples with the smallest average distances to their K nearest neighbors. For example, using K = 2, this table shows the distances between the minority and majority class samples:\n\n\n\n\n\n\n\n\n\n\n\n\nMinority\n\nMajority Point Labels\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n7\n0.46\n0.41\n1.41\n2.33\n3.10\n3.73\n\n\n8\n0.83\n0.64\n1.22\n1.62\n2.26\n2.94\n\n\n9\n1.77\n2.00\n1.01\n0.12\n0.95\n1.53\n\n\n10\n2.99\n3.06\n2.40\n1.29\n0.57\n0.89\n\n\n2NN Mean\n0.65\n0.53\n1.11\n0.71\n0.76\n1.21\n\n\n\n\n\n\n\n\n\n\n\nFor the first majority-class observation, its closest minority-class samples are #7 and #8 (each column’s two nearest neighbors are highlighted with a background accent color). The average distance between its two neighbors is 0.65. To balance the data, NearMiss-1 would retain four samples: #1, #2, #4, and #5.\nThe algorithm for the second NearMiss variant focuses on using the K farthest neighbors, i.e., those with the largest distance between points. However, the instructions from (mani2003knn?) are somewhat ambiguous:\n\nThe second method (NearMiss-2) selects [majority] examples that are close to all [minority] examples. In this method, examples are selected based on their average distances to the three farthest [minority] examples.\n\nIt is unclear whether the largest or the smallest average distance is used to determine which majority class data to keep. We’ll use the minimum average distance here.\nThis table highlights the two furthest neighbors (i.e., “2FN”) and their average distances:\n\n\n\n\n\n\n\n\n\n\n\n\nMinority\n\nMajority Point Labels\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n7\n0.46\n0.41\n1.41\n2.33\n3.10\n3.73\n\n\n8\n0.83\n0.64\n1.22\n1.62\n2.26\n2.94\n\n\n9\n1.77\n2.00\n1.01\n0.12\n0.95\n1.53\n\n\n10\n2.99\n3.06\n2.40\n1.29\n0.57\n0.89\n\n\n2FN Mean\n2.38\n2.53\n1.90\n1.98\n2.68\n3.34\n\n\n\n\n\n\n\n\n\n\n\nNote that the neighbors are the opposite of those in the previous table. In this instance, we would retain samples #1, #2, #3, and #4.\nNearMiss-3 is a two-step process. First, the nearest K1 nearest neighbors are found for each minority sample. Any majority-class samples that were not on the list of neighbors are excluded. After this, we determine the K2 nearest neighbors of the majority class samples. Of these, the majority class observations with the largest average distance are retained.\nIn our example, we’ll use two neighbors for both calculations. In the first stage, the majority sample #3 is not a neighbor of any minority-class point, so it is discarded.\nThis table shows the reduced distance matrix and the average distances:\n\n\n\n\n\n\n\n\n\n\n\n\nMinority\n\nMajority Point Labels\n\n\n\n1\n2\n4\n5\n6\n\n\n\n\n7\n0.46\n0.41\n2.33\n3.10\n3.73\n\n\n8\n0.83\n0.64\n1.62\n2.26\n2.94\n\n\n9\n1.77\n2.00\n0.12\n0.95\n1.53\n\n\n10\n2.99\n3.06\n1.29\n0.57\n0.89\n\n\n2NN Mean\n0.65\n0.53\n0.71\n0.76\n1.21\n\n\n\n\n\n\n\n\n\n\n\nBased on these distances, NearMiss-3 selects #1, #4, #5, and #6 to include in the majority class.\nTomek Links (ivan1976two?) is a filtering method that is the opposite of NearMiss. In this case, we will remove data points that are close to some theoretical class boundary. The goal is to create a sort of gap between the predictor distributions for each class. For each class, we determine the (single) nearest neighbor from the opposite class. Two samples form a Tomek Link when they are each other’s nearest neighbors and belong to opposing classes. We would remove both data points that form such a link. (kubat1997addressing?) describe algorithms with similar goals.\nAn easy example from Figure 20.10 is the closeness of samples #4 and #9. They are extremely close to one another, yet in different classes. A counter-example is sample #3; its neighbor is #9, but the converse is not true since #9 and #4 are neighbors too. For this data set, only samples #1, #3, #6, and #8 are not Tomek Links and these are retained.\nWhen applied to the myopia data, the number of myopic samples is reduced from 60 to 39 and the number of non-myopic rows in the training set drops from 402 to 381. While the reduction in events might compromise the model’s effectiveness, Figure 20.8 shows that the FDA class boundaries produced with the entire data set and the reduced version are extremely similar.\nAnother potential downside to removing Tomek Links is increased instability. Using a single nearest-neighbor approach yields high variance, since small changes in the data can lead to substantially different solutions. That added variance in the training set might be passed along to the classifier.\nRemoving Tomek Links is a process that can be applied to data sets with balanced or unbalanced class frequencies; the goal is to reduce overlap in predictor distributions so the model can find a clearer trend in the data. It can also be used in conjunction with other sampling or synthesis methods, preferably afterwards.\n\n\n20.5.1.6 Boarderline Methods\n\n\n20.5.1.7 Comparisons Using the Myopia Data\nThe top panel of Figure 20.11 shows the model configuration statistics for different sampling and synthesis methods. The visualization emulates an ROC curve by plotting the sensitivity versus the false positive rate (i.e., one minus specificity) to demonstrate the trade-offs between the two types of errors. The downsampling panel shows a tight cluster of model results for both types of models, located in the region where sensitivity and specificity are approximately equal. Unsurprisingly, this is due to the equal frequencies of the two classes in the training data (post-sampling). However, this will not always be the case for other data sets. Near-miss sampling has a similar pattern with tighter clustering of model configuration results.\n\n\n\n\n\n\n\n\nFigure 20.11: Cross-validated statistics for the subsampling and synthesis methods.\n\n\n\n\n\nThe two synthetic methods showed slightly different patterns. ROSE and SMOTE with logistic regression yielded another tight cluster of results, with somewhat smaller statistics than those obtained through downsampling. For boosting, both SMOTE and ROSE demonstrated greater versatility in the tuning parameter results, enabling trade-offs between sensitivity and specificity. However, the pattern of the configurations favored specificity over sensitivity, especially for SMOTE.\nThe bottom panel of Figure 20.11 shows that, when sensitivity is improved by these tools, there can be a corresponding increase in the Brier scores. The boosting results show that a high sensitivity raises the Brier to values that would indicate a complete lack of calibration in the predicted probability estimates. This is deliberate; the goal of rebalancing is to have parity in the class frequencies. This enables the model to increase the likelihood of identifying events in the data. The consequence of doing so is that our class probability distributions are biased towards overestimating the probability of an event. However, when assessing the model, we use unbalanced data, and the increased probability of an event is not consistent with the true event rate (as seen outside of the balanced training set).\nIt is worth noting that we could also optimize the threshold to further fine-tune the sensitivity and specificity values. Unlike the analysis in the previous section, the custom thresholds are much less likely to be excessively small (or large). However, if this is required, we are probably better off using unsampled training sets or using cost-sensitive learning (shown in the next section).\nWe can see that these tools have an effect on the hard class prediction metrics, such as sensitivity and specificity. Although Figure 20.11 indicates that the calibration properties are not good, meaning that probability estimates do not reflect the rate at which events occur. However, it is possible that the probabilities can help discriminate between the classes for some threshold. For the four sampling methods used on these data, their ROC AUC values for the best logistic regression models ranged from 0.823 to 0.862 The range for the boosted tree models was 0.84 to 0.861. Are these any better or worse than the models that were developed in Section 20.4? Using the bootstrap techniques in Section 14.2, we linked the predictions by row for all of these models, created 2,000 bootstrap samples, and then computed the difference in the area under the ROC curves. Figure 20.12 shows the results. For these data, there doesn’t appear to be any improvement in the ROC AUCs when subsampling and synthesis tools are used. Note that the x-axis scale is fairly small; these AUC values are very similar to one another. One comparison did not cover zero, indicating that using logistic regression with SMOTE had a slightly worse AUC than simply using the training set as is.\n\n\n\n\n\n\n\n\nFigure 20.12: Bootstrap 90% confidence intervals to assess whether subsampling and synthesis methods improve the area under the ROC curve. Negative values indicate that the AUC values were higher for models developed in Section 20.4.\n\n\n\n\n\nThese results highlights the idea that rebalancing tools can help a model do better at finding events and that the user should only focus on hard class predictions (and perhaps avoid exposing the users to the probability estimates).\n\n\n\n20.5.2 Cost-Sensitive Learning\nWe’ve previously talked about the consequences of making an incorrect prediction. For example, Section 15.3 showed a model that predicts whether mushrooms are poisonous or not. We could think about this in terms of the general cost of a prediction (not too dissimilar to the discussion related to SVM models).\nIf we have \\(C\\) classes, we could write down a cost matrix that defines the specific costs \\(\\mathcal{c}_{j|k}\\) of incorrectly predicting a data point to be class \\(j\\) when it is truly class \\(k\\). In many instances, the diagonal terms of this matrix are zero since there is no real “cost” to making the correct prediction. There are cases where different types of benefits are associated with different types of correct predictions. See REF for an example. However, for the purposes of our discussion here, we will assume that the diagonal is zero.\nIf we have this cost structure, we can use the predicted class probabilities to estimate the expected costs for a collection of data by averaging a set of individual costs.\nLet’s say that \\(C=3\\) and the cost matrix is\n\\[\n\\mathcal{C} =\n\\begin{bmatrix}\n0.00 & 1.00 & 3.00 \\\\\n0.50 & 0.00 & 2.00 \\\\\n0.25 & 0.75 & 0.00\n\\end{bmatrix}\n\\]\nwhere the true values are in columns, and the predicted class levels are rows. For example, the cost of predicting a data point that is Class 3 as Class 1 is \\(\\mathcal{c}_{1|3} = 3.00\\). Here are some examples of probabilities and their individual costs:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTruth\n\nProbability Estimates\n\nCost\n\n\nA\nB\nC\n\n\n\n\nA\n0.33\n0.33\n0.33\n0.25\n\n\nB\n0.33\n0.33\n0.33\n0.58\n\n\nC\n0.33\n0.33\n0.33\n1.67\n\n\nA\n0.10\n0.50\n0.40\n0.35\n\n\nC\n0.40\n0.50\n0.10\n2.20\n\n\n\n\n\n\n\nIf our ML model can utilize custom objective functions, minimizing the expected cost is a fairly rational way to ensure your model is fit for purpose. If not, we can use the expected cost to help optimize the tuning parameters of our model (as we did in Figure 20.6).\nAnother way some researchers have parameterized cost is to collapse the matrix, resulting in a general cost for misclassifying a sample, regardless of what it was misclassified as. In this case, we sum the cost matrix over columns, resulting in one cost per class. XXX describes these as class weights. In our example matrix shown above, the class weights are \\(\\mathcal{C}_1 = 0.75\\), \\(\\mathcal{C}_2 = 1.75\\), and \\(\\mathcal{C}_3 = 5.00\\). For a two-class outcome (e.g., myopia or not), the class weights are the same as the two off-diagonal entries in the cost matrix.\nIf we consider class-specific costs as weights, we can generalize many performance metrics to be cost-sensitive. For example, we can make the Brier score cost-sensitive using a modified version of Equation 15.25\n\\[\nBrier = \\frac{1}{WC}\\sum_{i=1}^n\\sum_{k=1}^C w_k(y_{ik} - \\hat{p}_{ik})^2\n\\tag{20.1}\\]\nwhere \\(w_k\\) is the cost or weight for class \\(k\\) and \\(W\\) is the sum of the \\(n\\) weights. If we have two classes and \\(w_1 = 5\\) and \\(w_2 = 1\\), every data point whose true outcome is the first class will have five-fold more influence over the objective function. In this way, we can train the model to ensure that it performs better in predicting specific classes than others. This is the basis for cost-sensitive learning6.\nIn practice, our implementations of ML model metrics may or may not enable cost-sensitive learning. If not, they may have the capacity to handle case weights. These are numeric values associated with each row of the data set that specify how it should affect the computations. There are many types of case weights for different purposes. Here are a few examples:\n\nFrequency weights are integers that specify how often the specific predictor pattern occurs in the data. If there were two categorical predictions with a total of 25 combinations, a very large data set can be compressed by using a case weight that reflects how often each of the 25 patterns occurs. Frequency weights would be assigned to both the training and testing sets.\nImportance weights are numbers (decimal and non-negative) that reflect how important the row is to the model fit. This is the type of weight that we’ve discussed above. Importance weights should only be assigned to the training set; the test set should not use them, as it should accurately reflect the state of the data in the real world.\n\nIf the metric implementation allows for case weights, we can use importance weights to make them sensitive to different classes.\n\nWhile not related to class imbalances, importance weights are also useful for time-series data, where we would like more recent data to have a higher priority in the model fit. It can also be useful for dealing with problematic/anomalous data in a model. For example, data collected during the global COVID-19 pandemic can have a negative impact on fits, but it may still need to be included. Assigning these row small case weights may solve those issues.\n\nNote that the scale of class weights may differ across metrics and/or models (as we’ll see below). For one type of model, minority class weights ranging from 1 to 50 might have the same performance as another model/metric that ranges from 1 to 5. There is some trial and error associated with tuning the weights.\nIf your problem has a well-known cost structure, likely expressed in units of currency, there is probably little ambiguity about it. Otherwise, we might use class weights to either decrease the likelihood of tragic errors from occurring in the model or to help the model overcome a severe class imbalance. In other words, the “importance” in importance weights could mean many different things.\nCART book, resampling weights versus case weights\nTo visualize an example, Figure 20.13 shows the same data as Figure 20.3, but in this case, case weights were used so that the myopic data (in red) had a 15-fold larger impact on the objective function than the non-myopic children. The same FDA model was trained on the data, but it used the case weights. The result is a class boundary that bends in the middle, allowing it to encompass a greater portion of the minority class than the original fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.13: A flexible discriminant fit to the myopia training set using cost-sensitive learning. The myopic subjects had a 15-fold higher cost in the objective function and 43 of the 60 events (in red) are identified using the default 50% threshold. This figure is comparable with Figure 20.3 and ?fig-myopia-down-fda.\n\n\n\nCase weights based on the class can also approximate some of the subsampling methods shown in the previous section. For example, instead of dropping training set rows from the data, we can adjust the importance weights so that the sum of the weights in the majority class equals the sum of the weights in the minority class. By doing this, we may be able to obtain a better model since the majority of the majority class is not lost. Similarly, an approximation to upsampling would only increase the minority class weights so that the class weight totals are in equilibrium. This would also result in faster training times than basic oversampling.\nIt is worth noting that some stochastic ensemble implementations (e.g., boosting and random forests) often confuse the terms “case weights” and “sampling weights.” In these instances, the weights are used to preferentially sample the training set while fitting the models (usually trees) without directly impacting the objective function. This process is more akin to downsampling and upsampling than to cost-sensitive learning.\nTo demonstrate with the full myopia data set, boosted trees and regularized logistic regression were used once again. The lightgbm implementation was used and, while their documentation is unclear about how the weights are used, it appears that the weights are applied to the objective function and not just the sampling mechanism7.\nThe same grids were used as in the previous section for both models. For the case weights ranges for each model, some investigation was required to find appropriate changes in the performance metrics. While both parameter ranges were explored in log2 units, starting at 1.0, the boosted model used a maximum weight of 210, whereas logistic regression used maximum weights of 24. The majority class weights were kept at a value of 1.0 for both models. For each model, a sequence of weights was created in a 1D grid, and these were crossed with the original grids.\nFigure 20.14 shows the results where the ranges of the weights have been standardized across the two models to be within [0, 1]. In the top panel, another ROC-like visualization is used to demonstrate the effect of using higher weights for the myopic samples. We can see that both models tend to increase sensitivity as the weights increase8. This is not always the case, as some tuning parameters for the model were suboptimal on their own, and adjusting the class weights did not yield a significant change. For the most part, the increase in sensitivity comes with the price of reduced specificity. The different class weights enable us to select the optimal trade-off between the two.\nAs with subsampling and synthesis methods, the class probabilities generated by this model become increasingly inconsistent with reality as the cost of the minority class increases. The bottom panel shows the degradation in the Brier score tracks with increased sensitivity. Once again, we caution against using these probability estimates with users, as they may not accurately emulate the true likelihood of the event.\n\n\n\n\n\n\n\n\nFigure 20.14: Cross-validated statistics for the cost-sensitive models.\n\n\n\n\n\nWhen comparing subsampling/synthetic methods to cost-sensitive learning, the latter appears to be more versatile than the former. It enables the user to have more direct control over the trade-off between false-negative and false-positive rates. The downside to cost-sensitive learning is that it may be dependent on the implementation of the method, whereas subsampling/synthetic tools can be used anywhere, as they are specialized preprocessors.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Class Imbalances</span>"
    ]
  },
  {
    "objectID": "chapters/cls-imbalance.html#sec-prevalence-again",
    "href": "chapters/cls-imbalance.html#sec-prevalence-again",
    "title": "20  Class Imbalances",
    "section": "20.6 Revenge of the Prevalence",
    "text": "20.6 Revenge of the Prevalence\nIt is worth revisiting the discussion from Section 15.5 on how the event rate (i.e., prevalence) affects important statistics. We’ll continue to use the myopia data for discussion.\nIf we assume that the rate of myopia in the population of interest is consistent with our data set (13.1%), we can gain a deeper understanding of how well the model performs. Given our estimates of sensitivity and specificity, the unconditional performance statistics are:\n\npositive predictive value (PPV): 38.2%\nnegative predictive value (NPV): 95.7%\n\nEven though our sensitivity and specificity ended up being fairly balanced, the statistics that convey to true ability to predict are not.\nThis could exacerbate the potential confusion that can occur when explaining the results of a prediction. Continuing the hypothetical conversation from Section 20.4, the explanation of the PPV might be:\n\n“We understand that this rule of thumb might seem overly stringent. That said, from our data, we know that when we predict that a child is myopic, the probability that they are actually myopic is roughly 38%. This is because most children are not myopic but we do find some evidence to make the diagnosis.”\n\nConversely, if the child had a very small predicted probability and a myopia diagnosis was not used, the explanation would be:\n\n“Even though the chances that your child is myopic are greater than zero, from our data we know that when we predict that a child is not myopic, the probability that they are actually not myopic is overwhelmingly high (roughly 96%). This is because most children are not myopic and there is not enough evidence to disprove this for your child.”\n\nThis is much less ambiguous than the previous hypothetical conversation.\nWe can try to optimize the model for PPV and NPV in the same way as we did for sensitivity and specificity. However, to do this, the prevalence must be well known for the specific population that will be predicted by the model and should be stable over time.\nFor example, (covidcdc?) calculated COVID-19 cases and deaths, and estimated cumulative incidence for March and April 2020. The mortality rate varied geographically. For example, during that period, New York City had an estimated mortality rate of 5.3%, while the remainder of the state of New York had an estimated rate of 2.2%. Thankfully, in 2025, these rates are much lower.\nThis illustrates that computations that depend on the event rate can be challenging, as the values can systematically change under different conditions and over time.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Class Imbalances</span>"
    ]
  },
  {
    "objectID": "chapters/cls-imbalance.html#chapter-references",
    "href": "chapters/cls-imbalance.html#chapter-references",
    "title": "20  Class Imbalances",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nHosmer, D, S Lemeshow, and R Sturdivant. 2013. Applied Logistic Regression. John Wiley & Sons.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Class Imbalances</span>"
    ]
  },
  {
    "objectID": "chapters/cls-imbalance.html#footnotes",
    "href": "chapters/cls-imbalance.html#footnotes",
    "title": "20  Class Imbalances",
    "section": "",
    "text": "We use the original implementation of the glmnet model, which features a capability where, for a specific mixture value, a single model fit contains results for multiple penalty values. For this reason, we can evaluate a regular grid with 72 candidates with only 4 model fits, one for each mixture value. A regular grid exploits this feature, but a space-filling design would nullify this advantage.↩︎\nOr a validation set if multiple resamples are not used.↩︎\nWhen adding the threshold to the optimization grid, it might be helpful to start with a space-filling design and crossing it with a vector of thresholds. This allows us to have data on the thresholds for each of the candidate points. There is minimal overhead associated with this approach, as the thresholding computations occur after preprocessing and model fitting. For example, if an initial grid with 25 points is crossed with 10 threshold values, there are still only 25 model configurations to train, which takes the longest time to compute.↩︎\nAnd validation set, if any↩︎\nIf there are more than two classes, you can substitute “non-majority” class here.↩︎\nNote that our previous use of costs in Figure 20.6 was after the model was trained. In this section, we will use weights/costs so that the model adapts to the cost structure during training.↩︎\nA GitHub issue (https://github.com/microsoft/LightGBM/issues/1299) has evidence that the weights are a “multiplication applied to every positive label weight” and shows some C++ code to that effect.↩︎\nAs with the previous section, these statistics use the default 50% threshold.↩︎",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Class Imbalances</span>"
    ]
  },
  {
    "objectID": "chapters/news.html#section-12",
    "href": "chapters/news.html#section-12",
    "title": "News",
    "section": "2023-10-09",
    "text": "2023-10-09\nFirst committed versions.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section",
    "href": "chapters/news.html#section",
    "title": "News",
    "section": "",
    "text": "Logistic Regression\nNonlinear Discriminants\nK-Nearest Neighbors\nNeural Networks via Multilayer Perceptrons",
    "crumbs": [
      "News"
    ]
  }
]