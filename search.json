[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "",
    "text": "Preface\nWelcome! This is a work in progress. We want to create a practical guide to developing quality predictive models from tabular data. We’ll publish materials here as we create them and welcome community contributions in the form of discussions, suggestions, and edits.\nWe also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).\nOur intention is to write these materials and, when we feel we’re done, pick a publishing partner to produce a print version.\nThe book takes a holistic view of the predictive modeling process and focuses on a few areas that are usually left out of similar works. For example, the effectiveness of the model can be driven by how the predictors are represented. Because of this, we tightly couple feature engineering methods with machine learning models. Also, quite a lot of work happens after we have determined our best model and created the final fit. These post-modeling activities are an important part of the model development process and will be described in detail.\nWe deliberately avoid using the term “artificial intelligence.” Eugen Rochko’s (@Gargron@mastodon.social) comment on Mastodon does a good job of summarizing our reservations regarding the term:\nTo cite this website, we suggest:\n@online{aml4td,\n  author = {Kuhn, M and Johnson, K},\n  title = {{Applied Machine Learning for Tabular Data}},\n  year = {2023},\n  url = { https://aml4td.org},\n  urldate = {2024-01-02}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "License",
    "text": "License\n\nThis work is licensed under CC BY-NC-SA 4.0\n\nThis license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, for noncommercial purposes only. If others modify or adapt the material, they must license the modified material under identical terms.\n\nBY: Credit must be given to you, the creator.\nNC: Only noncommercial use of your work is permitted. Noncommercial means not primarily intended for or directed towards commercial advantage or monetary compensation.\nSA: Adaptations must be shared under the same terms.\n\nOur goal is to have an open book where people can reuse and reference the materials but can’t just put their names on them and resell them (without our permission).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Intended Audience",
    "text": "Intended Audience\nOur intended audience includes data analysts of many types: statisticians, data scientists, professors and instructors of machine learning courses, laboratory scientists, and anyone else who desires to understand how to create a model for prediction. We don’t expect readers to be experts in these methods or the math behind them. Instead, our approach throughout this work is applied. That is, we want readers to use this material to build intuition about the predictive modeling process. What are good and bad ideas for the modeling process? What pitfalls should we look out for? How can we be confident that the model will be predictive for new samples? What are advantages and disadvantages of different types of models? These are just some of the questions that this work will address.\nSome background in modeling and statistics will be extremely useful. Having seen or used basic regression models is good, and an understanding of basic statistical concepts such as variance, correlation, populations, samples, etc., is needed. There will also be some mathematical notation, so you’ll need to be able to grasp these abstractions. But we will keep this to those parts where it is absolutely necessary. There are a few more statistically sophisticated sections for some of the more advanced topics.\nIf you would like a more theoretical treatment of machine learning models, then we recommend Hastie, Tibshirani, and Friedman (2017). Other books for gaining a more in-depth understanding of machine learning are Bishop and Nasrabadi (2006), Arnold, Kane, and Lewis (2019) and, for more of a deep learning focus, Goodfellow, Bengio, and Courville (2016) and/or Prince (2023).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#is-there-code",
    "href": "index.html#is-there-code",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Is there code?",
    "text": "Is there code?\nWe definitely want to decouple the content of this work from specific software. One of our other books on modeling had computing sections. Many people found these sections to be a useful resource at the time of the book’s publication. However, code can quickly become outdated in today’s computational environment. In addition, this information takes up a lot of page space that would be better used for other topics.\nWe will create computing supplements to go along with the materials. Since we use R’s tidymodels framework for calculations, the supplement currently in-progress is:\n\ntidymodels.aml4td.org\n\nIf you are interested in working on a python/scikit-learn supplement, please file an issue",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#are-there-exercises",
    "href": "index.html#are-there-exercises",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Are there exercises?",
    "text": "Are there exercises?\nMany readers found the Exercise sections of Applied Predictive Modeling to be helpful for solidifying the concepts presented in each chapter. The current set can be found at exercises.aml4td.org",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-can-i-ask-questions",
    "href": "index.html#how-can-i-ask-questions",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "How can I ask questions?",
    "text": "How can I ask questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like cross-validated. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what I call Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.\nIf you think there is a bug, please file an issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Can I contribute?",
    "text": "Can I contribute?\nThere is a contributing page with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list. It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto was used to compile and render the materials\n\nQuarto 1.4.533\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.11: OK\n      Dart Sass version 1.69.5: OK\n      Deno version 1.37.2: OK\n[✓] Checking versions of quarto dependencies......OK\n[✓] Checking Quarto installation......OK\n      Version: 1.4.533\n[✓] Checking tools....................OK\n      TinyTeX: (external install)\n      Chromium: (not installed)\n[✓] Checking LaTeX....................OK\n      Using: TinyTex\n      Version: 2022\n[✓] Checking basic markdown render....OK\n[✓] Checking Python 3 installation....OK\n      Version: 3.8.13 (Conda)\n      Jupyter: (None)\n      Jupyter is not available in this Python installation.\n[✓] Checking R installation...........OK\n      Version: 4.3.2\n      LibPaths:\n      knitr: 1.45\n      rmarkdown: 2.25\n[✓] Checking Knitr engine render......OK\n\nR version 4.3.2 (2023-10-31) was used for the majority of the computations. torch 1.13.1 was also used. The versions of the primary R modeling and visualization packages used here are:\n\n\n\n\n\n\n\n\n\napplicable (0.1.0)\nbaguette (1.0.1)\nbestNormalize (1.9.1)\n\n\nbonsai (0.2.1)\nbroom (1.0.5)\nbrulee (0.2.0)\n\n\nC50 (0.1.8)\nCubist (0.4.2.1)\nDALEXtra (2.3.0)\n\n\ndbarts (0.9-23)\ndesirability2 (0.0.1)\ndials (1.2.0)\n\n\ndimRed (0.2.6)\ndiscrim (1.0.1)\ndoMC (1.3.8)\n\n\ndplyr (1.1.4)\ne1071 (1.7-13)\nearth (5.3.2)\n\n\nembed (1.1.3)\nfinetune (1.1.0)\nGA (3.2.3)\n\n\ngganimate (1.0.8)\nggiraph (0.8.7)\nggplot2 (3.4.4)\n\n\nglmnet (4.1-8)\ngt (0.10.0)\nhardhat (1.3.0)\n\n\nipred (0.9-14)\nirlba (2.3.5.1)\nkernlab (0.9-32)\n\n\nkknn (1.3.1)\nklaR (1.7-2)\nlightgbm (3.3.5)\n\n\nmda (0.5-4)\nmgcv (1.9-0)\nmixOmics (6.25.1)\n\n\nmodeldata (1.2.0)\nmodeldatatoo (0.2.1)\npamr (1.56.1)\n\n\nparsnip (1.1.1)\npartykit (1.2-20)\npatchwork (1.1.3)\n\n\nplsmod (1.0.0)\nprobably (1.0.2)\npROC (1.18.5)\n\n\npurrr (1.0.2)\nragg (1.2.6)\nranger (0.16.0)\n\n\nrecipes (1.0.8)\nrpart (4.1.21)\nrsample (1.2.0)\n\n\nrstudioapi (0.15.0)\nrules (1.0.2)\nsparsediscrim (0.3.0)\n\n\nsparseLDA (0.1-9)\nspatialsample (0.5.1)\nsplines2 (0.5.1)\n\n\nstacks (1.0.3)\nstopwords (2.3)\ntextrecipes (1.0.6.9000)\n\n\nthemis (1.0.2)\ntidymodels (1.1.1)\ntidyposterior (1.0.1)\n\n\ntidyr (1.3.0)\ntorch (0.11.0)\ntune (1.1.2)\n\n\nusethis (2.2.2)\nVBsparsePCA (0.1.0)\nworkflows (1.1.3)\n\n\nworkflowsets (1.0.1)\nxgboost (1.7.5.1)\nxrf (0.2.2)\n\n\nyardstick (1.2.0)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#chapter-references",
    "href": "index.html#chapter-references",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nArnold, T, M Kane, and B Lewis. 2019. A Computational Approach to Statistical Learning. Chapman; Hall/CRC.\n\n\nBishop, C M, and N M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nHastie, T, R Tibshirani, and J Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/news.html#errata",
    "href": "chapters/news.html#errata",
    "title": "News",
    "section": "Errata",
    "text": "Errata\nNo reported edits (yet).",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#changelog",
    "href": "chapters/news.html#changelog",
    "title": "News",
    "section": "Changelog",
    "text": "Changelog\n\n2024-01-02\nFixed various typos.\n\n\n2023-12-21\nTypo fix on main page.\n\n\n2023-12-19\nSmall updates.\nUpdated snapshot.\nThe background color of premade outputs has been changed to the pages background color (#16).\n\n\n2023-12-11\nNo new content.\nUpdated snapshot.\nIncludes new Google Scholar links for articles (#8)\n\n\n2023-11-17\nUpdated renv snapshot.\n\n\n2023-10-09\nFirst committed versions.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/contributing.html#software",
    "href": "chapters/contributing.html#software",
    "title": "Contributing",
    "section": "Software",
    "text": "Software\nRegarding R packages, the repository has a DESCRIPTION file as if it were an R package. This lets us specify precisely what packages and their versions should be installed. The packages listed in the imports field contain packages for modeling/analysis and packages used to make the website/book. Some basic system requirements are likely needed to install packages: Fortran, gdal, and others.\nThe main requirements are as follows.\n\nQuarto\nQuarto is an open-source scientific and technical publishing system. Quarto version 1.4.533 is used to compile the website.\nWe also use a few Quarto extensions. These should be installed from the project’s root directory via:\nquarto add quarto-ext/fontawesome\nquarto add quarto-ext/fancy-text\nquarto add leovan/quarto-pseudocode\n\n\nR and renv\nR version 4.3.2 (2023-10-31) is what we are currently using. We suggest using rig to manage R versions. There are several IDEs that you can use. We’ve used RStudio (&gt;= 2023.6.1.524).\nThe current strategy is to use the renv (&gt;= version 1.0.3) package to make this project more isolated, portable and reproducible.\nTo get package dependencies installed…\n\nWhen you open the website.Rproj file, the renv package should be automatically installed/updated (if neded). For example:\n# Bootstrapping renv 1.0.3 ---------------------------------------------------\n- Downloading renv ... OK\n- Installing renv  ... OK\n\nThe following package(s) will be installed:\n- BiocManager [1.30.22]\nThese packages will be installed into \"~/content/aml4td/renv/library/R-4.3/x86_64-apple-darwin20\".\nif you try to compile the book, you probably get and error:\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nYou can get more information using renv::status() but you can get them installed by first running renv::activate(). As an example:\n&gt; renv::activate()\n\nRestarting R session...\n\n- Project '~/content/aml4td' loaded. [renv 1.0.3]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nSince we have package versions recorded in the lockfile, we can installed them using renv::restore(). Here is an example of that output:\n&gt; renv::restore() \nThe following package(s) will be updated:\n\n# Bioconductor ---------------------------------------------------------------\n- mixOmics         [* -&gt; mixOmicsTeam/mixOmics]\n\n# CRAN -----------------------------------------------------------------------\n- BiocManager      [1.30.22 -&gt; 1.30.21.1]\n- lattice          [0.21-9 -&gt; 0.21-8]\n- Matrix           [1.6-1.1 -&gt; 1.6-0]\n- nlme             [3.1-163 -&gt; 3.1-162]\n- rpart            [4.1.21 -&gt; 4.1.19]\n- survival         [3.5-7 -&gt; 3.5-5]\n- abind            [* -&gt; 1.4-5]\n\n&lt;snip&gt;\n\n- zip              [* -&gt; 2.2.0]\n- zoo              [* -&gt; 1.8-12]\n\n# GitHub ---------------------------------------------------------------------\n- BiocParallel     [* -&gt; Bioconductor/BiocParallel@devel]\n- BiocVersion      [* -&gt; Bioconductor/BiocVersion@devel]\n- modeldatatoo     [* -&gt; tidymodels/modeldatatoo@HEAD]\n- parsnip          [* -&gt; tidymodels/parsnip@HEAD]\n- usethis          [* -&gt; r-lib/usethis@HEAD]\n\n# RSPM -----------------------------------------------------------------------\n- bslib            [* -&gt; 0.5.1]\n- fansi            [* -&gt; 1.0.5]\n- fontawesome      [* -&gt; 0.5.2]\n- ggplot2          [* -&gt; 3.4.4]\n- htmltools        [* -&gt; 0.5.6.1]\n- withr            [* -&gt; 2.5.1]\n\nDo you want to proceed? [Y/n]: y\n\n# Downloading packages -------------------------------------------------------\n- Downloading BiocManager from CRAN ...         OK [569 Kb in 0.19s]\n- Downloading nlme from CRAN ...                OK [828.7 Kb in 0.19s]\n- Downloading BH from CRAN ...                  OK [12.7 Mb in 0.4s]\n- Downloading BiocVersion from GitHub ...       OK [826 bytes in 0.37s]\n\n&lt;snip&gt;\n\nDepending on whether you have to install packages from source, you may need to install some system dependencies and try again (I had to install libgit2 the last time I did this).\nOnce you have everything installed, we recommend installing the underlying torch computational libraries. You can do this by loading the torch package A download will automatically begin if you need one.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#contributor-list",
    "href": "chapters/contributing.html#contributor-list",
    "title": "Contributing",
    "section": "Contributor List",
    "text": "Contributor List\nThe would like to thank users who have made a contribution to the project: @bmreiniger, @coatless, @syclik, and @tomsing1.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular",
    "href": "chapters/introduction.html#sec-nontabular",
    "title": "1  Introduction",
    "section": "1.1 Non-Tabular Data",
    "text": "1.1 Non-Tabular Data\nThe columns shown in Table 1.1 are defined within a small scope. For example, the delivery hour column encapsulates everything about that order characteristic:\n\nYou don’t need any other columns to define it.\nA single number is all that is required to describe that information.\n\nNon-tabular data does not have the same properties. Consider images, such as the one shown Figure 1.1 that depicts four cells from a biological experiment (Yu et al. 2007). These cells that have been stained with pigments that fluoresce at different wavelengths to “paint” different parts of the cells. In this image, blue reflects the part of the cell called the cytoskeleton, which encompasses the entire cell body. The red color corresponds to a stain that only attaches itself to the contents of the cell nucleus, specifically DNA. This image3 is 371 pixels by 341 pixels with quantification of two colors.\n\n\n\n\n\n\n\n\nFigure 1.1: An example image of several cells using two colors. Image from Yu et al. (2007).\n\n\n\n\n\nFrom a data perspective, this image is represented as a three-dimensional array (371 rows, 341 columns, and 2 colors). Probably the most important attribute of this data is the spatial relationships between pixels. It is critical to know where each value in the 3D array resides, but knowing what the nearby values are is at least as important.\nWe could “flatten” the array into 371 \\(\\times\\) 341 \\(\\times\\) 2 = 253,022 columns to use in a table4. However, this would break the spatial link between the data points; each column is no longer as self-contained as the columns from the delivery data. The result is that we have to consider the 3D array as the data instead of each row/column/color combination.\nAdditionally, images often have different dimensions which results in different array sizes. From a tabular data perspective, a flattened version of such data would have a different number of columns for different-sized images.\nVideos are an extension of image data if we were to consider it a four-dimensional array (with time as the additional dimension). The temporal aspects of such data are critical to understanding and predicting values.\nText data is often thought of as non-tabular data. As an example, consider text from Sanderson (2017):\n\n“The most important step a man can take. It’s not the first one, is it? It’s the next one. Always the next step, Dalinar.”\n\nWith text data, it is common to define the “token”: the unit of text that should be analyzed. This could be a paragraph, sentence, word, etc. Suppose that we used sentences as tokens. In this case, the table for this quote would have four rows. Where do we go from here? The sequence of words (or characters) is likely to be important, and this makes a case for keeping the sentences as strings. However, as will be seen shortly, we might be able to convert these four tokens to numeric columns in a way that preserves the information required for prediction.\nPerhaps a better text example relates to the structure of chemicals. A chemical is a three-dimensional molecule but is often described using a SMILES string (Engel and Gasteiger 2018, chap. 3). This is a textual description of molecule that lists its elements and how they relate to one another. For example, the SMILES string CC(C)CC1=CC=C(C=C1)C(C)C(=O)O defines the anti-inflammatory drug Ibuprofen. The letters describe the elements (C is carbon, O is oxygen, etc) and the other characters defines the bonds between the elements. The character sequence within this string is critical to understanding the data.\nOur motives for categorizing data as tabular and non-tabular are related to how we choose an appropriate model. Our opinion is that most modeling projects involve tabular data (or data that can be effectively represented as tabular). Models for non-tabular data are very specialized and, while these ML approaches are the most discussed in the social media, they tend not to be the best approach for tabular data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular-convert",
    "href": "chapters/introduction.html#sec-nontabular-convert",
    "title": "1  Introduction",
    "section": "1.2 Converting Non-Tabular Data to Tabular",
    "text": "1.2 Converting Non-Tabular Data to Tabular\nIn some situations, non-tabular data can be effectively converted to a tabular format, depending on the specific problem and data.\nFor some modeling projects using images, we might not be interested in every pixel. For Figure 1.1, we really care about the cells in the image. It is important to understand within-cell characteristics (e.g. shape or size) and some between-cell information (e.g., the distance between cells). The vast majority of the pixels don’t need to be analyzed.\nWe can pre-analyze the 3D arrays to determine which pixels in an image correspond to specific cells (or nuclei within a cell). Segmentation (Holmes and Huber 2018) is the process of estimating regions of an image that define some object(s). Figure 1.2 shows the same cells with lines generated from a simple segmentation.\n\n\n\n\n\n\n\n\nFigure 1.2: The cell segmentation results for the four neuroblastomas shown in Figure 1.1.\n\n\n\n\n\nOnce our cells have been segmented, we can compute various statistics of interest based on size, shape, or color. If a cell is our unit of data, we can create a tabular format where rows are cells and columns are cell characteristics (Table 1.2). The result is a data table that has more rows than the original non-tabular structure since there are multiple cells in an image. There are far fewer columns but these columns are designed to be more informative than the raw pixel data since the researchers define the cell characteristics that they desire.\n\n\n\n\n\n\n\n\n\n\n\nID\nEccentricity\n\nArea\n\nIntensity\n\n\nNucleus\nCell\nNucleus\nCell\nNucleus\nCell\n\n\n\n\n17\n0.494\n0.836\n\n3,352\n11,699\n\n0.274\n0.155\n\n\n18\n0.708\n0.550\n\n1,777\n4,980\n\n0.278\n0.210\n\n\n21\n0.495\n0.802\n\n1,274\n3,081\n\n0.326\n0.218\n\n\n22\n0.809\n0.975\n\n1,169\n3,933\n\n0.583\n0.229\n\n\n\n\n\n\n\n\nTable 1.2: Cells, such as those found in Figure 1.1, translated into a tabular format using three features for the nuclear and non-nuclear regions of the segmented cells.\n\n\n\n\nThis conversion to tabular data isn’t appropriate for all image analysis problems. If you want to know if an image contains a cat or not it probably isn’t a good idea.\nLet’s also consider the text examples. For the book quote, Table 1.3 shows a few simple predictors that are tabular in nature:\n\nPresence/absence columns for each word in the entire document. Table 1.3 shows columns corresponding to the words “always”, “can”, and “take”. These are represented as counts.\nSentence summaries, such as the number of commas, words, characters, first-person speech, etc.\n\nThere are many more ways to represent these data. More complex numeric embeddings that are built on separate large databases can reflect different parts of speech, text sequences (called n-grams), and others. Hvitfeldt and Silge (2021) and Boykis (2023) describe these and other tools for analyzing text data (in either format).\n\n\n\n\n\n\n\n\n\n\n\nSentence\n\"always\"\n\"can\"\n...\n\"take\"\nCharacters\nCommas\n\n\n\n\nThe most important step a man can take.\n0\n1\n...\n1\n32\n0\n\n\nIt's not the first one, is it?\n0\n0\n...\n0\n24\n1\n\n\nIt's the next one.\n0\n0\n...\n0\n15\n0\n\n\nAlways the next step, Dalinar.\n1\n0\n...\n0\n26\n1\n\n\n\n\n\n\n\n\nTable 1.3: An example of converting non-numeric data to a numeric, tabular format.\n\n\n\n\nFor the chemical structure of Ibuprofen, there is a rich field of molecular descriptors (Leach and Gillet 2003, chap. 3; Engel and Gasteiger 2018) that can be used to produce thousands of informative predictor columns. For example, we might be interested in the size of the molecule (perhaps measured by surface area), its electrical charge, or whether it contains specific sub-structures.\nThe process of determining the best data representations is called feature engineering. This is a critical task in machine learning and is often overlooked. Part 2 of this book spends some time looking at feature engineering methods (once the data are in a tabular format).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-types",
    "href": "chapters/introduction.html#sec-model-types",
    "title": "1  Introduction",
    "section": "1.3 Models and Machine Learning",
    "text": "1.3 Models and Machine Learning\nSince we have mentioned linear regression and regression trees, it makes sense to talk about possible machine learning scenarios. There are many, but we will focus on two types.\nRegression models have a numeric outcome (e.g., delivery time) and our goal is to predict that value. For some data sets we are mostly interested in ranking new data (as opposed to accurately predicting the value).\nThe other scenario that we discuss is classification models. In this case, the outcome is a qualitative categorical value (e.g., “cat” or “no cat” in an image). One interesting aspect of these models is that there are two main types of predictions:\n\nhard predictions correspond to the original outcome value.\nsoft predictions return the probability of each possible outcome (e.g., there is a 27% chance that a cat is in the image).\n\nThe number of categories is important; some models are specific to two classes, while others can accommodate any number of outcome categories. Most classification outcomes have mutually exclusive classes (one of all possible class values). There are also ordered class values where, while categorical, there is some type of ordering. For example, tumors are often classified in stages, with stage I being a localized malignancy and stage IV corresponding to one that has spread throughout the body.\nMulti-label outcomes can have multiple classes per row. For example, if we were trying to predict which languages that someone can speak, multilingual people would have multiple outcome values.\nWe are focused on prediction problems. This implies that our data will contain an outcome. Such scenarios falls under the broader class of supervised models. An unsupervised model is one where the is no true outcome value. Tools such as clustering, principal component analysis (PCA), and others look to quantify patterns in the data without some prediction goal. We’ll encounter unsupervised methods in our pre-model activities such as feature engineering described in Part 2.\nSeveral synonyms for machine learning include statistical learning, predictive modeling, pattern recognition, and others. While many specific models are designed for machine learning, we suggest that users think about machine learning as the problem and not the solution. Any model that can adequately predict the outcome values deserves the title. A commonly used argument for this position is that two models that are conventionally thought of as tools to make inferences (e.g., p-values) are linear regression and logistic regression. Equation 1.1 is a simplistic linear regression model. As will be seen in future chapters, linear and logistic regression models can produce highly accurate predictions for complex problems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-data-types",
    "href": "chapters/introduction.html#sec-data-types",
    "title": "1  Introduction",
    "section": "1.4 Data Characteristics",
    "text": "1.4 Data Characteristics\nIt’s helpful to discuss different nomenclature and types of data. With tabular data, there are a number of ways to refer to a row. The terms sample, instance, observation, and/or data point are commonly used. The first, “sample”, can be used in a few different contexts; we will also use it to describe a subset of a greater data collection (as in “we took a random sample of the data”.). The notation for the number of rows in a data set is \\(n\\) with subscripts for specificity (e.g., \\(n_{tr}\\)).\nFor predictors, there are also many synonyms. In statistical nomenclature, independent variable or covariate are used. More generic terms are attribute and descriptor. The term feature is generally equated to predictors, and we’ll use both. An additional adjective may also be assigned to “feature”. Consider the date and time of the orders shown in Table 1.1. The “original feature” is the data/time column in the data. The table includes two “derived features” in the form of the decimal hour and the day of the week. This distinction will be important when discussing feature engineering, importance scores, and explaining models. The number of overall predictors in a data set is symbolized with \\(p\\).\nOutcomes/target columns can also be called the dependent variable or response.\nFor numeric columns, we will make some distinctions. A real number is numeric with potentially fractional values (e.g., time to delivery). Integers are whole numbers, often reflecting counts. Binary data take two possible values that are almost always represented with zero and one.\nDense data describes a collection of numeric values with many possible values, also reflected by the delivery time and distances in Table 1.1. Sparse data have fewer possible values or when only a few values are contained in the observed data. The product counts in the delivery data are good examples. They are integer counts consisting mostly of zeros. The frequency of non-zero values decreases with the magnitiude of the counts.\nWhile numeric data are quantitative, qualitative data cannot be represented by a numeric scale5. The day of the week data are categories with seven possible values. Binary categorical data have two categories, such as alive/dead. The symbol \\(C\\) is used to describe the number of categories in a column.\nThe item columns in Table 1.1 are interesting too. If the order could only contain one item, we might configure that data with a qualitative single column. However, these data are multiple-choice and, as such, are shown in multiple integer columns with zero, reflecting that it was not in the order.\nvalue types, skewness, colinearity, distributions, ordinal",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-pipeline",
    "href": "chapters/introduction.html#sec-model-pipeline",
    "title": "1  Introduction",
    "section": "1.5 What Defines the Model?",
    "text": "1.5 What Defines the Model?\nWhen discussing the modeling process, it is traditional to think the parameter estimation steps occur only when the model is fit. However, as more complex techniques are used to prepare predictor values for the model, it is crucial to understand that important quantities are also being estimated before the model. Here are a few examples:\n\nImputation methods, discussed in ?sec-missing-data6, create sub-models that estimate missing predictor values. These are applied to the data before modeling to complete the data set.\nFeature selection tools (Kuhn and Johnson 2019) often compute measures of predictor importance to remove uninformative predictors before passing the data to the model.\nEffect encoding tools, discussed in ?sec-effect-encodings, use the effect of the predictor on the outcome as a predictor.\nPost-model adjustments to predictions, such as model calibration (Section 2.6 and ?sec-cls-calibration), are examples of postprocessing operations.\n\n\n\n\n\n\n\nThe modeling pipeline\n\n\n\nThese operations can profoundly affect the overall results, yet they are not usually considered part of “the model.” We’ll use the more expansive term “modeling pipeline” to describe any estimation for the model or operations before or after the model:\n\nmodel pipeline = preprocessing + supervised model + postprocessing\n\n\n\nThis is important to understand: a common pitfall in ML is only validating the model fitting step instead of the whole process. This can lead to performance statistics that can be extraordinarily optimistic and might deceive a practitioner into believing that their model is much better than it truly is.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-deep-learning",
    "href": "chapters/introduction.html#sec-deep-learning",
    "title": "1  Introduction",
    "section": "1.6 Deep Learning",
    "text": "1.6 Deep Learning\nWhile many models can be used for machine learning, neural networks are the elephant in the room. These complex, nonlinear models are frequently used in machine learning to the point where, in some domains, these models are synonymous with ML. One class of neural network models is deep learning (DL) models (Goodfellow, Bengio, and Courville 2016; Prince 2023). Here, “deep” means that the models have many structural layers, resulting in extraordinary amounts of complexity.\nFor example, Figure 1.3 shows a diagram of a type of deep learning model called a convolutional neural network that is often used for making predictions on images. It consists of several layers that manipulate the data. This particular model, dubbed VGG16, was proposed by Simonyan and Zisserman (2014).\nThe color image on the left is a data point. The first block has three layers. The two larger yellow boxes are convolutional layers. They move a small multidimensional array over the larger data array to compute localized features. For example, for an image that is 150 x 150 pixels, a convolution that uses a 3x3 filter will move this small rectangle across the array. At the (2,2) position, it takes the average of all adjacent pixel locations.\nAfter the convolutional operations, a smaller pooling layer is shown in orange. Pooling, usually by finding the maximum feature value across different image regions, compresses/downsamples the larger feature set so a smaller size without diminishing features with large signals.\nThe next three blocks consist of similar convolutional and pooling layers but of different dimensions. The idea behind these blocks is to reduce the image data to a smaller dimension width/height dimension in a way that extracts potentially important hierarchical features from the image.\nAfter block 5, the data are flattened into a single array, similar to the columns in a tabular data set. Up to this point, the network has been focused on preprocessing the image data. If our initial image were 150 pixels by 150 pixels, the flattening process would result in over 8 thousand predictors that would be used in a supervised ML model. The network would have over 14 million model parameters to estimate from the data to get to this point.\nAfter flattening, the final two layers correspond to the classic supervised neural network model (Bishop 1995). The original paper used four large supervised neural network layers; for simplicity, Figure 1.3 shows a single dense layer. This can add dozens of millions of additional parameters for a modest-sized dense layer.\n\n\n\n\n\n\n\n\nFigure 1.3: A simplified version of the convolutional neural network deep learning model proposed by Simonyan and Zisserman (2014). This version only includes a single dense layer (instead of four).\n\n\n\n\n\nThere are many types of deep learning models. Another example is recurrent neural networks, which create layers that are proficient at modeling data that occur in sequences such as time series data or sentences (i.e., a sequence of words).\nVery sophisticated deep learning models have generated, by far, the best predictive ability for images and similar data. It would seem natural to think these models would be just as effective for tabular data. However, this has not been the case. Machine learning competitions, which are very different from day-to-day machine learning work, have shown that many other models can consistently do as well or likely better on tabular data sets. This has generated an explosion of literature7 and social media posts that try to explain why this is the case. For example, see Borisov et al. (2022) and McElfresh et al. (2023).\nWe believe that exceptionally complex DL models are handicapped in several ways when taken out of the environments where they have been successful.\nFirst, the complexity of these models requires extreme amounts of data, and the constraints of such large data sets drive how the models are developed.\nFor example, keeping very large data sets in a computer’s memory is almost impossible. As discussed earlier, we use the data to find optimal values of our parameters for some objective functions (such as classification accuracy). Using traditional optimization procedures, this is very difficult if the data cannot be accessed simultaneously. Deep learning models use more modern optimization methods, such as stochastic gradient descent (SGD) methods, for estimating parameters. It does not simultaneously hold all of the data in memory and fits the model on small incremental batches of data. This is a very effective technique, but it is not an approach one would use with data set sizes that are more common (e.g., less than a million data points). SGD is less effective for smaller data sizes than traditional in-memory optimizers. Another consequence of huge data requirements is that model training can take an excruciatingly long time. As a result, efficient model development can require specialized hardware. It can also drive how models are optimized and compared. DL models are often optimized by sequentially adjusting the model. Other ML models can be optimized with a more methodical simultaneous design that is often more efficient, systematic, and effective than sequential optimizers used with deep learning.\nAnother issue with deep learning models is that their size and mathematical structure are associated with a very difficult optimization problem; their objective function is non-convex. We’d like to have an optimization problem that has some guarantees that a global optimal value exists (and can be found). That is often not the case for these models. For example, if we want to estimate the VGG16 model parameters with the goal of maximizing the classification accuracy for finding dogs in an image, it can be difficult to get a reliable estimate and, if we do, we don’t know if it is really the correct answer.\nBecause the optimization problem is so difficult, much of the user’s time is spent optimizing the model parameters or tuning the network structure to find the best results. When combined with how long it takes for a single model fit, deep learning models are very high maintenance compared to other models when used on tabular data. They can probably produce a model that is marginally better than the others, but doing so requires an inordinate amount of time, effort, and constraints. DL models are superior for certain hard problems with large data sets. For other situations, other ML models can go probably go further and faster8.\nFinally, there are some data-centric reasons that the successfulness of deep learning models doesn’t automatically translate to tabular data. There are characteristics of tabular data that don’t often occur in stereotypical DL applications. For example:\n\nIn some cases, a group of predictors might be highly correlated with one another. This can compromise some of the mathematical operations used to estimate parameters in neural networks (and many other models).\nUnless we have extensive prior experience with our data, we don’t know which are informative and which are irrelevant for predicting an outcome. As the number of non-informative predictors increases, the performance of neural networks decreases. See Section 19.1 of Kuhn and Johnson (2013).\nPredictors with “irregular” distributions, such as skewness or heavy tails, can harm performance (McElfresh et al. 2023).\nMissing predictor data are not naturally handled in basic neural networks.\n“Small” sample sizes or data dimensions with at least as many predictors as rows.\n\nas well as others. These issues can be overcome by adding layers to a network designed to counter specific challenges. However, in the end, this ends up adding more complexity. We propose that there are a multitude of other models, and many of them are resistant or robust to these types of data-specific characteristics. It is better to have different tools in our toolbox; making a more complex hammer may not help us put a screw in a wall.\nThe deep learning literature has resulted in many collateral benefits for the broader machine learning field. We’ll describe a few techniques that have improved machine learning in subsequent chapters,",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-aspects",
    "href": "chapters/introduction.html#sec-aspects",
    "title": "1  Introduction",
    "section": "1.7 Modeling Philosophies",
    "text": "1.7 Modeling Philosophies\ntalk about subject-matter knowledge/intuition vs “let the machine figure it out”. biased versus unbiased model development.\nassistive vs automated usage",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-outline",
    "href": "chapters/introduction.html#sec-outline",
    "title": "1  Introduction",
    "section": "1.8 Outline of These Materials",
    "text": "1.8 Outline of These Materials\nThis work is organized into parts:\n\nIntroduction: The next chapter shows an abbreviated example to illustrate important concepts used later.\nPreparation: These chapters discuss topics such as data splitting and feature engineering. These activities occur before the actual training of the machine learning model but are critically important.\nOptimization: To find the model that has the best performance, we often need to tune them so that they are effective but do not overfit. These chapters describe overfitting, methods to measure performance, and two different classes of optimization methods.\nClassification: Various models for classification are described in this part.\nRegression: These chapter describe models for numeric outcomes.\nCharacterization: Once you have a model, how do you describe it or its predictions? How do you know when you should question the results of your model? We’ll address these questions in this part.\nFinalization: Post-modeling activities such as monitoring performance are discussed.\n\nBefore diving into the process of creating a good model, let’s have a short case study. The next chapter is designed to give you a sense of the overall process and illustrate important ideas and concepts that will appear later in the materials.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#chapter-references",
    "href": "chapters/introduction.html#chapter-references",
    "title": "1  Introduction",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nBishop, C. 1995. Neural Networks for Pattern Recognition. Oxford: Oxford University Press.\n\n\nBorisov, V, T Leemann, K Seßler, J Haug, M Pawelczyk, and G Kasneci. 2022. “Deep Neural Networks and Tabular Data: A Survey.” IEEE Transactions on Neural Networks and Learning Systems.\n\n\nBoykis, V. 2023. “What are embeddings?” https://doi.org/10.5281/zenodo.8015029.\n\n\nEngel, T, and J Gasteiger. 2018. Chemoinformatics : A Textbook . Weinheim: Wiley-VCH.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nHolmes, S, and W Huber. 2018. Modern Statistics for Modern Biology. Cambridge University Press.\n\n\nHvitfeldt, E, and J Silge. 2021. Supervised Machine Learning for Text Analysis in R. CRC Press.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\n———. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nLeach, A, and V Gillet. 2003. An Introduction to Chemoinformatics. Springer.\n\n\nMcElfresh, D, S Khandagale, J Valverde, V Prasad, G Ramakrishnan, M Goldblum, and C White. 2023. “When Do Neural Nets Outperform Boosted Trees on Tabular Data?” arXiv.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.\n\n\nSanderson, B. 2017. Oathbringer. Tor Books.\n\n\nSimonyan, K, and A Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv.\n\n\nYu, W, HK Lee, S Hariharan, WY Bu, and S Ahmed. 2007. “CCDB:6843, Mus Musculus, Neuroblastoma.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-delivery-times",
    "href": "chapters/whole-game.html#sec-delivery-times",
    "title": "2  The Whole Game",
    "section": "2.1 Predicting Delivery Time",
    "text": "2.1 Predicting Delivery Time\nThe illustrative example focuses on predicting the food delivery time (i.e., the time from the initial order to receiving the food). There is a collection of data containing 10,012 orders from a specific restaurant. The predictors, previously shown in Table 1.1, include:\n\nThe time, in decimal hours, of the order.\nThe day of the week for the order.\nThe approximate distance between the restaurant and the delivery location.\nA set of 27 predictors that count the number of distinct menu items in the order.\n\nThe outcome is the time1 (in minutes).\nThe next section describes the process of splitting the overall data pool into different subsets and signifies the start of the modeling process. However, how we make that split depends on some aspects of the data. Specifically, it is a good idea to examine the distribution of our outcome data (i.e., delivery time) to determine how to randomly split the data2.\nFigure 2.1 (panel (a)) uses a histogram to examine the distribution of the outcome data. It is right-skewed and there is a subset of long delivery times that stand out from the mainstream of the data. The most likely delivery time is about 27 minutes, but there is also a hint of a second mode a few minutes earlier.\n\n\n\n\n\n\n\n\nFigure 2.1: Histrograms of the training set outcome data with and without a transformation.\n\n\n\n\n\nThis skewness might affect how we split the data and is discussed in the next section. Another important question is: should we model a transformed version of the delivery times? If we have large outlying values, the model might be inappropriately pulled to these values, often at the expense of the overall quality of fit. One approach is to model the logarithm of the times (as shown in Equation 1.1). Figure 2.1 (panel (b)) shows the distribution of the log (base 2) deliveries. It is more symmetric, and the potential second mode is more pronounced. This isn’t a bad idea, and the primary consequence is how we interpret some of the metrics that are used to quantify how well the model fits the data3. In the end, we decided to model the data in the original units but did try the analysis both ways (and did not find huge differences in the results).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-data-spending-whole-game",
    "href": "chapters/whole-game.html#sec-data-spending-whole-game",
    "title": "2  The Whole Game",
    "section": "2.2 Data Spending",
    "text": "2.2 Data Spending\nAn important aspect of machine learning is to use the right data at the right time. We don’t start modeling with the entire pool of data. As will be seen in ?sec-resampling, we need to reserve some data to evaluate the model and make sure that those data do not overlap with those used to fit the model.\nOften, our data spending strategy is to split the data into at least two subsets:\n\nThe training set is used to develop the model pipeline. It is used for investigating, fitting, and comparing them.\nThe test set is a smaller fraction of the data that is only used at the very end to independently validate how well we did.\n\nThe training set is used for a lot of our activities. We do need to evaluate multiple model pipelines along the way. We reserve the test set for the final assessment and should not use it for development. There are two main strategies for characterizing model efficacy during development.\nThe first is to resample the training set. This is an iterative computational tool to find good statistical estimates of model performance using the training set.\nThe other option is to initially split the data into three subsets: the training set, the testing set, and the validation set. Validation sets are small subset used for evaluating the model repeatedly during development.\nGenerally, we suggest using resampling unless the initial data pool is “sufficiently large.” For these data, 10,012 food orders is probably sufficient to choose a validation set over the resampling approach.\nTo split the data, we’ll allocate 60% for the training set and then use 20% for the validation set, and the remaining 20% for testing. The splitting will be conducted by stratification. To ensure that our delivery time distributions are approximately the same, it is temporarily “binned” into four quartiles. The three-way split is executed within each quartiles and the overall training/validation/testing sets are assembled by aggregating the corresponding data from the quartiles. As a result, the respective sample sizes are 6004/2004/2004 deliveries.\nAs previously stated, the majority of our time is spent with the training set samples. The first step in any model-building process is to understand the data, which can most easily be done through visualizations of the training set.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-eda-whole-game",
    "href": "chapters/whole-game.html#sec-eda-whole-game",
    "title": "2  The Whole Game",
    "section": "2.3 Exploratory Data Analysis",
    "text": "2.3 Exploratory Data Analysis\nOur goal now is to determine if there are aspects of predictors that would affect how we provide them to the model. We should look at individual variables as well as combinations multiple variables.\nTo start, for the dense numeric predictors, how would we characterize their relationship to the outcome? Is it linear, nonlinear, or random? Let’s start with the distance predictor. Figure 2.2(a) shows the values with the “scatter plot smoother” fit. The line is a flexible nonlinear approach that can adapt to any apparent pattern in the plot4. First, the plot shows that the distance distribution is also right-skewed (although a separate histogram would better illustrate this). The smoother shows a slightly nonlinear trend, although the nonlinearity is mainly in the right tail of the distances and might be an artifact of the small sample size in that region. Regardless, it appears to be an informative predictor.\n\n\n\n\n\n\n\n\nFigure 2.2: Visualizations of relationship between the outcome and several predictors using the training set.\n\n\n\n\n\nPanel (b) has a similar visualization for the order time. In this case, there is another nonlinear trend where the order increases with the hour, then peaks around 17:30, then begins to decrease. There is also an increase in delivery time variation as the order time approaches the peak.\nThe day of the week is visualized via box plots in panel (c). There is an increasing trend in both mean and variation as we move from Monday to Friday/Saturday, then a decrease on Sunday. Again, this appears to be an important predictor.\nThese three panels show relationships between individual predictors and delivery times. It is possible that multiple predictors can act in concert; their effect on the outcome occurs jointly. Panel (d) of Figure 2.2 shows that there is an interaction effect between the day and hour of the order. Mondays and Tuesdays have shorter delivery times that don’t change much over hours. Conversely, on Fridays and Saturdays, the delivery times are generally longer, with much higher peaks. This might explain the increase in delivery time variation that was seen in Panel (b). Now that we know of the existence of this interaction, we might add additional model terms to help the model understand and exploit this pattern in the training set.\nFor the 27 itemized frequency counts, we might want to know if there was a change in delivery time when a specific item was in the order (relative to those where it is not purchased). To do this, mean delivery times were computed for orders with and without the item. The ratio of these times was computed and then converted to the percent increase in time. This is used to quantify the effect of item on delivery time. We’d also like to know what the variation is on this statistic and a technique called the bootstrap (Davison and Hinkley 1997) was used to create 90% confidence intervals for the ratio. Figure 2.3 shows the results.\n\n\n\n\n\n\n\n\nFigure 2.3: Ranking the items by the increase in delivery time when ordered at least once.\n\n\n\n\n\nThere is a subset of items that increase the delivery time (relative to the experimental noise), many that seem irrelevant, and one that decreases the time.\nMore investigations into the data would be pursued to better understand the data and help us with feature engineering. For brevity, we’ll stop here and introduce three ML models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-development-whole-game",
    "href": "chapters/whole-game.html#sec-model-development-whole-game",
    "title": "2  The Whole Game",
    "section": "2.4 Model Development",
    "text": "2.4 Model Development\nWhich model to use? There is a multitude of tools that can be used to represent and predict these data. We’ll illustrate three in this section and the essential concepts to consider along the way.\nBefore proceeding, we need to pick a performance metric. This is used to quantify how well the model fits the data. For any model fit, the residual (usually denoted as \\(\\epsilon\\), also called the error) is the difference between the model prediction and the observed outcome. We’d like our model to produce residuals near zero. One metric5 is the mean absolute error (MAE). For a data set with \\(n\\) data points, the equation is\n\\[\nMAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| = \\frac{1}{n}\\sum_{i=1}^n |\\epsilon_i|\n\\tag{2.1}\\]\nwhere \\(\\hat{y}\\) is the predicted numeric outcome value and \\(\\epsilon\\) is the model error. The MAE units are the same as the outcome which, in this case, is minutes. We desire to minimize this statistic.\nTo get started, we’ll use a basic linear model for these data.\n\nLinear Regression\nLinear regression is probably the most well-known statistical model in history. It can predict a numeric outcome using one or more predictors using the equation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\ldots + \\beta_px_{ip} + \\epsilon_i\n\\tag{2.2}\\]\nwhere \\(y_i\\) is the outcome for the \\(i^{th}\\) data point (out of \\(n_{tr}\\)), and \\(x_{ij}\\) is value for the \\(j^{th}\\) predictor (out of \\(p\\)).\nFor Equation 2.2, there are many ways to estimate the model parameters (\\(\\beta\\)’s). Ordinary least squares is used most often. It finds the values of the \\(\\beta_j\\) that that minimize sum of squared errors (SSE). ?sec-reg-linear discusses several other ways to estimate the model parameters that use other criteria (variations of the SSE).\nThe main assumptions about the data for ordinary least squares are that they are independent and identically distributed. Given what we know about the problem, both assumptions are at least mildly incorrect. Orders are being processed simultaneously (affecting independence), and our visualizations have indicated that there is an increase in variation with the mean (i.e., not identically distributed). However, we are most interested in prediction here, so we can often overlook slight to moderate violations of these assumptions6\nWhat are our \\(x_{ij}\\) values in Equation 2.2? We can’t give the model equations values of \"Monday\" or \"Friday\"; OLS requires numbers in its equations. A simple workaround is to create binary indicator variables for six of the seven days of the week. For example, the indicator for Friday would have a value of one for orders placed on a Friday and zero otherwise7.\nAlso, for Equation 2.2, what do we do about the numeric predictors that appear to have nonlinear trends? We can augment the predictor by describing it with multiple columns. Equation 2.2 shows that linear regression is linear in the parameters (not the predictors). We can add terms such as \\(x_{ij} = distance_i^2\\) to allow the fit to be nonlinear. Alternatively, an effective tool for enabling nonlinear trends is spline functions8. These are special polynomial terms. For example, we could represent that original predictor with 10 spline terms (i.e., columns in the data) for the order hour. As we add more terms, the model fit has increased capacity to be nonlinear. We’ll use 10 separate splines for both distance and the order hour (in lieu of the original data columns.)\nHow do we encode the interaction seen in Figure 2.2(d)? We have six indicator columns for the order day and ten spline columns for the order hour. We can facilitate having different nonlinear hour trends for each day by making 60 cross-product terms. For example, the interactions for Fridays would multiply each of the ten spline terms by the binary indicator associated with Fridays.\nThese feature engineering operations result in a model with \\(p = 114\\) \\(\\beta\\) parameters. We fit this model to the training set to estimate the model parameters. To evaluate it, we use the trained model to predict the validation set, and these predictions are the substrate to compute the MAE. For this model, the validation set MAE is 1.609 (i.e., 1 minute and 36 seconds). If we think of MAE as an average error, this doesn’t seem too bad. However, what does this look like? Let’s plot the observed times against predicted values to get a better sense of how well this model did. Figure 2.4 has the results which also feature a scatter plot smoother.\n\n\n\n\n\n\n\n\nFigure 2.4: A scatter plot of the observed and predicted delivery times, via a linear regression model, for the validation set. The red line is a smooth trend estimate.\n\n\n\n\n\nThe best case would be that the points arrange themselves tightly around the diagonal line. The model slightly under-predicts for very rapid deliveries but substantially under-predicts for times greater than 40 minutes. However, overall, the model works well for the majority of the deliveries.\nFrom here, our next step would be to investigate poorly predicted samples and determine if there is some commonality to them. For example, are they short distances that are ordered late on a Friday, etc? If there appears to be a pattern, we would add additional model terms to compensate for the flaw and see if the validation set MAE decreases. We would iterate between exploratory analysis of the residuals, adding or subtracting features, then refitting the model.\nFor illustration, we stop here and take a look at very different model.\n\n\nRule-Based Ensemble\nCubist is a rule-based ensemble method. It creates a rule set by first generating a regression tree that is composed of a hierarchical set of if/then statements9. Figure 2.5 shows a simple tree for our data. A set of splits on different predictors results in six terminal nodes (at the bottom of the tree). A rule is a distinct path through the tree to a terminal node. For example, Node 11 in the figure has the corresponding rule: hour &gt;= 14.77 & day in {Fri, Sat} & distance &gt;= 4.045. For each rule, Cubist fits a linear regression to the data that the rule covers (\\(n = 261\\) for Node 11). It builds new rule sets sequentially and uses many rules/regression equations to predict new samples.\n\n\n\n\n\n\n\n\nFigure 2.5: An example of a regression tree used to create a set of six rules.\n\n\n\n\n\nThe method isolates different parts of the predictor space and, with these subsets, models the data using linear functions. The creation of multiple rules enables the model to emulate nonlinear functions easily. When a rule consists of multiple predictors, this constitutes an interaction effect. Additionally, the data preprocessing requirements are minimal: it can internally account for any missing predictor values, and there is no need to convert categorical predictors into binary indicator columns, and so on.\nWhen fit to the training set, the Cubist model created an ensemble with 105,327 rules. On average, the rules consisted of 2.7 variables and the the regression models contained 12.7 predictors. Two example rules, and their regression models, were:\n\nif\n   hour &lt;= 14.252\n   day in {Fri, Sat}\nthen\n   outcome = -23.039 + 2.85 hour + 1.25 distance + 0.4 item_24\n             + 0.4 item_08 + 0.6 item_01 + 0.6 item_10 + 0.5 item_21\n             + 0.3 item_09 + 0.4 item_23 + 0.2 item_03 + 0.2 item_06\n\nand\n\nif\n   hour &gt; 15.828\n   hour &lt;= 18.395\n   distance &lt;= 4.35\n   item_10 &gt; 0\n   day = Thu\nthen\n   outcome = 11.29956 + 4.24 distance + 14.3 item_01 + 4.3 item_10\n             + 2.1 item_02 + 0.03 hour\n\nThese two examples are reasonable and understandable. However, Cubist blends many rules together to the points where is becomes a black-box model. For example, the first data point in the validation set is affected by 5,916 rules (about 6% of the total ensemble).\nDoes this complexity help? The validation set estimated MAE at 1 minute and 24 seconds, which is an improvement over the linear regression model. Figure 2.6 visualizes the observed and predicted plot. The model under-predicts fewer points with long delivery times than the previous model but does contain a few very large residuals.\n\n\n\n\n\n\n\n\nFigure 2.6: A scatter plot of the observed and predicted delivery times from the Cubist model, for the validation set.\n\n\n\n\n\nWould this model have benefited from using the feature set from the linear regression analysis? Not really. Refitting with those predictors had an MAE of 1 minute and 26 seconds. In this particular case, the original Cubist was able to estimate nonlinear trends and interactions without additional feature engineering.\nThere is more that we can do with the Cubist model but, in this initial model screening phase, we should focus our efforts on evaluating a disparate set of models and features to understand what works, what doesn’t work, and the difficulty level of the problem. Let’s try one additional model.\n\n\nNeural Network\nA neural network is a highly nonlinear modeling technique. They relate the predictors to the outcome through intermediate substructures, called layers, that contain hidden units. The hidden units allow the predictors to affect the outcomes differently10 allowing the model to isolate essential patterns in the data. As the number of hidden units (or layers of hidden units) increases, so does the complexity of the model. We’ll only consider a single layer network with multiple hidden units for these data.\nNeural networks, like linear regression, require numbers as inputs. We’ll convert the day of the week predictor to indicators. Additionally, the model requires that all of the predictors be in the same units. There are a few ways to accomplish this. First, we will compute the training set mean and standard deviation of each feature. To standardize them we subtract the mean and divide the result by the standard deviation. As a result, all features have a zero mean and a unit standard deviation. A common scaling of the predictor columns is a necessary precondition for fitting neural networks\nOne question about this model: how many hidden units should we use11? This decision will determine the complexity of the model. As models become model complex, they have more capacity to find complicated relationships between the predictors and response. However, added complexity adds to the risk of overfitting. This situation, discussed in ?sec-model-complexity-and-overfitting, occurs when a model fits exceptionally well to the training data but fails for new data when it finds patterns in the training data that are artifacts (i.e., do not generalize for other data sets). How should we choose the number of hidden units?\nUnfortunately, no equation can directly compute an estimate the number of units from the data. Values such as these are referred to as tuning parameters or hyperparameters. However, one solution is to extend the validation process previously shown for the linear regression and Cubist models. We will consider a candidate set of tuning parameter values ranging from 3 to 100 hidden units. We’ll fit a model to the training set for each of these 98 values and use the validation set MAE values to select an appropriate value. The results of this process is shown in Figure 2.7.\n\n\n\n\n\n\n\n\nFigure 2.7: The relationship between the number of hidden units and the mean absolute deviation from the validation set data.\n\n\n\n\n\nA small number of hidden units performs poorly due to underfitting (i.e., insufficient complexity). Adding more improves the situation and, while the numerically best result corresponds to a value of 99, there is appreciable noise in the MAE values12 . Eventually, adding too much complexity will either result in the MAE values becoming larger due to overfitting or will drastically increase the time to fit each model. The MAE pattern in Figure 2.7 shows a plateau of values with a similar MAE once enought hidden units are added. Given the noise in the system, we’ll select a value of 48 hidden units, by determining the least complex model that is within 1% of the numerically best MAE. The MAE associated with the selected candidate value13 was 1 minute and 29 seconds (but is likely to be closer to 1 minute and 31 seconds, given the noise in Figure 2.7).\nFigure 2.8 shows the familiar plot of the observed and predicted delivery times. There are fewer large residuals than the Cubist model but the underfitting for longer delivery times appears more pronounced. There is also a hint of under-predicted times for fast deliveries.\n\n\n\n\n\n\n\n\nFigure 2.8: A scatter plot of the validation set observed and predicted delivery times from a neural network with 48 hidden units.\n\n\n\n\n\nThese results suggest that the Cubist and neural network models have roughly equivocal performance14. Like Cubist, the finalized model is very complex; it is a highly nonlinear regression with 5,172,847 model parameters (i.e., slopes and intercepts).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-selection-whole-game",
    "href": "chapters/whole-game.html#sec-model-selection-whole-game",
    "title": "2  The Whole Game",
    "section": "2.5 Choosing Between Model",
    "text": "2.5 Choosing Between Model\nThere are a variety of criteria used to rank and compare model fits. Obviously, performance is important. Also, it is good practice to choose a model that is as simplistic as possible (subject to having acceptable performance). There are other practical factors too. If a model is to be deployed, the size of the files(s) required to automate predictions can be a constraint. There may also be data constraints; which predictors are easy or inexpensive to obtain? Does the number of predictors affect how quickly predictions can be made?\nWhile prediction is the focus for ML models, we often have to explain the model and/or predicted values to consumers. While we can develop explainers for any model, some types are easier to explain than others.\nFor this chapter, we have three models whose MAE values are within about 12 seconds of one another. Each has the same deficiency: they all significantly underpredict long delivery times. In fact, a few poor predictions may be a limiting factor for any regression metric; it might be impossible for one model’s MAE to really stand out from the others.\nWe’ll select the linear regression model as our final model due to its relative simplicity and linear structure.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-calibration-whole-game",
    "href": "chapters/whole-game.html#sec-calibration-whole-game",
    "title": "2  The Whole Game",
    "section": "2.6 Calibration",
    "text": "2.6 Calibration\nThere is one option for remedying the systematic underprediction of long delivery times. ?sec-reg-calibration describes calibration methods that might be able to correct such issues. To do so, we use the held out predictions to estimate the average unwanted trend and remove it from new predictions. For the linear regression results, the estimated average trend is almost identical to the smooth line shown in Figure 2.4.\nFor our test set predictions, we’ll remove this trend and, hopefully, this will recenter the long predictions to be closer to the diagonal line.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-test-results-whole-game",
    "href": "chapters/whole-game.html#sec-test-results-whole-game",
    "title": "2  The Whole Game",
    "section": "2.7 Test Set Results",
    "text": "2.7 Test Set Results\nWe can predict the test set using the linear regression fit from the training set. Recall that the validation set MAE was 1 minute and 36 seconds. Coincidentally, without the additional calibration, the test set value was also 1 minute and 36 seconds. The similarity of these results gives us confidence that the performance statistics we estimated during development are consistent with the test set. The test set plot of the predictions is shown in Figure 2.9(a).\nThe calibration results are shown in panel (b). There does appear to be an improvement in long delivery times; these are closer to the diagonal line. The MAE value is slightly improved: 1 minute and 32 seconds. The small difference in these results and the similar MAE values across models indicates that the few large residuals are probably the gating factor for these data.\n\n\n\n\n\n\n\n\nFigure 2.9: Observed versus predicted test set delivery times for the final model. Results are shown for pre-calibrated and post-calibration.\n\n\n\n\n\nAt this point, we should spend a fair amount of time documenting the model and our development process. It might be helpful to employ inferential analyses to explain to interested parties which model components were most important (e.g., how much the interaction or nonlinear terms improve the model, etc.). It is very important to document where the model does not work well, its intended use, and the boundaries of the training set. Finally, if the model is deployed, we should also monitor its effectiveness over time and also understand if the sample population is changing over time too.\nThe next section accentuates important aspects of this case study.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-themes-whole-game",
    "href": "chapters/whole-game.html#sec-themes-whole-game",
    "title": "2  The Whole Game",
    "section": "2.8 Themes",
    "text": "2.8 Themes\nAt a high level, let’s review the process that we just used to create a model; Figure 2.10 highlights several themes15. The overall process involves initial data splitting, model development, then model selection, and validation. We’ll look at each of these in turn to discuss important points.\n\n\n\n\n\n\n\n\nFigure 2.10: A general high level view of the process of creating a predictive model. The diagram reflects which steps use the test set, validation set, or the entire training set. Figure 2.11 describes the operations labeled “model development”.\n\n\n\n\n\n\nData Spending\nAlthough discussed in Chapter 3, how we allocate data to specific tasks (e.g., model building, evaluating performance, etc.) is an essential aspect of modeling. In our analysis, a minority of the data were explicitly allocated to measure how well the model worked. The test set is meant to be unbiased since it was never used to develop the model. This philosophy of empirical validation is a crucial feature for judging how models function.\nThe majority of the data ends up in the training set. The core activities of creating, optimizing, and selecting are accomplished with these data.\nFigure 2.10 highlights how we use the right data at the right time. A lack of a data usage methodology (or an inappropriate one) can lead to having a false sense of the quality of the model.\nFor example, when the linear regression model was calibrated, we chose to use the held-out samples as the substrate for estimating the unwanted trend. We could have used the re-predicted training set data but, as will be discussed in ?sec-model-complexity-and-overfitting, these values can be unrealistically optimistic. Also, we chose not to compute the MAE on the held-out predictions after they were used to estimate the calibration. This dual use of the same data would result in an MAE value that is smaller than it should be. There are other ways of estimating the utility of the calibration prior to the test set; these are discussed in @?sec-reg-calibration.\n\n\nInvestigate Many Options\nFor the delivery data, three different models were evaluated. It is our experience that some modeling practitioners have a favorite model that is relied on indiscriminately (boosted trees immediately come to mind). The “No Free Lunch” Theorem (Wolpert 1996) argues that, without substantive information about the modeling problem and data, no single model will always do better than any other model. Because of this, a strong case can be made to try various techniques and then determine which to focus on. In our example, simple data visualizations elucidated the relationships between the outcome and the predictors. Given this knowledge, we might exclude some models from consideration, but there is still a wide variety of techniques to evaluate.\nFor individual models, Figure 2.11 summarizes the cyclic process of speculation, configuration, and validation of model parameters and features. For these data, we discovered most of the important trends prior to our first model fit. As previously mentioned, we would cycle through a few rounds where we try to make improvments for each of the models.\n\n\n\n\n\n\n\n\nFigure 2.11: The cycle of developing a specific type of model. EDA standard for exploratory data analysis.\n\n\n\n\n\nFor novel data sets, it is a challenge to know a priori what good choices are for tuning parameters, feature engineering methods, and predictors. The modeler must iterate using choices informed by their experience and knowledge of the problem.\n\n\nPredictor Representations\nThis example revolves around a small set of predictors. In most situations, there will be numerous predictors of different types (e.g., quantitative, qualitative, etc.). It is unclear which predictors should be included in the model for new modeling projects and how they should be represented.\nWhen considering how to approach a modeling project, the stereotype is that the user focuses on which new, fancy model to apply to the data. The reality is that many of the most effective decisions that users must confront are which predictors to use and in what format. Thoughtfully adding the right feature representations often obviates the need (and overhead and risks) for complex models. That is one of the lessons from this particular data set.\nWe’ll put the feature engineering process on equal footing with model training For this reason, Figure 2.11 shows that the choice of model features and parameters are both important.\nWhile Cubist and the neural network were able to internally determine the more elaborate patterns in the data, our discovery of these same motifs has a higher value. We learned something about the nature of the data and can relate them to the model (via feature engineering) and the end-users16. Later, in ?sec-interactions, we’ll see how a black-box model can be used to suggests additional interactions to add to our linear regression model. This has the effect of eliminating some of the large residuals seen in Figure 2.4.\n\n\nModel Selection\nAt some point in the process, a specific model pipeline must be chosen. This chapter demonstrated two types of selection. First, we chose some models over others: the linear regression model did as well as more complex models and was the final selection. In this case, we decided between model pipelines. There was also a second type of model selection shown. The number of hidden units for the neural network was determined by trying different values and assessing the results. This was also model selection, where we decided on the configuration of a neural network (as defined by the number of hidden units). In this case, we selected within different neural networks.\nIn either case, we relied on data separate from the training set to produce quantitative efficacy assessments to help choose. This book aims to help the user gain intuition regarding the strengths and weaknesses of different models to make informed decisions.\n\n\nMeasuring Performance\nBefore using the test set, two techniques were used to determine the model’s effectiveness. First, quantitative assessments of statistics (i.e., MAE) from the validation set help the user understand how each technique would perform on new data. The other tool was to create simple visualizations of a model, such as plotting the observed and predicted values, to discover areas of the data where the model does particularly well or poorly. This qualitative information is lost when the model is gauged only using summary statistics and is critical for improving models. There are many more visualization methods for displaying the outcomes, their predictions, and other data. The figures shown here are pretty simplistic.\nNote that almost all of our approaches for model evaluation are data-driven in a way where we determine if the predicted values are “close” to the actual values. This focus on empirical validation is critical in proving that a model pipeline is fit for purpose. Our general mantra is\n\n\n\n\n\n\nImportant\n\n\n\nAlways have a separate piece of data that can contradict what you believe.\n\n\nFor models that require specific probabilistic assumptions, it is a good idea to check these too, primarily if a model will also be used for inference. However, before considering questions regarding theoretical assumptions, empirical validation should be used to ensure that the model can explain the data well enough to be worth our inferences. For example, if our validation set MAE was around 15 minutes, the model predictions would not show much fidelity to the actual times. Any inferences generated from such a model might be suspect.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-summary-whole-game",
    "href": "chapters/whole-game.html#sec-summary-whole-game",
    "title": "2  The Whole Game",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nAt face value, model building appears straightforward: pick a modeling technique, plug in the data, and generate a prediction. While this approach will yield a predictive model, it unlikely to generate a reliable, trustworthy model. To achieve this we must first understand the data and the objective of the modeling. We finally proceed to building, evaluating, optimizing, and selecting models after these steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#chapter-references",
    "href": "chapters/whole-game.html#chapter-references",
    "title": "2  The Whole Game",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nPerkins, D. 2010. Making Learning Whole. Wiley.\n\n\nWolpert, D. 1996. “The Lack of a Priori Distinctions Between Learning Algorithms.” Neural Computation 8 (7): 1341–90.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-ames-intro",
    "href": "chapters/initial-data-splitting.html#sec-ames-intro",
    "title": "3  Initial Data Splitting",
    "section": "3.1 The Ames Housing Data",
    "text": "3.1 The Ames Housing Data\nThese data, originally published by De Cock (2011), are an excellent teaching example. Data were collected for 2,930 houses in Ames, Iowa, via the local assessor’s office. A variety of different characteristics of the houses were measured. Chapter 4 of Kuhn and Silge (2022) contains a detailed examination of these data. For illustration, we will focus on a smaller set of predictors, summarized in Tables 3.1 and 3.2. The geographic locations of the properties are shown in Figure 3.2.\n\n\n\n\n\n\n\n\n  \n    \n    \n      Column\n      Min\n      Median\n      Max\n      Std. Dev.\n      Skewness\n      Distribution\n    \n  \n  \n    Baths\n     0.0\n      2.0\n      5.0\n     0.64\n 0.3\n\n    Gross Living Area\n   334.0\n  1,442.0\n  5,642.0\n   505.51\n 1.3\n\n    Latitude\n    42.0\n     42.0\n     42.1\n     0.02\n-0.5\n\n    Longitude\n   -93.7\n    -93.6\n    -93.6\n     0.03\n-0.3\n\n    Lot Area\n 1,300.0\n  9,436.5\n215,245.0\n 7,880.02\n12.8\n\n    Sale Price\n12,789.0\n160,000.0\n755,000.0\n79,886.69\n 1.7\n\n    Year Built\n 1,872.0\n  1,973.0\n  2,010.0\n    30.25\n-0.6\n\n    Year Sold\n 2,006.0\n  2,008.0\n  2,010.0\n     1.32\n 0.1\n\n  \n  \n  \n\n\n\n\n\nTable 3.1: A summary of numeric predictors in the Ames housing data.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Column\n      # Values\n      Most Frequent (n)\n      Least Frequent (n)\n      Distribution\n    \n  \n  \n    Building Type\n5\nSingle-Family Detached (2425)\nTwo-Family Conversion (62)\n\n    Central Air\n2\nYes (2734)\nNo (196)\n\n    Neighborhood\n28\nNorth Ames (443)\nLandmark (1)\n\n  \n  \n  \n\n\n\n\n\nTable 3.2: A summary of categorical predictors in the Ames housing data.\n\n\n\n\nAs shown in Table 3.1, the sale price distribution is fairly right-skewed. For this reason, and because we do not want to be able to predict negative prices, the outcome is analyzed on the log (base-10) scale.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-train-test",
    "href": "chapters/initial-data-splitting.html#sec-train-test",
    "title": "3  Initial Data Splitting",
    "section": "3.2 Training and Testing Sets",
    "text": "3.2 Training and Testing Sets\nOne of the first decisions is to decide which samples will be used to evaluate performance. We should evaluate the model with samples that were not used to build or fine-tune it. An “external sample” will help us obtain an unbiased sense of model effectiveness. A selection of samples can be set aside to evaluate the final model. The training data set is the general term for the samples used to create the model. The remaining samples, or a subset of them, are placed in the testing data set. The testing data set is exclusively used to quantify how well the model works on an independent set of data. It should only be accessed once to validate the final model candidate.\nHow much data should be allocated to the training and testing sets? This depends on several characteristics, such as the total number of samples, the distribution of the response, and the type of model to be built. For example, suppose the outcome is binary and one class has far fewer samples than the other. In that case, the number of samples selected for training will depend on the number of samples in the minority class. Finally, the more tuning parameters required for a model, the larger the training set sample size will need to be. In general, a decent rule of thumb is that 75% could be used from training.\nWhen the initial data pool is small, a strong case can be made that a test set should be avoided because every sample may be needed for model building. Additionally, the size of the test set may not have sufficient power or precision to make reasonable judgments. Several researchers (J. Martin and Hirschberg 1996; Hawkins, Basak, and Mills 2003; Molinaro 2005) show that validation using a single test set can be a poor choice. Hawkins, Basak, and Mills (2003) concisely summarizes this point:\n\n“hold-out samples of tolerable size […] do not match the cross-validation itself for reliability in assessing model fit and are hard to motivate”.\n\nResampling methods (?sec-resampling), such as cross-validation, are an effective tool that indicates if overfitting is occurring. Although resampling techniques can be misapplied, such as the example shown in Ambroise and McLachlan (2002), they often produce performance estimates superior to a single test set because they evaluate many alternate versions of the data.\n\n\n\n\n\n\nImportant\n\n\n\nOverfitting is the greatest danger in predictive modeling. It can occur subtly and silently. You cannot be too paranoid about overfitting.\n\n\nFor this reason, it is crucial to have a systematic plan for using the data during modeling and ensure that everyone sticks to the program. This can be particularly important in cases where the modeling efforts are collaborations between multiple people or institutions. We have had experiences where a well-meaning person included the test set during model training and showed stakeholders artificially good results. For these situations, it might be a good idea to have a third party split the data and blind the outcomes of the test set. In this way, we minimize the possibility of accidentally using the test set (or people peeking at the test set results).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-leakage",
    "href": "chapters/initial-data-splitting.html#sec-leakage",
    "title": "3  Initial Data Splitting",
    "section": "3.3 Information Leakage",
    "text": "3.3 Information Leakage\nInformation leakage (a.k.a data leakage) is another aspect of data handling to consider at the onset of a modeling project. This occurs when the model has access to data that it should not. For example,\n\nUsing the distribution of the predictor data in the test set (or other future data) to inform the model.\nIncluding identical or statistically related data in training and test sets.\nExploiting inadvertent features that are situationally confounded with the outcome.\n\nAn example of the last item we experienced may be familiar to some readers. A laboratory was producing experimental results to evaluate the difference between two treatments for a particular disorder. The laboratory was under time constraints due to an impending move to another building. They prioritized samples corresponding to the new treatment since these were more interesting. Once finished, they moved to their new home and processed the samples from the standard treatment.\nOnce the data were examined, there was an enormous difference between the two treatment sets. Fortuitously, one sample was processed twice: before and after they moved. The two replicate data points for this biological sample also showed a large difference. This means that the signal seen in the data was potentially driven by the changes incurred by the laboratory move and not due to the treatment type.\nThis type of issue can frequently occur. See, for example, Baggerly, Morris, and Coombes (2004), Kaufman et al. (2012), or Kapoor and Narayanan (2023).\nAnother example occurs in the Ames housing data set. These data were produced by the local assessor’s office, whose job is to appraise the house and estimate the property’s value. The data set contains several quality fields for things like the heating system, kitchen, fireplace, garage, and so on. These are subjective results based on the assessor’s experience. These variables are in a qualitative, ordinal format: “poor”, “fair”, “good”, etc. While these variables correlate well with the sale price, they are actually outcomes and not predictors. For this reason, it is inappropriate to use them as independent variables.\nFinally, the test set must emulate the data that will be seen “in the wild”, i.e., in future samples. We have had experiences where the person in charge of the initial data split had a strong interest in putting the “most difficult” samples in the test set. The prevalence of such samples should be consistent with their prevalence in the population that the model is predicting.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "href": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.4 Simple Data Splitting",
    "text": "3.4 Simple Data Splitting\nWhen splitting the data, it is vital to think about the model’s purpose and how the predictions will be used. The most important issue is whether the model will predict the same population found in the current data collection. For example, for the Ames data, the purpose is to predict new houses in the town. This definition implies a measure of interpolation since we are primarily concerned with what is happening in Ames. The existing data capture the types of properties that might be seen in the future.\nAs a counter-example, Chapter 4 of Kuhn and Johnson (2019) highlights a prediction problem in which a model is used to predict the future ridership of commuters on the Chicago elevated trains. This data set has daily records of how many commuters ride the train, and temporal factors highly affect the patterns. In this case, the population we will predict is future ridership. Given the heavy influence of time on the outcome, this implies that we will be extrapolating outside the range of existing data.\nIn cases of temporal extrapolation, the most common approach to creating the training and testing set is to keep the most recent data in the test set. In general, it is crucial to have the data used to evaluate the model be as close to the population to be predicted. For times series data, a deterministic split is best for partitioning the data.\nWhen interpolation is the focus, the simplest way to split the data into a training and test set is to take a simple random sample. If we desire the test set to contain 25% of the data, we randomly generate an appropriately sized selection of row numbers to allocate sales to the test set. The remainder is placed in the training set.\nWhat is the appropriate percentage? Like many other problems, this depends on the characteristics of the data (e.g., size) and the modeling context. Our general rule of thumb is that one-fourth of the data can go into testing. The criticality of this choice is driven by how much data is available. The split size is not terribly important if a massive amount of data is available. When data are limited, deciding how much data to withhold from training can be challenging.\nT. Martin et al. (2012) compares different methods of splitting data, including random sampling, dissimilarity sampling, and other methods.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "href": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "title": "3  Initial Data Splitting",
    "section": "3.5 Using the Outcome",
    "text": "3.5 Using the Outcome\nSimple random sampling does not control for any data attributes, such as the percentage of data in the classes. When one class has a disproportionately small frequency compared to the others (discussed in ?sec-imbalances), the distribution of the outcomes may be substantially different between the training and test sets.\nWhen splitting the data, stratified random sampling (Kohavi 1995) applies random sampling within sub-groups (such as the classes) to account for the outcome. In this way, there is a higher likelihood that the outcome distributions will match. When an outcome is a number, we use a similar strategy; the numeric values are broken into similar groups (e.g., low, medium, and high) and execute the randomization within these groups.\nLet’s use the Ames data to demonstrate stratification. The outcome is the sale price of a house. Figure 3.1(a) shows the distribution of the outcomes with vertical lines that separate 20% partitions of the data. Panel (b) shows that the outcome distributions are nearly identical after partitioning into training and testing sets.\n\n\n\n\n\n\n\n\nFigure 3.1: (a) A density plot of the sale price of houses in Ames with vertical lines that indicate regions that cover 20% of the data. The ‘rug’ on the axis shows the individual data points. (b) Density plots of the training set outcomes (solid red) and test set outcomes (dashed blue) for the Ames data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "href": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "title": "3  Initial Data Splitting",
    "section": "3.6 Using the Predictors",
    "text": "3.6 Using the Predictors\nAlternatively, we can split the data based on the predictor values. Willett (1999) and Clark (1997) proposed data splitting based on maximum dissimilarity sampling. The dissimilarity between two samples can be measured in several ways. The simplest method uses the distance between the predictor values for two samples. If the distance is small, the points are nearby. Larger distances between points are indicative of dissimilarity. To use dissimilarity as a tool for data splitting, we should initialize the training set with a single sample. We calculate the dissimilarity between this initial sample and the unallocated samples. The unallocated sample that is most dissimilar is added to the training set. A method is needed to allocate more instances to the training set to determine the dissimilarities between groups of points (i.e., the two in the training set and the unallocated points). One approach is to use the average or minimum of the dissimilarities. For example, to measure the dissimilarities between the two samples in the training set and a single unallocated point, we can determine the two dissimilarities and average them. The third point added to the training is chosen as having the maximum average dissimilarity to the existing set. This process continues until we achieve the targeted training set size.\nFigure 3.2 illustrates this process for the Ames housing data. Starting with a data point near the middle of the town, dissimilarity sampling selected 50 data points using scaled longitude and latitude as predictors. As the sampling proceeds, the algorithm initially chooses samples near the outskirts of the data, especially if they are outliers. Overall, the selected data points cover the space with no redundancy.\n\n\n\n\n\n\n\n\nFigure 3.2: Maximum dissimilarity sampling of 25 points in the Ames data. The small black circles are individual properties. Larger, lighter colors indidicate earlier selection.\n\n\n\n\n\nFor this example, the two predictors used for splitting were numeric. In this case, we typically use simple distance functions to define dissimilarity. Many other functions are possible. The Gower distance (Gower 1971) is a good alternative when a data set has non-numeric predictors. ?sec-cls-knn discusses this metric in more detail.\n\n\n\n\n\n\nWarning\n\n\n\nWhile this analysis nicely illustrates the dissimilarity sampling process, it is flawed since it ignores the issue of spatial autocorrelation (Mahoney et al. 2023). This is the idea that things close to one another act more similarly than objects farther away. ?sec-spatial-resampling discusses this data-splitting issue in more detail.\n\n\nThere are various other methods to split the data using the predictor set. For example, Kennard and Stone (1969) describes an algorithm that attempts to sequentially select points to be uniformly distributed in the space defined by the splitting variables. Similarly, Vakayil and Joseph (2022) proposed a data splitting method called twinning, where a split of the data is sought that minimizes an aggregate distance between points in the training and testing set. Twinning uses the energy distance of Székely and Rizzo (2013), which measures the equality of distributions, to make the two data sets similar. Any variables can be used in the distance calculations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "href": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.7 Multi-Level Data",
    "text": "3.7 Multi-Level Data\nThere are cases where the rows of a data set may not be statistically independent. This often occurs when multiple data points are collected on individual people, such as\n\nPatients in medical studies may have data collected over time.\nPurchase histories of individual customers in a retail database.\n\nIn these and other situations, the data within a person tend to be correlated. This means that the data from a specific person have a higher correlation than data between people. There are many names for this type of data: multi-level data, hierarchical data, longitudinal data, random effect data, profile data, functional data, and so on. In some cases, there are multiple layers of data hierarchies.\nNote that the variable that indicates the person is generally not a predictor; we would not be making predictions about individual people. People, in this example, are sampled from the broader population. In this case, we are more concerned with the population rather than the individuals sampled from that population.\nThis aspect of the data differentiates it from the neighborhood predictor in the Ames data. The houses within each neighborhood may be more similar to one another than houses between neighborhoods. However, the difference is that we want to make predictions using information from these specific neighborhoods. Therefore, we will include neighborhood as a predictor since the individual neighborhoods are not a selected subset of those in the town; instead, the data contain all of the neighborhoods currently in the city.1\nChapter 9 of Kuhn and Johnson (2019) has a broad discussion on this topic with an illustrative example.\nWhen splitting multi-level data into a training and test set, the data are split at the subject level (as opposed to the row level). Each subject would have multiple rows in the data, and all of the subject’s rows must be allocated to either the training or the test set. In essence, we conduct random sampling on the subject identifiers to partition the data, and all of their data are added to either the training or test set.\nIf stratification is required, the process becomes more complicated. Often, the outcome data can vary within a subject. To stratify to balance the outcome distribution, we need a way to quantify the outcome per subject. For regression models, the mean of each subject’s outcome might be an excellent choice to summarize them. Analogously, the mode of categorical outcomes may suffice as an input into the stratification procedure.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-three-way-split",
    "href": "chapters/initial-data-splitting.html#sec-three-way-split",
    "title": "3  Initial Data Splitting",
    "section": "3.8 Validation Sets",
    "text": "3.8 Validation Sets\nAs previously discussed, validation sets are a separate partition of the data that function as a precursor for the testing set. It allows us to obtain performance estimates on our model(s) during the development cycle. These are commonly used in deep learning and other domains where the initial data sizes range from very large to massive. This additional partition is often created simultaneously with the training and testing sets.\nValidation sets serve the same purpose as resampling methods described in ?sec-resampling and we can consider them single resamples of the training data. Methods like bootstrapping or cross-validation use many alternative versions of the training set to compute performance statistics. When our data are extensive, multiple resamples are computationally expensive without significantly improving the precision of our estimates.\nWithout loss of generalization, we will treat the validation set as a particular case of resampling where there is a single resample of the training set. This difference is not substantive and allows us to have a common framework for measuring model efficacy (before the testing set).\nWe’ll see validation sets discussed in ?sec-validation and used in Sections TODO and TODO.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#chapter-references",
    "href": "chapters/initial-data-splitting.html#chapter-references",
    "title": "3  Initial Data Splitting",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBaggerly, K, J Morris, and K Coombes. 2004. “Reproducibility of SELDI-TOF Protein Patterns in Serum: Comparing Datasets from Different Experiments.” Bioinformatics 20 (5): 777–85.\n\n\nClark, R. 1997. “OptiSim: An Extended Dissimilarity Delection Method for Finding Diverse Representative Subsets.” Journal of Chemical Information and Computer Sciences 37 (6): 1181–88.\n\n\nDe Cock, D. 2011. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19 (3).\n\n\nGower, J. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71.\n\n\nHawkins, D, S Basak, and D Mills. 2003. “Assessing Model Fit by Cross-Validation.” Journal of Chemical Information and Computer Sciences 43 (2): 579–86.\n\n\nKapoor, S, and A Narayanan. 2023. “Leakage and the Reproducibility Crisis in Machine-Learning-Based Science.” Patterns 4 (9).\n\n\nKaufman, S, S Rosset, C Perlich, and O Stitelman. 2012. “Leakage in Data Mining: Formulation, Detection, and Avoidance.” ACM Transactions on Knowledge Discovery from Data 6 (4): 1–21.\n\n\nKennard, R W, and L A Stone. 1969. “Computer Aided Design of Experiments.” Technometrics 11 (1): 137–48.\n\n\nKohavi, R. 1995. “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.” International Joint Conference on Artificial Intelligence 14: 1137–45.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nKuhn, M, and J Silge. 2022. Tidy Modeling with R. O’Reilly Media, Inc.\n\n\nMahoney, M J, L K Johnson, J Silge, H Frick, M Kuhn, and C M Beier. 2023. “Assessing the Performance of Spatial Cross-Validation Approaches for Models of Spatially Structured Data.” arXiv.\n\n\nMartin, J, and D Hirschberg. 1996. “Small Sample Statistics for Classification Error Rates I: Error Rate Measurements.” Department of Informatics and Computer Science Technical Report.\n\n\nMartin, T, P Harten, D Young, E Muratov, A Golbraikh, H Zhu, and A Tropsha. 2012. “Does Rational Selection of Training and Test Sets Improve the Outcome of QSAR Modeling?” Journal of Chemical Information and Modeling 52 (10): 2570–78.\n\n\nMolinaro, A. 2005. “Prediction Error Estimation: A Comparison of Resampling Methods.” Bioinformatics 21 (15): 3301–7.\n\n\nSzékely, G J, and M L Rizzo. 2013. “Energy Statistics: A Class of Statistics Based on Distances.” Journal of Statistical Planning and Inference 143 (8): 1249–72.\n\n\nVakayil, A, and V R Joseph. 2022. “Data Twinning.” Statistical Analysis and Data Mining: The ASA Data Science Journal 15 (5): 598–610.\n\n\nWillett, P. 1999. “Dissimilarity-Based Algorithms for Selecting Structurally Diverse Sets of Compounds.” Journal of Computational Biology 6 (3): 447–57.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#footnotes",
    "href": "chapters/whole-game.html#footnotes",
    "title": "2  The Whole Game",
    "section": "",
    "text": "For event time data, it is possible to get incomplete data due to censoring. Suppose that, after 10 minutes you don’t have your order yet. You don’t know the exact delivery time but you do know that it is at least 10 minutes. There are specialized tools for analyzing and modeling censored data. Thankfully, all of our data are complete since we know the exact time of delivery.↩︎\nOverall, we advise looking at the other columns in the data after the splitting.↩︎\nThese are described more in ?sec-outcome-transformation.↩︎\nThe two strategies for creating flexible, smooth fits are splines and generalized additive models. These are described in detail in Sections ?sec-splines and ?sec-cls-gam, respectively. If you have used the ggplot2 function geom_smooth() function, you’ve used a smoother. ↩︎\nDiscussed in ?sec-reg-metrics↩︎\nIf we want to make inferences on the parameters, such as p-values and confidence intervals, we we’ll need to make more specific assumptions (such as normality of the errors).↩︎\nDifferent ways to convert categorical predictors to numeric predictors are detailed in ?sec-categorical-predictors.↩︎\n?sec-splines↩︎\nThe details for regression trees and Cubist are in ?sec-reg-trees↩︎\nSee Sections ?sec-cls-nnet and ?sec-reg-nnet for discussions and details.↩︎\nThis model description is deliberately vague (so as not to confuse readers who are new to this model). Other details: each model could be trained for up to 5,000 epochs. However, a small amount of the training set was randomly set aside to monitor the sums of squared errors at each epoch. Early stopping was triggered when this out-of-sample loss degraded for 10 consecutive epochs. A rectified linear unit activation function was used between the predictor and hidden layers. A cyclic learning rate scheduler was used with a maximum rate of 0.1 and a weight decay value of 0.01 was applied. In a more complete analysis, all of these values would also be tuned.↩︎\nThe noisy profile suggests that our validation set might not be large enough to differentiate between subtle differences within or between models. If the problem required more precision in the performance statistics, a more thorough resampling method would be appropriate, such as cross-validation. These are described in Chapter ?sec-resampling.↩︎\nTo some degree, this is an artificially optimistic estimate since we are using the same data to pick the best result and estimate the model’s overall performance. This is called optimization bias and it is discussed more in ?sec-nested-resampling. For the characteristics of these data, the optimism is likely to be small, especially compared to the noise in the MAE results.↩︎\nWe will revisit this statement in ?sec-comparing-boot, where methods for estimating confidence intervals for validation and test results are presented.↩︎\nWhen resampling is used in place of a validation set, ?sec-resampling has a similar diagram that reflects the subtle differences in data usage.↩︎\nIn fact, the day-to-day users of the data are the best resources for feature engineering.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#footnotes",
    "href": "chapters/initial-data-splitting.html#footnotes",
    "title": "3  Initial Data Splitting",
    "section": "",
    "text": "If you are familiar with non-Bayesian approaches to multi-level data, such as mixed effects models, this is the same as the difference between random and fixed effects. ↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Non-tabular data are later in this chapter.↩︎\nThis is a linear regression model.↩︎\nThis is a smaller version of a larger image that is 1392 pixels by 1040 pixels.↩︎\nThe original image would produce 2,895,360 flattened features.↩︎\nAlso known as “discrete” or “nominal” data.↩︎\nA note to readers: these ?sec-* references are to sections or chapters that are planned by not (entirely) written yet. They will resolve to actual references as we add more content. ↩︎\nTo be honest, the literature in this area has been fairly poor (as of this writing).↩︎\nThat said, simple neural network models can be very effective. While still a bit more high maintenance than other models, it’s a good idea to give them a try (as we will see in the following chapter).↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  }
]