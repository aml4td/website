[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "",
    "text": "Preface\nWelcome! This is a work in progress. We want to create a practical guide to creating quality predictive models from tabular data. We’ll publish materials here as we create them and welcome community contributions in the form of discussions, suggestions, and edits.\nWe also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).\nOur intention is to write these materials and, when we feel we’re done, pick a publishing partner to produce a print version.\nThe book takes a holistic view of this process and focuses on a few areas that are usually left out of similar works. For example, the effectiveness of the model can be driven by how the predictors are represented. We tightly couple feature engineering methods with machine learning models. Also, quite a lot of work happens after we have determined our best model and created the final fit. Post-modeling activities are also described here.\nTo cite this work, we suggest:\n@online{aml4td,\n  Author = {Kuhn, M and Johnson, K},\n  title = {{Applied Machine Learning for Tabular Data}},\n  year = {2023},\n  url = { https://aml4td.org},\n  urldate = {2023-11-20}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "License",
    "text": "License\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Our goal is to have an open book where people can reuse and reference the materials but can’t just put their names on them and resell them (without our permission).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Intended Audience",
    "text": "Intended Audience\nOur intended audience is data analysts of many types: statisticians, data scientists, laboratory scientists, and anyone else who needs to create a model for prediction. We don’t expect readers to be experts in these methods or the math behind them. Our approach is applied. We want readers to have intuition about what are good and bad ideas for their data and what to look out for.\nSome background in modeling and statistics is required. Having seen or used basic regression models is good, and understanding basic statistical concepts, such as variance, correlation, populations, samples, etc., is needed. There is some mathematical notation, so you’ll need to be able to read those. There are a few more statistically sophisticated sections, but these are not pivotal topics.\nIf you want a more theoretical treatment of machine learning models, we recommend Hastie et al. (2017). Other books for learning more about machine learning are Bishop and Nasrabadi (2006), Arnold et al. (2019) and, for more of a deep learning focus, Goodfellow et al. (2016).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#is-there-code",
    "href": "index.html#is-there-code",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Is there code?",
    "text": "Is there code?\nWe definitely want to decouple the content from specific software. One of our other books on modeling had computing sections. They are nice to peruse, and people enjoyed having them there. However, they can quickly become outdated and take up a lot of space.\nThe sources for the material use R, and early adopters can find them in the GitHub repository.\nHowever, we will create other websites with computing supplements for R and Python (hopefully more in the future). Those will become finalized around the time we finish the first version of the materials.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-can-i-ask-questions",
    "href": "index.html#how-can-i-ask-questions",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "How can I ask questions?",
    "text": "How can I ask questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like cross-validated. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what I call Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.\nIf you think there is a bug, please file an issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Can I contribute?",
    "text": "Can I contribute?\nThere is a contributing page with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list. It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto was used to compile and render the materials\n\nQuarto 1.4.376\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.8: OK\n      Dart Sass version 1.55.0: OK\n      Deno version 1.33.4: OK\n[✓] Checking versions of quarto dependencies......OK\n[✓] Checking Quarto installation......OK\n      Version: 1.4.376\n[✓] Checking tools....................OK\n      TinyTeX: (external install)\n      Chromium: (not installed)\n[✓] Checking LaTeX....................OK\n      Using: TinyTex\n      Version: 2022\n[✓] Checking basic markdown render....OK\n[✓] Checking Python 3 installation....OK\n      Version: 3.11.4\n      Jupyter: (None)\n      Jupyter is not available in this Python installation.\n[✓] Checking R installation...........OK\n      Version: 4.3.1\n      LibPaths:\n        - /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n      knitr: 1.45\n      rmarkdown: 2.25\n[✓] Checking Knitr engine render......OK\n\nR version 4.3.1 (2023-06-16) was used for the majority of the computations. torch 1.13.1 was also used. The versions of the primary R modeling and visualization packages used here are:\n\n\n\n\n\n\n\n\n\n\napplicable (0.1.0)\nbaguette (1.0.1.9000)\nbestNormalize (1.9.0)\n\n\nbonsai (0.2.1.9000)\nbroom (1.0.5)\nbrulee (0.2.0)\n\n\nC50 (0.1.8)\nCubist (0.4.2.1)\nDALEXtra (2.3.0)\n\n\ndbarts (0.9-23)\ndesirability2 (0.0.1)\ndials (1.2.0.9000)\n\n\ndimRed (0.2.6)\ndiscrim (1.0.1.9000)\ndoMC (1.3.8)\n\n\ndplyr (1.1.3)\ne1071 (1.7-13)\nearth (5.3.2)\n\n\nembed (1.1.1)\nfinetune (1.1.0.9001)\nGA (3.2.3)\n\n\ngganimate (1.0.8)\nggiraph (0.8.7)\nggplot2 (3.4.4)\n\n\nglmnet (4.1-8)\ngt (0.10.0)\nhardhat (1.3.0.9000)\n\n\nipred (0.9-14)\nirlba (2.3.5.1)\nkernlab (0.9-32)\n\n\nkknn (1.3.1)\nklaR (1.7-2)\nlightgbm (3.3.5)\n\n\nmda (0.5-4)\nmgcv (1.9-0)\nmixOmics (6.24.0)\n\n\nmodeldata (1.2.0.9000)\nmodeldatatoo (0.1.0)\npamr (1.56.1)\n\n\nparsnip (1.1.1.9002)\npartykit (1.2-20)\npatchwork (1.1.3)\n\n\nplsmod (1.0.0.9000)\nprobably (1.0.2)\npROC (1.18.4)\n\n\npurrr (1.0.2)\nragg (1.2.5)\nranger (0.15.1)\n\n\nrecipes (1.0.8.9000)\nrpart (4.1.19)\nrsample (1.2.0.9000)\n\n\nrstudioapi (0.15.0)\nrules (1.0.2.9000)\nsparsediscrim (0.3.0)\n\n\nsparseLDA (0.1-9)\nspatialsample (0.5.1.9000)\nsplines2 (0.5.0)\n\n\nstacks (1.0.3.9000)\nstopwords (2.3)\ntextfeatures (0.3.3)\n\n\ntextrecipes (1.0.3)\nthemis (1.0.2.9000)\ntidymodels (1.1.1.9000)\n\n\ntidyposterior (1.0.0)\ntidyr (1.3.0)\ntorch (0.11.0)\n\n\ntune (1.1.2.9000)\nusethis (2.2.2)\nVBsparsePCA (0.1.0)\n\n\nworkflows (1.1.3.9000)\nworkflowsets (1.0.1)\nxgboost (1.7.5.1)\n\n\nxrf (0.2.2)\nyardstick (1.2.0.9001)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#chapter-references",
    "href": "index.html#chapter-references",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nArnold, T., Kane, M., and Lewis, B. (2019), A Computational Approach to Statistical Learning, CRC Press.\n\n\nBishop, C. M., and Nasrabadi, N. M. (2006), Pattern Recognition and Machine Learning, Springer.\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016), Deep learning, MIT press.\n\n\nHastie, T., Tibshirani, R., and Friedman, J. (2017), The Elements of Statistical Learning: Data Mining, Inference and Prediction, Springer.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/news.html#errata",
    "href": "chapters/news.html#errata",
    "title": "News",
    "section": "Errata",
    "text": "Errata\nNo reported edits (yet).",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#changelog",
    "href": "chapters/news.html#changelog",
    "title": "News",
    "section": "Changelog",
    "text": "Changelog\n\n2023-11-17\nUpdated renv snapshot.\n\n\n2023-10-09\nFirst committed versions.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/contributing.html#software",
    "href": "chapters/contributing.html#software",
    "title": "Contributing",
    "section": "Software",
    "text": "Software\nRegarding R packages, the repository has a DESCRIPTION file as if it were an R package. This lets us specify precisely what packages and their versions should be installed. The packages listed in the imports field contain packages for modeling/analysis and packages used to make the website/book. Some basic system requirements are likely needed to install packages: Fortran, gdal, and others.\nThe main requirements are as follows.\n\nQuarto\nQuarto is an open-source scientific and technical publishing system. Quarto version 1.4.376 is used to compile the website.\nWe also use a few Quarto extensions. These should be installed from the project’s root directory via:\nquarto add quarto-ext/fontawesome\nquarto add quarto-ext/fancy-text\nquarto add leovan/quarto-pseudocode\n\n\nR and renv\nR version 4.3.1 (2023-06-16) is what we are currently using. We suggest using rig to manage R versions. There are several IDEs that you can use. We’ve used RStudio (&gt;= 2023.6.1.524).\nThe current strategy is to use the renv (&gt;= version 1.0.2) package to make this project more isolated, portable and reproducible.\nTo get package dependencies installed…\n\nWhen you open the website.Rproj file, the renv package should be automatically installed/updated (if neded). For example:\n# Bootstrapping renv 1.0.3 ---------------------------------------------------\n- Downloading renv ... OK\n- Installing renv  ... OK\n\nThe following package(s) will be installed:\n- BiocManager [1.30.22]\nThese packages will be installed into \"~/content/aml4td/renv/library/R-4.3/x86_64-apple-darwin20\".\nif you try to compile the book, you probably get and error:\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nYou can get more information using renv::status() but you can get them installed by first running renv::activate(). As an example:\n&gt; renv::activate()\n\nRestarting R session...\n\n- Project '~/content/aml4td' loaded. [renv 1.0.3]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nSince we have package versions recorded in the lockfile, we can installed them using renv::restore(). Here is an example of that output:\n&gt; renv::restore() \nThe following package(s) will be updated:\n\n# Bioconductor ---------------------------------------------------------------\n- mixOmics         [* -&gt; mixOmicsTeam/mixOmics]\n\n# CRAN -----------------------------------------------------------------------\n- BiocManager      [1.30.22 -&gt; 1.30.21.1]\n- lattice          [0.21-9 -&gt; 0.21-8]\n- Matrix           [1.6-1.1 -&gt; 1.6-0]\n- nlme             [3.1-163 -&gt; 3.1-162]\n- rpart            [4.1.21 -&gt; 4.1.19]\n- survival         [3.5-7 -&gt; 3.5-5]\n- abind            [* -&gt; 1.4-5]\n\n&lt;snip&gt;\n\n- zip              [* -&gt; 2.2.0]\n- zoo              [* -&gt; 1.8-12]\n\n# GitHub ---------------------------------------------------------------------\n- BiocParallel     [* -&gt; Bioconductor/BiocParallel@devel]\n- BiocVersion      [* -&gt; Bioconductor/BiocVersion@devel]\n- modeldatatoo     [* -&gt; tidymodels/modeldatatoo@HEAD]\n- parsnip          [* -&gt; tidymodels/parsnip@HEAD]\n- usethis          [* -&gt; r-lib/usethis@HEAD]\n\n# RSPM -----------------------------------------------------------------------\n- bslib            [* -&gt; 0.5.1]\n- fansi            [* -&gt; 1.0.5]\n- fontawesome      [* -&gt; 0.5.2]\n- ggplot2          [* -&gt; 3.4.4]\n- htmltools        [* -&gt; 0.5.6.1]\n- withr            [* -&gt; 2.5.1]\n\nDo you want to proceed? [Y/n]: y\n\n# Downloading packages -------------------------------------------------------\n- Downloading BiocManager from CRAN ...         OK [569 Kb in 0.19s]\n- Downloading nlme from CRAN ...                OK [828.7 Kb in 0.19s]\n- Downloading BH from CRAN ...                  OK [12.7 Mb in 0.4s]\n- Downloading BiocVersion from GitHub ...       OK [826 bytes in 0.37s]\n\n&lt;snip&gt;\n\nDepending on whether you have to install packages from source, you may need to install some system dependencies and try again (I had to install libgit2 the last time I did this).\nOnce you have everything installed, we recommend installing the underlying torch computational libraries. You can do this by loading the torch package A download will automatically begin if you need one.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#contributor-list",
    "href": "chapters/contributing.html#contributor-list",
    "title": "Contributing",
    "section": "Contributor List",
    "text": "Contributor List\nThe would like to thank users who have made a contribution to the project:",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular",
    "href": "chapters/introduction.html#sec-nontabular",
    "title": "1  Introduction",
    "section": "1.1 Non-Tabular Data",
    "text": "1.1 Non-Tabular Data\nThe columns shown in Table 1.1 are defined within a small scope. For example, the delivery hour column encapsulates everything about that order characteristic:\n\nYou don’t need any other columns to define it.\nA single number is all that is required to describe that information.\n\nNon-tabular data does not have the same properties. Consider images, such as the one shown Figure 1.1 that depicts four cells from a biological experiment (Yu et al. 2007). These cells that have been stained with pigments that fluoresce at different wavelengths to “paint” different parts of the cells. In this image, blue reflects the part of the cell called the cytoskeleton, which encompasses the entire cell body. The red color corresponds to a stain that only attaches itself to the contents of the cell nucleus, specifically DNA. This image3 is 371 pixels by 341 pixels with quantification of two colors.\n\n\n\n\n\n\n\n\nFigure 1.1: An example image of several cells using two colors. Image from Yu et al. (2007).\n\n\n\n\n\nFrom a data perspective, this image is represented as a three-dimensional array (371 rows, 341 columns, and 2 colors). Probably the most important attribute of this data is the spatial relationships between pixels. It is critical to know where each value in the 3D array resides, but knowing what the nearby values are is at least as important. We could “flatten” the array into 371 \\(\\times\\) 341 \\(\\times\\) 2 = 253,022 columns to use in a table4. However, this would break the spatial link between the data points; each column is no longer as self-contained as the columns from the delivery data. The result is that we have to consider the 3D array as the data instead of each pixel location/color combination.\nAdditionally, images often have different dimensions which results in different array sizes. From a tabular data perspective, a flattened version of such data would have a different number of columns for different-sized images.\nVideos are an extension of image data if we were to consider it a four-dimensional array (with time as the additional dimension). The temporal aspects of such data are critical to understanding and predicting values.\nText data is often thought of as non-tabular data. As an example, consider text from Sanderson (2017):\n\n“The most important step a man can take. It’s not the first one, is it? It’s the next one. Always the next step, Dalinar.”\n\nWith text data, it is common to define the “token”: the unit of text that should be analyzed. This could be a paragraph, sentence, word, etc. Suppose that we used sentences as tokens. In this case, the table for this quote would have four rows. Where do we go from here? The sequence of words (or characters) is likely to be important, and this makes a case for keeping the sentences as strings. However, as will be seen shortly, we might be able to convert these four tokens to numeric columns in a way that preserves the information required for prediction.\nPerhaps a better text example relates to the structure of chemicals. A chemical is a three-dimensional molecule but is often described using a SMILES string (Engel and Gasteiger 2018,chap. 3). This is a textual description of molecule that lists its elements and how they relate to one another. For example, the SMILES string CC(C)CC1=CC=C(C=C1)C(C)C(=O)O defines the anti-inflammatory drug Ibuprofen. The letters describe the elements (C is carbon, O is oxygen, etc) and the other characters defines the bonds between the elements. The character sequence within this string is critical to understanding the data.\nOur motives for categorizing data as tabular and non-tabular are related to how we choose an appropriate model. Our opinion is that most modeling projects involve tabular data (or data that can be effectively represented as tabular). Models for non-tabular data are very specialized and, while these ML approaches are the most discussed in the social media, they tend not to be the best approach for tabular data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular-convert",
    "href": "chapters/introduction.html#sec-nontabular-convert",
    "title": "1  Introduction",
    "section": "1.2 Converting Non-Tabular Data to Tabular",
    "text": "1.2 Converting Non-Tabular Data to Tabular\nIn some situations, non-tabular data can be effectively converted to a tabular format, depending on the specific problem and data.\nFor some modeling projects using images, we might not be interested in every pixel. For Figure 1.1, we really care about the cells in the image. It is important to understand within-cell characteristics (e.g. shape or size) and some between-cell information (e.g., the distance between cells). The vast majority of the pixels don’t need to be analyzed.\nWe can pre-analyze the 3D arrays to determine which pixels in an image correspond to specific cells (or nuclei within a cell). Segmentation (Holmes and Huber 2018) is the process of estimating regions of an image that define some object(s). Figure 1.2 shows the same cells with lines generated from a simple segmentation.\n\n\n\n\n\n\n\n\nFigure 1.2: The cell segmentation results for the four neuroblastomas shown in Figure 1.1.\n\n\n\n\n\nOnce our cells have been segmented, we can compute various statistics of interest based on size, shape, or color. If a cell is our unit of data, we can create a tabular format where rows are cells and columns are cell characteristics (Table 1.2). The result is a data table that has more rows than the original non-tabular structure since there are multiple cells in an image. There are far fewer columns but these columns are designed to be more informative than the raw pixel data since the researchers define the cell characteristics that the desire.\n\n\n\n\n\n\n\n\n\n\n\n\nID\nEccentricity\n\nArea\n\nIntensity\n\n\nNucleus\nCell\nNucleus\nCell\nNucleus\nCell\n\n\n\n\n17\n0.494\n0.836\n\n3,352\n11,699\n\n0.274\n0.155\n\n\n18\n0.708\n0.550\n\n1,777\n4,980\n\n0.278\n0.210\n\n\n21\n0.495\n0.802\n\n1,274\n3,081\n\n0.326\n0.218\n\n\n22\n0.809\n0.975\n\n1,169\n3,933\n\n0.583\n0.229\n\n\n\n\n\n\n\n\n\nTable 1.2: Cells, such as those found in Figure 1.1, translated into a tabular format using three features for the nuclear and non-nuclear regions of the segmented cells.\n\n\n\n\nThis conversion to tabular data isn’t appropriate for all image analysis problems. If you want to know if an image contains a cat or not it probably isn’t a good idea.\nLet’s also consider the text examples. For the book quote, Table 1.3 shows a few simple predictors that are tabular in nature:\n\nPresence/absence columns for each word in the entire document. Table 1.3 shows columns corresponding to the words “always”, “can”, and “take”. These are represented as counts.\nSentence summaries, such as the number of commas, words, characters, first-person speech, etc.\n\nThere are many more ways to represent these data. More complex numeric embeddings that are built on separate large databases can reflect different parts of speech, text sequences (called n-grams), and others. Hvitfeldt and Silge (2021) describes these and other tools for analyzing tabular versions of text.\n\n\n\n\n\n\n\n\n\n\n\n\nSentence\n\"always\"\n\"can\"\n...\n\"take\"\nCharacters\nCommas\n\n\n\n\nThe most important step a man can take.\n0\n1\n...\n1\n32\n0\n\n\nIt's not the first one, is it?\n0\n0\n...\n0\n24\n1\n\n\nIt's the next one.\n0\n0\n...\n0\n15\n0\n\n\nAlways the next step, Dalinar.\n1\n0\n...\n0\n26\n1\n\n\n\n\n\n\n\n\n\nTable 1.3: An example of converting non-numeric data to a numeric, tabular format.\n\n\n\n\nFor the chemical structure of Ibuprofen, there is a rich field of molecular descriptors (Engel and Gasteiger 2018; Leach and Gillet 2003,chap. 3) that can be used to produce thousands of informative predictor columns. For example, we might be interested in the size of the molecule (perhaps measured by surface area), its electrical charge, or whether it contains specific sub-structures.\nThe process of determining the best data representations is called feature engineering. This is a critical task in machine learning and is often overlooked. Part 2 of this book spends some time looking at feature engineering methods (once the data are in a tabular format).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-types",
    "href": "chapters/introduction.html#sec-model-types",
    "title": "1  Introduction",
    "section": "1.3 Models and Machine Learning",
    "text": "1.3 Models and Machine Learning\nSince we have mentioned linear regression and regression trees, it makes sense to talk about possible machine learning scenarios. There are many, but we will focus on two types.\nRegression models have a numeric outcome (e.g., delivery time) and our goal is to predict that value. There are some cases where our outcome is a number, but we are mostly interested in ranking new data (as opposed to accurately predicting the value).\nThe other scenario that we discuss is classification models. In this case, the outcome is a qualitative categorical value (e.g., “cat” or “no cat” in an image). One interesting aspect of these models is that there are two main types of predictions:\n\nhard predictions correspond to the original outcome value.\nsoft predictions return the probability of each possible outcome (e.g., there is a 27% chance that a cat is in the image).\n\nThe number of categories is important; some models are specific to two classes, while others can accommodate any number of outcome categories. Most classification outcomes have mutually exclusive classes (one of all possible class values). There are also ordered class values where, while categorical, there is some type of ordering. For example, tumors are often classified in stages, with stage I being a localized malignancy and stage IV corresponding to one that has spread throughout the body.\nMulti-label outcomes can have multiple classes per row. For example, if we were trying to predict which languages that someone can speak, multilingual people would have multiple outcome values.\nWe are focused on prediction problems. This implies that our data will contain an outcome. Such scenarios falls under the broader class of supervised models. An unsupervised model is one where the is no true outcome value. Tools such as clustering, principal component analysis (PCA), and others look to quantify patterns in the data without some prediction goal. We’ll encounter unsupervised methods in our pre-model activities such as feature engineering described in Part 2.\nSeveral synonyms for machine learning include statistical learning, predictive modeling, pattern recognition, and others. While many specific models are designed for machine learning, we suggest that users think about machine learning as the problem and not the solution. Any model that can adequately predict the outcome values deserves the title. A commonly used argument for this position is that two models that are conventionally thought of as tools to make inferences (e.g., p-values) are linear regression and logistic regression. Equation 1.1 is a simplistic linear regression model. As will be seen in future chapters, linear and logistic regression models can produce highly accurate predictions for complex problems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-data-types",
    "href": "chapters/introduction.html#sec-data-types",
    "title": "1  Introduction",
    "section": "1.4 Data Characteristics",
    "text": "1.4 Data Characteristics\nIt’s helpful to discuss different nomenclature and types of data. With tabular data, there are a number of ways to refer to a row. The terms sample, instance, observation, and/or data point are commonly used. The first, “sample”, can be used in a few different contexts; we will also use it to describe a subset of a greater data collection (as in “we took a random sample of the data”.).\nFor predictors, there are also many synonyms. In statistical nomenclature, independent variable or covariate are used. More generic terms are attribute and descriptor. The term feature is generally equated to predictors, and we’ll use both. An additional adjective may also be assigned to “feature”. Consider the date and time of the orders shown in Table 1.1. The “original feature” is the data/time column in the data. The table includes two “derived features” in the form of the decimal hour and the day of the week. This distinction will be important when discussing feature engineering, importance scores, and explaining models.\nOutcomes/target columns can also be called the dependent variable or response.\nFor numeric columns, we will make some distinctions. A real number is numeric with potentially fractional values (e.g., time to delivery). Integers are whole numbers, often reflecting counts. Binary data take two possible values that are almost always represented with zero and one.\nDense data describes a collection of numeric values with many possible values, also reflected by the delivery time and distances in Table 1.1. Sparse data have fewer possible values or when only a few values are contained in the observed data. The product counts in the delivery data are good examples. They are integer counts and are mostly represented by zeros and some ones. There are very few values greater than two.\nWhile numeric data are quantitative, qualitative data cannot be represented by a numeric scale5. The day of the week data are categories with seven possible values. Binary categorical data have two categories, such as alive/dead.\nThe item columns in Table 1.1 are interesting too. If the order could only contain one item, we might configure that data with a qualitative single column. However, these data are multiple-choice and, as such, are shown in multiple integer columns with zero, reflecting that it was not in the order.\nvalue types, skewness, colinearity, distributions, ordinal",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-pipeline",
    "href": "chapters/introduction.html#sec-model-pipeline",
    "title": "1  Introduction",
    "section": "1.5 What Defines the Model?",
    "text": "1.5 What Defines the Model?\nWhen discussing the modeling process, it is traditional to think the parameter estimation steps occur only when the model is fit. However, as more complex techniques are used to prepare predictor values for the model, it is crucial to understand that important quantities are also being estimated before the model. Here are a few examples:\n\nImputation methods, discussed in Chapter TODO, create sub-models that estimate missing predictor values. These are applied to the data before modeling to complete the data set.\nFeature selection tools (Kuhn and Johnson 2019) often compute measures of predictor importance to remove uninformative predictors before passing the data to the model.\nEffect encoding tools, discussed in Section TODO, embed the effect of the predictor on the outcome into the model as a predictor.\nPost-model adjustments to predictions, such as model calibration (Section 2.6 and TODO), are examples of postprocessing operations.\n\n\n\n\n\n\n\nThe modeling pipeline\n\n\n\nThese operations can profoundly affect the overall results, yet they are not usually considered part of “the model.” We’ll use the more expansive term “modeling pipeline” to describe any estimation for the model or operations before or after the model:\n\nmodel pipeline = preprocessing + supervised model + postprocessing\n\n\n\nThis is important to understand: a common pitfall in ML is only validating the model fitting step instead of the whole process. This can lead to performance statistics that can be extraordinarily optimistic and might deceive a practitioner into believing that their model is much better than it truly is.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-deep-learning",
    "href": "chapters/introduction.html#sec-deep-learning",
    "title": "1  Introduction",
    "section": "1.6 Deep Learning",
    "text": "1.6 Deep Learning\nWhile many models can be used for machine learning, neural networks are the elephant in the room. These complex, nonlinear models are frequently used in machine learning to the point where, in some domains, these models are synonymous with ML. One class of neural network models is deep learning (DL) models (Goodfellow et al. 2016; Prince 2023). Here, “deep” means that the models have many structural layers, resulting in extraordinary amount of complexity defined by the tools.\nFor example, Figure 1.3 shows a diagram of a type of deep learning model called a convolutional neural network that is often used for making predictions on images. It consists of several layers that manipulate the data. This particular model, dubbed VGG16, was proposed by Simonyan and Zisserman (2014).\nThe color image on the left is a data point. The first block has three layers. The two larger yellow boxes are convolutional layers. They move a small multidimensional array over the larger data array to compute localized features. For example, for an image that is 150 x 150 pixels, a convolution that uses a 3x3 filter will move this small rectangle across the array. At the (2,2) position, it takes the average of all adjacent pixel locations.\nAfter the convolutional operations, a smaller pooling layer is shown in orange. Pooling, usually by finding the maximum feature value across different image regions, compresses/downsamples the larger feature set so a smaller size without diminishing features with large signals.\nThe next three blocks consist of similar convolutional and pooling layers but of different dimensions. The idea behind these blocks is to reduce the image data to a smaller dimension width/height dimension in a way that extracts potentially important hierarchical features from the image.\nAfter block 5, the data are flattened into a single array, similar to the columns in a tabular data set. Up to this point, the network has been focused on preprocessing the image data. If our initial image were 150 pixels by 150 pixels, the flattening process would result in over 8 thousand predictors that would be used in a supervised ML model. The network would have over 14 million model parameters to estimate from the data to get to this point.\nAfter flattening, the final two layers correspond to the classic supervised neural network model (Bishop 1995). The original paper used four large supervised neural network layers; for simplicity, Figure 1.3 shows a single dense layer. This can add dozens of millions of additional parameters for a modest-sized dense layer.\n\n\n\n\n\n\n\n\nFigure 1.3: A simplified version of the convolutional neural network deep learning model proposed by Simonyan and Zisserman (2014). This version only includes a single dense layer (instead of four).\n\n\n\n\n\nThere are many types of deep learning models. Another example is recurrent neural networks, which create layers that are proficient at modeling data that occur in sequences such as time series data or sentences (i.e., a sequence of words).\nVery sophisticated deep learning models have generated, by far, the best predictive ability for images and similar data. It would seem natural to think these models would be just as effective for tabular data. However, this has not been the case. Machine learning competitions, which are very different from day-to-day machine learning work, have shown that many other models can consistently do as well or likely better on tabular data sets. This has generated an explosion of literature6 and social media posts that try to explain why this is the case. For example, see Borisov et al. (2022) and McElfresh et al. (2023).\nWe believe that exceptionally complex DL models are handicapped in several ways when taken out of the environments where they have been successful.\nFirst, the complexity of these models requires extreme amounts of data, and the constraints of such large data sets drive how the models are developed.\nFor example, keeping very large data sets in a computer’s memory is almost impossible. As discussed earlier, we use the data to find optimal values of our parameters for some objective functions (such as classification accuracy). Using traditional optimization procedures, this is very difficult if the data cannot be accessed simultaneously. Deep learning models use more modern optimization methods, such as stochastic gradient descent (SGD) methods, for estimating parameters. It does not simultaneously hold all of the data in memory and fits the model on small incremental batches of data. This is a very effective technique, but it is not an approach one would use with data set sizes that are more common (e.g., less than a million data points). SGD is less effective for smaller data sizes than traditional in-memory optimizers. Another consequence of huge data requirements is that model training can take an excruciatingly long time. As a result, efficient model development can require specialized hardware. It can also drive how models are optimized and compared. DL models are often optimized by sequentially adjusting the model. Other ML models can be optimized with a more methodical simultaneous design that is often more efficient, systematic, and effective than sequential optimizers used with deep learning.\nAnother issue with deep learning models is that their size and mathematical structure are associated with a very difficult optimization problem; their objective function is non-convex. We’d like to have an optimization problem that has some guarantees that a global optimal value exists (or can be found). That is often not the case for these models. For example, if we want to estimate the VGG16 model parameters with the goal of maximizing the classification accuracy for finding dogs in an image, it can be difficult to get a reliable example and, if we do, we don’t know if it is really the correct answer.\nBecause the optimization problem is so difficult, much of the user’s time is spent optimizing the model parameters or tuning the network structure to find the best results. When combined with how long it takes for a single model fit, deep learning models are very high maintenance compared to other models when used on tabular data. They can probably produce a model that is marginally better than the others, but doing so requires an inordinate amount of time, effort, and constraints. DL models are superior for certain hard problems with large data sets. For other situations, other ML models can go probably go further and faster7.\nFinally, there are some data-centric reasons that deep learning models don’t automatically translate to tabular data. There are characteristics of tabular data that don’t often occur in stereotypical DL applications. For example:\n\nIn some cases, a group of predictors might be highly correlated with one another. This can compromise some of the mathematical operations used to estimate parameters in neural networks (and many other models).\nUnless we have extensive prior experience with our data, we don’t know which are informative and which are irrelevant for predicting an outcome. As the number of non-informative predictors increases, the performance of neural networks decreases. See Section 19.1 of Kuhn and Johnson (2013).\nPredictors with “irregular” distributions, such as skewness or heavy tails, can harm performance (McElfresh et al. 2023).\nMissing predictor data are not naturally handled in basic neural networks.\n“Small” sample sizes or data dimensions with at least as many predictors as rows.\n\nas well as others. These issues can be overcome by adding layers to a network designed to counter specific challenges. However, in the end, this ends up adding more complexity. We propose that there are a multitude of other models, and many of them are resistant or robust to these types of data-specific characteristics. It is better to have different tools in our toolbox; making a more complex hammer may not help us put a screw in a wall.\nThe deep learning literature has resulted in many collateral benefits for the broader machine learning field. We’ll describe a few techniques that have improved machine learning, especially for smaller neural networks.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-aspects",
    "href": "chapters/introduction.html#sec-aspects",
    "title": "1  Introduction",
    "section": "1.7 Modeling Philosophies",
    "text": "1.7 Modeling Philosophies\ntalk about subject-matter knowledge/intuition vs “let the machine figure it out”. biased versus unbiased model development.\nassistive vs automated usage",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-outline",
    "href": "chapters/introduction.html#sec-outline",
    "title": "1  Introduction",
    "section": "1.8 Outline of These Materials",
    "text": "1.8 Outline of These Materials\nThis work is organized into parts:\n\nIntroduction: The next chapter shows an abbreviated example to illustrate important concepts used later.\nPreparation: These chapters discuss topics such as data splitting and feature engineering. These activities occur before the actual training of the machine learning model but are critically important.\nOptimization: To find the model that has the best performance, we often need to tune them so that they are effective but do not overfit. These chapters describe overfitting, methods to measure performance, and two different classes of optimization methods.\nClassification: Classification models have outcomes that are not numbers (e.g. yes/no, etc). Various models for classification are described in this part.\nRegression: Models that predict some sort of number are often called regression models. These chapter describe such models.\nCharacterization: Once you have a model, how do you describe it or its predictions? How do you know when you should question the results of your model? We’ll address these questions in this part.\nFinalization: Post-modeling activities such as monitoring performance are discussed.\n\nBefore diving into the process of creating a good model, let’s have a short case study. The next chapter is designed to give you a sense of the overall process and illustrate important ideas and concepts that will appear later in the materials.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#chapter-references",
    "href": "chapters/introduction.html#chapter-references",
    "title": "1  Introduction",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nBishop, C. (1995), Neural networks for pattern recognition, Oxford: Oxford University Press.\n\n\nBorisov, V., Leemann, T., Seßler, K., Haug, J., Pawelczyk, M., and Kasneci, G. (2022), “Deep neural networks and tabular data: A survey,” IEEE Transactions on Neural Networks and Learning Systems, IEEE.\n\n\nEngel, T, and Gasteiger, J. (2018), Chemoinformatics : A Textbook , Weinheim: Wiley-VCH.\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016), Deep learning, MIT press.\n\n\nHolmes, S., and Huber, W. (2018), Modern statistics for modern biology, Cambridge University Press.\n\n\nHvitfeldt, E., and Silge, J. (2021), Supervised machine learning for text analysis in R, CRC Press.\n\n\nKuhn, M., and Johnson, K. (2013), Applied Predictive Modeling, Springer.\n\n\nKuhn, M., and Johnson, K. (2019), Feature Engineering and Selection: A Practical Approach for Predictive Models, CRC Press.\n\n\nLeach, A., and Gillet, V. (2003), An Introduction to Chemoinformatics, Springer.\n\n\nMcElfresh, D., Khandagale, S., Valverde, J., Prasad, V., Ramakrishnan, G., Goldblum, M., and White, C. (2023), “When do neural nets outperform boosted trees on tabular data?” arXiv.\n\n\nPrince, S. (2023), Understanding deep learning, MIT press.\n\n\nSanderson, B. (2017), Oathbringer, Tor Books.\n\n\nSimonyan, K., and Zisserman, A. (2014), “Very deep convolutional networks for large-scale image recognition,” arXiv.\n\n\nYu, W, Lee, HK, Hariharan, S, Bu, W., and Ahmed, S. (2007), “CCDB:6843, mus musculus, neuroblastoma.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-delivery-times",
    "href": "chapters/whole-game.html#sec-delivery-times",
    "title": "2  The Whole Game",
    "section": "2.1 Predicting Delivery Time",
    "text": "2.1 Predicting Delivery Time\nThe illustrative example focuses on predicting the food delivery time (i.e., the time from the initial order to receiving the food). There is a collection of data containing 10,012 orders from a specific restaurant. The predictors, previously shown in Table 1.1, include:\n\nThe time, in decimal hours, of the order.\nThe day of the week for the order.\nThe approximate distance between the restaurant and the delivery location.\nA set of 27 predictors that count the number of distinct menu items in the order.\n\nThe outcome is the time1 (in minutes).\nThe next section describes the process of splitting the overall data pool into different subsets and signifies the start of the modeling process. However, how we make that split depends on some aspects of the data. Specifically, it is a good idea to examine the distribution of our outcome data (i.e., delivery time) to determine how to randomly split the data2.\nFigure 2.1 (panel (a)) uses a histogram to examine the distribution of the outcome data. It is right-skewed and there is a subset of long delivery times that stand out from the mainstream of the data. The most likely delivery time is about 27 minutes, but there is also a hint of a second mode a few minutes earlier.\n\n\n\n\n\n\n\n\nFigure 2.1: Histrograms of the training set outcome data with and without a transformation.\n\n\n\n\n\nThis skewness might affect how we split the data and is discussed in the next section. Another important question is: should we model a transformed version of the delivery times? If we have large outlying values, the model might be inappropriately pulled to these values, often at the expense of the overall quality of fit. One approach is to model the logarithm of the times (as shown in Equation 1.1). Figure 2.1 (panel (b)) shows the distribution of the log (base 2) deliveries. It is more symmetric, and the potential second mode is more pronounced. This isn’t a bad idea, and the primary consequence is how we interpret some of the metrics that are used to quantify how well the model fits the data3. In the end, we decided to model the data in the original units but did try the analysis both ways (and did not find huge differences in the results).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-data-spending-whole-game",
    "href": "chapters/whole-game.html#sec-data-spending-whole-game",
    "title": "2  The Whole Game",
    "section": "2.2 Data Spending",
    "text": "2.2 Data Spending\nAn important aspect of machine learning is to use the right data at the right time. We don’t start modeling with the entire pool of data. As will be seen in Section TODO, we need to reserve some data to evaluate the model and make sure that those data do not overlap with those used to fit the model.\nOften, our data spending strategy is to split the data into at least two subsets:\n\nThe training set is used to develop the model pipeline. It is used for investigating, fitting, and comparing them.\nThe test set is a smaller fraction of the data that is only used at the very end to independently validate how well we did.\n\nThe training set is used for a lot of our activities. We do need to evaluate multiple model pipelines along the way. We reserve the test set for this function and should not use it for development. There are two main strategies for characterizing model efficacy during development.\nThe first is to resample the training set. This is an iterative computational tool to find good statistical estimates of model performance using the training set.\nThe other option is to initially split the data into three subsets: the training set, the testing set, and the validation set. The latter is another data partition, smaller than the training set, that is used for evaluating the model repeatedly during development.\nGenerally, we suggest using resampling unless the initial data pool is “sufficiently large.” For these data, 10,012 food orders is probably sufficient to choose a validation set over the resampling approach.\nTo split the data, we’ll allocate 60% for the training set and then use 20% for the validation set, and the remaining 20% for testing. The splitting will be conducted by stratification. To ensure that our delivery time distributions are approximately the same, it is temporarily “binned” into four quartiles. The three-way split is executed within each quartiles and the overall training/validation/testing sets are assembled by aggregating the corresponding data from the quartiles. As a result, the respective sample sizes are 6004/2004/2004 deliveries.\nAs previously stated, the majority of our time is spent with the training set samples. The first step in any model-building process is to understand the data, which can most easily be done through visualizations of the training set.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-eda-whole-game",
    "href": "chapters/whole-game.html#sec-eda-whole-game",
    "title": "2  The Whole Game",
    "section": "2.3 Exploratory Data Analysis",
    "text": "2.3 Exploratory Data Analysis\nOur goal now is to determine if there are aspects of predictors that would affect how we provide them to the model. We should look at individual variables as well as combinations multiple variables.\nTo start, for the dense numeric predictors, how would we characterize their relationship to the outcome? Is it linear, nonlinear, or random? Let’s start with the distance predictor. Figure 2.2(a) shows the values with the “scatter plot smoother” fit. The line is a flexible nonlinear approach that can adapt to any apparent pattern in the plot4. First, the plot shows that the distance distribution is also right-skewed (although a separate histogram would better illustrate this). The smoother shows a slightly nonlinear trend, although the nonlinearity is mainly in the right tail of the distances and might be an artifact of the small sample size in that region. It is an informative variable that we can use in the model.\n\n\n\n\n\n\n\n\nFigure 2.2: Visualizations of relationship between the outcome and several predictors using the training set.\n\n\n\n\n\nPanel (b) has a similar visualization for the order time. In this case, there is another nonlinear trend where the order increases with the hour, then peaks around 17:30, then begins to decrease. There is also an increase in delivery time variation as the order time approaches the peak.\nThe day of the week is visualized via box plots in panel (c). There is an increasing trend in both mean and variation as we move from Monday to Friday/Saturday, then a decrease on Sunday. Again, this appears to be an important predictor.\nThese three panels show relationships between individual predictors and delivery times. It is possible that multiple predictors can act in concert; their effect on the outcome occurs jointly. Panel (d) of Figure 2.2 shows that there is an interaction effect between the day and hour of the order. Mondays and Tuesdays have shorter delivery times that don’t change much over hours. Conversely, on Fridays and Saturdays, the delivery times are generally longer, with much higher peaks. This might explain the increase in delivery time variation that was seen in Panel (b). Now that we know of the existence of this interaction, we might add additional model terms to help the model understand and exploit this pattern in the training set.\nFor the 27 itemized frequency counts, we might want to know if there was a change in delivery time when a specific item was in the order (relative to those where it is not purchased). To do this, mean delivery times were computed for orders with and without the item. The ratio of these times was computed and then converted to the percent increase in time. This is used to quantify the effect of item on delivery time. We’d also like to know what the variation is on this statistic and a technique called the bootstrap (Davison and Hinkley 1997) was used to create 90% confidence intervals for the ratio. Figure 2.3 shows the results.\n\n\n\n\n\n\n\n\nFigure 2.3: Ranking the items by the increase in delivery time when ordered at least once.\n\n\n\n\n\nThere is a subset of items that increase the delivery time (relative to the experimental noise), many that seem irrelevant, and one that decreases the time.\nMore investigations into the data would be pursued to better understand the data and help us with feature engineering. For brevity, we’ll stop here and introduce three ML models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-development-whole-game",
    "href": "chapters/whole-game.html#sec-model-development-whole-game",
    "title": "2  The Whole Game",
    "section": "2.4 Model Development",
    "text": "2.4 Model Development\nWhich model to use? There is a multitude of tools that can be used to represent and predict these data. We’ll illustrate three in this section and the essential concepts to consider along the way.\nBefore proceeding, we need to pick a performance metric. This is used to quantify how well the model fits the data. For any model fit, the residual (usually denoted as \\(\\epsilon\\), also called the error) is the difference between the model prediction and the observed outcome. We’d like our model to produce residuals near zero. One metric5 is the mean absolute error (MAE). For a data set with \\(n\\) data points, the equation is\n\\[\nMAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| = \\frac{1}{n}\\sum_{i=1}^n |\\epsilon_i|\n\\tag{2.1}\\]\nwhere \\(\\hat{y}\\) is the predicted numeric outcome value and \\(\\epsilon\\) is the model error. The MAE units are the same as the outcome which, in this case, is minutes. We desire to minimize this statistic.\nTo get started, we’ll use a basic linear model for these data.\n\nLinear Regression\nLinear regression is probably the most well-known statistical model in history. It can predict a numeric outcome using one or more predictors using the equation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\ldots + \\beta_px_{ip} + \\epsilon_i\n\\tag{2.2}\\]\nwhere \\(y_i\\) is the outcome for the \\(i^{th}\\) data point (out of \\(n_{tr}\\)), and \\(x_{ij}\\) is value for the \\(j^{th}\\) predictor (out of \\(p\\)).\nFor Equation 2.2, there are many ways to estimate the model parameters (\\(\\beta\\)’s). Ordinary least squares is used most often. It finds the values of the \\(\\beta_j\\) that that minimize sum of squared errors (SSE). Chapter TODO discussed several other ways to estimate the model parameters that use other criteria (variations of the SSE).\nThe main assumptions about the data for ordinary least squares are that they are independent and identically distributed. Given what we know about the problem, both assumptions are at least mildly incorrect. Orders are being processed simultaneously (affecting independence), and our visualizations have indicated that there is an increase in variation with the mean (i.e., not identically distributed). However, we are most interested in prediction here, so we can often overlook slight to moderate violations of these assumptions6\nWhat are our \\(x_{ij}\\) values in Equation 2.2? We can’t give the model equations values of \"Monday\" or \"Friday\"; OLS requires numbers in its equations. A simple workaround is to create binary indicator variables for six of the seven days of the week. For example, the indicator for Friday would have a value of one for orders placed on a Friday and zero otherwise7.\nAlso, for Equation 2.2, what do we do about the numeric predictors that appear to have nonlinear trends? We can emulate the approach taken by the smoothing methods and augment the predictor columns. Equation 2.2 shows that linear regression is linear in the parameters (not the predictors). We can add terms such as \\(x_{ij} = distance_i^2\\) to allow the fit to be nonlinear. An effective tool for enabling nonlinear trends is spline functions8. These are special polynomial terms. For example, we could represent that original predictor with 10 spline terms (i.e., columns in the data) for the order hour. As we add more terms, the model fit has increased capacity to be nonlinear. We’ll use 10 separate splines for both distance and the order hour (in lieu of the original data columns.)\nHow do we encode the interaction seen in Figure 2.2(d)? We have six indicator columns for the order day and ten spline columns for the order hour. We can facilitate having different nonlinear hour trends for each day by making 60 cross-product terms. For example, the interactions for Fridays would multiply each of the ten spline terms by the binary indicator associated with Fridays.\nThese feature engineering operations result in a model with \\(p = 114\\) \\(\\beta\\) parameters. We fit this model to the training set to estimate the model parameters. To evaluate it, we use the trained model to predict the validation set, and these predictions are the substrate to compute the MAE. For this model, the validation set MAE is 1.609 (i.e., 1 minute and 36 seconds). If we think of MAE as an average error, this doesn’t seem too bad. However, what does this look like? Let’s plot the observed times against predicted values to get a better sense of how well this model did. Figure 2.4 has the results which also feature a scatter plot smoother.\n\n\n\n\n\n\n\n\nFigure 2.4: A scatter plot of the observed and predicted delivery times, via a linear regression model, for the validation set. The red line is a smooth trend estimate.\n\n\n\n\n\nThe best case would be that the points arrange themselves tightly around the diagonal line. The model slightly under-predicts for very rapid deliveries but substantially under-predicts for times greater than 40 minutes. However, overall, the model works well for the majority of the deliveries.\nFrom here, our next step would be to investigate poorly predicted samples and determine if there is some commonality to them. For example, are they short distances that are ordered late on a Friday, etc? If there appears to be a pattern, we would add additional model terms to compensate for the flaw and see if the validation set MAE decreases. We would iterate between exploratory analysis of the residuals, adding or subtracting features, then refitting the model.\nFor illustration, we stop here and take a look at very different model.\n\n\nRule-Based Ensemble\nCubist is a rule-based ensemble method. It creates a rule set by first generating a regression tree that is composed of a hierarchical set of if/then statements9. Figure 2.5 shows a simple tree for our data. A set of splits on different predictors results in six terminal nodes (at the bottom of the tree). A rule is a distinct path through the tree to a terminal node. For example, Node 11 in the figure has the corresponding rule: hour &gt;= 14.77 & day in {Fri, Sat} & distance &gt;= 4.045. For each rule, Cubist fits a linear regression to the data that the rule covers (\\(n = 261\\) for Node 11). It builds new rule sets sequentially and uses many rules/regression equations to predict new samples.\n\n\n\n\n\n\n\n\nFigure 2.5: An example of a regression tree used to create a set of six rules.\n\n\n\n\n\nThe method isolates different parts of the predictor space and models the data using linear functions. The local nature of the rules enables the model to emulate nonlinear functions easily. When a rule consists of multiple predictors, this constitutes an interaction effect. Additionally, the data preprocessing requirements are minimal: it can internally account for any missing predictor values, and there is no need to convert categorical predictors into binary indicator columns, and so on.\nWhen fit to the training set, the Cubist model created an ensemble with 105,327 rules. On average, the rules consisted of 2.7 variables and the the regression models contained 12.7 predictors. Two example rules, and their regression models, were:\n\nif\n   hour &lt;= 14.252\n   day in {Fri, Sat}\nthen\n   outcome = -23.039 + 2.85 hour + 1.25 distance + 0.4 item_24\n             + 0.4 item_08 + 0.6 item_01 + 0.6 item_10 + 0.5 item_21\n             + 0.3 item_09 + 0.4 item_23 + 0.2 item_03 + 0.2 item_06\n\nand\n\nif\n   hour &gt; 15.828\n   hour &lt;= 18.395\n   distance &lt;= 4.35\n   item_10 &gt; 0\n   day = Thu\nthen\n   outcome = 11.29956 + 4.24 distance + 14.3 item_01 + 4.3 item_10\n             + 2.1 item_02 + 0.03 hour\n\nThese two examples are reasonable and understandable. However, blending so many rules means that this model is a complex, black-box model. For example, the first data point in the validation set is affected by 5,916 rules (about 6% of the total ensemble).\nDoes this complexity help? The validation set estimated MAE at 1 minute and 24 seconds, which is an improvement over the linear regression model. Figure 2.6 visualizes the observed and predicted plot. The model under-predicts fewer points with long delivery times than the previous model but does contain a few very large residuals.\n\n\n\n\n\n\n\n\nFigure 2.6: A scatter plot of the observed and predicted delivery times from the Cubist model, for the validation set.\n\n\n\n\n\nWould this model have benefited from using the feature set from the linear regression analysis? Not really. Refitting with those predictors had an MAE of 1 minute and 26 seconds. In this particular case, the original Cubist was able to estimate nonlinear trends and interactions without additional feature engineering.\nThere is more that we can do with the Cubist model but, in this initial model screening phase, we should focus our efforts on evaluating a disparate set of models and features to understand what works, what doesn’t work, and the difficulty level of the problem. Let’s try one additional model.\n\n\nNeural Network\nA neural network is a highly nonlinear modeling technique. They relate the predictors to the outcome through intermediate substructures, called layers, that contain hidden units. The hidden units allow the predictors to affect the outcomes differently10 allowing the model to isolate essential patterns in the data. As the number of hidden units (or layers of hidden units) increases, so does the complexity of the model. We’ll only consider a single layer network with multiple hidden units for these data.\nNeural networks, like linear regression, require numbers as inputs. We’ll convert the day of the week predictor to indicators. Additionally, the model requires that all of the predictors be in the same units. There are a few ways to accomplish this. First, we will compute the training set mean and standard deviation of each feature. To standardize them we subtract the mean and divide the result by the standard deviation. As a result, all features have a zero mean and a unit standard deviation.\nOne question about this model: how many hidden units should we use11? This decision will determine the complexity of the model. As models become model complex, they have more capacity to find complicated relationships between the predictors and response. However, added complexity adds to the risk of overfitting. This situation, discussed in Chapter TODO, occurs when a model fits exceptionally well to the training data but fails for new data by finding patterns in the training data that are artifacts and do not generalize for other data sets. How should we choose the number of hidden units?\nUnfortunately, no equation can directly compute an estimate the number of units from the data. Values such as these are referred to as tuning parameters or hyperparameters. However, one solution is to extend the resampling process previously shown for the linear regression and Cubist models. We will consider a candidate set of tuning parameter values ranging from 3 to 100 hidden units. We’ll fit a model to the training set for each of these 98 values and use the validation set MAE values to select an appropriate value. The results of this process is shown in Figure 2.7.\n\n\n\n\n\n\n\n\nFigure 2.7: The relationship between the number of hidden units and the mean absolute deviation from the validation set data.\n\n\n\n\n\nA small number of hidden units performs poorly due to underfitting (i.e., insufficient complexity). Adding more improves the situation and, while the numerically best result corresponds to a value of 99, there is appreciable noise in the MAE values12 . Eventually, adding too much complexity will either result in the MAE values becoming larger due to overfitting or will drastically increase the time to fit each model. The pattern shows a plateau of values with a similar MAE is the absolute optimal value. Given the noise in the system, we’ll select a value of 48 hidden units, by determining the least complex model that is within 1% of the numerically best MAE. The MAE associated with the selected candidate value13 was 1 minute and 29 seconds (but is likely to be closer to 1 minute and 31 seconds, given the noise in Figure 2.7).\nFigure 2.8 shows the familiar plot of the observed and predicted delivery times. There are fewer large residuals than the Cubist model but the underfitting for longer delivery times appears more pronounced. There is also a hint of under-predicted times for fast deliveries.\n\n\n\n\n\n\n\n\nFigure 2.8: A scatter plot of the validation set observed and predicted delivery times from a neural network with 48 hidden units.\n\n\n\n\n\nThese results suggest that the Cubist and neural network models have roughly equivocal performance14. Like Cubist, the finalized model is very complex; it is a highly nonlinear regression with 5,743,264 model parameters (i.e., slopes and intercepts).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-selection-whole-game",
    "href": "chapters/whole-game.html#sec-model-selection-whole-game",
    "title": "2  The Whole Game",
    "section": "2.5 Choosing Between Model",
    "text": "2.5 Choosing Between Model\nThere are a variety of criteria used to rank and compare model fits. Obviously, performance is important. Also, it is good practice to choose a model that is as simplistic as possible (subject to having acceptable performance). There are other practical factors too. If a model is to be deployed, the size of the files(s) required to automate predictions can be a constraint. There may also be data constraints; which predictors are easy or inexpensive to obtain? Does the number of predictors affect how quickly predictions can be made?\nWhile prediction is the focus for ML models, we often have to explain the model and/or predicted values to consumers. While we can develop explainers for any model, some types are easier to explain than others.\nFor this chapter, we have three models whose MAE values are within about 12 seconds of one another. Each has the same deficiency: they all significantly underpredict long delivery times15. In fact, a few poor predictions may be a limiting factor for any regression metric; it might be impossible for one model’s MAE to really stand out from the others.\nWe’ll select the linear regression model as our final model due to its relative simplicity and linear structure.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-calibration-whole-game",
    "href": "chapters/whole-game.html#sec-calibration-whole-game",
    "title": "2  The Whole Game",
    "section": "2.6 Calibration",
    "text": "2.6 Calibration\nThere is one option for remedying the systematic underprediction of long delivery times. Section TODO describes calibration methods that might be able to correct such issues. To do so, we use the held out predictions to estimate the average unwanted trend and remove it from new predictions. For the linear regression results, the estimated average trend is almost identical to the smooth line shown in Figure 2.4.\nFor our test set predictions, we’ll remove this trend and, hopefully, this will recenter the long predictions to be closer to the diagonal line.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-test-results-whole-game",
    "href": "chapters/whole-game.html#sec-test-results-whole-game",
    "title": "2  The Whole Game",
    "section": "2.7 Test Set Results",
    "text": "2.7 Test Set Results\nWe can predict the test set using the linear regression fit from the training set. Recall that the validation set MAE was 1 minute and 36 seconds. Without the additional calibration, the test set value was also 1 minute and 36 seconds. The similarity of these results gives us confidence that the performance statistics we estimated during development are consistent with the test set. The test set plot of the predictions is shown in Figure 2.9(a).\nThe calibration results are shown in panel (b). There does appear to be an improvement in long delivery times; these are closer to the diagonal line. The MAE value is slightly improved: 1 minute and 32 seconds. The small difference in these results and the similar MAE values across models indicates that the few large residuals are probably the gating factor for these data.\n\n\n\n\n\n\n\n\nFigure 2.9: Observed versus predicted test set delivery times for the final model. Results are shown for pre-calibrated and post-calibration.\n\n\n\n\n\nAt this point, we should spend a fair amount of time documenting the model and our development process. It might be helpful to employ inferential analyses to explain to interested parties which model components were most important (e.g., how much the interaction or nonlinear terms improve the model, etc.). It is very important to document where the model does not work well, its intended use, and the boundaries of the training set. Finally, if the model is deployed, we should also monitor its effectiveness over time and also understand if the sample population is changing over time too.\nThe next section accentuates important aspects of this case study.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-themes-whole-game",
    "href": "chapters/whole-game.html#sec-themes-whole-game",
    "title": "2  The Whole Game",
    "section": "2.8 Themes",
    "text": "2.8 Themes\nAt a high level, let’s review the process that we just used to create a model; Figure 2.10 highlights several themes16. The overall process involves initial data splitting, model development, then model selection, and validation. We’ll look at each of these in turn to discuss important points.\n\n\n\n\n\n\n\n\nFigure 2.10: A general high level view of the process of creating a predictive model. The diagram reflects which steps use the test set, validation set, or the entire training set. Figure 2.11 describes the operations labeled “model development”.\n\n\n\n\n\n\nData Spending\nAlthough discussed in Chapter TODO, how we allocate data to specific tasks (e.g., model building, evaluating performance, etc.) is an essential aspect of modeling. In our analysis, a minority of the data were explicitly allocated to measure how well the model worked. The test set is meant to be unbiased since it was never used to develop the model. This philosophy of empirical validation is a crucial feature for judging how models function.\nThe majority of the data ends up in the training set. The core activities of creating, optimizing, and selecting are accomplished with these data.\nFigure 2.10 highlights how we use the right data at the right time. A lack of a data usage methodology (or an inappropriate one) can lead to having a false sense of the quality of the model.\nFor example, when the linear regression model was calibrated, we chose to use the held-out samples as the substrate for estimating the unwanted trend. We could have used the re-predicted training set data but, as will be discussed in Section TODO, these values can be unrealistically optimistic. Also, we chose not to compute the MAE on the held-out predictions after they were used to estimate the calibration. This dual use of the same data would result in an MAE value that is smaller than it should be. There are other ways of estimating the utility of the calibration prior to the test set; these are discussed in Section TODO.\n\n\nInvestigate Many Options\nFor the delivery data, three different models were evaluated. It is our experience that some modeling practitioners have a favorite model that is relied on indiscriminately (boosted trees immediately come to mind). The “No Free Lunch” Theorem (Wolpert 1996) argues that, without substantive information about the modeling problem and data, no single model will always do better than any other model. Because of this, a strong case can be made to try various techniques and then determine which to focus on. In our example, simple data visualizations elucidated the relationships between the outcome and the predictors. Given this knowledge, we might exclude some models from consideration, but there is still a wide variety of techniques to evaluate.\nFor individual models, Figure 2.11 summarizes the cyclic process of speculation, configuration, and validation of model parameters and features. For these data, we discovered most of the important trends prior to our first model fit. As previously mentioned, we would cycle through a few rounds where we try to make improvments for each of the models.\n\n\n\n\n\n\n\n\nFigure 2.11: The cycle of developing a specific type of model. EDA standard for exploratory data analysis.\n\n\n\n\n\nFor novel data sets, it is a challenge to know a priori what good choices are for tuning parameters, feature engineering methods, and predictors. The modeler must iterate using choices informed by their experience and knowledge of the problem.\n\n\nPredictor Representations\nThis example revolves around a small set of predictors. In most situations, there will be numerous predictors of different types (e.g., quantitative, qualitative, etc.). It is unclear which predictors should be included in the model for new modeling projects and how they should be represented.\nWhen considering how to approach a modeling project, the stereotype focuses on which new, fancy model to adopt. The reality is that many of the most effective decisions that users must confront are which predictors to use and in what format. Thoughtfully adding the right feature representations often obviates the need (and overhead and risks) for complex models. That is one of the lessons from this particular data set.\nWe’ll put the feature engineering process on equal footing with model training For this reason, Figure 2.11 describes the cyclic nature of developing a single model and its corresponding features. These two processes go hand-in-hand.\nWhile Cubist and the neural network were able to internally determine the more elaborate patterns in the data, our discovery of these same motifs has a higher value. We learned something about the nature of the data and can relate them to the model (via feature engineering) and the end-users. In fact, the day-to-day users of the data are the best resources for feature engineering.\n\n\nModel Selection\nAt some point in the process, a specific model pipeline must be chosen. This chapter demonstrated two types of selection. First, we chose some models over others: the linear regression model did as well as more complex models and was the final selection. In this case, we decided between model pipelines. There was also a second type of model selection shown. The number of hidden units for the neural network was determined by trying different values and assessing the results. This was also model selection, where we decided on the configuration of a neural network (as defined by the number of hidden units). In this case, we selected within different neural networks.\nIn either case, we relied on data separate from the training set to produce quantitative efficacy assessments to help choose. This book aims to help the user gain intuition regarding the strengths and weaknesses of different models to make informed decisions.\n\n\nMeasuring Performance\nBefore using the test set, two techniques were used to determine the model’s effectiveness. First, quantitative assessments of statistics (i.e., MAE) from the validation set help the user understand how each technique would perform on new data. The other tool was to create simple visualizations of a model, such as plotting the observed and predicted values, to discover areas of the data where the model does particularly well or poorly. This qualitative information is lost when the model is gauged only using summary statistics and is critical for improving models. There are many more visualization methods for displaying the outcomes, their predictions, and other data. The figures shown here are pretty simplistic.\nNote that almost all of our approaches for model evaluation are data-driven in a way where we determine if the predicted values are “close” to the actual values. This focus on empirical validation is critical in proving that a model pipeline is fit for purpose. Our general mantra is\n\n\n\n\n\n\nImportant\n\n\n\nAlways have a separate piece of data that can contradict what you believe.\n\n\nFor models with parametric assumptions, it is a good idea to check these too, primarily if a model will also be used for inference. However, before considering questions regarding theoretical assumptions, empirical validation should be used to ensure that the model can explain the data well enough to be worth our inferences. For example, if our validation set MAE was around 15 minutes, the model predictions would not show much fidelity to the actual times. Any inferences generated from such a model might be suspect.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-summary-whole-game",
    "href": "chapters/whole-game.html#sec-summary-whole-game",
    "title": "2  The Whole Game",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nAt face value, model building appears straightforward: pick a modeling technique, plug in the data, and generate a prediction. While this approach will yield a predictive model, it will likely not generate a reliable, trustworthy model. To achieve this we must first understand the data and the objective of the modeling. We finally proceed to building, evaluating, optimizing, and selecting models after these steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#chapter-references",
    "href": "chapters/whole-game.html#chapter-references",
    "title": "2  The Whole Game",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nDavison, A., and Hinkley, D. (1997), Bootstrap Methods and Their Application, Cambridge University Press.\n\n\nPerkins, D. (2010), Making Learning Whole, Wiley.\n\n\nWolpert, D. (1996), “The lack of a priori distinctions between learning algorithms,” Neural Computation, 8, 1341–1390.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  }
]