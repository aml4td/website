[
  {
    "objectID": "chapters/iterative-search.html",
    "href": "chapters/iterative-search.html",
    "title": "12  Iterative Search",
    "section": "",
    "text": "12.1 Example: Predicting Barley Amounts using Support Vector Machines\nGrid search is a static procedure; we predetermine which candidates will be evaluated before beginning. How can we adaptively optimize tuning parameters in a sequential manner? Perhaps more importantly, when is this a good approach?\nPreviously, we’ve seen that a plateau of good performance in the parameter space is possible. This is often the case but will not always be true. If the region of optimal performance is small, we must create a large space-filling design to find it. When there are many tuning parameters, the computational expense can be unreasonable. An even worse situation is one where the previously described speed-up techniques (such as the submodel trick from Section 11.3.1) are not applicable. Racing can be very efficient, but if the training set is very large, it may be that using a validation set is more appropriate than multiple resamples; in this case, racing cannot be used. Finally, there are practical considerations when the data set is very large. Most parallel processing techniques require the data to be in memory for each parallel worker, restricting their utility.\nGenerally, we are not often constrained by these issues for models used for tabular data. However, there are a few models that might be better optimized sequentially. The first two that come to mind are large neural networks1. and support vector machines2 (SVMs) (Schölkopf and Smola 2001; Shawe-Taylor and Cristianini 2004). For the former, as the number of layers increases, so does the number of tuning parameters. Neural networks also have many important tuning parameters associated with the model’s training, such as the learning rate, regularization parameters, etc. These models also require more preprocessing than others; they are suspectable to the effects of uninformative predictors, missing data, and collinearity. Depending on the data set, there can be many pre-model tuning parameters.\nSupport vector machines are models with fewer tuning parameters than neural networks but with similar preprocessing requirements. Unfortunately, the tuning parameter space can have large areas of poor performance with “islands” where the model works well. The location of these will change from data set to data set. We’ll see an example of this shortly where two model parameters are tuned.\nThese two models, in particular, are more likely to benefit from an optimization method that chooses candidates as the process evolves.\nIn theory, any iterative optimization procedures can be used. In general, gradient methods, such as steepest descent or Newton’s method, are the most commonly used technique for nonlinear optimization. These tools are suboptimal when it comes to parameter tuning but play important parts in other areas of machine learning and, for this reason, they will be discussed later.\nDerivative-free techniques can be helpful for model tuning. Traditional examples are genetic algorithms, simulated annealing, particle swarm optimization, etc. We’ll consider the first two of these in Sections 12.3 and 12.4. Currently, the most well-known iterative tool for tuning models is Bayesian optimization. This will be examined in Section 12.6. However, before this, Section 12.5 will flesh out what it means for a model to be Bayesian.\nTo get started, let’s revisit a data set.\nWe’ll return to the data previously seen in Section 7.1 where we want to predict the percentage of barley oil in a mixture. Recall that the predictors are extremely correlated with one another. In Chapter 7, we considered embedding methods like PCA to preprocess the data and these models were able to achieve RMSE values of about 6% (shown in Figure 7.6).\nIn this chapter, we’ll model these data in two different scenarios. First, we’ll use them as a “toy problem” where only two tuning parameters are optimized for a support vector machine model. This is a little unrealistic, but it allows us to visualize how iterative search methods work in a 2D space. Second, in Section 12.7, we’ll get serious and optimize a larger group of parameters for neural networks simultaneously with additional parameters for a specialized preprocessing technique.\nLet’s start with the toy example that uses an SVM regression model. This highly flexible nonlinear model can represent the predictor data in higher dimensions using a kernel transformation (Hofmann, Schölkopf, and Smola 2008). This type of function combines numeric predictor vectors from two data points using a dot product. There are many different types of kernel functions, but for a polynomial kernel, it is:\n\\[\nk(\\boldsymbol{x}_1, \\boldsymbol{x}_2) = (a\\boldsymbol{x}_1'\\boldsymbol{x}_2 + b)^q\n\\tag{12.1}\\]\nwhere \\(a\\) is called the scaling factor, \\(b\\) is a constant offset value, and \\(q\\) is the polynomial degree. The dot product of predictor vectors (\\(\\boldsymbol{x}_i\\)) measures both angle and distance between points. Note that for the kernel function to work in an appropriate way, the two vectors must have elements with consistent units (i.e., they have been standardized).\nThe kernel function operates much like a polynomial basis expansion; it projects a data point into a much larger, nonlinear space. The idea is that a more complex representation of the predictors might enable the model to make better predictions.\nFor our toy example, we’ll take the 550 predictors, project them to a reduced space of 10 principal components, and standardize those features to have the same mean and standard deviation. A quartic polynomial with zero offset is applied, and the scale parameter, \\(a\\), will be tuned. This parameter helps define how much influence the dot product has in the polynomial expansion.\nThe most commonly adjusted parameter in SVMs is the cost value, which is independent of the chosen kernel function. This parameter determines how strongly the model is penalized for incorrect predictions on the training set. A higher cost value pushes the SVM to create a more complex model. As shown earlier in Figure 9.1, small cost values result in the SVM making less effort to classify samples correctly, leading to underfitting. In contrast, extremely high cost values cause the model to overfit to the training set.\nJust as in Chapter 7, the RMSE will be computed from a validation set and these statistics will be used to guide our efforts.\nLet’s define a wide space for our two tuning parameters: cost will vary from 2-10 to 210 and the scale factor3 is allowed to range from 10-10 to 10-0.1.\nFigure 12.1 visualizes the RMSE across these ranges4 where darker colors indicate smaller RMSE values. The lower left diagonal area is a virtual “dead zone” with very large RMSE results that don’t appear to change much. There is also a diagonal wedge of good performance (symbolized by the darker colors). The figure shows the location of the smallest RMSE value and a diagonal ridge of parameter combinations with nearly equal results. Any points on this line would produce good models (at least for this toy example). Note that there is a small locale of excessively large RMSE values in the upper right. Therefore, increasing both tuning parameters to their upper limits is a bad idea.\nFigure 12.1: A visualization of model performance when only the SVM cost and scaling factor parameters are optimized. It shows the combination with the smallest RMSE and a ridge of candidate values with nearly equivalent performance.\nIn the next section, we explore gradient descent optimization and explain why it may not be suitable for model tuning. Following that, we delve into two traditional global search methods—simulated annealing and genetic algorithms—and their application in parameter space exploration. We then shift our focus to Bayesian optimization, concluding with an in-depth analysis of the barley prediction problem.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-gradient-opt",
    "href": "chapters/iterative-search.html#sec-gradient-opt",
    "title": "12  Iterative Search",
    "section": "12.2 Sidebar: Gradient-Based Optimization",
    "text": "12.2 Sidebar: Gradient-Based Optimization\nTo formalize the concept of optimization, we need an objective function that defines what we are trying to optimize. This function, often called a loss function (specifically to be minimized), is denoted by \\(\\psi()\\). The parameters that modify \\(\\psi()\\) are represented by \\(\\boldsymbol{\\theta}\\), a \\(p \\times 1\\) vector of real numbers. For simplicity, we will assume that \\(\\psi()\\) is smooth and generally differentiable. Additionally, without loss of generality, we will assume that smaller values of \\(\\psi()\\) are better.\nWe’ll denote the first derivative (\\(\\psi'(\\boldsymbol{\\theta})\\)), for simplicity, as \\(g(\\boldsymbol{\\theta})\\) (\\(p\\times 1\\)). The matrix of second deriviatives, called the Hessian matrix, is symbolized as \\(H(\\boldsymbol{\\theta})\\) (\\(p\\times  p\\)).\nWe start with an initial guess, \\(\\boldsymbol{\\theta}_0\\), and compute the gradient at this point, yielding a \\(p\\)-dimensional directional vector. To get to our next parameter value, simple gradient descent uses the update:\n\\[\n\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\alpha\\:g(\\boldsymbol{\\theta}_i)\n\\tag{12.2}\\]\nThe value \\(\\alpha\\) defines how far to move in the chosen direction. It can either be a fixed constant5 or adjusted using a secondary method called a line search. In a line search, \\(\\alpha\\) is incrementally increased until the objective function worsens.\nWe proceed to iterate this process until some measure of convergence is achieved. For example, the optimization could be halted if the objective function does not improve more than a very small value. Lu (2022) and Zhang (2019) are excellent introductions to gradient-based optimization that is focused on training models.\nAs a simple demonstration, Figure 12.2 shows \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\) for values of \\(\\theta\\) between \\(\\pm 10.0\\). We want to minimize this function. There is a global minimum at about \\(\\theta \\approx 6.85\\) while there are local minima at \\(\\theta = -10.0\\) and \\(\\theta \\approx -1.72\\). These are false solutions where some search procedures might become trapped.\n\n\n\n\n\n\n\n\n#| label: shiny-grad-descent\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-gd.R\")\n\napp\n\n\n\nFigure 12.2: An example of simple gradient descent for the function \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\).\n\n\n\n\n\n\n\nThe figure enables the choice of \\(\\theta_0\\), the value of \\(\\alpha\\), and how many iterations to used. Consider a few configurations:\n\nStarting at \\(\\theta = 3\\), a learning rate of \\(\\alpha = 2.0\\) is inappropriate. The search initially moves towards the global minimum but then reverses course and jumps past the best value, then becomes trapped around one of the local optima.\nIf we keep \\(\\theta = 3\\) and decrease learning rate to \\(\\alpha = 1.0\\), we quickly find the best result.\nHowever, if we tried decreasing the learning rate too low, say \\(\\alpha = 0.01\\), the optimization moves too slowly.\n\nThe learning rate plays a crucial role in the optimization process. Additionally, the starting value is significant; values below 2.0 consistently fail to reach the optimum.\nThis gradient descent algorithm outlined above is extremely basic. There are far more complex versions, the most well-known of which is the Newton-Raphson method (a.k.a. Newton’s Method) that incorporates the second derivative matrix \\(H(\\boldsymbol{\\theta})\\) in the updating formula6.\nWe often know when gradient-based optimization will work well. If we know the equation for the objective function, we can determine its properties, such as whether it is a convex function, and theory can tell us if a global optimum can be found via the use of gradients. For example, squared error loss \\(\\psi(\\boldsymbol{\\theta}) = (y-\\hat{y})^2\\), is convex when the relationship for the predicted value \\(\\hat{y}\\) is a well-behaved function of \\(\\boldsymbol{\\theta}\\) (such as in linear regression).\nHowever, there are additional considerations when it comes to optimizations for predictive models. First, since data are not deterministic, the loss function is not only a random variable but can be excessively noisy. This noise can have a detrimental effect on how well the optimization proceeds.\nSecond, our objective function is often a performance metric, such as RMSE. However, not all metrics are mathematically well-behaved—they may lack smoothness or convexity. In the case of deep neural networks, even when the objective function is simple (e.g., squared error loss), the model equations can create a non-convex optimization landscape. As a result, the optimized parameters may correspond to a local optimum rather than a global one.\nThis issue arises because traditional gradient-based methods are inherently greedy. They optimize by moving in the direction that appears most favorable based on the current estimates of \\(\\boldsymbol{\\theta}\\). While this approach is generally effective, it can lead to the optimization process becoming trapped in a local optimum, particularly with complex or non-standard objective functions.\nThere are additional complications when the tuning parameters, \\(\\boldsymbol{\\theta}\\), are not real numbers. For instance, the number of spline terms is an integer, but the update equation might produce a fractional value for this number. Other parameters are qualitative, such as the choice of activation function in a neural network, which cannot be represented as continuous numerical values.\nDuring model tuning, we know to avoid repredicting the training set (due to overfitting). Procedures such as resampling make the evaluation of \\(\\psi()\\) computationally expensive since multiple models should be trained. Unless we use symbolic gradient equations, the numerical process of approximating the gradient vector \\(g(\\boldsymbol{\\theta})\\) can require a large number of function evaluations.\nLater, we’ll discuss stochastic gradient descent (SGD) (Prince 2023, chap. 6). This method is commonly used to train complex networks with large amounts of training data. SGD approximates the gradient by using only a small subset of the data at a time. Although this introduces some variation in the direction of descent, it can be beneficial as it helps the algorithm escape from local minima. Additionally, when dealing with large datasets, computer memory may not be able to store all the data at once. In such cases, SGD is often the only viable option because it doesn’t require the entire dataset to be loaded into memory and can also be much faster.\nUntil then, the next three chapters will describe different stochastic optimization, gradient-free methods that are well-suited for parameter tuning since they can sidestep some of the issues described above.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-sim-anneal",
    "href": "chapters/iterative-search.html#sec-sim-anneal",
    "title": "12  Iterative Search",
    "section": "12.3 Simulated Annealing",
    "text": "12.3 Simulated Annealing\nNon-greedy search methods are not constrained to always proceed in the absolute best direction (as defined by the gradient). One such method is simulated annealing (SA)(Kirkpatrick, Gelatt, and Vecchi 1983; Spall 2005). It is a controlled random search that moves in random directions but with some amount of control over the path. It can also incorporate restarts if the algorithm moves into clearly poor regions.\nGiven an initial solution, simulated annealing (SA) creates a random perturbation of the current candidate solution, typically within a small local neighborhood. The objective function is then evaluated for the new candidate and compared to the previous solution. If the new candidate results in an improvement, the process moves forward by using it to make the next step. If the new candidate is worse, there are two options:\n\nWe can accept the current solution as “suboptimal” and use it as the basis for the next perturbation, or\nwe can discard the current solution and treat it as if it never occurred. The next candidate point will then be a perturbation of the last “acceptable” solution.\n\nFor our SVM example, suppose we start with a candidate where\n\\[x_0 = \\left[log_2 (cost), log_{10} (scale)\\right] = [-10, -0.1]\\]\nand had an associated RMSE value of 6.10%. For the next candidate, a perturbation of this values is created, say \\(x_1 = [-7.11, -0.225]\\). Suppose that the corresponding RMSE was measured at 5.97%. Since \\(x_1\\) has a better performance metric it is automatically accepted and is used to create the next parameter.\nHowever, suppose that the RMSE value for \\(x_1\\) was 7.00%, meaning that the new candidate did worse than the initial one. In this case, simulated annealing generates a probability threshold for accepting the worse solution. Typically, the probability depends on two quantities:\nSimulated annealing generates a probability threshold for accepting the worse solution. Typically, the probability depends on two quantities:\n\nThe difference between the current and previous objective function values. If the new candidate is nearly as good as the current solution, the probability of acceptance will be higher compared to a candidate that is significantly worse.\nThe probability of acceptance should decrease over time as the search progresses. This is often achieved using an exponentially decaying function, known as the “cooling schedule.”\n\nAn often used equation for the probability, assuming that smaller values are better, is\n\\[\nPr[accept] = \\exp\\Bigl[-i\\bigl(\\hat{Q}(\\boldsymbol{\\theta}_{i}) - \\hat{Q}(\\boldsymbol{\\theta}_{i-1})\\bigr)\\Bigr]\n\\]\nwhere \\(i\\) is the iteration number. To compare 6.1% versus 7.0%, the acceptance probability is 0.407. To make the determination, a random uniform number \\(\\mathcal{U}\\) is generated and, if \\(\\mathcal{U} \\le Pr[accept]\\), we accept \\(\\boldsymbol{\\theta}_{i}\\) and use it to make \\(\\boldsymbol{\\theta}_{i+1}\\). Note that the probability “cools” over time; if this difference were to occur at a later iteration, say \\(i = 5\\), the probability would drop to 0.0111.\nOne small matter is related to the scale of the objective function. The difference in the exponent is very sensitive to scale. If, for example, instead of percentages we were to use the proportions 0.61 and 0.70, the probability of acceptance would change from 40.7% to 91.4%. One way to mitigate this issue is to use a normalized difference by dividing the raw difference by the previous objective function (i.e., (0.70-0.61) / 0.70). This is the approach used in the SA analyses here.\nThis process continues until either a pre-defined number of iterations is reached or there is no improvement after a certain number of iterations. The best result found during the optimization process is used as the final value, as there is no formal concept of “convergence” for this method. Additionally, as mentioned earlier, a restart rule can prevent simulated annealing from getting stuck in suboptimal regions if no better results have been found within a certain timeframe. When the process is restarted, it can either continue from the best candidate found in previous iterations or start from a random point in the parameter space.\nHow should the candidates be perturbed from iteration to iteration? When the tuning parameters are all numeric, we can create a random distance and angle from the current values. A similar process can be used for integers by “flooring” them to the nearest whole number. For qualitative tuning parameters, a random subset of parameters is chosen to change to a different value chosen at random. The amount of change should be large enough to search the parameters space and potentially get out of a local optimum.\nIt is important to perform computations on the parameters in their transformed space to ensure that the full range of possible values is treated equally. For example, when working with the SVM cost parameter, we use its log (base 2) scale. When perturbing the parameter values, we make sure to adjust them in the log space rather than the original scale. This approach applies to all other search methods discussed in this chapter.\nTo illustrate, a very small initial space-filling design with three candidates was generated and evaluated. The results are in Table 12.1. To start the SA search7, we will start with the candidate with the smallest RMSE and proceed for 50 iterations without a rule for early stopping. A restart to the last known best results was enforced after eight suboptimal iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRMSE (%)\nCost (log-2)\nScale (log-10)\n\n\n\n\n6.10\n-10\n-0.1\n\n\n7.33\n10\n-5.0\n\n\n17.04\n0\n-10.0\n\n\n\n\n\n\n\n\nTable 12.1: The initial set of candidates used for iterative search with the toy example from Figure 12.1. One candidate does poorly while the other two have relatively similar results.\n\n\n\n\n\n\n\nFigure 12.3 contains an animation of the results of the SA search. In the figure, the initial points are represented by open circles, and a grey diagonal line shows the ridge of values that corresponds to the best RMSE results.\n\n\n\n\n\n\n\n\n#| label: shiny-sa-example\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A two-dimensional parameter space with axes for the scale parameter and the SVM cost is shown. Three initial points are shown. The SA algorithm progresses from a point with low cost and a large value of the scale factor to meander to the ridge of optimal performance, starting several times along the way. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-sa.R\")\n\napp\n\n\n\nFigure 12.3: An example of how simulated annealing can investigate the tuning parameter space. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n\n\n\n\n\n\nDuring the search, there were 4 iterations where a new global best result was discovered (iterations 1, 14, 15, and 40). There were also 5 restarts at iterations 9, 23, 31, 39, and 48. In the end, the best results occurred with a cost value of 2-2.5 and a scale factor of 10-0.98. The corresponding validation set RMSE was 5.78%. With this random seed, the search gets near the ridge of best performance shown in Figure 12.1 but only lingers there for short times. It does spend a fair amount of time meandering in regions of poor performance.\nSimulated annealing has several attractive qualities. First, the process of generating new candidates works with any type of parameter, whether real, integer, or qualitative. This is not true for the other two iterative methods we’ll discuss. Additionally, the perturbation process is very fast, meaning there is minimal computational overhead to compute the next objective function value. Since we are effectively generating new candidate sets, we can also apply constraints to individual parameters or groups of parameters. For example, if a tuning parameter is restricted to odd integers, this would not pose a significant problem for simulated annealing.\nThere are a few downsides to this method. Compared to the other search methods, SA makes small incremental changes. If we start far away from the optimum, many iterations might be required to reach it. One way to mitigate this issue is to do a small space-filling design and start from the best point (as we did). In fact, applying SA search after a grid search (perhaps using racing) can be a good way to verify that the grid search was effective.\nAnother disadvantage is that a single candidate is processed at a time. If the training set size is not excessive, we could parallel process the multiple candidates simultaneously. We could make a batch of perturbations and pick the best value to keep or apply the probabilistic process of accepting a poor value.\nThis optimization took 31.5s per candidate to execute for these data.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-genetic-algo",
    "href": "chapters/iterative-search.html#sec-genetic-algo",
    "title": "12  Iterative Search",
    "section": "12.4 Genetic Algorithms",
    "text": "12.4 Genetic Algorithms\nGenetic algorithms (GAs) (Mitchell 1996; Eiben and Smith 2015) are an optimization method that mimics the process of evolution through natural selection. While GAs are not ideally suited for parameter tuning in our case—since a standard GA search typically requires hundreds to millions of objective function evaluations—we will limit the search to a smaller scale for practical reasons. Nevertheless, genetic algorithms can often find solutions near the optimal value fairly quickly. Additionally, as previously mentioned, there is usually a region of acceptable candidates. Finally, GAs are highly unlikely to become trapped in a locally optimal solution.\nInstead of search iterations, genetic algorithms are counted in generations. A generation is a group of candidate values that are evaluated at the same time (as a batch)8. Once their corresponding performance metrics are computed, a small set of the best candidates is selected and is used to create the next generation via reproduction. Reproduction would entail combining a pair of “parent” candidates by swapping information9, and then random mutations can be applied. Once the next generation is created, the process continues for some pre-defined time limit or maximum number of generations.\nHow, exactly, does this work? The first step is to pick a numerical representation for each candidate. Let’s consider methods for our main types of data.\n\nReal-Valued Parameters\nFor real numbers, there are two encoding methods: one option is to keep them as-is (i.e., floating point values) and another converts the values to a binary encoding.\nWhen keeping the values as real numbers, we can make two children from a pair of well-performing candidates via a linear combination. For candidate vectors \\(\\boldsymbol{\\theta}_j\\), we can use:\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}^{\\:kid}_1 &= \\alpha \\boldsymbol{\\theta}^{\\:par}_1 + (1 - \\alpha) \\boldsymbol{\\theta}^{\\:par}_2 \\notag \\\\\n\\boldsymbol{\\theta}^{\\:kid}_2 &= (1-\\alpha) \\boldsymbol{\\theta}^{\\:par}_1 + \\alpha \\boldsymbol{\\theta}^{\\:par}_2 \\notag \\\\\n\\end{align}\n\\]\nwhere \\(\\alpha\\) is a random standard uniform number. Notice that the two children’s candidate values will always be in-between the values of their parents.\nFor mutation, a candidate’s values are either locally perturbed or simulated with rate \\(\\pi_m\\). For example, we might mutate the log cost value by simulating a random number across the range that defines its search space.\nBinary encodings for real numbers were suggested in the early days of genetic algorithms. In this case, each candidate value is encoded as a set of binary integers. For example, consider a log2 cost value of -2.34. To convert this to binary, we multiply it by 100 (assuming a limit of two decimal places) to convert it to an integer. If we use 8 binary digits (a.k.a. “bits”) to represent 234, we get 11101010. If the candidates could have both positive and negative numbers, we can add an extra bit at the start that is 1 when the value is positive, yielding: 011101010.\nThe method of cross-over was often used for the reproduction of binary representations. For a single cross-over, a random location between digits was created for each candidate value, and the binary digits were swapped. For example, a representation with five bits might have parents ABCDE and VWXYZ. If they were crossed over between the second and third elements, the children would be ABXYZ and VWCDE. There are reproduction methods that use multiple cross-over points to create a more granular sharing of information.\nThere are systematic bias that can occur when crossing binary representations as described by Rana (1999) and Soule (2009). For example, there are positional biases. For example, if the first bits (A and V) capture the sign of the value, the sign is more likely to follow the initial bits than the later bits to an offspring.\nFor mutating a binary representation, each child’s bit would be flipped at a rate of \\(\\pi_m\\).\nAfter these reproduction and mutation, the values are decoded into real numbers.\n\n\nInteger Parameters\nFor integers, the same approach can be used as real numbers, but after the usual operations, the decimal values are coerced to integers via rounding. If a binary encoding is used, the same process can be used for integers as real numbers; they are all just bits to the encoding process.\n\n\nQualitative Parameters\nFor qualitative tuning parameters, one (inelegant) approach is to encode them into values on the real line or as integers. For example, for a parameter with values “red,”, “blue,” and “green”, we could map them to bins of [0, 1/3), [1/3, 2/3), and [2/3, 1]. From here, we reproduce and mutate them as described above, then convert them back to their non-numeric categories. This is more palatable when there is a natural ordering of the values but is otherwise a workable but unfortunate approach. Mutation is simple though; at rate \\(\\pi_m\\), a value is flipped to a random selection of the possible values.\n\n\n12.4.1 Assembling Generations\nNow that we know how to generate new candidates, we can form new generations. The first step is to determine the population size. Typically, the minimum population size within a generation is 25 to 50 candidates. However, this may be infeasible depending on the computational cost of the model and the characteristics of the training set, such as its size. While fewer candidates can be used, the risk is that we may fail to sample any acceptable results. In such cases, combining the best results will only produce more mediocre candidates. Increasing the mutation rate can help mitigate this issue. For the first iteration, random sampling of the parameter space is commonly used. It is highly recommended to use a space-filling design for the initial candidate set to ensure a more comprehensive exploration.\nFinally, there is the choice of which candidates to reproduce at each generation. There are myriad techniques for selecting which candidates are used to make the next generation of candidates. The simplest is to pick two parents by randomly selecting them with probabilities that are proportional to their performance (i.e., the best candidates are chosen most often). There is also the idea of elitism in selection. Depending on the size of the generation, we could retain a few of the best-performing candidates from the previous generation. The performance values of these candidates would not have to be recomputed (saving time), and their information is likely to persist in a few good candidates for the next generation.\n\n\n12.4.2 Two Parameter Example\nTo illustrate genetic algorithms in two dimensions, a population size of 8 was used for 7 generations. These values are not optimal defaults but were selected to align with the previous SA search and the optimization method discussed in the next section. The two tuning parameters were kept as floating-point values. Parental selection was based on sampling weights proportional to the RMSE values (with smaller values being better). The mutation rate was set at 10%, and elitism was applied by retaining the best candidate from each generation. All computations within a generation were performed in parallel. On average, the optimization took 17.2s per candidate. Figure 12.4 illustrates the results of the search.\n\n\n\n\n\n\n\n\n#| label: shiny-ga-example\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A two-dimensional parameter space with axes for the scale parameter and the SVM cost is shown. Eight initial points are shown, one near the ridge of best results. As generations increase, new generations are gathered around the ridge. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-ga.R\")\n\napp\n\n\n\nFigure 12.4: Several generations of a genetic algorithm that search tuning parameter space. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n\n\n\n\n\n\nA space-filling design was used to ensure that the initial population was diverse. Fortuitously, one design point was very close to the ridge, with an RMSE of 5.83%. After this, there were 4 generations with better candidates (with almost identical performance): 5.82%, 5.772%, 5.765%, and 5.764%. We can see that, after three generations, the search is concentrated around the ridge of optimal performance. In later generations, some candidates have outliers in one dimension; this is the effect of mutation during reproduction.\n\n\n12.4.3 Summary\nThis small toy example is not ideal for demonstrating genetic algorithms. The example’s low parameter dimensionality and the relatively low computational cost per candidate might give the impression that genetic algorithms are a universal solution to optimization problems. While genetic algorithms are versatile and powerful tools for global optimization, they come with limitations. Like simulated annealing, they have minimal overhead between generations, but in most real-world applications, there are more than just two parameters. This typically means that larger populations and more generations are needed. If the dataset is not excessively large, parallel processing can help manage the increased computational demands.10.\n\nNow we’ll take a look at what makes a Bayesian model Bayesian.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes",
    "href": "chapters/iterative-search.html#sec-bayes",
    "title": "12  Iterative Search",
    "section": "12.5 Sidebar: Bayesian Models",
    "text": "12.5 Sidebar: Bayesian Models\nWe’ve superficially described an application of Bayesian analysis in Section 6.4.3. Before discussing Bayesian optimization, we should give a general description of Bayesian analysis, especially since it will appear again several times after this chapter.\nMany models make probabilistic assumptions about their data or parameters. For example, a linear regression model has the form\n\\[\ny_i = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p + \\epsilon_i\n\\]\nUsing ordinary least squares estimation, we can make assumptions regarding the model errors (\\(\\epsilon_i\\)). We can assume that the residuals are independent of one another and follow a Gaussian distribution with zero mean and a constant standard deviation. From there, it follows that the regression parameters (\\(\\beta\\) coefficients) also follow Gaussian distributions.\nBased on these assumptions, our objective function is the Gaussian likelihood function, generally denoted as \\(\\ell(z|\\theta)\\), although we often maximize the log of this value. We fix the outcome and predictor data and try to find values of \\(\\sigma\\) and the \\(\\beta\\) parameters that maximize the objective function (log \\(\\ell(z|\\theta)\\)). This process is maximum likelihood estimation.\n\nOne important point is that each model parameter is treated as a single value. Our maximum likelihood estimate (MLE) is a point estimate and, based on our assumptions about the residuals, we know the distribution of the MLEs.\n\nThe consequence of this is that we cannot make inferences about the true, unknown model parameters since they are considered to be single points. Instead, our inference focuses on the MLEs. This leads to the circuitous explanation of hypothesis tests and confidence intervals. For example, the explaination of a 90% confidence interval is:\n\n“We believe that if we were to repeat this experiment a large number of times, the true parameter value would fall between \\(L\\) and \\(U\\) 90% of the time.”\n\nA Bayesian approach takes a different perspective on probability assumptions. It assumes that the unknown parameters are drawn from a prior distribution that represents our beliefs or knowledge about them before observing the data. We denote this prior distribution as \\(\\pi(\\theta)\\). The term “prior” is crucial because the modeler should define this distribution before seeing the observed data.\nFor example, consider our model for the time to deliver food in Equation 1.1. A reasonable prior for the regression parameter associated with distance would have a distribution that assigns zero probability to negative values, since we would never expect (log) delivery times to decrease with distance. If we worked at the restaurant, we could be more specific based on our experience. We might believe that, all other factors being equal, each additional mile from the restaurant doubles the delivery time. Using this assumption, we could define a probability distribution that reflects this belief.\nOnce we have our prior distributions for our parameters and assumptions regarding our data distributions, we can write down the equations required to estimate our results. Bayes’ Rule is a basic probability statement that combines the prior distribution with our likelihood function:\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) \\ell(x|\\theta)} {\\pi(x)}\n\\tag{12.3}\\]\n\\(\\pi(\\theta|x)\\) is the posterior distribution: the probability distribution of our parameters, given the observed data. This is the endpoint for any Bayesian analysis.\nAnother important point is that Bayesian estimation has a much more difficult goal than maximum likelihood estimation. The latter needs to find point estimates of its parameters while Bayesian estimation has to estimate the entire posterior distribution \\(\\pi(\\theta|x)\\). In a moment, we’ll look at a simple problem with a simple solution. However, in most other cases, the computational requirements for Bayesian estimation are considerably higher. That’s the bad news.\nThe good news is that, once we find the posterior distribution, it is incredibly useful. First, we can make direct statements about parameter values. Unlike confidence intervals, Bayesian methods allow us to say things like\n\n“We believe that there is a 90% probability that the true value of the parameter is between \\(L\\) and \\(U\\).”\n\nIt also lets us easily make similar statements regarding more complex combinations of parameters (such as ratios, etc).\nFinally, we should mention the effect of the prior on the computations. It does pull the likelihood \\(\\ell(x|\\theta)\\) towards it. For example, suppose that for Equation 1.1 we used a highly restrictive prior for \\(\\beta_1\\) that was a uniform distribution between [0.7, 0.9]. In that case, the posterior would be confined to this range11. That said, the effect of the prior on the posterior decreases as our training set size increases. We saw this in Section 6.4.3 where two travel agents were contrasted; one with many reservations in the data and another with very few.\nTo illustrate, let’s look at a very simple example.\n\n12.5.1 A Single Proportion\nFor the forestry data discussed earlier in Section 10.8, the outcome is categorical, with the values “yes” and “no” representing the question “Is this location forested?” We can use the training dataset to estimate the probability (\\(\\pi\\)) of the event occurring. The simplest estimate is the sample proportion, which is calculated by dividing the number of occurrences of the event (e.g., “yes” responses) by the total number of data points. This gives the estimate \\(\\hat{\\pi}\\) = 56.167%).\nTo estimate this rate using maximum likelihood estimation, we might assume that the data follow a binomial distribution with theoretical probability \\(\\pi\\). From there we can solve equations that find a value of \\(\\pi\\) that correspond to the largest likelihood. It turns out that the sample proportion is also the maximum likelihood estimate.\nInstead of treating the unknown parameter as a single value, Bayesian methods propose that the parameter comes from a distribution of possible values. This distribution reflects our prior understanding or belief about the parameter based on what we know of the situation. For this reason, it is called a prior distribution. If we think that locations in Washington state are very likely to be forested, we would choose a distribution that has more area for higher probabilities.\nFor a binomial model, an example of a prior is the Beta distribution. The Beta distribution is very flexable and has values range between zero and one. It is indexed by two parameters: \\(\\alpha\\) and \\(\\beta\\). Figure 12.5 shows different versions of the Beta distribution for different values.\nTo settle on a specific prior, we would posit different questions such as:\n\n“Are there rate values that we would never believe possible?”\n“What is the most likely value that we would expect and how certain are we of that?”\n“Do we think that the distribution of possible values is symmetric?”\n\nand so on.\nFrom these answers, we would experiment with different values of \\(\\alpha\\) and \\(\\beta\\) until we find a combination that encapsulates what we believe. If we think that larger probabilities of forestation are more likely, we might choose a Beta prior that has values of \\(\\alpha\\) that are larger than \\(\\beta\\), which places more mass on larger values (as seen below). This is an example of an informative prior, albeit a weak one. As our prior distribution is more peaked it reflects that, before seeing any data, we have strong beliefs. If we honestly have no idea, we could choose a uniform distribution between zero and one using \\(\\alpha = \\beta = 1\\).\n\n\n\n\n\n\n\n\nFigure 12.5: Examples of the Beta distribution for different values of \\(\\alpha\\) and \\(\\beta\\).\n\n\n\n\n\nWe’ll use values of \\(\\alpha = 5\\) and \\(\\beta = 3\\). If the training set has \\(n_{tr}\\) points and \\(n_+\\) of them are known to be forested, the ordinary sample proportion estimate is \\(\\hat{p} = n_+ / n_{tr}\\). In a Bayesian analysis, the final estimate is a function of both the prior distribution and our observed data. For a Beta prior, the Bayesian estimate is\n\\[\n\\hat{p}_{BB} = \\frac{n_+ +\\alpha}{n_{tr} + \\alpha + \\beta}\n\\tag{12.4}\\]\nIf, before seeing the data, we had chosen \\(\\alpha = 5\\) and \\(\\beta = 1\\), we estimate that \\(\\hat{p}_{BB} = 56.201\\)%. This is pretty close to our simple estimate because there is so much data in the training set (\\(n_{tr}\\) = 4,832) that the influence of the prior is severely diminished. As a counter-example, suppose we took a very small sample that resulted in \\(n_y\\) = 1 and \\(n_{tr}\\) = 3, the prior would pull the estimate from the MLE of 33.3% to 54.5%.\nNow suppose that for regulatory purposes, we were required to produce an interval estimate for our parameter. From these data and our prior, we could say that the true probability of forestation has a 90% chance of being between 55% and 57.3%.\nWe did mention that it can be very difficult to compute the posterior distribution. We deliberately chose the Beta distribution because, if we assume a binomial distribution for our data and a \\(Beta(\\alpha, \\beta)\\) prior, the posterior is \\(Beta(\\alpha + n_y, n_{tr} - n_y + \\beta)\\). More often than not, we will not have a simple analytical solution for the posterior. That said, we’ll see this happen again in Section 12.6.2.\nFor a Bayesian model, the predictions also have a posterior distribution, reflecting the probability of a wide range of values. As with non-Bayesian models, we often summarize the posterior using the most likely value (perhaps using the mean or mode of the distribution). We can also measure the uncertainty in the prediction using the estimated standard deviation.\n\n\n12.5.2 What is not Bayesian?\nMost other methods fall into the category of “Frequentist.” The terms “Frequentist” and “Bayesian” correspond to different philosophies of interpreting probability. They are not the only two philosophies, but between them, they account for the vast majority of data analytic methods.\nMaximum likelihood estimation is a good example of a Frequentist approach. As we said, when training a model, the optimal parameters are the ones that maximize the probability that we actually did get the data that we observed. If we flip a coin 100 times and 90 of them come up heads, MLE would not choose parameter values that are inconsistent with the results from our 100 observed results.\nCoin flipping does present a good window into how both philosophies operate. Consider the probability of getting heads when tossing a specific coin. The two points of view are:\nFrequentists:\n\nThe probability of being heads is a single unknown value representing the long-term rate at which one would get heads with that coin after infinite tosses. Once we toss the coin, there is no uncertainty. If it is heads, the probability that it is heads is exactly 100%.\n\nBayesians:\n\nThe probability of heads is a random variable with an unknown distribution. We can incorporate our beliefs about that distribution into our analyses, often making them somewhat biased. We update our beliefs with data so that our inferences and predictions are a combination of both. We can make direct probabilistic statements about unknown parameters so if a coin was heads, we can still estimate the probability that it would be heads.\n\nIf you are unsure about the tool that you are using, terms like “maximum likelihood estimation”, “confidence intervals”, and “hypothesis testing” should tell you that it takes a Frequentist view. Terms such as “prior,” “posterior,” or “credible interval” are more indicative of a Bayesian methodology.\nBland and Altman (1998) and Fornacon-Wood et al. (2022) are fairly non-technical discussions of the two camps, while Wagenmakers et al. (2008) discusses more mathematical differences. Discussions on this subject can be vigorous. Gill, Sabin, and Schmid (2005)’s discussion of how medical doctors make decisions is interesting and elicited numerous letters to the British Medical Journal (Chitty 2005; Hutchon 2005; McCrossin 2005). Others have had pragmatic, if not emphatic, thoughts on the subject (Breiman 1997).\n\nTo summarize:\n\nBayesian models require a prior distribution for all of our model parameters.\nThe prior and observed data are combined into a posterior distribution.\nDifferent statistics can be estimated from the posterior, including individual predictions.\n\n\n\nWe’ll encounter even more Bayesian methods in subsequent chapters. For example, in Chapter 14, both Frequentist and Bayesian approaches to inference are discussed. In ?sec-cls-metrics, we’ll describe the process of developing a prior for muticlass problem based on Wordle scores and how priors can affect our results.\nNow that we know more about Bayesian statistics, let’s see how they can be used to tune models.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes-opt",
    "href": "chapters/iterative-search.html#sec-bayes-opt",
    "title": "12  Iterative Search",
    "section": "12.6 Bayesian Optimization",
    "text": "12.6 Bayesian Optimization\nBayesian optimization (BO) is a search procedure that uses an overarching statistical model to predict the next set of candidate values based on past performance. It is currently the most used iterative search routine for optimizing models. The technique originated with Močkus (1975) and was further developed by Jones, Schonlau, and Welch (1998). It is often tied to a particular Bayesian predictive model: the Gaussian process (GP) regression model (Binois and Wycoff 2022).\nBayesian optimization and Gaussian proceses have been vibrant research areas and our discussions will scratch the surface. More recent surveys, Gramacy (2020) and Garnett (2023), are comprehensive resources for these methodologies.\nHowever, before diving into the details of Gaussian proceseses, we should discuss how any Bayesian method can be used for optimization. Similar to the racing model from Equation 11.1, the Bayesian model’s outcome will be our estimated performance statistic (i.e., \\(\\hat{Q}\\)). The predictors for this model will be the tuning parameter values (\\(\\boldsymbol{\\theta}\\)):\n\\[\nQ_j = f(\\boldsymbol{\\theta}_j)+ \\epsilon_{j}\n\\tag{12.5}\\]\nwhere \\(j\\) is the iteration number, \\(f(\\cdot)\\) is a surrogate model, and \\(\\epsilon\\) is an error term. For the barley data, we’ve used RMSE as our primary metric. Using a Bayesian model for \\(f(\\cdot)\\) enables us to predict what we expected the RMSE to be for different candidate points.\nSince a Bayesian model produces a posterior (predictive) distribution, we can obtain the distribution of the performance metric for a potential new candidate. We often summarize this predictive distribution with the mean and standard deviation. Figure 12.6 shows two hypothetical candidates and their respective predictive distributions. The first candidate has a slightly better predicted RMSE (on average). The area under the distribution curve to the right of the vertical line reflects the probability of being worse than the current solution which, for this candidate, is 16%.\n\n\n\n\n\n\n\n\nFigure 12.6: An example of a choice between two candidates where performance is gauged by RMSE. One candidate has slightly better predicted mean performance and another where the mean RMSE is predicted to be much lower but with high uncertainty.\n\n\n\n\n\nThe second candidate has a mean RMSE that is predicted to have about a 2-fold improvement over the first but comes with considerably large uncertainty. On average, this is a better choice but there is more area to the right of the vertical line. The probability of the second candidate having worse results than the current best is 26%.\nWhich option is better? The first candidate is a safer choice, while the second comes with higher risk but also a greater potential reward. The decision often depends on our perspective. However, it’s important to understand why the second candidate has such a large variance in outcomes.\nWe’ve previously discussed the variance-bias tradeoff and demonstrated that large uncertainty in the performance metric can often be explained by model variance. Recall that models might vary due to predictive instability caused by a model that is far more complex than is required, such as the polynomial example in Section 8.4.\nHowever, there is another reason that the noise in our performance statistic can be large. Back in Section 3.9, we described spatial autocorrelation, where objects are more similar to closer objects than to objects further away. If our Bayesian model reflects spatial variability, the increased uncertainty can be attributed to how each candidate relates spatially to the existing set. If our model’s variance in Figure 12.6 is based on spatial effects, it implies that the first candidate is closer to the current collection of tuning parameter candidates. Conversely, the large uncertainty for the second candidate suggests that it is placed far away from existing results.\nThis example illustrates two different search strategies: exploitation and extrapolation.\n\nExploitation involves focusing the search near existing data, leveraging known results to refine and optimize solutions within familiar territory.\nExtrapolation emphasizes exploring new, untested areas more aggressively in search of novel candidates that could lead to model improvements.\n\nUsing the Bayesian model in Equation 12.5 is beneficial because it allows for both mean and variance predictions, which can be weighted differently when evaluating potential new candidates. If our surrogate Bayesian model effectively captures spatial effects, we can leverage this to optimize the model. The next section outlines the overall optimization process.\n\n12.6.1 How Does Bayesian Optimization Work?\nThe search process begins with an initial set of candidates (\\(s_0\\)) and their corresponding performance statistics (_j$). These data are used to build a surrogate model, where the performance metric serves as the outcome and the candidate values act as predictors. The model then predicts the mean and standard deviation of the metric for new candidate values.\nTo guide the search, we use an acquisition function—an objective function that combines the predicted mean and standard deviation into a single value (Garnett 2023, chap. 7). The next candidate to be evaluated is the one that optimizes this acquisition function. This process is then repeated iteratively.\nOne of the most well-known acquisition functions is based on the notion of expected improvement (Jones, Schonlau, and Welch 1998, sec. 4). If the current best metric value is denoted as \\(\\hat{Q}_{opt}\\), the expected improvement is the positive part of \\(\\delta(\\hat{\\boldsymbol{\\theta}}) = Q_{opt} - \\mu(\\hat{\\boldsymbol{\\theta}})\\), where \\(\\mu(\\hat{\\boldsymbol{\\theta}})\\) is the mean of the posterior prediction (assuming that smaller metrics are better). They found that, probabilistically, the expected improvement is\n\\[\nEI(\\hat{\\boldsymbol{\\theta}}; Q_{opt}) = \\delta(\\hat{\\boldsymbol{\\theta}}) \\Phi\\left(\\frac{\\delta(\\hat{\\boldsymbol{\\theta}})}{\\sigma(\\hat{\\boldsymbol{\\theta}})}\\right) + \\sigma(\\hat{\\boldsymbol{\\theta}}) \\phi\\left(\\frac{\\delta(\\hat{\\boldsymbol{\\theta}})}{\\sigma(\\hat{\\boldsymbol{\\theta}})}\\right)\n\\tag{12.6}\\]\nwhere \\(\\Phi(\\cdot)\\) is the cumulative standard normal and \\(\\phi(\\cdot)\\) is the standard normal density.\nExpected improvement balances both mean and variance, but their influence can shift throughout the optimization process. Consider a parameter space where most regions have already been sampled to some extent. If the Bayesian model is influenced by spatial variation, the remaining unexplored areas will generally have low variance (\\(\\sigma\\)), causing one term in Equation 12.6 to dominate the optimization. In contrast, during the early iterations, variance tends to be much higher. Unless there is a strong mean effect, spatial variance will dominate, leading the search to focus more on extrapolation.\nAdditionally, the expected improvement can be altered to include a tradeoff factor \\(\\xi\\) so that improvement is the positive part of\n\\[\n\\delta(\\hat{\\boldsymbol{\\theta}}) = Q_{opt} - \\mu(\\hat{\\boldsymbol{\\theta}}) + \\xi\n\\]\nThis effectively inflates the value of the current best and the resulting value of \\(\\delta(\\hat{\\boldsymbol{\\theta}})\\) helps emphasize extrapolation. \\(\\xi\\) could be a single constant or a function that changes the tradeoff value as a function of iterations.\nSimilar to simulated annealing, sampling continues until reaching a predefined limit on iterations or computational time. Additionally, an early stopping rule can be used to end the search early if no new best result is found within a set number of iterations.\nLet’s see how this approach works in a single dimension. Figure 12.8 shows how expected improvement can be used to find the global minimum of \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\). The light grey line in the lower panel represents the true value of \\(\\psi(\\theta)\\) across the parameter space. The process starts with \\(s_0 = 2\\) candidate points at -8.0 and 3.0. From these two points, the surrogate model is fit and the dark line in the lower panel shows the predicted objective function values across the space. The magnitude of the line represents the mean of the posterior. Notice that the predicted line fails to capture the true trend, which is not unexpected with two data points.\nIf we optimized only on the mean prediction, we would select the next point near \\(\\theta = 3\\) since it has the lowest predicted value. This approach relies entirely on exploitation.\nThe color of the line depicts the predicted variance, which drops to zero near the existing points and explodes in the regions in between our two points. If we focused solely on the variance of the objective function, we would choose the next point in an area far from the two sampled points, such as the interval \\(-6 \\le \\theta \\le 1\\) or \\(\\theta \\ge 4\\).\nBy combining the mean and variance predictions through expected improvement, we strike a balance between the two. In the first iteration, two areas near (but not exactly at) \\(\\theta = 3.0\\) are predicted to have large expected improvements. The region on the left is sampled first, and once \\(\\psi(\\theta = 3)\\) is evaluated, the next point moves closer to the true global optimum. This process continues, alternating between sampling near the best result and exploring regions with few nearby candidate values. After 15 iterations, the search is very close to the true global optimum.\n\n\n\n\n#| label: shiny-bayes-opt\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A single curve is shown with alternating peaks and valleys. The x-axis is the parameter, and the y-axis shows the objective function. Above this is another curve where the y-axis is expected improvement. As the optimization proceeds, the top curve shows several peaks of expected improvement, and the candidate points begin to cluster around the global minimum. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\nload(url(\"https://raw.githubusercontent.com/aml4td/website/main/RData/bayesian_opt_1d.RData\"))\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-bo-1d.R\")\n\napp\n\n\n\nFigure 12.7: An example of Bayesian optimization for the function \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\). The top plot shows the expected improvement for the current iteration. The bottom panel shows the true curve in grey and the mean prediction line (colored by the predicted standard deviation). The vertical line indicates the location of the next candidate.\n\n\n\nNumerous other acquisition functions can be used. For example, instead of expected improvement, we can maximize the probability of improvement:\n\\[\nPI(\\hat{\\boldsymbol{\\theta}}) = \\Phi\\left(\\frac{\\tau - \\mu(\\hat{\\boldsymbol{\\theta}})}{\\sigma(\\hat{\\boldsymbol{\\theta}})}\\right)\n\\]\nwhere \\(\\tau\\) is some performance metric goal (e.g., an R2 of 80%). This is generally deprecated in favor of EI for a few reasons. Most importantly, PI does not directly incorporate the current best value. Additionally, the value of \\(\\tau\\) is arbitrary and the search can be inconsistent across values of \\(\\tau\\) (Jones 2001).\nAnother acquisition function is based on confidence bounds (Cox and John 1992). For minimizing the performance statistic, the lower bound would be computed:\n\\[\nCB(\\hat{\\boldsymbol{\\theta}}) = \\mu(\\hat{\\boldsymbol{\\theta}}) - \\kappa\\: \\sigma(\\hat{\\boldsymbol{\\theta}})\n\\]\nwhere \\(\\kappa\\) sets the confidence level and often has values that are much smaller than range used for statistical inference (e.g., centered around 1.64). This multiplier also modulates between extrapolation (large \\(\\kappa\\)) and exploitation (small \\(\\kappa\\)) for the search.\nBayesian optimization offers more than just different acquisition functions; there are also various other modifications that can enhance its efficiency. For instance, similar to simulated annealing, the default approach samples one candidate at a time. While this works, it can be computationally inefficient, especially when models can be fitted in parallel. To improve this, modifications allow for multiple candidates to be simulated at once. In this approach, the candidate with the highest predicted acquisition value is chosen, and its predicted performance is treated as if it were derived from an actual assessment. A new GP model is then created using this imputed data point, and the process continues by acquiring the next point. This cycle repeats until the desired number of new candidates is obtained (see Azimi, Fern, and Fern (2010)). Alternatively, we can focus on the most promising areas of the parameter space and sample several different points. For example, in iteration five of Figure 12.7, where there are three peaks showing improvement, we could sample each of these peaks and create a batch of three new candidates to evaluate all at once, rather than individually.\nNow that the general optimization process has been outlined, let’s focus on the most popular model associated with Bayesian optimization.\n\n\n12.6.2 Gaussian Process Models\nGPs (Williams and Rasmussen 2006; Garnett 2023) are often motivated as special cases of stochastic processes, which are the collection of random variables that are indexed by one or more variables. The most common example of a stochastic process is the stock market. A stock price is a random variable that is indexed by time. A Gaussian process is just a stochastic process whose data are assumed to be multivariate normal.\nGaussian processes are different from basic multivariable normal distributions. Let’s say that we collected the heights and weights of 100 students in a specific grade. If we collect the heights and weights into a two-dimensional vector \\(\\boldsymbol{x} = [height, weight]\\), we might assume that the data is multivariate normal, with a 2D population mean vector and a 2 \\(\\times\\) 2 covariance matrix, i.e., \\(\\boldsymbol{x} \\sim N(\\boldsymbol{\\mu}, \\Sigma)\\). In this case, the data are a sample of people at a static time point.\nHowever, a Gaussian process is a sequence of data so we have to specify their means and covariances dynamically; there are mean and covariance functions instead of vectors and matrices. The mean function is often denoted as \\(\\mu(\\boldsymbol{x})\\). One way of writing the covariance function is \\(\\Sigma(\\boldsymbol{x}, \\boldsymbol{x}') = k(\\boldsymbol{x}, \\boldsymbol{x}') + \\sigma^2_{\\epsilon}I\\) where \\(k(\\cdot, \\cdot)\\) is a kernel function and \\(\\sigma_{\\epsilon}\\) is a constant error term.\nWe’ve seen kernels before, back in Equation 12.1 when they were associated with support vector machine models. For Gaussian processes, the kernel will define the variance between two data points in the process. Some kernels can be written as a function of the difference \\(\\delta_{ii'} = ||\\boldsymbol{x} - \\boldsymbol{x}'||\\). A common kernel is the squared exponential:\n\\[\nk(\\boldsymbol{x}, \\boldsymbol{x}') = \\exp\\left(-\\sigma d_{ii'}^2 \\right)\n\\tag{12.7}\\]\nNote that this covariance function has it’s own parameter \\(\\sigma\\), similar to \\(a\\), \\(b\\), and \\(q\\) in Equation 12.1 (\\(\\sigma\\) is unrelated to \\(\\sigma_{\\epsilon}\\)).\nThe squared exponential function shows that as the two vectors become more different from one another (i.e., further away), the exponentiated difference becomes large, indicating high variance (apart from \\(\\sigma^2_{\\epsilon}\\)). For two identical vectors, this covariance function yields a variance of \\(\\sigma^2_{\\epsilon}\\). Now, we can see why the previous section had a strong emphasis on spatial variability for Bayesian optimization. Figure 12.7 used this kernel function.\nA more general covariance function uses the Matern kernel:\n\\[\nk_{\\nu }(\\delta_{ii'})=\n{\\frac {2^{1-\\nu }}{\\Gamma (\\nu )}}{\\Bigg (}{\\sqrt {2\\nu }}{\\frac {\\delta_{ii'}}{\\rho }}{\\Bigg )}^{\\nu }\\mathcal{K}_{\\nu }{\\Bigg (}{\\sqrt {2\\nu }}{\\frac {\\delta_{ii'}}{\\rho }}{\\Bigg )}\n\\tag{12.8}\\]\nwhere \\(\\mathcal{K}(\\cdot)\\) is the Bessel function, \\(\\Gamma(\\cdot)\\) is the Gamma function, and \\(\\rho\\) and \\(\\nu\\) are tuning parameters.\nSo far, our covariance functions have assumed that all tuning parameters are numeric. A common, though simplistic, approach for incorporating qualitative parameters into a Gaussian process is to convert them into binary indicators and treat them as generic numeric predictors. However, this method is problematic because it’s unlikely that these binary indicators follow a Gaussian distribution. Additionally, if the qualitative tuning parameters have many levels, this approach can rapidly increase the dimensionality of the Gaussian space, which in turn raises the computational costs of training the GP model.\nAlternatively, special “categorical kernels” take integers corresponding to levels of a categorical predictor as inputs. For example, one based on an exchangeable correlation structure is:\n\\[\nk(x, x') = \\rho I(x \\ne x') + I(x = x')\n\\tag{12.9}\\]\nwhere \\(\\rho &lt; 1\\) (Joseph and Delaney 2007). More extensive discussions of models with mixed types can be found in Qian and Wu (2008), Zhou, Qian, and Zhou (2011), and Saves et al. (2023).\nMultiple kernels can be used to accommodate a pipeline with both quantitative and qualitative tuning parameters. Depending on the situation, an overall kernel can be created via addition or multiplication.\nGenton (2001), Part II of Shawe-Taylor and Cristianini (2004), and Chapter 4 of Garnett (2023) contain overviews of kernel functions, their different classes, and how they can be used and combined for data analysis.\nHow does this relate to iterative tuning parameter optimization? Suppose that our multivariate random variable is the concatenation of an outcome variable and one or more predictors, our vector of random variable could be thought of as \\(\\boldsymbol{d} = [y, x_1, ..., x_p]\\). In a model, we’d like to predict the value of \\(y\\) is for a specific set of \\(x\\) values. Mathematically, this is \\(Pr[y | \\boldsymbol{x}]\\) and, since this collection of data is multivariate normal, there are some nice linear algebra equations that can be used to compute this. This ties into Bayesian analysis since we can write Equation 12.3 in terms of probabilities:\n\\[\nPr[y | \\boldsymbol{x}] = \\frac{Pr[y] Pr[\\boldsymbol{x} | y]}{Pr[\\boldsymbol{x}]} = \\frac{(prior)\\times (likelihood)}{(evidence)}\n\\tag{12.10}\\]\nThe GP model is Bayesian because we have specified the distributions for the outcome and predictors (\\(Pr[y]\\) and \\(Pr[\\boldsymbol{x}]\\), respectively) simultaneously. The kernel function is the primary driver of the prior distribution of Gaussian processes.\nIn theory, each of these is easy to compute with a simple multivariate normal. However, we don’t know the value of \\(\\sigma^2_{\\epsilon}\\) nor do we know the values of any kernel parameters used by \\(k(\\cdot,\\cdot)\\). The error term can be estimated a variety of ways, including maximum likelihood, cross-validation, and others (see Ameli and Shadden (2022) for a survey of methods). For the kernel tuning parameters, it is common to maximize the marginal likelihood to estimate these structural parameters (Williams and Rasmussen 2006, chap. 5). The Gaussian nature of this model enables a number of relatively straightforward procedures for these purposes (that does not involve another layer of resampling).\nHowver, training Gaussian process models can be difficult and the computation cost of training these models increases cubically with the number of data points. In our application, the training set size is the number of tuning parameter candidates with performance statistics. However, after the initial GP model has been trained, there are updating algorithms that can prevent the model from being completely re-estimated from scratch thus reducing training time.\n\nIn summary, Gaussian process models are nonlinear models that can use tuning parameter values to make predictions on future performance statistics. They can predict the mean and variance of performance, the latter being mostly driven by spatial variability.\n\nIn Figure 12.7 we’ve seen how well Bayesian optimization works for a simple one-dimensional function. Let’s apply it in context by using it to optimize our two SVM tuning parameters.\n\n\n12.6.3 A Two Dimensional Illustration\nReturning the the SVM example, we initialize the GP model using the same three points shown in Table 12.1. We again use the squared exponential kernel in Equation 12.7 to measure variability.\nFigure 12.8 shows the results that help us select the first new candidate value. Due to the small number of points used to fit the GP, the EI surface is virtually flat. The flatness of the acquisition function results in a fairly uninformative choice: the first point is very close to the best candidate in the initial set.\nOnce this new point is sampled, the surrogate GP model produces a clear trend in EI indicating a strong preference for higher cost values (and no effect of the scale parameter). The selected candidate is closer to the ridge of optimal performance. After acquiring this point, the next iteration shows that EI is focused on the scale parameter value and ignores the cost parameter. By iteration 13, a candidate is chosen that is nearly optimal.\nThis example shows that Bayesian optimization can make enormous leaps in parameter space, frequently choosing values of parameters at their minimum or maximum values. This is the opposite of SA, which meanders through local parameter space.\n\n\n\n\n\n\n\n\n#| label: shiny-bo-example\n#| viewerHeight: 630\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A two-dimensional parameter space with axes for the scale parameter and the SVM cost is shown. Three initial points are shown. The BO algorithm progresses from a point with low cost and a large scale factor value, making large jumps to different regions of the parameter space; several iterations come close to the optimal ridge. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-bo.R\")\n\napp\n\n\n\nFigure 12.8: An illustration of Bayesian optimization. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n\n\n\n\n\n\nThe complete search produced new optimal values at iterations iterations 4, 9, 13, 42, and 44. The decrease in RMSE shows few meaningful improvements among negligible decreases: 6.098%, 5.993%, 5.947%, 5.773%, 5.771%, and 5.77%. The search process was able to effectively tune the SVM model and find very good parameters.\nThe cost of the Gaussian process fit does become more expensive as new candidates are sampled, making the process somewhat less efficient than the previous approaches. The average time to compute each candidate was 56.2s, although the cost of each increases with iterations.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes-opt-nnet",
    "href": "chapters/iterative-search.html#sec-bayes-opt-nnet",
    "title": "12  Iterative Search",
    "section": "12.7 Example: Tuning a Neural Network",
    "text": "12.7 Example: Tuning a Neural Network\nWe revisit the barley data, where the percentage of barley oil in a mixture is estimated using spectrographic measurements. Our previous work on these data addressed the high correlation between predictors via feature engineering with principal components.\nIn this chapter, we’ll take a more sophisticated approach to preprocessing this type of data using signal processing techniques, specifically the Savitzky-Golay (SG) method (Rinnan, Van Den Berg, and Engelsen 2009; Schafer 2011). This technique incorporates two main goals: to smooth the data from location to location (i.e., across the x-axis in Figure 7.1(b)) and to difference the data. Smoothing attempts to reduce measurement noise without diminishing the data patterns that could predict the outcome. Differencing can remove the correlation between locations and might also mitigate other issues, such as systematic background noise.\nFor smoothing, the SG procedure uses a moving window across a sample’s location and, within each window, uses a polynomial basis expansion to produce a smoother representation of the data. The window width and the polynomial degree are both tuning parameters.\nDifferencing has a single tuning parameter: the difference order. A zero-order difference leaves the data as-is (after smoothing). A first-order difference is the smoothed value minus the smoothed value for the next location (and so on). Savitzky and Golay’s method uses the polynomial estimates to estimate the differences/derivatives, instead of manually subtracting columns after smoothing.\nFigure 12.9 shows an example of a small set of locations from a single spectra in Figure 7.1. The top row shows the effect of smoothing when differencing is not used. For a linear smoother (i.e., a polynomial degree of one), the process can over-smooth the data such that the nonlinear portions of the raw data lose significant amounts of information. This issue goes away as the polynomial order is increased. However, the higher degree polynomial nearly interpolates between data points and may not be smoothing the data in any meaningful way.\nThe bottom panel shows the data when a first-order difference is used. The pattern is completely different since the data now represent the rate of change in the outcome rather than the absolute measurement. Again, larger window sizes oversmooth the data, and the highest degree of polynomial provides minimal smoothing.\n\n\n\n\n\n\n\n\nFigure 12.9: Demonstrations of the Savitzky-Golay preprocessing method for a small region for one specific sample previously shown in Figure 7.1.\n\n\n\n\n\nWe’ll want to optimize these preprocessing parameters simultaneously with the parameters for the supervised model. With this example, the supervised model of choice is a neural network with a single layer of hidden units. We’ll tune over the following parameters, described at length in ?sec-cls-nnet:\n\nThe number of hidden units in the layer (2 to 100).\nThe type of nonlinear function that connects the input layer and the hidden layer, called the “activation function”. Possible functions are “tanh”, exponential linear unit (“ELU”) functions, rectified linear unit (“ReLU”) functions, and log-sigmoidal functions.\nThe number of epochs of degraded performance before early stopping is enacted (3 to 20).\nThe total amount of penalization for the model (-10 to 0, in log-10 units).\nThe proportion of \\(L_1\\) penalty (a.k.a. Lasso penalization) (0 to 1).\nThe initial learning rate used during gradient descent (-2 to -0.5, in log-10 units).\nThe “scheduling” function to adjust the learning rate over iterations: “constant”, “cyclic”, or “exponential decay”.\n\nIn all, there are ten tuning parameters, two of which are qualitative.\nA space-filling design with 12 candidate points were initially processed to use as the starting point for Bayesian optimization and simulated annealing. When constructing the grid, there are two constraints on the preprocessing parameters:\n\nThe smoothing polynomial degree must be greater or equal to the differentiation order.\nThe window size must be greater than the polynomial degree.\n\nAn initial grid was created without constraints, and candidates who violated these rules were eliminated. The two constraints will also be applied when simulated annealing and Bayesian optimization create new candidates.\nTable 12.2 shows the settings and their corresponding validation set RMSE values, which ranged between 4.3% and 14.4%. At least two of the settings show good performance results, especially compared to the RMSE values previously achieved when embeddings and/or SVMs were used.\n\n\n\n\n\n\n\n\n\nRMSE (%)\nWindow\nDiff. Order\nDegree\nUnits\nPenalty (log-10)\nActivation\nLearn Rate (log-10)\nRate Schedule\nStop Iter.\nL1 Mixture\n\n\n\n\n4.32\n19\n2\n10\n27\n-7.89\nelu\n-1.84\nnone\n12\n42.11\n\n\n4.57\n19\n3\n7\n84\n-9.47\ntanh\n-0.82\ndecay\n8\n31.58\n\n\n4.72\n15\n2\n3\n2\n-2.63\ntanh\n-1.45\nnone\n16\n5.26\n\n\n4.87\n15\n1\n1\n69\n-5.79\nelu\n-1.29\ndecay\n4\n0.00\n\n\n4.91\n9\n2\n8\n100\n-7.37\nelu\n-1.05\ncyclic\n5\n94.74\n\n\n5.88\n17\n0\n3\n79\n-6.84\nrelu\n-1.92\ncyclic\n19\n73.68\n\n\n6.16\n11\n4\n6\n94\n-3.68\nelu\n-1.21\ndecay\n20\n10.53\n\n\n6.47\n13\n0\n9\n38\n-5.26\nlog sigmoid\n-0.74\ncyclic\n14\n15.79\n\n\n7.01\n13\n0\n5\n7\n-6.32\ntanh\n-1.37\ndecay\n3\n89.47\n\n\n7.72\n21\n1\n5\n53\n-1.58\nlog sigmoid\n-0.50\nnone\n15\n78.95\n\n\n10.57\n5\n1\n4\n17\n-0.53\nelu\n-1.68\ndecay\n13\n68.42\n\n\n14.38\n21\n3\n7\n58\n0.00\nrelu\n-1.53\ncyclic\n4\n52.63\n\n\n\n\n\n\n\n\nTable 12.2: The initial set of candidates used for iterative search methods.\n\n\n\nCan we improve on these results using iterative optimization? To begin, a simulated annealing search was initialized using the best results in Table 12.2 and ran for 50 iterations with no early stopping. If a new globally best result was not found within eight iterations, the search restarted from the previously best candidate. The search was restarted 3 times at iterations 15, 31, and 47 and improved candidates were found at iterations 2, 7, 23, 39. The overall best candidate had an RMSE value of 3.65%, with a 90% confidence interval of (3.41%, 3.9%). The improvement appears to be a legitimate improvement over the initial best configuration. The top panel of Figure 12.10 shows the RMSE values over iterations and Table 12.3 lists the best candidate value.\n\n\n\n\n\n\n\n\nFigure 12.10: Validation set RMSE results for the optimization. The validation set RMSE is shown on the y-axis with 90% confidence intervals. Orange points indicate iterations where a new optimal value was found and vertical lines indicate where the algorithm restarted from the previous best candidate.\n\n\n\n\n\nA Bayesian optimization was also used with a squared exponential kernel for the covariance function and used expected improvement to select candidates at each of the 50 iterations. This search found new best results at 3 iterations: 1, 10, and 47. Like the SA search, the best RMSE was substantially different than the one found in the initial grid: 3.58% with 90% confidence interval, (3.3%, 3.87%). The iterations and final values also be seen in Figure 12.10 and Table 12.3, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInitial Grid\nSimulated Annealing\nBayesian Optimzation\n\n\n\n\nDifferentiation Order\n2\n2\n1\n\n\nPolynomial Degree\n10\n4\n8\n\n\nWindow Size\n19\n19\n21\n\n\nHidden Units\n27\n27\n53\n\n\nActivation\nelu\nrelu\ntanh\n\n\nPenalty (log10)\n-7.89\n-9.00\n-8.22\n\n\nL1 Proportion\n0.421\n0.212\n0.112\n\n\nLearning Rate (log10)\n-1.84\n-1.39\n-1.97\n\n\nRate Schedule\nnone\ncyclic\ncyclic\n\n\nStopping Rule\n12\n14\n14\n\n\nRMSE\n4.32%\n3.65%\n3.58%\n\n\nConf. Int.\n(4.03%, 4.6%)\n(3.41%, 3.9%)\n(3.3%, 3.87%)\n\n\n\n\n\n\n\n\nTable 12.3: The best results from the initial space-filling design and the two iterative searches for a neural network using Savitzky-Golay preprocessing.\n\n\n\n\n\n\n\nMany of the parameter values for the final candidates from each search are very similar. In particular, the learning rates, regularization penalties, and SG window sizes where nearly the same.\nIn terms of computing time, simulated annealing took 1m 55.9s on average for each candidate and compared to 3m 22.3s required for BO (a 1.7-fold difference).",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-iterative-summary",
    "href": "chapters/iterative-search.html#sec-iterative-summary",
    "title": "12  Iterative Search",
    "section": "12.8 Summary",
    "text": "12.8 Summary\nThere are several different ways to tune models sequentially. For tabular data, these approaches are rarely the only efficient method for model optimization, but they may be preferable for certain models and large data sets.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#chapter-references",
    "href": "chapters/iterative-search.html#chapter-references",
    "title": "12  Iterative Search",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmeli, S, and S Shadden. 2022. “Noise Estimation in Gaussian Process Regression.” arXiv.\n\n\nAzimi, J, A Fern, and X Fern. 2010. “Batch Bayesian Optimization via Simulation Matching.” Advances in Neural Information Processing Systems 23.\n\n\nBinois, M, and N Wycoff. 2022. “A Survey on High-Dimensional Gaussian Process Modeling with Application to Bayesian Optimization.” ACM Transactions on Evolutionary Learning and Optimization 2 (2): 1–26.\n\n\nBland, J, and D Altman. 1998. “Bayesians and Frequentists.” Bmj 317 (7166): 1151–60.\n\n\nBreiman, L. 1997. “No Bayesians in Foxholes.” IEEE Expert 12 (6): 21–24.\n\n\nChitty, R. 2005. “Why Clinicians Are Natural Bayesians: Is There a Bayesian Doctor in the House?” BMJ: British Medical Journal 330 (7504): 1390.\n\n\nCox, D, and S John. 1992. “A Statistical Method for Global Optimization.” In [Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics, 1241–46. IEEE.\n\n\nEiben, A, and J Smith. 2015. Introduction to Evolutionary Computing. Springer.\n\n\nFornacon-Wood, I, H Mistry, C Johnson-Hart, C Faivre-Finn, J O’Connor, and G Price. 2022. “Understanding the Differences Between Bayesian and Frequentist Statistics.” International Journal of Radiation Oncology, Biology, Physics 112 (5): 1076–82.\n\n\nGarnett, R. 2023. Bayesian Optimization. Cambridge University Press.\n\n\nGenton, M. 2001. “Classes of Kernels for Machine Learning: A Statistics Perspective.” Journal of Machine Learning Research 2 (Dec): 299–312.\n\n\nGill, C, L Sabin, and C Schmid. 2005. “Why Clinicians Are Natural Bayesians.” Bmj 330 (7499): 1080–83.\n\n\nGramacy, R. 2020. Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.\n\n\nHofmann, T, B Schölkopf, and A Smola. 2008. “Kernel Methods in Machine Learning.” The Annals of Statistics 36 (3): 1171–1220.\n\n\nHutchon, D. 2005. “Why Clinicians Are Natural Bayesians: Bayesian Confusion.” BMJ: British Medical Journal 330 (7504): 1390.\n\n\nJones, D. 2001. “A Taxonomy of Global Optimization Methods Based on Response Surfaces.” Journal of Global Optimization 21: 345–83.\n\n\nJones, D, M Schonlau, and W Welch. 1998. “Efficient Global Optimization of Expensive Black-Box Functions.” Journal of Global Optimization 13: 455–92.\n\n\nJoseph, R, and J Delaney. 2007. “Functionally Induced Priors for the Analysis of Experiments.” Technometrics 49 (1): 1–11.\n\n\nKirkpatrick, S, D Gelatt, and M Vecchi. 1983. “Optimization by Simulated Annealing.” Science 220 (4598): 671–80.\n\n\nLu, J. 2022. Gradient Descent, Stochastic Optimization, and Other Tales. Eliva Press.\n\n\nMcCrossin, R. 2005. “Why Clinicians Are Natural Bayesians: Clinicians Have to Be Bayesians.” BMJ: British Medical Journal 330 (7504): 1390.\n\n\nMitchell, M. 1996. An Introduction to Genetic Algorithms. MIT Press.\n\n\nMočkus, J. 1975. “On Bayesian Methods for Seeking the Extremum.” In Optimization Techniques IFIP Technical Conference, 400–404. Springer.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.\n\n\nQian, P, and CF Jeff Wu. 2008. “Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments.” Technometrics 50 (2): 192–204.\n\n\nRana, S. 1999. “The Distributional Biases of Crossover Operators.” In Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation-Volume 1, 549–56.\n\n\nRinnan, A, F Van Den Berg, and S Engelsen. 2009. “Review of the Most Common Pre-Processing Techniques for Near-Infrared Spectra.” Trends in Analytical Chemistry 28 (10): 1201–22.\n\n\nSaves, P, Y Diouane, N Bartoli, T Lefebvre, and J Morlier. 2023. “A Mixed-Categorical Correlation Kernel for Gaussian Process.” Neurocomputing 550: 126472.\n\n\nSchafer, R. 2011. “What Is a Savitzky-Golay Filter?” IEEE Signal Processing Magazine 28 (4): 111–17.\n\n\nSchölkopf, B, and A Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. The MIT Press.\n\n\nShawe-Taylor, J., and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.\n\n\nSoule, T. 2009. “Crossover and Sampling Biases on Nearly Uniform Landscapes.” In Genetic Programming Theory and Practice VI, 1–15. Springer US.\n\n\nSpall, J. 2005. Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control. John Wiley & Sons.\n\n\nWagenmakers, E, M Lee, T Lodewyckx, and G Iverson. 2008. “Bayesian Evaluation of Informative Hypotheses.” In, edited by H Hoijtink, I Klugkist, and P Boelen, 181–207. New York, NY: Springer New York.\n\n\nWilliams, C, and C Rasmussen. 2006. Gaussian Processes for Machine Learning. MIT press Cambridge, MA.\n\n\nZhang, J. 2019. “Gradient Descent Based Optimization Algorithms for Deep Learning Models Training.” arXiv.\n\n\nZhou, Q, P Qian, and S Zhou. 2011. “A Simple Approach to Emulation for Computer Models with Qualitative and Quantitative Factors.” Technometrics 53 (3): 266–73.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#footnotes",
    "href": "chapters/iterative-search.html#footnotes",
    "title": "12  Iterative Search",
    "section": "",
    "text": "Sections ?sec-cls-svm and ?sec-reg-svm↩︎\nSections ?sec-cls-nnet and ?sec-reg-nnet↩︎\nWhy do these two parameters use different bases? It’s mostly a convention. Both parameters have valid values that can range across many orders of magnitude. The change in cost tends to occur more slowly than the scaling factor, so a smaller base of 2 is often used.↩︎\nThese are not simulated data, so this surface is an approximation of the true RMSE via the validation set using a very large regular grid. The RMSE results are estimates and would change if we used different random numbers for data splitting.↩︎\nWe’ve seen \\(\\alpha\\) before when it was called the learning rate (and will revisit it later in this chapter).↩︎\nHowever, computing the Hessian matrix increases the computational cost by about 3-fold.↩︎\nThese values will be also used as the initial substrate for Bayesian optimization.↩︎\nThink of the candidates within a generation as a population of people.↩︎\nAnalogous to chromosomes.↩︎\nIf you have access to high-performance computing infrastructure, the parallel processing can run the individuals in the population in parallel and, within these, any resampling iterations can also be parallelized.↩︎\nWe’ll see how the prior can affect some calculations later in ?sec-naive-bayes. ↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html",
    "href": "chapters/feature-selection.html",
    "title": "13  Feature Selection",
    "section": "",
    "text": "13.1 The Cost of Irrelevant Predictors\nIn Chapters 5 through 8, we described different methods for adding new predictors to the model (e.g., interactions, spline columns, etc.). We can add any predictors (or functions of predictors) based on first principles or our intuition or contextual knowledge.\nWe are also motivated to remove unwanted or irrelevant predictors. This can decrease model complexity, training time, computer memory, the cost of acquiring unimportant predictors, and so on. For some models, the presence of non-informative predictors can decrease performance (as we’ll see in Figure 13.1).\nWe may want to remove predictors solely based on their distributions; unsupervised feature selection focuses on just the predictor data.\nMore often, though, we are interested in supervised feature selection1: the removal of predictors that do not appear to have a relationship to the outcome. This implies that we have a statistic that can compute some measure of each predictor’s utility for predicting the outcome.\nWhile it is easy to add predictors to a model, it can be very difficult (and computationally expensive) to properly determine which predictors to remove. The reason comes back to the misuse (or over-use) of data. We’ve described the problem of using the same data to fit and evaluate a model. By naively re-predicting the training set, we can accidentally get overly optimistic performance statistics. To compensate for this, resampling repeatedly allocates some data for model fitting and other data for model evaluation.\nFor feature selection, we have the same problem when the same data are used to measure importance, filter the predictors, and then train the model on the smaller feature set. It is similar to repeatedly taking the same test and then measuring our knowledge using the grade for the last test repetition. It doesn’t accurately measure the quantity that is important to us. This is the reason that this chapter is situated here instead of in Part 2; understanding overfitting is crucial to effectively remove predictors.\nThis chapter will summarize the most important aspects of feature selection. This is a broad topic and a more extensive discussion can be found in Kuhn and Johnson (2019). First, we’ll use an example to quantify the effect of irrelevant predictors on different models. Next, the four main approaches for feature selection are described and illustrated.\nDoes it matter if the model is given extra predictors that are unrelated to the outcome? It depends on the model. To quantify this, data were simulated using the equation described in Hooker (2004) and used in Sorokina et al. (2008):\n\\[\ny_i = \\pi^{x_{i1} x_{i2}}  \\sqrt{ 2 x_{i3} } - asin(x_{i4}) + \\log(x_{i3}  + x_{i5}) - (x_{i9} / x_{i10}) \\sqrt{x_{i7} / x_{i8}} - x_{i2} x_{i7} + \\epsilon_i\n\\tag{13.1}\\]\nwhere predictors 1, 2, 3, 6, 7, and 9 are standard uniform while the others are uniform on [0.6, 1.0]. Note that \\(x_6\\) is not used in the equation. The errors \\(\\epsilon_i\\) are Gaussian with mean zero and a standard deviation2 of 0.25. Training and testing data sizes where simulated to be 1,000 and 100,000, respectively.\nTo assess the effect of useless predictors, between 10 and 100 extra columns of standard normal data were generated (unrelated to the outcome). Models were fit, tuned, and the RMSE of the test set was computed for each model. Twenty simulations were run for each combination of models and extra features3.\nThe percent difference from baseline RMSE was computed (baseline being the model with no extra columns). Figure 13.1 shows the results for the following models:\nThese models are described in detail in subsequent chapters. The colors of the points/lines signifies whether the model automatically removes unused predictors from the model equation (see Section 13.6 below).\nFigure 13.1: For different regression models, the percent increase in RMSE is shown as a function of the number of additional non-informative predictors.\nThe results show that a few models, K-nearest neighbors, neural networks, and support vector machines, have severely degraded performance as the number of predictors increases. This is most likely due to the use of cross- and/or dot-products of the predictor columns in their calculations. The extra predictors add significant noise to these calculations, and that noise propagates in a way that inhibits the models from accurately determining the underlying relationship with the outcome.\nHowever, several other models use these same calculations without being drastically affected. This is due to the models automatically performing feature selection during model training. Let’s briefly describe two sets of models that were largely unaffected by the extra columns.\nThe other unaffected models are tree-based. As seen in Figure 2.5, these types of models make different rules by selecting predictors to split the data. If a predictor was not used in any split, it is functionally independent of the outcome in the prediction equation. This provides some insensitivity to the presence of non-informative predictors, although there is slight degradation in RMSE as the number of columns increases.\nIn summary, irrelevant columns minimally affect ML models that automatically select features. Models that do not have this attribute can be crippled under the same circumstances.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-irrelevant-predictors",
    "href": "chapters/feature-selection.html#sec-irrelevant-predictors",
    "title": "13  Feature Selection",
    "section": "",
    "text": "Bagged regression trees\nBayesian additive regression trees (BART)\nBoosted trees (via lightGBM)\nCubist\nGeneralized additive models (GAMs)\nK-nearest neighbors (KNN)\nMultivariate adaptive regression splines (MARS)\nPenalized linear regression\nRandom forest\nRuleFit\nSingle-layer neural networks\nSupport vector machines (radial basis function kernel)\n\n\n\n\n\n\nThe first set includes penalized linear regression, generalized additive models, and RuleFit. These add a penalty to their calculations, restricting the model parameters from becoming abnormally large unless the underlying data warrant a large coefficient. This effectively reduces the impact of extra predictors by keeping their corresponding model coefficient at or near zero.\nThe other set o model models include MARS and Cubist. They selectively include predictors in their regression equations, only including those specifically selected as having value.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-feature-levels",
    "href": "chapters/feature-selection.html#sec-feature-levels",
    "title": "13  Feature Selection",
    "section": "13.2 Different Levels of Features",
    "text": "13.2 Different Levels of Features\nWhen we talk about selecting features, we should be clear about what level of computation we mean. There are often two levels:\n\nOriginal predictors are the unmodified version of the data.\nDerived predictors refer to the set of columns that are present after feature engineering.\n\nFor example, if our model requires all numeric inputs, a categorical predictor is often converted to binary indicators. A date-time column is the original predictor, while the binary indicators for individual days of the week are the derived predictor. Similarly, the 550 measurement predictors in the barley data are the original predictors, and embedded values, such as the PCA components, are the derived predictors.\nDepending on the context, our interest in feature selection could be at either level. If the original predictors are expensive to estimate, we would be more interested in removing them at the original level; if any of their derived predictors are important, we need the original.\nThis idea will arise again in ?sec-importance when we discuss variable importance scores for explaining models.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-selection-overfitting",
    "href": "chapters/feature-selection.html#sec-selection-overfitting",
    "title": "13  Feature Selection",
    "section": "13.3 Overfitting the Feature Set",
    "text": "13.3 Overfitting the Feature Set\nWe’ve previously discussed overfitting, where a model finds patterns in the training data that are not reproducible. It is also possible to overfit the predictor set by finding the training set predictors that appear to be connected to the outcome but do not show the same relationship in other data.\nThis is most likely to occur when the number of predictors (\\(p\\)) is much larger than the training set size (\\(n_{tr}\\)) or when no external validation is used to verify performance. Ambroise and McLachlan (2002) shows reanalyses of high-dimensional biology experiments using a backward selection algorithm for removing predictors. In some cases, they could find a model and predictor subset with perfect accuracy even when the training set outcome data were randomly shuffled.\nRecall back in Section 1.5 an argument was made that preprocessing methods and the supervised model encompassed the entire model pipeline. Technically, feature selection (supervised or unsupervised) falls into the definition of preprocessing. It is important to use data that is different from the data used to train the pipeline (i.e., select the predictors) is different from the data used to evaluate how well it worked.\nFor example, if we use a resampling scheme, we have to repeat the feature selection process within each resample. If we used 10-fold cross-validation, each of the 10 analysis sets (90% of the training set) would have its own set of selected predictors.\nAmbroise and McLachlan (2002) demonstrated that when feature selection was enacted once, and then the model with this subset was resampled, they could produce models with excessively optimistic performance metrics. However, when the performance statistics were computed with multiple realizations of feature selection, there was very little false optimism.\nAnother improper example of data usage that can be found in the literature, as well as in real-life analyses, occurs when the entire data set is used to select predictors then the initial split is used to create the training and test sets. This “bakes in” the false optimism from the start, the epitome of information leakage4.\nHowever, performing feature selection in each resample can multiplicatively increase the computational cost, especially if the model has many tuning parameters. For this reason, we tend to focus on methods that automatically select features during the normal process of training the model (as discussed below).\nOne scheme to understand if we are overfitting to the predictors is to create one or more artificial “sentinel predictors” that are random noise. Once the analysis that ranks/selects predictors is finished, we can see how high the algorithm ranks these deliberately irrelevant predictors. It is also possible to create a rule where predictors that rank higher than the sentinels are selected to be included in the model.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-unsupervised-selection",
    "href": "chapters/feature-selection.html#sec-unsupervised-selection",
    "title": "13  Feature Selection",
    "section": "13.4 Unsupervised Selection",
    "text": "13.4 Unsupervised Selection\nThere are occasions where the values of the predictors are unfavorable or possibly detrimental to the model. The simplest example is a zero-variance column, where all of the values of a predictor in the training set have the same value. There is no information in these predictors and, although the model might be able to tolerate such a predictor, taking the time to identify and remove such columns is advisable.\nBut what about situations where only one or two rows have non-zero values? It is difficult to filter on variance values since the predictors could have different units or ranges. The underlying problem occurs when very few unique values exist (i.e., “coarse data”), and one or two predictor values capture most of the rows. Perhaps we can identify this situation.\nConsider a hypothetical training set of 500 data points with a count-based predictor that has mostly zero counts. However, there are three data points with a count of 1 and one instance with a count of 2. The variance of these data is low, but it would be difficult to determine a cutpoint for removal. Kuhn (2008) developed an algorithm to find near-zero variance predictors, as defined by two criteria:\n\nThe freqeuncy ratio is the frequency of the most prevalent value over the second most frequent value. For our example, the ratio was 496/3 = 165.3.\nThe proportion of unique values, which is 3 / 500 = 0.006.\n\nWe can determine thresholds for both of these criteria to remove the predictor. The default behavior is to remove the column of the frequency ratio is greater than 19 and the proportion of unique values is less than 0.1. For the example, the predictor would be eliminated.\nAnother example of an unsupervised filter reduces between-predictor correlations. A high degree of correlation can reduce the effectiveness of some models. We can filter out predictors by choosing a threshold for the (absolute) pairwise correlations and finding the smallest subset of predictors to meet this criterion.\nA good example is Figure 13.2, which visualizes the correlation matrix for the fifteen predictors in the forestry data set (from Section 3.9). Predictors for the January minimum temperature and the annual mean temperature are highly correlated (corr = 0.93), and only one might be sufficient to produce a better model. Similar issues occur with other predictor pairs. If we apply a threshold of 0.50, then six predictors (dew temperature, annual precipitation, annual maximum temperature, annual mean temperature, annual minimum temperature, and maximum vapor) would be removed before model training.\n\n\n\n\n\n\n\n\nFigure 13.2: A heatmap of a correlation matrix for fifteen predictors spatial, ordered using a hierarchical clustering algorithm.\n\n\n\n\nSimilar redundancy can occur in categorical predictors. In this case, measures of similarity can be used to filter the predictors in the same way.\nUnsupervised filters are only needed in specific circumstances. Some models (such as trees) are resistant to the distributions of the predictor’s data5, while others are excessively sensitive.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-selection-methods",
    "href": "chapters/feature-selection.html#sec-selection-methods",
    "title": "13  Feature Selection",
    "section": "13.5 Classes of Supervised Methods",
    "text": "13.5 Classes of Supervised Methods\nThere are three main strategies for supervised feature selection. The first and most effective was discussed: automatic selection occurs when a subset of predictors is determined during model training. We’ll discuss this more in the next section.\nThe second supervised strategy is called wrappers. In this case, a sequential algorithm proposes feature subsets, fits the model with these subsets, and then determines a better subset from the results. This sounds similar to iterative optimization because it is. Many of the same tools can be used. Wrappers are the most thorough approach to searching the space of feature subsets, but this can come with an enormous computational cost.\nA third strategy is to use a filter to screen predictors before adding them to the model. For example, the analysis shown in Figure 2.3 computed how each item in a food order impacted the delivery time. We could have applied a filter where we only used food item predictors whose lower confidence interval for the increase in time was greater than one. The idea behind the filter is that it is applied once prior to the model fit.\nThe next few sections will describe the mechanics of these techniques. However, how we utilize these algorithms is tricky. We definitely want to avoid overfitting the predictor set to the training data.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-automatic-selection",
    "href": "chapters/feature-selection.html#sec-automatic-selection",
    "title": "13  Feature Selection",
    "section": "13.6 Automatic Selection",
    "text": "13.6 Automatic Selection\nTo more closely demonstrate how automatic feature selection works, let’s return to the Cubist model fit to the delivery data in Section 2.4. That model created a series of regression trees, and converted them to rules (i.e., a path through the tree), then fit regression models for every rule. Recall that there was a sizable number of rules (2,007) each with its own linear regression. Despite the size of the rule set, only 13 of the original 30 predictors in the data set were used in rules or model fits; 17 were judged to be irrelevant for predicting delivery times.\n\nThis shows that we can often reduce the number of features for free.\n\nDifferent models view the training data differently, and the list of relevant predictors will likely change from model to model. When we fit a MARS model (?sec-mars-reg) to the data, only 9 original predictors were used. If we use automatic selection methods to understand what is important, we should fit a variety of different models and create a consensus estimate of which predictors are “active” for the data set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-wrappers",
    "href": "chapters/feature-selection.html#sec-wrappers",
    "title": "13  Feature Selection",
    "section": "13.7 Wrapper Methods",
    "text": "13.7 Wrapper Methods\nWrapper methods (Kohavi and John 1998) use an overarching search method to optimize binary indicators, one for each potential predictor, that control which predictors are given to the model.\nThe most popular approach is recursive feature elimination (RFE), which starts with the complete set of predictors and uses a mechanism to rank each in terms of their utility for predicting the outcome (Guyon et al. 2002). Using this ordering, predictors are successively removed while performance is monitored. The performance profile determines the optimal subset size, and the most important predictors are included in the model. RFE is greedy, though; it predetermines the order that the predictors are eliminated and will never consider a subset that is inconsistent with this order.\nTwo global search methods described in Chapter 12, genetic algorithms (GA) and simulated annealing, can also be used. In this instance, the search space is the predictor space represented as binary indicators. The same process applies, though. For example, a GA would contain a population of different subsets, each producing a fitted model. The performance values enable the creation of the next generation via selection, reproduction, and mutation. In the case of simulated annealing, an initial subset is created and evaluated, and perturbations are created by randomly altering a small set of indicators (to create a “nearby” subset). Suboptimal subsets can be accepted similarly to our application for tuning parameters.\nHowever, for each of these cases, the details matter. First, we should probably use separate data to\n\nestimate performance for each candidate subset and\ndetermining how far the search should proceed.\n\nIf we have large data sets, we can partition them into separate validation sets for each purpose. Otherwise, resampling is probably the answer to solve one or both of these problems.\nThe need to optimize tuning parameters can greatly complicate this entire process. A set of optimal parameter values for large subset sizes may perform very poorly for small subsets. In some cases, such as \\(m_{try}\\) and the number of embedding dimensions, the tuning parameter depends on the subset size.\nIn this case, the traditional approach uses a nested resampling procedure similar to the one shown in Section 11.4. An inner resampling scheme tunes the model, and the optimized model is used to predict the assessment set from the outer resample. Even with parallel processing, this can become an onerous computational task.\nHowever, let’s illustrate a wrapper method using an artificial data set from the simulations described in Section 13.1. We’ll use Equation 13.1 and supplement the eight truly important predictors with twenty one noise columns. For simplicity, we’ll use a neural network to predict the outcome with a static set of tuning parameters. The training set consisted of 1,000 data points.\nSince feature selection is part of the broader model pipeline, 10-fold cross-validation was used to measure the effectiveness of 150 iterations of simulated annealing (Kuhn and Johnson 2019 Sect 12.2). A separate SA run is created for each resample; we use the analysis set to train the model on the current subset, and the assessment set is used to measure the RMSE.\nFigure 13.3 shows the results, where each point is the average of the 10 RMSE values for the SA at each iteration. The results clearly improve as the selection process mostly begins to include important predictors and discards a few of the noise columns. After approximately 120 iterations, the solution stabilizes. This appears to be a very straightforward search process that clearly indicates when to stop the SA search.\n\n\n\n\n\n\n\n\nFigure 13.3: The progress of using simulated annealing to select features for.a neural network. The dashed green line indicators the best RMSE value achievable for these simulated data.\n\n\n\n\n\nHowever, looks can be deceiving. The feature selection process is incredibly variable, and adding or removing an important predictor can cause severe changes in performance. Figure 13.3 is showing the average change in performance, and the relative smoothness of this curve is somewhat deceiving.\nFigure 13.4 shows what could have happened by visualizing each of the 10 searches. The results are much more dynamic and significantly vary from resample to resample. Most (but not all) searches contain one or two precipitous drops in the RMSE with many spikes of high errors (caused by a bad permutation of the solution). These cliffs and spikes do not reproducibly occur at the same iteration or in the same way.\n\n\n\n\n\n\n\n\nFigure 13.4: The individual SA runs that make up the data in Figure 13.3.\n\n\n\n\n\nWe should keep in mind that Figure 13.3 lets us know where, on average, the search should stop, but our actual search result (that uses the entire training set) is more likely to look like what we see in Figure 13.4.\nFor this particular search, the numerically best result occurred at iteration 113 and the final model contained 17 predictors where eight of the twenty noise columns are still contained in the model. Our resampled estimate of the RMSE, at this iteration, was 0.302 (a perfect model would have a value of 0.25). However, the plots of the SA performance profiles indicate a range, probably above 125 iterations, where the results are equivocal.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-filters",
    "href": "chapters/feature-selection.html#sec-filters",
    "title": "13  Feature Selection",
    "section": "13.8 Filter Methods",
    "text": "13.8 Filter Methods\nFilters can use any scoring function that quantifies how effectively the predictor predicts the outcome (Kohavi and John 1998; Duch 2006). One method is to use a univariate statistic to evaluate each predictor independently. For example, the area under the ROC curve (?sec-roc) can be computed for a binary classification outcome and a set of numeric predictors. The filter could be applied using these values by pre-defining a threshold for the statistic that defines “important enough.” Alternatively, the top \\(p^*\\) predictors can be retained and used in the supervised model. This works well unless predictors have ties in their statistics.\nThe example of using ROC curves raises an interesting question. Should we rank the predictors by their area under the curve or should we convert that analysis to a p-value to see if the area under the curve is greater than 0.5 (Hanley and McNeil 1983)? Our advice is to do the former. Hypothesis tests and p-values have their uses but the AUC estimates themselves are probably more numerically stable. To factor in uncertainty, we could filter on the lower confidence bound of the AUC. This is generally true for almost any ranking statistic that has a corresponding p-value.\nMultivariate methods can also be used. These simultaneously compute measures of importance for all predictors. This is usually preferred since univariate statistics can be compromised when the predictors are highly correlated and/or have important interaction effects.\nA variety of models have built-in methods for measuring importance. For example:\n\nMARS (Friedman 1991) and decision trees can compute the importance of each predictor by aggregating the improvement in the objective function when each predictor is used in a split or an artificial feature.\nSome neural networks can use the values of their coefficients in each layer to compute an aggregate measure of effectiveness for each feature (Garson 1991; Gevrey, Dimopoulos, and Lek 2003; Olden, Joy, and Death 2004).\nSimilarly, partial least squares models can compute importance scores from their loading estimates (Wold, Johansson, and Cocchi 1993; Farrés et al. 2015).\nAs mentioned in Section 8.1.2, BART (and H-statistics) can measure the importance of individual predictors using the same techniques to discover interaction effects.\n\nThere are also model-free tools for computing importance scores from an existing model based on permuting each predictor and measuring how performance metrics change. This approach was initially popularized by the random forest model (?sec-rand-forest-cls) but can be easily extended to any model. See Breiman (2001), Strobl et al. (2007), and ?sec-importance for more details. There are a few other methods worth mentioning:\n\nThe Relief allgorithm (Kira and Rendell 1992; Kononenko 1994) and its descendant methods randomly select training set points and use the outcome values of their nearest neighbors to aggregate statistics to measure importance.\nMinimum redundancy, maximum relevance analysis (MRMR) combines two statistics, one for importance and another for between-predictor correlation, to find a subset that has the best of both quantities (Peng, Long, and Ding 2005).\n\nThere are also tools to help combine multiple filters into a compound score (Karl et al. 2023). For example, we may want to blend the number of active predictors and multiple performance characteristics into a single score used to rank different tuning parameter candidates. One way to do this is to use desirability functions. These will be described in ?sec-combined-metrics, Also, Section 12.3.2 of Kuhn and Johnson (2019) shows an example of how desirability functions can be incorporated into feature selection.\nOnce an appropriate scoring function is determined, a tuning parameter can be added for how many predictors are to be retained. This is like RFE, but we optimize all the tuning parameters simultaneously and evaluate subsets in a potentially less greedy way.\nThere are two potential downsides, though. First, there is still the potential for enormous computational costs, especially if the ranking process is expensive. For example, if we use a grid search, there is significant potential for the same subset size to occur more than once in the grid6. In this case, we are repeating the same importance calculation multiple times.\nThe second potential problem is that we will use the same data set to determine optimal parameters and how many features to retain. This could very easily lead to overfitting and/or optimization bias, especially if our training set is not large.\nWe measured the predictors using the standard random forest importance score using the same data as the previous simulated annealing search. The number of features to retain was treated as an additional tuning parameter. Cross-validation was used to select the best candidate from the space-filling design with 50 candidate points. Figure 13.5(a) shows the tuning results. The number of selected predictors appears to have the largest effect on the results; performance worsens as we remove too many features (which probably includes informative ones). It also appears that too much penalization has a negative effect on reducing the RMSE.\nThe model with the smallest RMSE selected the top 9 most important predictors, 13 hidden units, a penalty value of 10-8.78, a learning rate of 10-1.47, and tanh activation. The resampled RMSE associated with the numerically best results was 0.28. When that model was fit on the entire training set, the holdout RMSE estimate was 0.282. One reason that these two estimates of performance are so close is the relatively high \\(n_{tr}/p\\) ratio (1,000 / 29 \\(\\approx\\) 34.5).\n\n\n\n\n\n\n\n\nFigure 13.5: Panel (a): The results of tuning a variance importance filter and the parameters from a single-layer neural network for predicting the simulated data from Equation 13.1. Panel (b): The frequency distribution of predictor selection in the 10 folds for the model with the smallest RMSE.\n\n\n\n\n\nRecall that the importance scores are computed on different analysis sets. For the best model, Figure 13.5(b) contains the frequency at which each predictor was selected. For the most part, the importance scores are pretty accurate; almost all informative features are consistently selected across folds. When the final model was fit on the entire training set, a single irrelevant predictor was included in the model (by random change) along with true predictors \\(x_{1}\\), \\(x_{2}\\), \\(x_{3}\\), \\(x_{4}\\), \\(x_{5}\\), \\(x_{7}\\), \\(x_{9}\\), and \\(x_{10}\\). These results are unusually clean, mostly due to the large sample size and relatively small predictor set.\nSimilar to the discussion in Section 10.10, the different predictor sets discovered in the resamples can give us a sense of our results’ consistency. They are “realizations” of what could have occurred. Once we fit the final model, we get the actual predictor set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#chapter-references",
    "href": "chapters/feature-selection.html#chapter-references",
    "title": "13  Feature Selection",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nDuch, W. 2006. “Filter Methods.” In Feature Extraction: Foundations and Applications, 89–117. Springer.\n\n\nFarrés, M, S Platikanov, S Tsakovski, and R Tauler. 2015. “Comparison of the Variable Importance in Projection (VIP) and of the Selectivity Ratio (SR) Methods for Variable Selection and Interpretation.” Journal of Chemometrics 29 (10): 528–36.\n\n\nFriedman, J. 1991. “Multivariate Adaptive Regression Splines.” The Annals of Statistics 19 (1): 1–67.\n\n\nGarson, D. 1991. “Interpreting Neural Network Connection Weights.” Artificial Intellegence Expert.\n\n\nGevrey, M, I Dimopoulos, and S Lek. 2003. “Review and Comparison of Methods to Study the Contribution of Variables in Artificial Neural Network Models.” Ecological Modelling 160 (3): 249–64.\n\n\nGuyon, I, J Weston, S Barnhill, and V Vapnik. 2002. “Gene Selection for Cancer Classification Using Support Vector Machines.” Machine Learning 46: 389–422.\n\n\nHanley, J, and B McNeil. 1983. “A Method of Comparing the Areas Under Receiver Operating Characteristic Curves Derived from the Same Cases.” Radiology 148 (3): 839–43.\n\n\nHooker, G. 2004. “Discovering Additive Structure in Black Box Functions.” In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 575–80.\n\n\nKarl, F, T Pielok, J Moosbauer, F Pfisterer, S Coors, M Binder, L Schneider, et al. 2023. “Multi-Objective Hyperparameter Optimization in Machine Learning—an Overview.” ACM Transactions on Evolutionary Learning and Optimization 3 (4): 1–50.\n\n\nKira, K, and L Rendell. 1992. “A Practical Approach to Feature Selection.” In Machine Learning Proceedings 1992, 249–56. Elsevier.\n\n\nKohavi, R, and G John. 1998. “The Wrapper Approach.” In Feature Extraction, Construction and Selection: A Data Mining Perspective, 33–50. Springer.\n\n\nKononenko, I. 1994. “Estimating Attributes: Analysis and Extensions of RELIEF.” In European Conference on Machine Learning, 171–82. Springer.\n\n\nKuhn, M. 2008. “Building Predictive Models in R Using the caret Package.” Journal of Statistical Software 28: 1–26.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nOlden, J, M Joy, and R Death. 2004. “An Accurate Comparison of Methods for Quantifying Variable Importance in Artificial Neural Networks Using Simulated Data.” Ecological Modelling 178 (3-4): 389–97.\n\n\nPeng, H, F Long, and C Ding. 2005. “Feature Selection Based on Mutual Information Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy.” IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (8): 1226–38.\n\n\nSorokina, D, R Caruana, M Riedewald, and D Fink. 2008. “Detecting Statistical Interactions with Additive Groves of Trees.” In Proceedings of the 25th International Conference on Machine Learning, 1000–1007.\n\n\nStrobl, C, AL Boulesteix, A Zeileis, and T Hothorn. 2007. “Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution.” BMC Bioinformatics 8: 1–21.\n\n\nWold, S, E Johansson, and M Cocchi. 1993. “PLS: Partial Least Squares Projections to Latent Structures.” In 3D QSAR in Drug Design: Theory, Methods and Applications., 523–50. Kluwer ESCOM Science Publisher.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#footnotes",
    "href": "chapters/feature-selection.html#footnotes",
    "title": "13  Feature Selection",
    "section": "",
    "text": "Recall that we use the terms “predictors” and “features” interchangeably. We favor the former term, but “features” is more commonly used when discussing the filtering of columns.↩︎\nFor this value of the simulated standard deviation, the best possible RMSE value is also 0.25.↩︎\nDetails and code can be found at https://github.com/topepo/noise_features_sim.↩︎\nRecall out mantra that “Always have a separate piece of data that can contradict what you believe.”↩︎\nZero-variance predictors do not harm these models, but determining such predictors computationally will speed up their training.↩︎\nThe same issue can also occur with iterative search↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html",
    "href": "chapters/comparing-models.html",
    "title": "14  Comparing Models",
    "section": "",
    "text": "14.1 Resampled Data Sets\nSo far, we have established several key fundamentals of building predictive models. To create an effective model, we must employ strategies to reduce overfitting, use empirical validation techniques to accurately measure predictive performance, and systematically search a tuning parameter space to identify values that optimize model quality. By following these approaches, we can have confidence that our model generalizes well to new, unseen data and provides a realistic estimate of its efficacy.\nWith this foundation in place, the next step is to select the best model for a given data set. When presented with new data, we typically evaluate multiple machine learning models, each with different sets of tuning parameters. Our goal is to select the model and parameter combination that yields the best results. But how do we determine which is truly the best?\nMaking this decision requires considering several factors. As briefly discussed in Chapter 2, practical characteristics such as model complexity, interpretability, and deployability play an important role. Predictive ability, however, is an objective measure that can be evaluated quantitatively. Comparing metrics across models raises an important question: How can we be sure that one model’s quality is truly superior to another’s? Or, conversely, how can we conclude that multiple models have no significant difference in performance?\nTo address these questions, we will examine both graphical and statistical techniques for comparing the performance statistics of different models. During the model tuning and training process, these comparisons may involve assessing various configurations of tuning parameters within a single model or comparing fundamentally different modeling approaches.\nEmpirical validation (e.g., resampling) serves as the backbone of the tuning and training process. It systematically partitions the data into multiple subsets, enabling repeated estimation of performance metrics. The variability from the different assessment sets provides critical information for selecting optimal tuning parameters or identifying the best-performing model.\nIn the first part of this chapter, we will focus on methods for evaluating models when replicate estimates of performance metrics are available. This includes techniques grounded in Frequentist and Bayesian frameworks. After that, we will explore the case of a single data set. This is relevant when a single holdout set is used during model development (i.e., a validation set) and also when the test set is evaluated.\nAs a refresher, resampling produces multiple variations of the training set. These are used to metric estimates, such as accuracy or R2, that should mimic the results we would see on data sets similar to the training set.\nThe tools in this section are very similar to those described for model racing in Section 11.3.3, and we’ll use the same notation as that section. Similarly, we’ve discussed the two main philosophies of statistical analysis (Frequentist and Bayesian) in Section 6.4.3 and Section 12.5.2. Many of those ideas play out again in this chapter.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-resamples",
    "href": "chapters/comparing-models.html#sec-compare-resamples",
    "title": "14  Comparing Models",
    "section": "",
    "text": "14.1.1 Statistical Foundations\nRegardless of whether we take a Frequentist or Bayesian philosophies to comparing model performance, both methods aim to identify key factors that influence the outcome, such as model-to-model effects. We seek to determine whether the effect of these factors is statistically significant—that is, unlikely to be due to random chance.\nTo illustrate these statistical techniques, we will use the same data that will be used in later chapters on classification: the Washington State forestation data introduced in Chapter 3. Our goal is to predict whether a location is classified as forested or non-forested. The data set includes predictors such as elevation, annual precipitation, longitude, latitude, and so on.\nWithout going into much detail, we’ll focus on comparing three modeling strategies that will be described in Part 4:\n\nA boosted tree ensemble using C5.0 trees as the base learner. This model was tuned over the number of trees in the ensemble and the amount of data required to make additional splits (a.k.a. \\(n_{min}\\)). Grid search found that 60 trees in the ensemble were optimal, in conjunction with \\(n_{min}\\) = 20 training set points.\nA logistic regression model that was trained using penalization (?sec-logistic-penalized). Feature engineering included an effect encoding parameterization of the county associated with locations, a small set of interaction terms (described in ?sec-forestation-eda), and ten spline terms for several of its numeric predictors. The optimal tuning parameter values were a penalization value of 10-4.67 and 0% Lasso penalization.\nA naive Bayes model (?sec-naive-bayes]) was used where the conditional distributions were modeled using a nonparametric density estimate. No feature preprocessing was used.\n\nOne difference between these analyses and those to come in Part 4 is that we’ve chosen to use classification accuracy as our metric1. Some of these results may differ from upcoming analyses. Previously, the spatially aware resampling scheme that was used generated 10 resamples of the training set. For this reason, there are 10 different accuracy “replicates” for each candidate value that was evaluated.\nSimilar to previous sections, we’ll produce models where the replicated performance statistics (denoted as \\(\\widehat{Q}_{ij}\\)) are the outcomes of the model (for resample \\(i\\) and model \\(j\\)). This is the same general approach shown in Equation 11.1 and Equation 12.5. We’ll assume that there are \\(M\\) models to compare (three in our case).\n\n\n14.1.2 Frequentist Hypothesis Testing Methods\nLet’s begin by building towards an appropriate statistical model that will enable us to formally compare the efficacy of models. To begin this illustration, let’s suppose that each of these models was built separately using different 10-fold partitions of the training data. In this scenario, the metrics on the first holdout fold would be unique, or independent, for each model. If this were the case, then we could represent the desired performance metric with the following linear statistical model:\n\\[\nQ_{ij} = \\beta_0 + \\sum_{j=1}^{M-1}\\beta_jx_{ij} + \\epsilon_{ij}\n\\tag{14.1}\\]\nIn this equation \\(\\beta_0\\) is the overall average performance statistic across all assessment sets and models2. The term \\(\\beta_{j}\\) represents the incremental impact on the outcome due to the \\(j^{th}\\) model (\\(j\\) = 1, 2, …, \\(M-1\\) = total number of models), and \\(x_{ij}\\) is an indicator function for the \\(j^{th}\\) model. In this example, the \\(\\beta_j\\) will be considered as a fixed effect. This indicates that these are the only models that we are considering. If we were to have taken a random sample of models, then the \\(\\beta_j\\) would be treated a random effect which would have a corresponding statistical distribution. The distinction between fixed and random effects will come into play shortly. Finally, \\(\\epsilon_{ij}\\)) represents the part of the performance that is not explainable by the \\(i^{th}\\) assessment set and \\(j^{th}\\) model. We’ll assume these residuals follow a Gaussian distribution with a mean of zero and an unknown variance3. The model, along with the statistical assumption about the distribution of the residuals, provides the statistical foundation for formally comparing models.\nFigure 14.1 illustrates these separate components for the assessment set with the largest accuracy value for the boosting ensemble, where the overall intercept is determined by the logistic regression. The horizontal dashed line represents the overall mean of the accuracy estimates, and the blue bar shows the mean of the boosting estimates alone. It is slightly higher than the average. For that model, the orange line shows the residual (\\(\\epsilon_{ij}\\) for the largest boosted tree accuracy replicate. The totality of such errors is used to measure the overall error in the model.\n\n\n\n\n\n\n\n\nFigure 14.1: An illustration of how variation is partitioned based on the effect of the model (blue) and unexplained causes (orange) when distinct cross-validation partitions are used to train each model.\n\n\n\n\n\nEquation 14.1 enables us to distinctly partition the total variability in the outcome data into two components:\n\nExplained variability: The portion attributable to differences among models, represented by the blue bar. This is the signal in the data set.\nUnexplained variability: The portion due to random or unaccounted-for factors, represented by the orange bar, is commonly called the noise.\n\nOur key question is whether the observed differences in the mean performance for each model reflect genuine differences among models or if they could simply be due to random variation. The Frequentist approach addresses this question using hypothesis testing. In this framework:\n\nThe null hypothesis (\\(H_0\\)) represents the assumption that all models have the same average performance. Formally, this can be written as:\n\\[\nH_0: \\beta_k = \\beta_m,\n\\] for all \\(k = m\\).\nThe alternative hypothesis (\\(H_A\\)) asserts that at least two models have significantly different average performance:\n\\[\nH_A: \\beta_{k} \\neq \\beta_{m},\n\\]\nfor any \\(k \\neq m\\).\n\nTo determine whether we have sufficient evidence to reject the null hypothesis, we compare:\n\nBetween-group variability (signal): Differences in mean performance across models.\n\nWithin-group variability (noise): Residual variation not explained by the models.\n\nThis comparison is conducted using a test whose statistic follows an F distribution (Kutner, Nachtsheim, and Neter 2004), which assesses whether the signal is large enough relative to the noise to conclude that differences among models are statistically significant.\nIn this case, the F-statistic is (3.06, 2, 27)4, with a corresponding p-value of 0.0633. The p-value is small. However, it is not small enough to conclude that there is strong evidence to reject the null hypothesis. Therefore, we would not conclude that there are differences in accuracy among the three models. However, remember that we treated these data as if the resamples were completely different, which was not the case. As you’ll see shortly, this is a seriously flawed analysis of our data.\nIn practice, when tuning and training models, we use the same resampling sets to evaluate their performance. This is a more efficient approach since we only need to set up the resampling scheme once. Moreover, the structure generated by using the same assessment sets will enable us to detect differences in quality between models more effectively.\nIn this setting, each assessment set forms what is known as a block, which is a group of units that are similar to each other. This grouping is an important piece of information that will aid in assessing whether models have statistically different performance. The predictive performance when tuning multiple models using the same cross-validation assessment sets can now be represented as:\n\\[\nQ_{ij} = (\\beta_0 + \\beta_{0i}) + \\sum_{j=1}^{M-1}\\beta_jx_{ij} + \\epsilon_{ij}\n\\tag{14.2}\\]\nNotice that Equation 14.2 is nearly the same as Equation 14.1, with the additional term \\(\\beta_{0i}\\). This term represents the incremental impact on performance due to the \\(i^{th}\\) assessment set (\\(i\\) = 1, 2,…, \\(B\\) resamples). In this model, the impact of the \\(B\\) resamples, represented by \\(\\beta_{0i}\\) are a random effect. The reason the resamples are considered as random effects is because the 10 folds were selected at random from an overall distribution of resamples. The impacts of each model are still considered as fixed effects. When a model contains both fixed and random effects, it is called a mixed effect model.\nFigure 14.2 illustrates these separate components for the assessment set with the largest accuracy value for the boosted tree. In this figure, the points are connected based on the fold. The green line shows that the residual for the best boosting accuracy can be explained by the fact that it comes from this specific fold (i.e., resample). For whatever reason, this specific assessment set was easier to predict since all models had the highest accuracies. The resample that is associated with each accuracy is a systematic and explainable effect in these data. It’s also a “nuisance parameter” because we don’t care about this specific trend.\nThe mixed model can quantify the effect of this nuisance parameter; we can subtract its effect from the overall error, yielding a much smaller value of \\(\\epsilon_{ij}\\) as evidenced by the updated vertical orange bar. Notice that the total amount of variability explained by the model is identical to Figure 14.1. However, the previously unexplainable variability from Figure 14.2 is now mostly explained by the variation due to the fold, thus reducing the unexplainable variability. We have made our analysis much more precise by accounting for the resample effect5.\n\n\n\n\n\n\n\n\nFigure 14.2: An illustration of how variation is partitioned based on the effect of the model (blue), folds (green), and unexplainable causes (orange) when the same cross-validation partitions are used to train each model. The lines connecting the points indicate the results for the same cross-validation fold.\n\n\n\n\n\nTo understand if there are differences between models, we again compare the variability explained by the different models with the unexplainable variability. The signal relative to the noise is now much stronger, enabling us to more easily see differences in model performance. The F-statistic from this model is (20.42, 2, 18) and has a corresponding p-value of &lt; 10-4. By utilizing the same cross-validation scheme across models, there is now strong evidence of a difference in performance among the models.\nThis result does not tell us which models are different. To understand that, we need to perform additional tests to compare each pair of models. These tests are referred to as post hoc tests because they are performed after the initial hypothesis test among all means. To learn more details about post hoc tests and the field of multiple comparisons, see Hsu (1996) and/or Dudoit and Van Der Laan (2008) .\n\n\n14.1.2.1 Post Hoc Pairwise Comparisons and Protecting Against False Positive Findings\nAfter detecting a significant effect in a one-way analysis of variance (ANOVA), the next step is to determine which specific groups differ from each other. This is achieved using post hoc pairwise comparisons, which assess all possible pairwise differences while controlling for false positive findings.\nWhen conducting multiple comparisons, the risk of making at least one Type I error (false positive) increases. If we compare many pairs of groups separately, the probability of incorrectly rejecting at least one null hypothesis accumulates, leading to spurious findings. If this issue is not addressed, then we will eventually conclude that models are different when they actually are not. To mitigate this, statistical procedures adjust for multiple comparisons to ensure that the overall false positive rate remains controlled. Two methods for this purpose are Tukey’s Honestly Significant Difference (HSD) (Tukey 1949) and the False Discovery Rate (FDR) procedures (Efron and Hastie 2016, chap. 15).\nTukey’s HSD test focuses on controlling the probability of making at least one false discovery; this is known as the familywise error rate (FWER). The HSD test is specifically designed for one-way ANOVA post hoc comparisons and ensures that the FWER remains at a pre-determined significance level (\\(\\alpha\\) = 0.05).\nThe test considers all possible pairwise differences while maintaining a constant error rate across comparisons. To contrast models \\(i\\) and \\(j\\), the test statistic is calculated as:\n\\[\nT_{HSD} = \\frac{\\bar{Q}_i - \\bar{Q}_j}{\\sqrt{MS_{within} / n}}\n\\]\nwhere, \\(\\bar{Q}_i\\) and \\(\\bar{Q}_j\\) are the sample means of each model’s outcome data, \\(MS_{within}\\) is the mean square error from the ANOVA, and \\(n\\) is the number of observations per group. The critical value for \\(T_{HSD}\\) is obtained from the studentized range distribution, which depends on the number of groups and the degrees of freedom from the ANOVA error term.\nWhen we desire to compare the performance of each pair of models, then Tukey’s HSD procedure will provide the appropriate adjustments to the p-values to protect against false positive findings as it becomes overly conservative.\nIn settings where many hypotheses are tested simultaneously, we may be more interested in the false discovery rate (FDR): the expected proportion of false positives among all rejected hypotheses. With many comparisons, this can be more appropriate than controlling the familywise error rate.\nThere are numerous FDR procedures in the literature. The method by Benjamini and Hochberg (1995) is the simplest. It ranks the p-values from all pairwise comparisons from smallest to largest (\\(p_1\\), \\(p_2\\), …, \\(p_m\\)) and compares them to an adjusted threshold:\n\nRank the p-values in ascending order, assigning each a rank \\(i\\).\nCompute the critical value for each comparison: \\[\np_{crit} = \\frac{i}{m} \\alpha\n\\] where \\(m\\) is the total number of comparisons and \\(\\alpha\\) is the desired FDR level.\nFind the largest \\(i\\) where \\(p_i \\leq p_{crit}\\), and reject all hypotheses with p-values at or below this threshold.\n\nUnlike Tukey’s method, FDR control is more appropriate when performing many comparisons. This procedure offers a balance between sensitivity (detecting true differences) and specificity (limiting false positives).\nNow, let’s compare how these methods perform when evaluating each pair of models in our example. Table 14.1 presents the estimated differences in accuracy, standard error, degrees of freedom, raw, unadjusted p-values, and the p-values adjusted using the Tukey and FDR methods for each pairwise comparison. Notice that the Tukey adjustment results in less significant p-values than the FDR adjustment. Regardless of the adjustment method, the results indicate that each model’s performance differs statistically significantly from the others. Specifically, the boosting model has a higher accuracy than the other two models.\n\n\n\n\n\n\n\n\n\n\n\nComparison\nEstimate\nSE\nDF\n\np-value Adjustment\n\n\n\nUnadjusted\nTukey\nFDR\n\n\n\n\nBoosting - Logistic\n0.79%\n0.67%\n18\n0.25544\n0.48276\n0.25544\n\n\nBoosting - Naive Bayes\n4.04%\n0.67%\n18\n0.00001\n0.00003\n0.00003\n\n\nLogistic - Naive Bayes\n3.25%\n0.67%\n18\n0.00013\n0.00036\n0.00019\n\n\n\n\n\n\n\n\nTable 14.1: Pairwise comparisons among the three models for predicting forestation classification. The table includes the estimated difference in accuracy, standard error, degrees of freedom, the unadjusted p-value, and the corresponding Tukey and FDR adjusted p-values.\n\n\n\n\nIn this table, we can see that we have no evidence that boosting and logistic regression have different accuracies, but both of these models are statistically different from naive Bayes (no matter the adjustment type).\n\n\n\n14.1.2.2 Comparing Performance using Equivalence Tests\nIn the previous section, we explored how to determine whether one model significantly outperforms another in a metric of choice. However, there are cases where we may instead want to assess whether the models perform similarly in a practically meaningful way. To do this, we use an equivalence test, which differs from the traditional hypothesis test, which aimed to detect differences between models.\nEquivalence testing is based on the concept that two models can be considered practically equivalent if their difference falls within a predefined margin of indifference, often denoted as \\(\\theta\\). This threshold represents the maximum difference that is considered practically insignificant. To illustrate equivalence testing, let’s begin by reviewing the hypothesis testing context for assessing differences between models. Let \\(\\beta_1\\) and \\(\\beta_2\\) represent the mean quality of two models. The null hypothesis for a standard two-sample test of difference is:\n\\[\nH_O:  \\beta_1 = \\beta_2\n\\]\nwhile the alternative hypothesis is:\n\\[\nH_A:  \\beta_1 \\neq \\beta_2\n\\]\nHowever, an equivalence test reverses this logic. The null hypothesis is that the models are not equivalent, and the alternative hypothesis is that the models are practically insignificant:\n\\[\nH_0:  |\\beta_1 - \\beta_2| \\geq \\theta\n\\tag{14.3}\\]\n\\[\nH_A:  |\\beta_1 - \\beta_2| \\lt \\theta\n\\tag{14.4}\\]\nUsing this framework, we might reject the null hypothesis and conclude that the models are equivalent within the predefined margin of indifference.\nA commonly used approach for equivalence testing is the Two One-Sided Test (TOST) procedure (Schuirmann 1987). This method involves conducting two separate one-sided hypothesis tests as defined in Equation 14.3 and Equation 14.4. Specifically, one test assesses the lower bound comparison while the other test assesses the upper bound comparison as follows:\nLower bound:\n\\[\nH_0:  \\beta_1 - \\beta_2 \\geq -\\theta\n\\]\n\\[\nH_A:  \\beta_1 - \\beta_2 &gt; -\\theta\n\\] Upper bound:\n\\[\nH_0:  \\beta_1 - \\beta_2 \\leq \\theta\n\\] \\[\nH_A:  \\beta_1 - \\beta_2 &gt; \\theta\n\\]\nEach test is conducted at a desired significance level \\(\\alpha\\), and if both null hypotheses are rejected, we conclude that the model performance is equivalent based on the predefined margin of indifference.\nSchuirmann showed that the two one-sided hypotheses can be practically evaluated by constructing a \\(100(1-2\\alpha)\\%\\) confidence interval for \\(\\beta_1 - \\beta_2\\). If this interval is contained within the boundaries of (\\(-\\theta\\), \\(\\theta\\)), then we can reject the null hypothesis and conclude that the two models have equivalent performance.\nSelecting an appropriate margin of indifference is crucial and should be determined prior to conducting the statistical test. What value should we choose? The answer to this question is problem-specific and depends on what we believe makes a sensible difference in performance.\nFrom the previous section, we saw that the largest expected difference between any pair of models was 4%. For the sake of this illustration, let’s suppose that the margin of practical indifference between models is at least \\(\\pm\\) 3 percent in accuracy.6\nTo perform the equivalence tests, we use the same ANOVA model as when testing for differences in Equation 14.2 and estimate the 90% confidence intervals about each pairwise difference. Because we are making multiple pairwise comparisons, the intervals need to be adjusted to minimize the chance of making a false positive finding. This adjustment can be done using either the Tukey or FDR procedures.\nFigure 14.3 presents the Tukey-adjusted 90% confidence intervals for each pairwise difference between models. For the forestation data, we would only conclude that the boosting ensemble and the logistic regression are equivalent.\n\n\n\n\n\n\n\n\nFigure 14.3: Pairwise comparisons of models using equivalence testing. The Tukey-adjusted 90% confidence interval of each pairwise difference is illustrated with vertical bands. For these comparisons, the margin of practical equivalence was set to +/- 3% accuracy.\n\n\n\n\n\n\n\n\n\n14.1.3 Comparisons Using Bayesian Models\nHow would we use a Bayesian approach to fit the model from Equation 14.2? To start, we can again assume a Gaussian distribution for the errors with standard deviation \\(\\sigma\\). One commonly used prior for this parameter is the exponential distribution; it is highly right-skewed and has a single parameter. Our strategy here is to use very uninformative priors so that the data have more say in the final posterior estimates than the prior. If our model were useless, the standard deviation would be very large and would probably be bounded by the standard deviation of the outcome. For our data, that value for the accuracy is 4.1%. We’ll specify an exponential prior to use this as the mean of its distribution.\nWe’ll also need to specify priors for the three \\(\\beta\\) parameters. A fairly wide Gaussian would suffice. We’ll use \\(\\beta_j \\sim N(0, 5)\\) for each.\nFor the random effects \\(\\beta_{0i}\\), the sample size is fairly low (10 resamples per model). Let’s specify a bell-shaped distribution with a wide tail: a \\(t\\)-distribution with a single degree of freedom. This would enable the Bayesian model to have some resample-to-resample effects that might be considered outliers if we assumed a Gaussian distribution.\nWe’ll discuss training Bayesian models in ?sec-logistic-bayes, so we’ll summarize here. Given our priors, there is no analytically defined distribution for the posterior. To train the model, we’ll use a Markov-chain Monte Carlo (MCMC) method to slowly iterate the posterior to an optimal location, not unlike the simulated annealing process previously described. To do this, we create chains of parameters. These are a controlled random walk from a starting point. Once that walk appears to converge, we continue generating random data points that are consistent with the optimized posterior. These random points will numerically approximate the posterior, and from these, we can make inferences. We’ll create 10 independent chains, and each will have a warm-up phase of 5,000 iterations and then another 5,000 samples to approximate the posterior. If all goes well, we will have a random sample of 50,000 posterior samples.\nThe resulting posterior distributions for the mean accuracy for each of the three models are shown in Figure 14.4 along with 90% credible intervals. The posteriors are relatively bell-shaped, and although there are discernible shifts between the models, there are overlaps in these distributions.\nWe can also use the posteriors to estimate how much of the error in the data was associated with the resample-to-resample effect (i.e., the previously described nuisance parameter). The percentage of total error attributable to the resample-to-resample effects is 82.5%. This illustrates how important it is to account for all of the systematic effects, wanted or unwanted, in the model.\n\n\n\n\n\n\n\n\nFigure 14.4: Posterior distributions of accuracy for each of the models. The horizontal bars represent the 90% credible intervals about the posterior accuracy values for each model.\n\n\n\n\n\nSince we are interested in contrasting models, we can get a sample of the posterior and take the difference between the parameters that correspond to the model terms (i.e., \\(\\beta\\) parameters). We can create a large number of these samples to form posterior distributions of the pairwise contrasts and compute credible intervals. Bayesian modeling does not use the null hypothesis testing paradigm as Frequentist methods, so it is unclear whether post hoc adjustments are needed or how that would be computed. See Gelman, Hill, and Yajima (2012) and Ogle et al. (2019) for discussions. Figure 14.5 illustrates posteriors for the pairwise contrasts.\n\n\n\n\n\n\n\n\nFigure 14.5: Posterior distributions of pairwise differences in accuracy for each pair of models. The vertical dashed lines represent the 90% credible intervals for each contrast.\n\n\n\n\n\nBased on these results, we would reach the same conclusion as the Frequentist results; the boosting and logistic regression models are the only pair to appear to have comparable accuracies. We can compute probability statements such as “There is a 85.4% probability that the boosting ensemble exhibits a higher accuracy than logistic regression.”\nIn addition to utilizing the posterior distribution to compute probabilities of difference, we can also use this distribution to calculate probabilities of similarity or equivalence. The region of practical equivalence (ROPE) defines a range of metric values that we believe contains practically insignificant results (Kruschke 2014). When using the equivalence testing from ?sec-comparing-models-equivalence-tests, we considered models equivalent if the accuracy was within 3%. Using this region, the posterior distribution of the parameter of interest is used to generate a probability that the parameter is within this region. If a large proportion of the posterior distribution falls within the ROPE, we can conclude that the difference is minimal.\nThe primary difference between the Frequentist and Bayesian equivalence testing lies in interpretation and flexibility. Frequentist equivalence testing is based on pre-specified significance levels and long-run error rates. Bayesian ROPE, on the other hand, provides a probability-based assessment of practical equivalence. This is particularly useful in predictive modeling, where slight differences in performance may not be operationally relevant.\nFigure 14.6 illustrates the ROPE for the forestation data, defined as \\(\\pm\\) 3% accuracy. The probability mass within this region, shown in blue, represents the likelihood that the models have equivalent performance based on this criterion. Table 14.2 summarizes the probabilities for each model comparison.\n\n\n\n\n\n\n\n\nFigure 14.6: Posterior distributions of pairwise differences in accuracy for each pair of models. The vertical dashed lines represent the region of practical equivalence. The probability mass within this region represents the probability that the models are practically equivalent within +/- 3% accuracy.\n\n\n\n\n\nFor instance, there is an overwhelming probability that boosting and logistic regression are practically equivalent. The other two comparisons (with naive Bayes) show scant evidence of the same.\n\n\n\n\n\n\n\n\n\n\n\nComparison\nProbability of Practical Equivalence\n\n\n\n\nBoosting vs Logistic\n0.997\n\n\nBoosting vs Naive Bayes\n0.081\n\n\nLogistic vs Naive Bayes\n0.362\n\n\n\n\n\n\n\n\nTable 14.2: Probability of practical equivalence between each pair of models’ performance.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-holdout",
    "href": "chapters/comparing-models.html#sec-compare-holdout",
    "title": "14  Comparing Models",
    "section": "14.2 Single Holdout Data Sets",
    "text": "14.2 Single Holdout Data Sets\nUnlike resampled data sets, validation sets and test sets offer a single “look” at how well the model operates. In the case of the validation set, we would repeatedly use it to guide our model development process, much like how we would rely on resamples. For the test set, it should only be evaluated once, hopefully on a single model, but perhaps to make a final choice between a select few. It is not intended for repeat use. In the discussions below, we’ll use the term “holdout set” to describe the validation or test set, without loss of generality. In this scenario, the focus will be on Frequentist methods by way of bootstrap sampling (Efron 1979, 2003), which we have seen previously.\nYou were first introduced to the bootstrap in Section 10.6, where it was applied to estimate model performance during tuning. In essence, the bootstrap involves resampling with replacement from the available data to estimate the sampling variability of a statistic. This variability can then be used to construct confidence intervals of the performance metrics of interest, offering a range of plausible values for model performance on similar validation sets.\nFigure 10.7 illustrates the bootstrap process during the model-building tuning phase. Recall that resampling with replacement from the training data generates multiple bootstrap samples of the same size as the original. Because sampling is done with replacement, some observations may appear multiple times within a sample, while others may be omitted. The omitted samples are used as holdout sets to evaluate model performance.\nTo adapt this idea to make predictions on the single holdout set, then repeatedly draw bootstrap samples from it. Each bootstrap sample is the same size as the holdout set and is created via random sampling with replacement (as illustrated in Figure 14.7). We compute our metric(s) of interest on each analysis set, which are just the bootstrap samples. The corresponding assessment sets are not used in this process.\n\n\n\n\n\n\n\n\nFigure 14.7: A schematic of bootstraps resamples created from a validation set.\n\n\n\n\n\nThis process yields a distribution of performance metrics, from which bootstrap confidence intervals can be derived. These intervals offer an estimate of the uncertainty around the holdout set’s performance and give us a sound way to express the expected range of performance across other holdout sets drawn from the same population.\nThere are several methods for computing bootstrap confidence intervals. See Davison and Hinkley (1997) and Chapter 11 of Efron and Hastie (2016). We’ll describe one that is straightforward and often used: percentile intervals. The idea is simple: for an interval with \\(100 (1 - \\alpha)\\)% confidence, we compute the \\(\\alpha/2\\) quantiles on both ends of the bootstrap distribution. For our 90% intervals, we determine the 5% and 95% quantiles. Since the method relies on estimating very small areas in the distributional tails, it is important to create many bootstrap samples (i.e., thousands) to guarantee dependable results.\nLet’s suppose that we considered a wide variety of models and feature sets, and still found our boosted tree and logistic regression to be the best. We’ve previously seen that, while the ensemble appears to perform best, the results are fairly similar, and the logistic model is simpler and perhaps more explainable. We might take both of these pipelines to the test set to make one final comparison before making a choice.\nWe fit the final models on the entire training set and then predict the test set of 1,371 locations. The accuracies for the boosting and logistic models were very close; they were 87.7% and 87.3%, respectively. Is this difference within the experimental noise or could there be a (statistical) difference?\nWe arrange the data so that there are separate columns for the boosted tree and logistic regression predictions (i.e., they are merged by the row number of the test set). Using this data, 5,000 bootstrap samples were created. From each of these, we can compute intervals for the accuracy of each and the difference in accuracy between them.\nFigure 14.8 shows the bootstrap distributions of the individual model results. They are fairly similar, with a small shift between them. Above each histogram are two 90% confidence intervals. One is the bootstrap percentile interval, and the other is a theoretically based asymmetric interval that assumes that the correct/incorrect data for each model follows a Bernoulli distribution. For each model, these intervals are almost identical. We computed each to demonstrate that the bootstrap has very good statistical performance (compared to its theoretical counterpart).\n\n\n\n\n\n\n\n\nFigure 14.8: For both models, the bootstrap distributions of the accuracy statistics are shown as histograms. The intervals at the top of each panel show the 90% confidence intervals computed using the bootstrap and via the traditional analytical method.\n\n\n\n\n\n\nThe confidence intervals between models substantially overlap. Ordinarily, we should take that as reliable information that there is no statistical difference between the two models. However, this is not true for this particular situation.\n\nThe issue is that the two sets of predictions are highly correlated. On the test set, they agree 93.1% of the time. Unlike our previous mixed model approach from Equation 14.2, the individual confidence intervals have no way to correct for this issue (since they evaluate one model at a time).\nWhen computing the difference between two things (A and B), the variance of their difference is:\n\\[\nVar[A-B] = Var[A] + Var[B] - 2Cov[A, B]\n\\tag{14.5}\\]\nWhen A and B are related, their covariance will be very high and, if we ignore it, the power of any statistical test on that difference can be severely underpowered. This is the same issue as shown in Section 14.1.2. When we didn’t account for the resampling effect, we accidentally sabotaged the test’s ability to find a difference (if there was one).\nFor the test set, we compute the difference in accuracy by resampling the matched pairs of predictions. From this, the bootstrap estimate of the difference was 0.4% with 90% bootstrap interval (-0.8%, 1.5%). This does indicate that the models have statistically indistinguishable accuracies, but the lower bound is very close to zero. With slightly different models or a different random split of the data, they might very well be considered different7. There is an overwhelmingly strong case to make that they are practically equivalent, though.\nAlso, as the data set size increases, the variation we expect to see in the summary statistic will decrease. This concept is illustrated in Figure 14.9. In this figure, we have randomly selected subsets of varying size from the data set and then computed a 90% confidence interval using these smaller sets. Notice that as the sample size decreases, the width of the confidence interval increases. Therefore, the smaller the data set, the higher the variability will be in the estimate of model quality, and the more important it is to understand the range of potential uncertainty. This pattern is true of any interval procedure.\n\n\n\n\n\n\n\n\nFigure 14.9: 90% confidence interval widths of accuracy for data sets of varying size, assuming the same performance as the boosted ensemble.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-conclusion",
    "href": "chapters/comparing-models.html#sec-compare-conclusion",
    "title": "14  Comparing Models",
    "section": "14.3 Conclusion",
    "text": "14.3 Conclusion\nSelecting the best predictive model involves assessing performance metrics and accounting for practical considerations. Thus far, this chapter has outlined both Frequentist and Bayesian approaches to comparing model performance, emphasizing the importance of appropriate statistical techniques when evaluating differences. The Frequentist approach provides a formal hypothesis testing framework, while the Bayesian approach offers a more direct probability-based interpretation of model differences. When there is just one data set for evaluation, like a validation set or test set, then the bootstrap technique can be used to create an interval about the performance metric (or differences between models).\nUltimately, while statistical significance can indicate meaningful differences between models, practical significance should also guide decision-making. Equivalence testing and Bayesian ROPE analysis help determine whether models perform similarly within a defined threshold of practical relevance. In our forestation classification example, these methods suggested that while the boosting model demonstrated superior predictive performance overall, differences between some models may not always be large enough to drive a clear choice.\nIn practice, model selection is rarely based on performance alone. Factors such as interpretability, computational cost, and deployment feasibility must also be considered. By combining quantitative performance assessment with practical domain knowledge, we can make informed decisions that balance accuracy with real-world constraints.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#chapter-references",
    "href": "chapters/comparing-models.html#chapter-references",
    "title": "14  Comparing Models",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300.\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nDudoit, S, and M Van Der Laan. 2008. Multiple Testing Procedures with Applications to Genomics. Springer.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nEfron, B. 2003. “Second Thoughts on the Bootstrap.” Statistical Science, 135–40.\n\n\nEfron, B, and T Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.\n\n\nGelman, A, J Hill, and M Yajima. 2012. “Why We (Usually) Don’t Have to Worry about Multiple Comparisons.” Journal of Research on Educational Effectiveness 5 (2): 189–211.\n\n\nHsu, J. 1996. Multiple Comparisons: Theory and Methods. Chapman; Hall/CRC.\n\n\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press.\n\n\nKutner, M, C Nachtsheim, and J Neter. 2004. Applied Linear Regression Models. McGraw-Hill/Irwin.\n\n\nOgle, K, D Peltier, M Fell, J Guo, H Kropp, and J Barber. 2019. “Should We Be Concerned about Multiple Comparisons in Hierarchical Bayesian Models?” Methods in Ecology and Evolution 10 (4): 553–64.\n\n\nSchuirmann, Donald J. 1987. “A Comparison of the Two One-Sided Tests Procedure and the Power Approach for Assessing the Equivalence of Average Bioavailability.” Journal of Pharmacokinetics and Biopharmaceutics 15: 657–80.\n\n\nTukey, John W. 1949. “Comparing Individual Means in the Analysis of Variance.” Biometrics, 99–114.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#footnotes",
    "href": "chapters/comparing-models.html#footnotes",
    "title": "14  Comparing Models",
    "section": "",
    "text": "This isn’t normally our first choice. However, the outcome classes are fairly balanced. Also, accuracy is intuitively understandable, and the broader set of classification metrics has yet to be described in detail.↩︎\nThis is another example of an analysis of variance (ANOVA) model.↩︎\nIt may seem odd that we would assume a Gaussian distribution for a variable with values between zero and one. Almost all performance statistics are means; accuracy is the average of 0/1 data. The Central Limit Theorem tells us that, despite the original data distribution, the mean tends to normality as the sample size increases. Assessment and test sets often have enough data points to trust this assumption.↩︎\nThis is common notation for reporting the F-statistic, where the first number represents the ratio of the mean square of the model to the mean square of the error. The second and third numbers represent the degrees of freedom of the numerator and denominator, respectively.↩︎\nThis again echoes the discussion of racing from Section 11.3.3.↩︎\nWhen performing an equivalence test, the margin of practical indifference should be specified before the test is performed.↩︎\nOne reason to avoid using metrics based on qualitative predictions (e.g., “yes” or “no”) is that they have a diminished capability to detect differences. They tend to contain far less information than their corresponding class probabilities. A metric such as the Brier score would have additional precision and might be able to tell the two models apart.↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "",
    "text": "Preface\nWelcome! This is a work in progress. We want to create a practical guide to developing quality predictive models from tabular data. We’ll publish materials here as we create them and welcome community contributions in the form of discussions, suggestions, and edits.\nWe also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).\nOur intention is to write these materials and, when we feel we’re done, pick a publishing partner to produce a print version.\nThe book takes a holistic view of the predictive modeling process and focuses on a few areas that are usually left out of similar works. For example, the effectiveness of the model can be driven by how the predictors are represented. Because of this, we tightly couple feature engineering methods with machine learning models. Also, quite a lot of work happens after we have determined our best model and created the final fit. These post-modeling activities are an important part of the model development process and will be described in detail.\nWe deliberately avoid using the term “artificial intelligence.” Eugen Rochko’s (@Gargron@mastodon.social) comment on Mastodon does a good job of summarizing our reservations regarding the term:\nTo cite this website, we suggest:\n@online{aml4td,\n  author = {Kuhn, M and Johnson, K},\n  title = {{Applied Machine Learning for Tabular Data}},\n  year = {2023},\n  url = { https://aml4td.org},\n  urldate = {2025-06-24}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "License",
    "text": "License\n\nThis work is licensed under CC BY-NC-SA 4.0\n\nThis license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, for noncommercial purposes only. If others modify or adapt the material, they must license the modified material under identical terms.\n\nBY: Credit must be given to you, the creator.\nNC: Only noncommercial use of your work is permitted. Noncommercial means not primarily intended for or directed towards commercial advantage or monetary compensation.\nSA: Adaptations must be shared under the same terms.\n\nOur goal is to have an open book where people can reuse and reference the materials but can’t just put their names on them and resell them (without our permission).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Intended Audience",
    "text": "Intended Audience\nOur intended audience includes data analysts of many types: statisticians, data scientists, professors and instructors of machine learning courses, laboratory scientists, and anyone else who desires to understand how to create a model for prediction. We don’t expect readers to be experts in these methods or the math behind them. Instead, our approach throughout this work is applied. That is, we want readers to use this material to build intuition about the predictive modeling process. What are good and bad ideas for the modeling process? What pitfalls should we look out for? How can we be confident that the model will be predictive for new samples? What are advantages and disadvantages of different types of models? These are just some of the questions that this work will address.\nSome background in modeling and statistics will be extremely useful. Having seen or used basic regression models is good, and an understanding of basic statistical concepts such as variance, correlation, populations, samples, etc., is needed. There will also be some mathematical notation, so you’ll need to be able to grasp these abstractions. But we will keep this to those parts where it is absolutely necessary. There are a few more statistically sophisticated sections for some of the more advanced topics.\nIf you would like a more theoretical treatment of machine learning models, then we recommend Hastie, Tibshirani, and Friedman (2017). Other books for gaining a more in-depth understanding of machine learning are Bishop and Nasrabadi (2006), Arnold, Kane, and Lewis (2019) and, for more of a deep learning focus, Goodfellow, Bengio, and Courville (2016) and/or Prince (2023).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#is-there-code",
    "href": "index.html#is-there-code",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Is There Code?",
    "text": "Is There Code?\nWe definitely want to decouple the content of this work from specific software. One of our other books on modeling had computing sections. Many people found these sections to be a useful resource at the time of the book’s publication. However, code can quickly become outdated in today’s computational environment. In addition, this information takes up a lot of page space that would be better used for other topics.\nWe will create computing supplements to go along with the materials. Since we use R’s tidymodels framework for calculations, the supplement currently in-progress is:\n\ntidymodels.aml4td.org\n\nIf you are interested in working on a python/scikit-learn supplement, please file an issue\nWhen there is code that corresponds to a particular section, there will be one or more icons at the ends of the section to link to the computing supplements, such as this:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-exercises",
    "href": "index.html#sec-exercises",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Are There Exercises?",
    "text": "Are There Exercises?\nMany readers found the Exercise sections of Applied Predictive Modeling to be helpful for solidifying the concepts presented in each chapter. The current set can be found at exercises.aml4td.org",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-help",
    "href": "index.html#sec-help",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "How can I Ask Questions?",
    "text": "How can I Ask Questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like cross-validated. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what I call Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.\nIf you think there is a bug, please file an issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Can I Contribute?",
    "text": "Can I Contribute?\nThere is a contributing page with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list. It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.\nAlso note that the aml4td project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto was used to compile and render the materials\n\nQuarto 1.7.31\n[✓] Checking environment information...\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.6.3: OK\n      Dart Sass version 1.85.1: OK\n      Deno version 1.46.3: OK\n      Typst version 0.13.0: OK\n[✓] Checking versions of quarto dependencies......OK\n[✓] Checking Quarto installation......OK\n      Version: 1.7.31\n[✓] Checking tools....................OK\n      TinyTeX: (external install)\n      Chromium: (not installed)\n[✓] Checking LaTeX....................OK\n      Using: TinyTex\n      Version: 2025\n[✓] Checking Chrome Headless....................OK\n      Using: Chrome found on system\n      Source: MacOS known location\n[✓] Checking basic markdown render....OK\n[✓] Checking Python 3 installation....OK\n      Version: 3.10.17\n      Jupyter: (None)\n      Jupyter is not available in this Python installation.\n[✓] Checking R installation...........OK\n      Version: 4.5.0\n      LibPaths:\n      knitr: 1.50\n      rmarkdown: 2.29\n[✓] Checking Knitr engine render......OK\n\nR version 4.5.0 (2025-04-11) was used for the majority of the computations. torch 2.5.1 was also used. The versions of the primary R modeling and visualization packages used here are:\n\n\n\n\n\n\n\n\n\naorsf (0.1.5)\napplicable (0.1.1)\naspline (0.2.0)\n\n\nbaguette (1.1.0)\nbestNormalize (1.9.1)\nbibtex (0.5.1)\n\n\nbonsai (0.3.2)\nbroom (1.0.8)\nbroom.mixed (0.2.9.6)\n\n\nbrulee (0.5.0)\nbslib (0.9.0)\nC50 (0.2.0)\n\n\ncolino (0.0.1)\nCubist (0.5.0)\nDALEXtra (2.3.0)\n\n\ndbarts (0.9-32)\nddalpha (1.3.16)\ndesirability2 (0.0.1)\n\n\ndials (1.4.0)\ndimRed (0.2.7)\ndiscrim (1.0.1)\n\n\ndoMC (1.3.8)\ndplyr (1.1.4)\ne1071 (1.7-16)\n\n\nearth (5.3.4)\nembed (1.1.5)\nemmeans (1.11.1)\n\n\nfastICA (1.2-7)\nfinetune (1.2.1)\nforested (0.1.0)\n\n\nfuture.mirai (0.10.0)\nGA (3.2.4)\ngganimate (1.0.9)\n\n\nggforce (0.5.0)\nggiraph (0.8.13)\nggplot2 (3.5.2)\n\n\nglmnet (4.1-9)\ngt (1.0.0)\nhardhat (1.4.1)\n\n\nheatmaply (1.5.0)\nhstats (1.2.1)\nipred (0.9-15)\n\n\nirlba (2.3.5.1)\njanitor (2.2.1)\nkernlab (0.9-33)\n\n\nklaR (1.7-3)\nleaflet (2.2.2)\nlightgbm (4.6.0)\n\n\nlme4 (1.1-37)\nMatrix (1.7-3)\nmda (0.5-5)\n\n\nmeasure (0.0.1.9000)\nmgcv (1.9-3)\nmixdir (0.3.0)\n\n\nmixOmics (6.32.0)\nmodeldata (1.4.0)\nmodeldatatoo (0.3.0)\n\n\nnaniar (1.1.0)\nnlme (3.1-168)\npamr (1.57)\n\n\nparsnip (1.3.2)\npartykit (1.2-24)\npatchwork (1.3.0)\n\n\nplsmod (1.0.0)\nprobably (1.1.0)\npROC (1.18.5)\n\n\npurrr (1.0.4)\nragg (1.4.0)\nranger (0.17.0)\n\n\nrecipes (1.3.1)\nrpart (4.1.24)\nrsample (1.3.0)\n\n\nRSpectra (0.16-2)\nrstudioapi (0.17.1)\nrules (1.0.2)\n\n\nsf (1.0-21)\nsfd (0.1.0)\nshinylive (0.3.0)\n\n\nsparsediscrim (0.3.0)\nsparseLDA (0.1-9)\nsparsevctrs (0.3.4)\n\n\nspatialsample (0.6.0)\nsplines2 (0.5.4)\nstacks (1.1.1)\n\n\nstopwords (2.3)\ntextrecipes (1.1.0)\nthemis (1.0.3)\n\n\ntidymodels (1.3.0)\ntidyposterior (1.0.1)\ntidyr (1.3.1)\n\n\ntidysdm (1.0.0)\ntorch (0.14.2)\ntune (1.3.0)\n\n\nusethis (3.1.0)\nuwot (0.2.3)\nVBsparsePCA (0.1.0)\n\n\nviridis (0.6.5)\nworkflows (1.2.0)\nworkflowsets (1.1.1)\n\n\nxgboost (1.7.11.1)\nxrf (0.2.2)\nyardstick (1.3.2)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#chapter-references",
    "href": "index.html#chapter-references",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nArnold, T, M Kane, and B Lewis. 2019. A Computational Approach to Statistical Learning. Chapman; Hall/CRC.\n\n\nBishop, C M, and N M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nHastie, T, R Tibshirani, and J Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.",
    "crumbs": [
      "Preface"
    ]
  }
]