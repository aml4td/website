[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "",
    "text": "Preface\nWelcome! This is a work in progress. We want to create a practical guide to developing quality predictive models from tabular data. We’ll publish materials here as we create them and welcome community contributions in the form of discussions, suggestions, and edits.\nWe also want these materials to be reusable and open. The sources are in the source GitHub repository with a Creative Commons license attached (see below).\nOur intention is to write these materials and, when we feel we’re done, pick a publishing partner to produce a print version.\nThe book takes a holistic view of the predictive modeling process and focuses on a few areas that are usually left out of similar works. For example, the effectiveness of the model can be driven by how the predictors are represented. Because of this, we tightly couple feature engineering methods with machine learning models. Also, quite a lot of work happens after we have determined our best model and created the final fit. These post-modeling activities are an important part of the model development process and will be described in detail.\nWe deliberately avoid using the term “artificial intelligence.” Eugen Rochko’s (@Gargron@mastodon.social) comment on Mastodon does a good job of summarizing our reservations regarding the term:\nTo cite this website, we suggest:\n@online{aml4td,\n  author = {Kuhn, M and Johnson, K},\n  title = {{Applied Machine Learning for Tabular Data}},\n  year = {2023},\n  url = { https://aml4td.org},\n  urldate = {2025-06-27}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "License",
    "text": "License\n\nThis work is licensed under CC BY-NC-SA 4.0\n\nThis license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, for noncommercial purposes only. If others modify or adapt the material, they must license the modified material under identical terms.\n\nBY: Credit must be given to you, the creator.\nNC: Only noncommercial use of your work is permitted. Noncommercial means not primarily intended for or directed towards commercial advantage or monetary compensation.\nSA: Adaptations must be shared under the same terms.\n\nOur goal is to have an open book where people can reuse and reference the materials but can’t just put their names on them and resell them (without our permission).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Intended Audience",
    "text": "Intended Audience\nOur intended audience includes data analysts of many types: statisticians, data scientists, professors and instructors of machine learning courses, laboratory scientists, and anyone else who desires to understand how to create a model for prediction. We don’t expect readers to be experts in these methods or the math behind them. Instead, our approach throughout this work is applied. That is, we want readers to use this material to build intuition about the predictive modeling process. What are good and bad ideas for the modeling process? What pitfalls should we look out for? How can we be confident that the model will be predictive for new samples? What are advantages and disadvantages of different types of models? These are just some of the questions that this work will address.\nSome background in modeling and statistics will be extremely useful. Having seen or used basic regression models is good, and an understanding of basic statistical concepts such as variance, correlation, populations, samples, etc., is needed. There will also be some mathematical notation, so you’ll need to be able to grasp these abstractions. But we will keep this to those parts where it is absolutely necessary. There are a few more statistically sophisticated sections for some of the more advanced topics.\nIf you would like a more theoretical treatment of machine learning models, then we recommend Hastie, Tibshirani, and Friedman (2017). Other books for gaining a more in-depth understanding of machine learning are Bishop and Nasrabadi (2006), Arnold, Kane, and Lewis (2019) and, for more of a deep learning focus, Goodfellow, Bengio, and Courville (2016) and/or Prince (2023).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#is-there-code",
    "href": "index.html#is-there-code",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Is There Code?",
    "text": "Is There Code?\nWe definitely want to decouple the content of this work from specific software. One of our other books on modeling had computing sections. Many people found these sections to be a useful resource at the time of the book’s publication. However, code can quickly become outdated in today’s computational environment. In addition, this information takes up a lot of page space that would be better used for other topics.\nWe will create computing supplements to go along with the materials. Since we use R’s tidymodels framework for calculations, the supplement currently in-progress is:\n\ntidymodels.aml4td.org\n\nIf you are interested in working on a python/scikit-learn supplement, please file an issue\nWhen there is code that corresponds to a particular section, there will be one or more icons at the ends of the section to link to the computing supplements, such as this:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-exercises",
    "href": "index.html#sec-exercises",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Are There Exercises?",
    "text": "Are There Exercises?\nMany readers found the Exercise sections of Applied Predictive Modeling to be helpful for solidifying the concepts presented in each chapter. The current set can be found at exercises.aml4td.org",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-help",
    "href": "index.html#sec-help",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "How can I Ask Questions?",
    "text": "How can I Ask Questions?\nIf you have questions about the content, it is probably best to ask on a public forum, like cross-validated. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.\nIf you want a direct answer from us, you should follow what I call Yihui’s Rule: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.\nIf you think there is a bug, please file an issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Can I Contribute?",
    "text": "Can I Contribute?\nThere is a contributing page with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.\nIf you just want to fix a typo, you can make a pull request to alter the appropriate .qmd file.\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list. It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.\nAlso note that the aml4td project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#computing-notes",
    "href": "index.html#computing-notes",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Computing Notes",
    "text": "Computing Notes\nQuarto was used to compile and render the materials\n\nQuarto 1.7.31\n[✓] Checking environment information...\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.6.3: OK\n      Dart Sass version 1.85.1: OK\n      Deno version 1.46.3: OK\n      Typst version 0.13.0: OK\n[✓] Checking versions of quarto dependencies......OK\n[✓] Checking Quarto installation......OK\n      Version: 1.7.31\n[✓] Checking tools....................OK\n      TinyTeX: (external install)\n      Chromium: (not installed)\n[✓] Checking LaTeX....................OK\n      Using: TinyTex\n      Version: 2025\n[✓] Checking Chrome Headless....................OK\n      Using: Chrome found on system\n      Source: MacOS known location\n[✓] Checking basic markdown render....OK\n[✓] Checking Python 3 installation....OK\n      Version: 3.10.17\n      Jupyter: (None)\n      Jupyter is not available in this Python installation.\n[✓] Checking R installation...........OK\n      Version: 4.5.0\n      LibPaths:\n      knitr: 1.50\n      rmarkdown: 2.29\n[✓] Checking Knitr engine render......OK\n\nR version 4.5.0 (2025-04-11) was used for the majority of the computations. torch 2.5.1 was also used. The versions of the primary R modeling and visualization packages used here are:\n\n\n\n\n\n\n\n\n\naorsf (0.1.5)\napplicable (0.1.1)\naspline (0.2.0)\n\n\nbaguette (1.1.0)\nbestNormalize (1.9.1)\nbibtex (0.5.1)\n\n\nbonsai (0.3.2)\nbroom (1.0.8)\nbroom.mixed (0.2.9.6)\n\n\nbrulee (0.5.0)\nbslib (0.9.0)\nC50 (0.2.0)\n\n\ncolino (0.0.1)\nCubist (0.5.0)\nDALEXtra (2.3.0)\n\n\ndbarts (0.9-32)\nddalpha (1.3.16)\ndesirability2 (0.0.1)\n\n\ndials (1.4.0)\ndimRed (0.2.7)\ndiscrim (1.0.1)\n\n\ndoMC (1.3.8)\ndplyr (1.1.4)\ne1071 (1.7-16)\n\n\nearth (5.3.4)\nembed (1.1.5)\nemmeans (1.11.1)\n\n\nfastICA (1.2-7)\nfinetune (1.2.1)\nforested (0.1.0)\n\n\nfuture.mirai (0.10.0)\nGA (3.2.4)\ngganimate (1.0.9)\n\n\nggforce (0.5.0)\nggiraph (0.8.13)\nggplot2 (3.5.2)\n\n\nglmnet (4.1-9)\ngt (1.0.0)\nhardhat (1.4.1)\n\n\nheatmaply (1.5.0)\nhstats (1.2.1)\nipred (0.9-15)\n\n\nirlba (2.3.5.1)\njanitor (2.2.1)\nkernlab (0.9-33)\n\n\nklaR (1.7-3)\nleaflet (2.2.2)\nlightgbm (4.6.0)\n\n\nlme4 (1.1-37)\nMatrix (1.7-3)\nmda (0.5-5)\n\n\nmeasure (0.0.1.9000)\nmgcv (1.9-3)\nmixdir (0.3.0)\n\n\nmixOmics (6.32.0)\nmodeldata (1.4.0)\nmodeldatatoo (0.3.0)\n\n\nnaniar (1.1.0)\nnlme (3.1-168)\npamr (1.57)\n\n\nparsnip (1.3.2)\npartykit (1.2-24)\npatchwork (1.3.0)\n\n\nplsmod (1.0.0)\nprobably (1.1.0)\npROC (1.18.5)\n\n\npurrr (1.0.4)\nragg (1.4.0)\nranger (0.17.0)\n\n\nrecipes (1.3.1)\nrpart (4.1.24)\nrsample (1.3.0)\n\n\nRSpectra (0.16-2)\nrstudioapi (0.17.1)\nrules (1.0.2)\n\n\nsf (1.0-21)\nsfd (0.1.0)\nshinylive (0.3.0)\n\n\nsparsediscrim (0.3.0)\nsparseLDA (0.1-9)\nsparsevctrs (0.3.4)\n\n\nspatialsample (0.6.0)\nsplines2 (0.5.4)\nstacks (1.1.1)\n\n\nstopwords (2.3)\ntextrecipes (1.1.0)\nthemis (1.0.3)\n\n\ntidymodels (1.3.0)\ntidyposterior (1.0.1)\ntidyr (1.3.1)\n\n\ntidysdm (1.0.0)\ntorch (0.14.2)\ntune (1.3.0)\n\n\nusethis (3.1.0)\nuwot (0.2.3)\nVBsparsePCA (0.1.0)\n\n\nviridis (0.6.5)\nworkflows (1.2.0)\nworkflowsets (1.1.1)\n\n\nxgboost (1.7.11.1)\nxrf (0.2.2)\nyardstick (1.3.2)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#chapter-references",
    "href": "index.html#chapter-references",
    "title": "Applied Machine Learning for Tabular Data",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nArnold, T, M Kane, and B Lewis. 2019. A Computational Approach to Statistical Learning. Chapman; Hall/CRC.\n\n\nBishop, C M, and N M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nHastie, T, R Tibshirani, and J Friedman. 2017. The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/news.html",
    "href": "chapters/news.html",
    "title": "News",
    "section": "",
    "text": "2025-06-24\nA new chapter (14  Comparing Models) on model comparisons was added, concluding Part 3 of the book.\nAlso, a small section on permutation statistics was added to 9  Overfitting.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-1",
    "href": "chapters/news.html#section-1",
    "title": "News",
    "section": "2025-05-21",
    "text": "2025-05-21\nNew chapters on iterative optimization and feature selection.\nWhenever a section has a corresponding computing supplement, an icon (e.g.,  ) is shown that directly links to that section.\nNon-user facing changes:\n\nSome prep work for future light/dark mode.\nMove to mirai for parallel processing.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-2",
    "href": "chapters/news.html#section-2",
    "title": "News",
    "section": "2024-10-01",
    "text": "2024-10-01\nEnabled google analytics.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-3",
    "href": "chapters/news.html#section-3",
    "title": "News",
    "section": "2024-09-23",
    "text": "2024-09-23\nChapter 4 is now on missing data, bumping subsequent chapter numbers.\nNew chapters on resampling and grid search are now online.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-4",
    "href": "chapters/news.html#section-4",
    "title": "News",
    "section": "2024-06-17",
    "text": "2024-06-17\nSeveral new chapters on embeddings, splines/interactions/discretization, and overfitting. Also, shinylive is used for interactive visualizations of concepts.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-5",
    "href": "chapters/news.html#section-5",
    "title": "News",
    "section": "2024-01-02",
    "text": "2024-01-02\nFixed various typos.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-6",
    "href": "chapters/news.html#section-6",
    "title": "News",
    "section": "2023-12-21",
    "text": "2023-12-21\nTypo fix on main page.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-7",
    "href": "chapters/news.html#section-7",
    "title": "News",
    "section": "2023-12-19",
    "text": "2023-12-19\nSmall updates.\nUpdated snapshot.\nThe background color of premade outputs has been changed to the pages background color (#16).",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-8",
    "href": "chapters/news.html#section-8",
    "title": "News",
    "section": "2023-12-11",
    "text": "2023-12-11\nNo new content.\nUpdated snapshot.\nIncludes new Google Scholar links for articles (#8)",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-9",
    "href": "chapters/news.html#section-9",
    "title": "News",
    "section": "2023-11-17",
    "text": "2023-11-17\nUpdated renv snapshot.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/news.html#section-10",
    "href": "chapters/news.html#section-10",
    "title": "News",
    "section": "2023-10-09",
    "text": "2023-10-09\nFirst committed versions.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "chapters/contributing.html",
    "href": "chapters/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Software\nPlease note that the aml4td project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nIf you plan to do anything beyond fixing a typo, the best thing you can do is to open an issue and discuss changes before you spend a lot of time doing them.\nIf you don’t have a lot of experience with git or GitHub, take a look at the wonderful Happy Git and GitHub for the useR.\nIf you want to contribute, some general advice is:\nA merged PR will make you appear in the contributor list (see below). It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.\nYou will mostly work with the *.qmd files in the chapters directory.\nHere is a list of the elements in the repo:\nRegarding R packages, the repository has a DESCRIPTION file as if it were an R package. This lets us specify precisely what packages and their versions should be installed. The packages listed in the imports field contain packages for modeling/analysis and packages used to make the website/book. Some basic system requirements are likely needed to install packages: Fortran, gdal, and others.\nThe main requirements are as follows.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#software",
    "href": "chapters/contributing.html#software",
    "title": "Contributing",
    "section": "",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing system. Quarto version 1.7.31 is used to compile the website.\nWe also use a few Quarto extensions. These should be installed from the project’s root directory via:\nquarto add quarto-ext/fontawesome\nquarto add quarto-ext/shinylive\nquarto add quarto-ext/fancy-text\nquarto add leovan/quarto-pseudocode\n\n\nR and renv\nR version 4.5.0 (2025-04-11) is what we are currently using. We suggest using rig to manage R versions. There are several IDEs that you can use. We’ve used RStudio (&gt;= 2023.6.1.524).\nThe current strategy is to use the renv (&gt;= version 1.1.4) package to make this project more isolated, portable and reproducible.\nTo get package dependencies installed…\n\nWhen you open the website.Rproj file, the renv package should be automatically installed/updated (if neded). For example:\n# Bootstrapping renv 1.0.3 ---------------------------------------------------\n- Downloading renv ... OK\n- Installing renv  ... OK\n\nThe following package(s) will be installed:\n- BiocManager [1.30.22]\nThese packages will be installed into \"~/content/aml4td/renv/library/R-4.3/x86_64-apple-darwin20\".\nif you try to compile the book, you probably get and error:\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nYou can get more information using renv::status() but you can get them installed by first running renv::activate(). As an example:\n&gt; renv::activate()\n\nRestarting R session...\n\n- Project '~/content/aml4td' loaded. [renv 1.0.3]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nSince we have package versions recorded in the lockfile, we can installed them using renv::restore(). Here is an example of that output:\n&gt; renv::restore() \nThe following package(s) will be updated:\n\n# Bioconductor ---------------------------------------------------------------\n- mixOmics         [* -&gt; mixOmicsTeam/mixOmics]\n\n# CRAN -----------------------------------------------------------------------\n- BiocManager      [1.30.22 -&gt; 1.30.21.1]\n- lattice          [0.21-9 -&gt; 0.21-8]\n- Matrix           [1.6-1.1 -&gt; 1.6-0]\n- nlme             [3.1-163 -&gt; 3.1-162]\n- rpart            [4.1.21 -&gt; 4.1.19]\n- survival         [3.5-7 -&gt; 3.5-5]\n- abind            [* -&gt; 1.4-5]\n\n&lt;snip&gt;\n\n- zip              [* -&gt; 2.2.0]\n- zoo              [* -&gt; 1.8-12]\n\n# GitHub ---------------------------------------------------------------------\n- BiocParallel     [* -&gt; Bioconductor/BiocParallel@devel]\n- BiocVersion      [* -&gt; Bioconductor/BiocVersion@devel]\n- modeldatatoo     [* -&gt; tidymodels/modeldatatoo@HEAD]\n- parsnip          [* -&gt; tidymodels/parsnip@HEAD]\n- usethis          [* -&gt; r-lib/usethis@HEAD]\n\n# RSPM -----------------------------------------------------------------------\n- bslib            [* -&gt; 0.5.1]\n- fansi            [* -&gt; 1.0.5]\n- fontawesome      [* -&gt; 0.5.2]\n- ggplot2          [* -&gt; 3.4.4]\n- htmltools        [* -&gt; 0.5.6.1]\n- withr            [* -&gt; 2.5.1]\n\nDo you want to proceed? [Y/n]: y\n\n# Downloading packages -------------------------------------------------------\n- Downloading BiocManager from CRAN ...         OK [569 Kb in 0.19s]\n- Downloading nlme from CRAN ...                OK [828.7 Kb in 0.19s]\n- Downloading BH from CRAN ...                  OK [12.7 Mb in 0.4s]\n- Downloading BiocVersion from GitHub ...       OK [826 bytes in 0.37s]\n\n&lt;snip&gt;\n\nDepending on whether you have to install packages from source, you may need to install some system dependencies and try again (I had to install libgit2 the last time I did this). It is also recommended that you use the install of gfortran suggested by CRAN (esp avoiding the homebrew version on macOS). See the gfortran-for-macOS repository.\nOnce you have everything installed, we recommend installing the underlying torch computational libraries. You can do this by loading the torch package A download will automatically begin if you need one.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing.html#contributor-list",
    "href": "chapters/contributing.html#contributor-list",
    "title": "Contributing",
    "section": "Contributor List",
    "text": "Contributor List\nThe would like to thank those who have made a contribution to the project: @amy-palmer, @bmreiniger, @coatless, @krz, @syclik, and @tomsing1.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Non-Tabular Data\nMachine learning (ML) models are mathematical equations that take inputs, called predictors, and try to estimate some future output value. The output, often called an outcome or target, can be numbers, categories, or other types of values.\nFor example, in the next chapter, we try to predict how long it takes to deliver food ordered from a restaurant. The outcome is the time from the initial order (in minutes). There are multiple predictors, including: the distance from the restaurant to the delivery location, the date/time of the order, and which items were included in the order. These data are tabular; they can be arranged in a table-like way (such as a spreadsheet or database table) where variables are arranged in columns and individual data points (i.e., instances of food orders) in rows, as shown in Table 1.11.\nNote that the predictor values are almost always known. For future data, the outcome is not; it is a machine learning model’s job to predict unknown outcome values.\nHow does it do that? A specific machine learning model has a defined mathematical prediction equation defining exactly how the predictors relate to the outcome. We’ll see two very different prediction equations shortly.\nThe prediction equation includes some unknown parameters. These are placeholders for values that help us best predict the outcomes. The process of model training (also called “fitting” or “estimation”) takes our existing predictor and outcome data and uses them to find the “optimal” values of these unknown parameters. Once we estimate these unknowns, we can use them and specific predictor values to estimate future outcome values.\nHere is a simple example of a prediction equation with a single predictor (the distance) and two unknown parameters that have been estimated:\n\\[\ndelivery\\:time = \\exp\\left[2.944 + 0.096 \\: distance\\right]\n\\tag{1.1}\\]\nWe could use this equation for new orders:\nand so on.\nThis is an incredibly reductive approach; it assumes that the relationship between the distance and time are log-linearly related and is not influenced by any other characteristics of the process. On the bright side, the exponential function ensures that there are no negative time predictions, and it is straightforward to explain.\nHow did we arrive at values of 2.944 and 0.096? They were estimated from data once we made a few assumptions about our model. We started off by proposing a format for the prediction equation: an additive, log-linear function2 that includes unknown parameters \\(\\beta_0\\) and \\(\\beta_1\\):\n\\[\n{ \\color{darkseagreen} \\underbrace{\\color{black} log(time)}_{\\text{the outcome}} } =\n{ \\color{darkseagreen} \\overbrace{\\color{black} \\beta_0+\\beta_1\\, distance}^{\\text{an additive function of the predictors}} } +  \n{ \\color{darkseagreen} \\underbrace{\\color{black} \\epsilon}_{\\text{the errors}} }\n\\]\nTo estimate the parameters, we chose some type of criterion to quantify what it means to be “good” values (called an objective function or performance metric). Here we will pick values of the \\(\\beta\\) parameters that minimize the squared error, with the error (\\(\\epsilon\\)) defined as the difference in the observed and predicted times (on the log scale).\nBased on this criterion, we can define intermediate equations that, if we solve them, will result in the best possible parameter estimates given the data and our choices for the prediction equation and objective function.\nSadly, Equation 1.1, while better than a random guess, is not very effective. It does the best it can with a the single predictor. It just isn’t very accurate.\nTo illustrate the diversity of different ML models, Equation 1.2 shows a completely different kind of prediction equation. This was estimated from a model called a regression tree. The function \\(I(\\cdot)\\) is one if the logical statement is true and zero otherwise. Two predictors (distance and day of the week) were used in this case.\n\\[\n\\begin{align}\ntime = \\:&23.0\\, I\\left(distance &lt;  4.465   \\text{ miles and } day \\in \\{Mon, Tue, Wed, Sun\\}\\right)  + \\notag \\\\\n       \\:&27.0\\, I\\left(distance &lt;  4.465   \\text{ miles and } day \\in \\{Thu, Fri, Sat\\}\\right)  + \\notag \\\\\n       \\:&29.8\\, I\\left(distance \\ge  4.465 \\text{ miles and } day \\in \\{Mon, Tue, Wed, Sun\\}\\right)  + \\notag \\\\\n       \\:&36.5\\, I\\left(distance \\ge  4.465 \\text{ miles and } day \\in \\{Thu, Fri, Sat\\}\\right)\\notag\n\\end{align}\n\\tag{1.2}\\]\nA regression tree determines the best predictors(s) to “split” the data, making smaller and smaller subsets of the data. The goal is to make the subsets within each split have about the same outcome value. The coefficients for each logical group are the average of the delivery times for each subset defined by the statement with the \\(I(\\cdot)\\) terms. Note that these logical statements, called rules, are mutually exclusive; only one of the four subsets will be true for any data point being predicted.\nLike the linear regression model that produced Equation 1.1, the performance of Equation 1.2 is quite poor. There are only four possible predicted delivery times. This particular model is not known for its predictive ability, but it can be highly predictive when combined with other regression trees into an ensemble model.\nThis book is focused on the practice of applying various ML models to data to make accurate predictions. Before proceeding, we’ll discuss different types of the data used to estimate models and then explore some aspects of ML models, particularly neural networks.\nThe columns shown in Table 1.1 are defined within a small scope. For example, the delivery hour column encapsulates everything about that order characteristic:\nNon-tabular data does not have the same properties. Consider images, such as the one shown Figure 1.1 that depicts four cells from a biological experiment (Yu et al. 2007). These cells that have been stained with pigments that fluoresce at different wavelengths to “paint” different parts of the cells. In this image, blue reflects the part of the cell called the cytoskeleton, which encompasses the entire cell body. The red color corresponds to a stain that only attaches itself to the contents of the cell nucleus, specifically DNA. This image3 is 371 pixels by 341 pixels with quantification of two colors.\nFigure 1.1: An example image of several cells using two colors. Image from Yu et al. (2007).\nFrom a data perspective, this image is represented as a three-dimensional array (371 rows, 341 columns, and 2 colors). Probably the most important attribute of this data is the spatial relationships between pixels. It is critical to know where each value in the 3D array resides, but knowing what the nearby values are is at least as important.\nWe could “flatten” the array into 371 \\(\\times\\) 341 \\(\\times\\) 2 = 253,022 columns to use in a table4. However, this would break the spatial link between the data points; each column is no longer as self-contained as the columns from the delivery data. The result is that we have to consider the 3D array as the data instead of each row/column/color combination.\nAdditionally, images often have different dimensions which results in different array sizes. From a tabular data perspective, a flattened version of such data would have a different number of columns for different-sized images.\nVideos are an extension of image data if we were to consider it a four-dimensional array (with time as the additional dimension). The temporal aspects of such data are critical to understanding and predicting values.\nText data are often thought of as non-tabular data. As an example, consider text from Sanderson (2017):\nWith text data, it is common to define the “token”: the unit of text that should be analyzed. This could be a paragraph, sentence, word, etc. Suppose that we used sentences as tokens. In this case, the table for this quote would have four rows. Where do we go from here? The sequence of words (or characters) is likely to be important, and this makes a case for keeping the sentences as strings. However, as will be seen shortly, we might be able to convert these four tokens to numeric columns in a way that preserves the information required for prediction.\nPerhaps a better text example relates to the structure of chemicals. A chemical is a three-dimensional molecule but is often described using a SMILES string (Engel and Gasteiger 2018, chap. 3). This is a textual description of molecule that lists its elements and how they relate to one another. For example, the SMILES string CC(C)CC1=CC=C(C=C1)C(C)C(=O)O defines the anti-inflammatory drug Ibuprofen. The letters describe the elements (C is carbon, O is oxygen, etc) and the other characters defines the bonds between the elements. The character sequence within this string is critical to understanding the data.\nOur motives for categorizing data as tabular and non-tabular are related to how we choose an appropriate model. Our opinion is that most modeling projects involve tabular data (or data that can be effectively represented as tabular). Models for non-tabular data are very specialized and, while these ML approaches are the most discussed in the social media, they tend not to be the best approach for tabular data.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular",
    "href": "chapters/introduction.html#sec-nontabular",
    "title": "1  Introduction",
    "section": "",
    "text": "You don’t need any other columns to define it.\nA single number is all that is required to describe that information.\n\n\n\n\n\n\n\n\n\n“The most important step a man can take. It’s not the first one, is it? It’s the next one. Always the next step, Dalinar.”",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-nontabular-convert",
    "href": "chapters/introduction.html#sec-nontabular-convert",
    "title": "1  Introduction",
    "section": "1.2 Converting Non-Tabular Data to Tabular",
    "text": "1.2 Converting Non-Tabular Data to Tabular\nIn some situations, non-tabular data can be effectively converted to a tabular format, depending on the specific problem and data.\nFor some modeling projects using images, we might not be interested in every pixel. For Figure 1.1, we really care about the cells in the image. It is important to understand within-cell characteristics (e.g. shape or size) and some between-cell information (e.g., the distance between cells). The vast majority of the pixels don’t need to be analyzed.\nWe can pre-analyze the 3D arrays to determine which pixels in an image correspond to specific cells (or nuclei within a cell). Segmentation (Holmes and Huber 2018) is the process of estimating regions of an image that define some object(s). Figure 1.2 shows the same cells with lines generated from a simple segmentation.\n\n\n\n\n\n\n\n\nFigure 1.2: The cell segmentation results for the four neuroblastomas shown in Figure 1.1.\n\n\n\n\n\nOnce our cells have been segmented, we can compute various statistics of interest based on size, shape, or color. If a cell is our unit of data, we can create a tabular format where rows are cells and columns are cell characteristics (Table 1.2). The result is a data table that has more rows than the original non-tabular structure since there are multiple cells in an image. There are far fewer columns but these columns are designed to be more informative than the raw pixel data since the researchers define the cell characteristics that they desire.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\n\nEccentricity\n\n\n\nArea\n\n\n\nIntensity\n\n\n\nNucleus\nCell\nNucleus\nCell\nNucleus\nCell\n\n\n\n\n17\n0.494\n0.836\n\n3,352\n11,699\n\n0.274\n0.155\n\n\n18\n0.708\n0.550\n\n1,777\n4,980\n\n0.278\n0.210\n\n\n21\n0.495\n0.802\n\n1,274\n3,081\n\n0.326\n0.218\n\n\n22\n0.809\n0.975\n\n1,169\n3,933\n\n0.583\n0.229\n\n\n\n\n\n\n\n\nTable 1.2: Cells, such as those found in Figure 1.1, translated into a tabular format using three features for the nuclear and non-nuclear regions of the segmented cells.\n\n\n\n\nThis conversion to tabular data isn’t appropriate for all image analysis problems. If you want to know if an image contains a cat or not it probably isn’t a good idea.\nLet’s also consider the text examples. For the book quote, Table 1.3 shows a few simple predictors that are tabular in nature:\n\nPresence/absence columns for each word in the entire document. Table 1.3 shows columns corresponding to the words “always”, “can”, and “take”. These are represented as counts.\nSentence summaries, such as the number of commas, words, characters, first-person speech, etc.\n\nThere are many more ways to represent these data. More complex numeric embeddings that are built on separate large databases can reflect different parts of speech, text sequences (called n-grams), and others. Hvitfeldt and Silge (2021) and Boykis (2023) describe these and other tools for analyzing text data (in either format).\n\n\n\n\n\n\n\n\n\n\n\nSentence\n\"always\"\n\"can\"\n...\n\"take\"\nCharacters\nCommas\n\n\n\n\nThe most important step a man can take.\n0\n1\n...\n1\n32\n0\n\n\nIt's not the first one, is it?\n0\n0\n...\n0\n24\n1\n\n\nIt's the next one.\n0\n0\n...\n0\n15\n0\n\n\nAlways the next step, Dalinar.\n1\n0\n...\n0\n26\n1\n\n\n\n\n\n\n\n\nTable 1.3: An example of converting non-numeric data to a numeric, tabular format.\n\n\n\n\nFor the chemical structure of Ibuprofen, there is a rich field of molecular descriptors (Leach and Gillet 2003, chap. 3; Engel and Gasteiger 2018) that can be used to produce thousands of informative predictor columns. For example, we might be interested in the size of the molecule (perhaps measured by surface area), its electrical charge, or whether it contains specific sub-structures.\nThe process of determining the best data representations is called feature engineering. This is a critical task in machine learning and is often overlooked. Part 2 of this book spends some time looking at feature engineering methods (once the data are in a tabular format).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-types",
    "href": "chapters/introduction.html#sec-model-types",
    "title": "1  Introduction",
    "section": "1.3 Models and Machine Learning",
    "text": "1.3 Models and Machine Learning\nSince we have mentioned linear regression and regression trees, it makes sense to talk about possible machine learning scenarios. There are many, but we will focus on two types.\nRegression models have a numeric outcome (e.g., delivery time) and our goal is to predict that value. For some data sets we are mostly interested in ranking new data (as opposed to accurately predicting the value).\nThe other scenario that we discuss is classification models. In this case, the outcome is a qualitative categorical value (e.g., “cat” or “no cat” in an image). One interesting aspect of these models is that there are two main types of predictions:\n\nhard predictions correspond to the original outcome value.\nsoft predictions return the probability of each possible outcome (e.g., there is a 27% chance that a cat is in the image).\n\nThe number of categories is important; some models are specific to two classes, while others can accommodate any number of outcome categories. Most classification outcomes have mutually exclusive classes (one of all possible class values). There are also ordered class values where, while categorical, there is some type of ordering. For example, tumors are often classified in stages, with stage I being a localized malignancy and stage IV corresponding to one that has spread throughout the body.\nMulti-label outcomes can have multiple classes per row. For example, if we were trying to predict which languages that someone can speak, multilingual people would have multiple outcome values.\nWe are focused on prediction problems. This implies that our data will contain an outcome. Such scenarios falls under the broader class of supervised models. An unsupervised model is one where the is no true outcome value. Tools such as clustering, principal component analysis (PCA), and others look to quantify patterns in the data without some prediction goal. We’ll encounter unsupervised methods in our pre-model activities such as feature engineering described in Part 2.\nSeveral synonyms for machine learning include statistical learning, predictive modeling, pattern recognition, and others. While many specific models are designed for machine learning, we suggest that users think about machine learning as the problem and not the solution. Any model that can adequately predict the outcome values deserves the title. A commonly used argument for this position is that two models that are conventionally thought of as tools to make inferences (e.g., p-values) are linear regression and logistic regression. Equation 1.1 is a simplistic linear regression model. As will be seen in future chapters, linear and logistic regression models can produce highly accurate predictions for complex problems.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-data-types",
    "href": "chapters/introduction.html#sec-data-types",
    "title": "1  Introduction",
    "section": "1.4 Data Characteristics",
    "text": "1.4 Data Characteristics\nIt’s helpful to discuss different nomenclature and types of data. With tabular data, there are a number of ways to refer to a row. The terms sample, instance, observation, and/or data point are commonly used. The first, “sample”, can be used in a few different contexts; we will also use it to describe a subset of a greater data collection (as in “we took a random sample of the data”.). The notation for the number of rows in a data set is \\(n\\) with subscripts for specificity (e.g., \\(n_{tr}\\)).\nFor predictors, there are also many synonyms. In statistical nomenclature, independent variable or covariate are used. More generic terms are attribute and descriptor. The term feature is generally equated to predictors, and we’ll use both. An additional adjective may also be assigned to “feature”. Consider the date and time of the orders shown in Table 1.1. The “original feature” is the data/time column in the data. The table includes two “derived features” in the form of the decimal hour and the day of the week. This distinction will be important when discussing feature engineering, importance scores, and explaining models. The number of overall predictors in a data set is symbolized with \\(p\\).\nOutcomes/target columns can also be called the dependent variable or response.\nFor numeric columns, we will make some distinctions. A real number is numeric with potentially fractional values (e.g., time to delivery). Integers are whole numbers, often reflecting counts. Binary data take two possible values that are almost always represented with zero and one.\nDense data describes a collection of numeric values with many possible values, also reflected by the delivery time and distances in Table 1.1. Sparse data have fewer possible values or when only a few values are contained in the observed data. The product counts in the delivery data are good examples. They are integer counts consisting mostly of zeros. The frequency of non-zero values decreases with the magnitiude of the counts.\nWhile numeric data are quantitative, qualitative data cannot be represented by a numeric scale5. The day of the week data are categories with seven possible values. Binary categorical data have two categories, such as alive/dead. The symbol \\(C\\) is used to describe the number of categories in a column.\nThe item columns in Table 1.1 are interesting too. If the order could only contain one item, we might configure that data with a qualitative single column. However, these data are multiple-choice and, as such, are shown in multiple integer columns with zero, reflecting that it was not in the order.\nvalue types, skewness, colinearity, distributions, ordinal",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-model-pipeline",
    "href": "chapters/introduction.html#sec-model-pipeline",
    "title": "1  Introduction",
    "section": "1.5 What Defines the Model?",
    "text": "1.5 What Defines the Model?\nWhen discussing the modeling process, it is traditional to think the parameter estimation steps occur only when the model is fit. However, as more complex techniques are used to prepare predictor values for the model, it is crucial to understand that important quantities are also being estimated before the model. Here are a few examples:\n\nImputation methods, discussed in Chapter 4, create sub-models that estimate missing predictor values. These are applied to the data before modeling to complete the data set.\nFeature selection tools (Kuhn and Johnson 2019) often compute measures of predictor importance to remove uninformative predictors before passing the data to the model.\nEffect encoding tools, discussed in Section 6.4.3, use the effect of the predictor on the outcome as a predictor.\nPost-model adjustments to predictions, such as model calibration (Section 2.6 and ?sec-cls-calibration), are examples of postprocessing operations6.\n\n\nThese operations can profoundly affect the overall results, yet they are not usually considered part of “the model.” We’ll use the more expansive term “modeling pipeline” to describe any estimation for the model or operations before or after the model:\n\nmodel pipeline = preprocessing + supervised model + postprocessing\n\n\nThis is important to understand: a common pitfall in ML is only validating the model fitting step instead of the whole process. This can lead to performance statistics that can be extraordinarily optimistic and might deceive a practitioner into believing that their model is much better than it truly is.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-deep-learning",
    "href": "chapters/introduction.html#sec-deep-learning",
    "title": "1  Introduction",
    "section": "1.6 Deep Learning",
    "text": "1.6 Deep Learning\nWhile many models can be used for machine learning, neural networks are the elephant in the room. These complex, nonlinear models are frequently used in machine learning to the point where, in some domains, these models are synonymous with ML. One class of neural network models is deep learning (DL) models (Goodfellow, Bengio, and Courville 2016; Prince 2023). Here, “deep” means that the models have many structural layers, resulting in extraordinary amounts of complexity.\nFor example, Figure 1.3 shows a diagram of a type of deep learning model called a convolutional neural network that is often used for making predictions on images. It consists of several layers that manipulate the data. This particular model, dubbed VGG16, was proposed by Simonyan and Zisserman (2014).\nThe color image on the left is a data point. The first block has three layers. The two larger yellow boxes are convolutional layers. They move a small multidimensional array over the larger data array to compute localized features. For example, for an image that is 150 x 150 pixels, a convolution that uses a 3x3 filter will move this small rectangle across the array. At the (2,2) position, it takes the average of all adjacent pixel locations.\nAfter the convolutional operations, a smaller pooling layer is shown in orange. Pooling, usually by finding the maximum feature value across different image regions, compresses/downsamples the larger feature set so a smaller size without diminishing features with large signals.\nThe next three blocks consist of similar convolutional and pooling layers but of different dimensions. The idea behind these blocks is to reduce the image data to a smaller dimension width/height dimension in a way that extracts potentially important hierarchical features from the image.\nAfter block 5, the data are flattened into a single array, similar to the columns in a tabular data set. Up to this point, the network has been focused on preprocessing the image data. If our initial image were 150 pixels by 150 pixels, the flattening process would result in over 8 thousand predictors that would be used in a supervised ML model. The network would have over 14 million model parameters to estimate from the data to get to this point.\nAfter flattening, the final two layers correspond to the classic supervised neural network model (Bishop 1995). The original paper used four large supervised neural network layers; for simplicity, Figure 1.3 shows a single dense layer. This can add dozens of millions of additional parameters for a modest-sized dense layer.\n\n\n\n\n\n\n\n\nFigure 1.3: A simplified version of the convolutional neural network deep learning model proposed by Simonyan and Zisserman (2014). This version only includes a single dense layer (instead of four).\n\n\n\n\n\nThere are many types of deep learning models. Another example is recurrent neural networks, which create layers that are proficient at modeling data that occur in sequences such as time series data or sentences (i.e., a sequence of words).\nVery sophisticated deep learning models have generated, by far, the best predictive ability for images and similar data. It would seem natural to think these models would be just as effective for tabular data. However, this has not been the case. Machine learning competitions, which are very different from day-to-day machine learning work, have shown that many other models can consistently do as well or likely better on tabular data sets. This has generated an explosion of literature7 and social media posts that try to explain why this is the case. For example, see Borisov et al. (2022) and McElfresh et al. (2023).\nWe believe that exceptionally complex DL models are handicapped in several ways when taken out of the environments where they have been successful.\nFirst, the complexity of these models requires extreme amounts of data, and the constraints of such large data sets drive how the models are developed.\nFor example, keeping very large data sets in a computer’s memory is almost impossible. As discussed earlier, we use the data to find optimal values of our parameters for some objective functions (such as classification accuracy). Using traditional optimization procedures, this is very difficult if the data cannot be accessed simultaneously. Deep learning models use more modern optimization methods, such as stochastic gradient descent (SGD) methods, for estimating parameters. It does not simultaneously hold all of the data in memory and fits the model on small incremental batches of data. This is a very effective technique, but it is not an approach one would use with data set sizes that are more common (e.g., less than a million data points). SGD is less effective for smaller data sizes than traditional in-memory optimizers. Another consequence of huge data requirements is that model training can take an excruciatingly long time. As a result, efficient model development can require specialized hardware. It can also drive how models are optimized and compared. DL models are often optimized by sequentially adjusting the model. Other ML models can be optimized with a more methodical simultaneous design that is often more efficient, systematic, and effective than sequential optimizers used with deep learning.\nAnother issue with deep learning models is that their size and mathematical structure are associated with a very difficult optimization problem; their objective function is non-convex. We’d like to have an optimization problem that has some guarantees that a global optimal value exists (and can be found). That is often not the case for these models. For example, if we want to estimate the VGG16 model parameters with the goal of maximizing the classification accuracy for finding dogs in an image, it can be difficult to get a reliable estimate and, if we do, we don’t know if it is really the correct answer.\nBecause the optimization problem is so difficult, much of the user’s time is spent optimizing the model parameters or tuning the network structure to find the best results. When combined with how long it takes for a single model fit, deep learning models are very high maintenance compared to other models when used on tabular data. They can probably produce a model that is marginally better than the others, but doing so requires an inordinate amount of time, effort, and constraints. DL models are superior for certain hard problems with large data sets. For other situations, other ML models can go probably go further and faster8.\nFinally, there are some data-centric reasons that the successfulness of deep learning models doesn’t automatically translate to tabular data. There are characteristics of tabular data that don’t often occur in stereotypical DL applications. For example:\n\nIn some cases, a group of predictors might be highly correlated with one another. This can compromise some of the mathematical operations used to estimate parameters in neural networks (and many other models).\nUnless we have extensive prior experience with our data, we don’t know which are informative and which are irrelevant for predicting an outcome. As the number of non-informative predictors increases, the performance of neural networks decreases. See Section 19.1 of Kuhn and Johnson (2013).\nPredictors with “irregular” distributions, such as skewness or heavy tails, can harm performance (McElfresh et al. 2023).\nMissing predictor data are not naturally handled in basic neural networks.\n“Small” sample sizes or data dimensions with at least as many predictors as rows.\n\nas well as others. These issues can be overcome by adding layers to a network designed to counter specific challenges. However, in the end, this ends up adding more complexity. We propose that there are a multitude of other models, and many of them are resistant or robust to these types of data-specific characteristics. It is better to have different tools in our toolbox; making a more complex hammer may not help us put a screw in a wall.\nThe deep learning literature has resulted in many collateral benefits for the broader machine learning field. We’ll describe a few techniques that have improved machine learning in subsequent chapters,",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-aspects",
    "href": "chapters/introduction.html#sec-aspects",
    "title": "1  Introduction",
    "section": "1.7 Modeling Philosophies",
    "text": "1.7 Modeling Philosophies\ntalk about subject-matter knowledge/intuition vs “let the machine figure it out”. biased versus unbiased model development.\nassistive vs automated usage",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#sec-outline",
    "href": "chapters/introduction.html#sec-outline",
    "title": "1  Introduction",
    "section": "1.8 Outline of These Materials",
    "text": "1.8 Outline of These Materials\nThis work is organized into parts:\n\nIntroduction: The next chapter shows an abbreviated example to illustrate important concepts used later.\nPreparation: These chapters discuss topics such as data splitting and feature engineering. These activities occur before the actual training of the machine learning model but are critically important.\nOptimization: To find the model that has the best performance, we often need to tune them so that they are effective but do not overfit. These chapters describe overfitting, methods to measure performance, and two different classes of optimization methods.\nClassification: Various models for classification are described in this part.\nRegression: These chapter describe models for numeric outcomes.\nCharacterization: Once you have a model, how do you describe it or its predictions? How do you know when you should question the results of your model? We’ll address these questions in this part.\nFinalization: Post-modeling activities such as monitoring performance are discussed.\n\nBefore diving into the process of creating a good model, let’s have a short case study. The next chapter is designed to give you a sense of the overall process and illustrate important ideas and concepts that will appear later in the materials.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#chapter-references",
    "href": "chapters/introduction.html#chapter-references",
    "title": "1  Introduction",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nBishop, C. 1995. Neural Networks for Pattern Recognition. Oxford: Oxford University Press.\n\n\nBorisov, V, T Leemann, K Seßler, J Haug, M Pawelczyk, and G Kasneci. 2022. “Deep Neural Networks and Tabular Data: A Survey.” IEEE Transactions on Neural Networks and Learning Systems.\n\n\nBoykis, V. 2023. “What are embeddings?” https://doi.org/10.5281/zenodo.8015029.\n\n\nEngel, T, and J Gasteiger. 2018. Chemoinformatics : A Textbook . Weinheim: Wiley-VCH.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT press.\n\n\nHolmes, S, and W Huber. 2018. Modern Statistics for Modern Biology. Cambridge University Press.\n\n\nHvitfeldt, E, and J Silge. 2021. Supervised Machine Learning for Text Analysis in R. CRC Press.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nLeach, A, and V Gillet. 2003. An Introduction to Chemoinformatics. Springer.\n\n\nMcElfresh, D, S Khandagale, J Valverde, V Prasad, G Ramakrishnan, M Goldblum, and C White. 2023. “When Do Neural Nets Outperform Boosted Trees on Tabular Data?” arXiv.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.\n\n\nSanderson, B. 2017. Oathbringer. Tor Books.\n\n\nSimonyan, K, and A Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv.\n\n\nYu, W, HK Lee, S Hariharan, WY Bu, and S Ahmed. 2007. “CCDB:6843, Mus Musculus, Neuroblastoma.”",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Non-tabular data are later in this chapter.↩︎\nThis is a linear regression model.↩︎\nThis is a smaller version of a larger image that is 1392 pixels by 1040 pixels.↩︎\nThe original image would produce 2,895,360 flattened features.↩︎\nAlso known as “discrete” or “nominal” data.↩︎\nA note to readers: these ?sec-* references are to sections or chapters that are planned by not (entirely) written yet. They will resolve to actual references as we add more content.↩︎\nTo be honest, the literature in this area has been fairly poor (as of this writing).↩︎\nThat said, simple neural network models can be very effective. While still a bit more high maintenance than other models, it’s a good idea to give them a try (as we will see in the following chapter).↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html",
    "href": "chapters/whole-game.html",
    "title": "2  The Whole Game",
    "section": "",
    "text": "2.1 Predicting Delivery Time\nMany resources for teaching machine learning gloss over some crucial details that might appear to be tangential to the model. As a result, they omit parts that are critical to the overall project.\nThe notion of “the whole game” is to illustrate machine learning in a way that reflects the complexity of the entire process. Perkins (2010) uses this phrase in the context of learning baseball. Instead of focusing players on one or two key aspects of the game, it is better to be exposed (at some level) to everything. In his words, a smaller focus “was kind of like batting practice without knowing the whole game.”\nThis chapter is a self-contained case study.\nOur goal is to provide a somewhat abbreviated, high-level overview to help readers understand the overall strategy before we get into the specific set of tactics that we might use. This chapter previews what is to come and sets the stage to discuss some important themes that may not be obvious at first glance.\nWe’ll avoid many details in this chapter by copiously referencing upcoming chapters. Also, in this an upcoming chapters, there are icons (e.g., ) that link to seperate pages that illustrate how to perforrm the calculations.\nThe illustrative example focuses on predicting the food delivery time (i.e., the time from the initial order to receiving the food). There is a collection of data containing 10,012 orders from a specific restaurant. The predictors, previously shown in Table 1.1, include:\nThe outcome is the time1 (in minutes).\nThe next section describes the process of splitting the overall data pool into different subsets and signifies the start of the modeling process. However, how we make that split depends on some aspects of the data. Specifically, it is a good idea to examine the distribution of our outcome data (i.e., delivery time) to determine how to randomly split the data2.\nFigure 2.1 (panel (a)) uses a histogram to examine the distribution of the outcome data. It is right-skewed and there is a subset of long delivery times that stand out from the mainstream of the data. The most likely delivery time is about 27 minutes, but there is also a hint of a second mode a few minutes earlier.\nFigure 2.1: Histrograms of the training set outcome data with and without a transformation.\nThis skewness might affect how we split the data and is discussed in the next section. Another important question is: should we model a transformed version of the delivery times? If we have large outlying values, the model might be inappropriately pulled to these values, often at the expense of the overall quality of fit. One approach is to model the logarithm of the times (as shown in Equation 1.1). Figure 2.1 (panel (b)) shows the distribution of the log (base 2) deliveries. It is more symmetric, and the potential second mode is more pronounced. This isn’t a bad idea, and the primary consequence is how we interpret some of the metrics that are used to quantify how well the model fits the data3. In the end, we decided to model the data in the original units but did try the analysis both ways (and did not find huge differences in the results).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-delivery-times",
    "href": "chapters/whole-game.html#sec-delivery-times",
    "title": "2  The Whole Game",
    "section": "",
    "text": "The time, in decimal hours, of the order.\nThe day of the week for the order.\nThe approximate distance between the restaurant and the delivery location.\nA set of 27 predictors that count the number of distinct menu items in the order.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-data-spending-whole-game",
    "href": "chapters/whole-game.html#sec-data-spending-whole-game",
    "title": "2  The Whole Game",
    "section": "2.2 Data Spending",
    "text": "2.2 Data Spending\nAn important aspect of machine learning is to use the right data at the right time. We don’t start modeling with the entire pool of data. As will be seen in Chapter 10, we need to reserve some data to evaluate the model and make sure that those data do not overlap with those used to fit the model.\nOften, our data spending strategy is to split the data into at least two subsets:\n\nThe training set is used to develop the model pipeline. It is used for investigating, fitting, and comparing them.\nThe test set is a smaller fraction of the data that is only used at the very end to independently validate how well we did.\n\nThe training set is used for a lot of our activities. We do need to evaluate multiple model pipelines along the way. We reserve the test set for the final assessment and should not use it for development. There are two main strategies for characterizing model efficacy during development.\nThe first is to resample the training set. This is an iterative computational tool to find good statistical estimates of model performance using the training set.\nThe other option is to initially split the data into three subsets: the training set, the testing set, and the validation set. Validation sets are small subset used for evaluating the model repeatedly during development.\nGenerally, we suggest using resampling unless the initial data pool is “sufficiently large.” For these data, 10,012 food orders is probably sufficient to choose a validation set over the resampling approach.\nTo split the data, we’ll allocate 60% for the training set and then use 20% for the validation set, and the remaining 20% for testing. The splitting will be conducted by stratification. To ensure that our delivery time distributions are approximately the same, it is temporarily “binned” into four quartiles. The three-way split is executed within each quartiles and the overall training/validation/testing sets are assembled by aggregating the corresponding data from the quartiles. As a result, the respective sample sizes are 6004/2004/2004 deliveries.\nAs previously stated, the majority of our time is spent with the training set samples. The first step in any model-building process is to understand the data, which can most easily be done through visualizations of the training set.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-eda-whole-game",
    "href": "chapters/whole-game.html#sec-eda-whole-game",
    "title": "2  The Whole Game",
    "section": "2.3 Exploratory Data Analysis",
    "text": "2.3 Exploratory Data Analysis\nOur goal now is to determine if there are aspects of predictors that would affect how we provide them to the model. We should look at individual variables as well as combinations multiple variables.\nTo start, for the dense numeric predictors, how would we characterize their relationship to the outcome? Is it linear, nonlinear, or random? Let’s start with the distance predictor. Figure 2.2(a) shows the values with the “scatter plot smoother” fit. The line is a flexible nonlinear approach that can adapt to any apparent pattern in the plot4. First, the plot shows that the distance distribution is also right-skewed (although a separate histogram would better illustrate this). The smoother shows a slightly nonlinear trend, although the nonlinearity is mainly in the right tail of the distances and might be an artifact of the small sample size in that region. Regardless, it appears to be an informative predictor.\n\n\n\n\n\n\n\n\nFigure 2.2: Visualizations of relationship between the outcome and several predictors using the training set.\n\n\n\n\n\nPanel (b) has a similar visualization for the order time. In this case, there is another nonlinear trend where the order increases with the hour, then peaks around 17:30, then begins to decrease. There is also an increase in delivery time variation as the order time approaches the peak.\nThe day of the week is visualized via box plots in panel (c). There is an increasing trend in both mean and variation as we move from Monday to Friday/Saturday, then a decrease on Sunday. Again, this appears to be an important predictor.\nThese three panels show relationships between individual predictors and delivery times. It is possible that multiple predictors can act in concert; their effect on the outcome occurs jointly. Panel (d) of Figure 2.2 shows that there is an interaction effect between the day and hour of the order. Mondays and Tuesdays have shorter delivery times that don’t change much over hours. Conversely, on Fridays and Saturdays, the delivery times are generally longer, with much higher peaks. This might explain the increase in delivery time variation that was seen in Panel (b). Now that we know of the existence of this interaction, we might add additional model terms to help the model understand and exploit this pattern in the training set.\nFor the 27 itemized frequency counts, we might want to know if there was a change in delivery time when a specific item was in the order (relative to those where it is not purchased). To do this, mean delivery times were computed for orders with and without the item. The ratio of these times was computed and then converted to the percent increase in time. This is used to quantify the effect of item on delivery time. We’d also like to know what the variation is on this statistic and a technique called the bootstrap (Davison and Hinkley 1997) was used to create 90% confidence intervals for the ratio. Figure 2.3 shows the results.\n\n\n\n\n\n\n\n\nFigure 2.3: Ranking the items by the increase in delivery time when ordered at least once.\n\n\n\n\n\nThere is a subset of items that increase the delivery time (relative to the experimental noise), many that seem irrelevant, and one that decreases the time.\nMore investigations into the data would be pursued to better understand the data and help us with feature engineering. For brevity, we’ll stop here and introduce three ML models.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-development-whole-game",
    "href": "chapters/whole-game.html#sec-model-development-whole-game",
    "title": "2  The Whole Game",
    "section": "2.4 Model Development",
    "text": "2.4 Model Development\nWhich model to use? There is a multitude of tools that can be used to represent and predict these data. We’ll illustrate three in this section and the essential concepts to consider along the way.\nBefore proceeding, we need to pick a performance metric. This is used to quantify how well the model fits the data. For any model fit, the residual (usually denoted as \\(\\epsilon\\), also called the error) is the difference between the model prediction and the observed outcome. We’d like our model to produce residuals near zero. One metric5 is the mean absolute error (MAE). For a data set with \\(n\\) data points, the equation is\n\\[\nMAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| = \\frac{1}{n}\\sum_{i=1}^n |\\epsilon_i|\n\\tag{2.1}\\]\nwhere \\(\\hat{y}\\) is the predicted numeric outcome value and \\(\\epsilon\\) is the model error. The MAE units are the same as the outcome which, in this case, is minutes. We desire to minimize this statistic.\nTo get started, we’ll use a basic linear model for these data.\n\n\nLinear Regression\nLinear regression is probably the most well-known statistical model in history. It can predict a numeric outcome using one or more predictors using the equation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\ldots + \\beta_px_{ip} + \\epsilon_i\n\\tag{2.2}\\]\nwhere \\(y_i\\) is the outcome for the \\(i^{th}\\) data point (out of \\(n_{tr}\\)), and \\(x_{ij}\\) is value for the \\(j^{th}\\) predictor (out of \\(p\\)).\nFor Equation 2.2, there are many ways to estimate the model parameters (\\(\\beta\\)’s). Ordinary least squares is used most often. It finds the values of the \\(\\beta_j\\) that that minimize sum of squared errors (SSE). ?sec-reg-linear discusses several other ways to estimate the model parameters that use other criteria (variations of the SSE).\nThe main assumptions about the data for ordinary least squares are that they are independent and identically distributed. Given what we know about the problem, both assumptions are at least mildly incorrect. Orders are being processed simultaneously (affecting independence), and our visualizations have indicated that there is an increase in variation with the mean (i.e., not identically distributed). However, we are most interested in prediction here, so we can often overlook slight to moderate violations of these assumptions6\nWhat are our \\(x_{ij}\\) values in Equation 2.2? We can’t give the model equations values of \"Monday\" or \"Friday\"; OLS requires numbers in its equations. A simple workaround is to create binary indicator variables for six of the seven days of the week. For example, the indicator for Friday would have a value of one for orders placed on a Friday and zero otherwise7.\nAlso, for Equation 2.2, what do we do about the numeric predictors that appear to have nonlinear trends? We can augment the predictor by describing it with multiple columns. Equation 2.2 shows that linear regression is linear in the parameters (not the predictors). We can add terms such as \\(x_{ij} = distance_i^2\\) to allow the fit to be nonlinear. Alternatively, an effective tool for enabling nonlinear trends is spline functions8. These are special polynomial terms. For example, we could represent that original predictor with 10 spline terms (i.e., columns in the data) for the order hour. As we add more terms, the model fit has increased capacity to be nonlinear. We’ll use 10 separate splines for both distance and the order hour (in lieu of the original data columns.)\nHow do we encode the interaction seen in Figure 2.2(d)? We have six indicator columns for the order day and ten spline columns for the order hour. We can facilitate having different nonlinear hour trends for each day by making 60 cross-product terms. For example, the interactions for Fridays would multiply each of the ten spline terms by the binary indicator associated with Fridays.\nThese feature engineering operations result in a model with \\(p = 114\\) \\(\\beta\\) parameters. We fit this model to the training set to estimate the model parameters. To evaluate it, we use the trained model to predict the validation set, and these predictions are the substrate to compute the MAE. For this model, the validation set MAE is 1.609 (i.e., 1 minute and 36 seconds). If we think of MAE as an average error, this doesn’t seem too bad. However, what does this look like? Let’s plot the observed times against predicted values to get a better sense of how well this model did. Figure 2.4 has the results which also feature a scatter plot smoother.\n\n\n\n\n\n\n\n\nFigure 2.4: A scatter plot of the observed and predicted delivery times, via a linear regression model, for the validation set. The red line is a smooth trend estimate.\n\n\n\n\n\nThe best case would be that the points arrange themselves tightly around the diagonal line. The model slightly under-predicts for very rapid deliveries but substantially under-predicts for times greater than 40 minutes. However, overall, the model works well for the majority of the deliveries.\nFrom here, our next step would be to investigate poorly predicted samples and determine if there is some commonality to them. For example, are they short distances that are ordered late on a Friday, etc? If there appears to be a pattern, we would add additional model terms to compensate for the flaw and see if the validation set MAE decreases. We would iterate between exploratory analysis of the residuals, adding or subtracting features, then refitting the model.\nFor illustration, we stop here9 and take a look at very different model.\n\n\n\nRule-Based Ensemble\nCubist is a rule-based ensemble method. It creates a rule set by first generating a regression tree that is composed of a hierarchical set of if/then statements10. Figure 2.5 shows a simple tree for our data. A set of splits on different predictors results in six terminal nodes (at the bottom of the tree). A rule is a distinct path through the tree to a terminal node. For example, Node 11 in the figure has the corresponding rule: hour &gt;= 14.77 & day in {Fri, Sat} & distance &gt;= 4.045. For each rule, Cubist fits a linear regression to the data that the rule covers (\\(n = 261\\) for Node 11). It builds new rule sets sequentially and uses many rules/regression equations to predict new samples.\n\n\n\n\n\n\n\n\nFigure 2.5: An example of a regression tree used to create a set of six rules.\n\n\n\n\n\nThe method isolates different parts of the predictor space and, with these subsets, models the data using linear functions. The creation of multiple rules enables the model to emulate nonlinear functions easily. When a rule consists of multiple predictors, this constitutes an interaction effect. Additionally, the data preprocessing requirements are minimal: it can internally account for any missing predictor values, and there is no need to convert categorical predictors into binary indicator columns, and so on.\nWhen fit to the training set, the Cubist model created an ensemble with 2,007 rules. On average, the rules consisted of 2.7 variables and the the regression models contained 12.5 predictors. Two example rules, and their regression models, were:\n\nif\n   hour &lt;= 14.252\n   day in {Fri, Sat}\nthen\n   outcome = -23.039 + 2.85 hour + 1.25 distance + 0.4 item_24\n             + 0.4 item_08 + 0.6 item_01 + 0.6 item_10 + 0.5 item_21\n             + 0.3 item_09 + 0.4 item_23 + 0.2 item_03 + 0.2 item_06\n\nand\n\nif\n   hour &gt; 15.828\n   hour &lt;= 18.395\n   distance &lt;= 4.35\n   item_10 &gt; 0\n   day = Thu\nthen\n   outcome = 11.29956 + 4.24 distance + 14.3 item_01 + 4.3 item_10\n             + 2.1 item_02 + 0.03 hour\n\nThese two examples are reasonable and understandable. However, Cubist blends many rules together to the points where is becomes a black-box model. For example, the first data point in the validation set is affected by 115 rules (about 6% of the total ensemble).\nDoes this complexity help? The validation set estimated MAE at 1 minute and 24 seconds, which is an improvement over the linear regression model. Figure 2.6 visualizes the observed and predicted plot. The model under-predicts fewer points with long delivery times than the previous model but does contain a few very large residuals.\n\n\n\n\n\n\n\n\nFigure 2.6: A scatter plot of the observed and predicted delivery times from the Cubist model, for the validation set.\n\n\n\n\n\nWould this model have benefited from using the feature set from the linear regression analysis? Not really. Refitting with those predictors had an MAE of 1 minute and 26 seconds. In this particular case, the original Cubist was able to estimate nonlinear trends and interactions without additional feature engineering.\nThere is more that we can do with the Cubist model but, in this initial model screening phase, we should focus our efforts on evaluating a disparate set of models and features to understand what works, what doesn’t work, and the difficulty level of the problem. Let’s try one additional model.\n\n\n\nNeural Network\nA neural network is a highly nonlinear modeling technique. They relate the predictors to the outcome through intermediate substructures, called layers, that contain hidden units. The hidden units allow the predictors to affect the outcomes differently11 allowing the model to isolate essential patterns in the data. As the number of hidden units (or layers of hidden units) increases, so does the complexity of the model. We’ll only consider a single layer network with multiple hidden units for these data.\nNeural networks, like linear regression, require numbers as inputs. We’ll convert the day of the week predictor to indicators. Additionally, the model requires that all of the predictors be in the same units. There are a few ways to accomplish this. First, we will compute the training set mean and standard deviation of each feature. To standardize them we subtract the mean and divide the result by the standard deviation. As a result, all features have a zero mean and a unit standard deviation. A common scaling of the predictor columns is a necessary precondition for fitting neural networks\nOne question about this model is: how many hidden units should we use12? This decision will determine the complexity of the model. As models become model complex, they have more capacity to find complicated relationships between the predictors and response. However, added complexity increases the risk of overfitting. This situation, discussed in Section 9.1, occurs when a model fits exceptionally well to the training data but fails for new data when it finds patterns in the training data that are artifacts (i.e., do not generalize for other data sets). How should we choose the number of hidden units?\nUnfortunately, no equation can directly compute an estimate the number of units from the data. Values such as these are referred to as tuning parameters or hyperparameters. However, one solution is to extend the validation process previously shown for the linear regression and Cubist models. We will consider a candidate set of tuning parameter values ranging from 3 to 100 hidden units. We’ll fit a model to the training set for each of these 98 values and use the validation set MAE values to select an appropriate value. The results of this process is shown in Figure 2.7.\n\n\n\n\n\n\n\n\nFigure 2.7: The relationship between the number of hidden units and the mean absolute deviation from the validation set data.\n\n\n\n\n\nA small number of hidden units performs poorly due to underfitting (i.e., insufficient complexity). Adding more improves the situation and, while the numerically best result corresponds to a value of 75, there is appreciable noise in the MAE values13 . Eventually, adding too much complexity will either result in the MAE values becoming larger due to overfitting or will drastically increase the time to fit each model. The MAE pattern in Figure 2.7 shows a plateau of values with a similar MAE once enought hidden units are added. Given the noise in the system, we’ll select a value of 24 hidden units, by determining the least complex model that is within 1% of the numerically best MAE. The MAE associated with the selected candidate value14 was 1 minute and 28 seconds (but is likely to be closer to 1 minute and 30 seconds, given the noise in Figure 2.7).\nFigure 2.8 shows the familiar plot of the observed and predicted delivery times. There are fewer large residuals than the Cubist model but the underfitting for longer delivery times appears more pronounced. There is also a hint of under-predicted times for fast deliveries.\n\n\n\n\n\n\n\n\nFigure 2.8: A scatter plot of the validation set observed and predicted delivery times from a neural network with 24 hidden units.\n\n\n\n\n\nThese results suggest that the Cubist and neural network models have roughly equivocal performance15. Like Cubist, the finalized model is very complex; it is a highly nonlinear regression with 31,115 model parameters (i.e., slopes and intercepts).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-model-selection-whole-game",
    "href": "chapters/whole-game.html#sec-model-selection-whole-game",
    "title": "2  The Whole Game",
    "section": "2.5 Choosing Between Model",
    "text": "2.5 Choosing Between Model\nThere are a variety of criteria used to rank and compare model fits. Obviously, performance is important. Also, it is good practice to choose a model that is as simplistic as possible (subject to having acceptable performance). There are other practical factors too. If a model is to be deployed, the size of the files(s) required to automate predictions can be a constraint. There may also be data constraints; which predictors are easy or inexpensive to obtain? Does the number of predictors affect how quickly predictions can be made?\nWhile prediction is the focus for ML models, we often have to explain the model and/or predicted values to consumers. While we can develop explainers for any model, some types are easier to explain than others.\nFor this chapter, we have three models whose MAE values are within about 12 seconds of one another. Each has the same deficiency: they all significantly underpredict long delivery times. In fact, a few poor predictions may be a limiting factor for any regression metric; it might be impossible for one model’s MAE to really stand out from the others.\nWe’ll select the linear regression model as our final model due to its relative simplicity and linear structure.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-calibration-whole-game",
    "href": "chapters/whole-game.html#sec-calibration-whole-game",
    "title": "2  The Whole Game",
    "section": "2.6 Calibration",
    "text": "2.6 Calibration\nThere is one option for remedying the systematic underprediction of long delivery times. ?sec-reg-calibration describes calibration methods that might be able to correct such issues. To do so, we use the held out predictions to estimate the average unwanted trend and remove it from new predictions. For the linear regression results, the estimated average trend is almost identical to the smooth line shown in Figure 2.4.\nFor our test set predictions, we’ll remove this trend and, hopefully, this will recenter the long predictions to be closer to the diagonal line.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-test-results-whole-game",
    "href": "chapters/whole-game.html#sec-test-results-whole-game",
    "title": "2  The Whole Game",
    "section": "2.7 Test Set Results",
    "text": "2.7 Test Set Results\nWe can predict the test set using the linear regression fit from the training set. Recall that the validation set MAE was 1 minute and 36 seconds. Coincidentally, without the additional calibration, the test set value was also 1 minute and 36 seconds. The similarity of these results gives us confidence that the performance statistics we estimated during development are consistent with the test set. The test set plot of the predictions is shown in Figure 2.9(a).\nThe calibration results are shown in panel (b). There does appear to be an improvement in long delivery times; these are closer to the diagonal line. The MAE value is slightly improved: 1 minute and 32 seconds. The small difference in these results and the similar MAE values across models indicates that the few large residuals are probably the gating factor for these data.\n\n\n\n\n\n\n\n\nFigure 2.9: Observed versus predicted test set delivery times for the final model. Results are shown for pre-calibrated and post-calibration.\n\n\n\n\n\nAt this point, we should spend a fair amount of time documenting the model and our development process. It might be helpful to employ inferential analyses to explain to interested parties which model components were most important (e.g., how much the interaction or nonlinear terms improve the model, etc.). It is very important to document where the model does not work well, its intended use, and the boundaries of the training set. Finally, if the model is deployed, we should also monitor its effectiveness over time and also understand if the sample population is changing over time too.\nThe next section accentuates important aspects of this case study.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-themes-whole-game",
    "href": "chapters/whole-game.html#sec-themes-whole-game",
    "title": "2  The Whole Game",
    "section": "2.8 Themes",
    "text": "2.8 Themes\nAt a high level, let’s review the process that we just used to create a model; Figure 2.10 highlights several themes16. The overall process involves initial data splitting, model development, then model selection, and validation. We’ll look at each of these in turn to discuss important points.\n\n\n\n\n\n\n\n\nFigure 2.10: A general high level view of the process of creating a predictive model. The diagram reflects which steps use the test set, validation set, or the entire training set. Figure 2.11 describes the operations labeled “model development”.\n\n\n\n\n\n\nData Spending\nAlthough discussed in Chapter 3, how we allocate data to specific tasks (e.g., model building, evaluating performance, etc.) is an essential aspect of modeling. In our analysis, a minority of the data were explicitly allocated to measure how well the model worked. The test set is meant to be unbiased since it was never used to develop the model. This philosophy of empirical validation is a crucial feature for judging how models function.\nThe majority of the data ends up in the training set. The core activities of creating, optimizing, and selecting are accomplished with these data.\nFigure 2.10 highlights how we use the right data at the right time. A lack of a data usage methodology (or an inappropriate one) can lead to having a false sense of the quality of the model.\nFor example, when the linear regression model was calibrated, we chose to use the held-out samples as the substrate for estimating the unwanted trend. We could have used the re-predicted training set data but, as will be discussed in Section 9.1, these values can be unrealistically optimistic. Also, we chose not to compute the MAE on the held-out predictions after they were used to estimate the calibration. This dual use of the same data would result in an MAE value that is smaller than it should be. There are other ways of estimating the utility of the calibration prior to the test set; these are discussed in ?sec-reg-calibration.\n\n\nInvestigate Many Options\nFor the delivery data, three different models were evaluated. It is our experience that some modeling practitioners have a favorite model that is relied on indiscriminately (boosted trees immediately come to mind). The “No Free Lunch” Theorem (Wolpert 1996) argues that, without substantive information about the modeling problem and data, no single model will always do better than any other model. Because of this, a strong case can be made to try various techniques and then determine which to focus on. In our example, simple data visualizations elucidated the relationships between the outcome and the predictors. Given this knowledge, we might exclude some models from consideration, but there is still a wide variety of techniques to evaluate.\nFor individual models, Figure 2.11 summarizes the cyclic process of speculation, configuration, and validation of model parameters and features. For these data, we discovered most of the important trends prior to our first model fit. As previously mentioned, we would cycle through a few rounds where we try to make improvments for each of the models.\n\n\n\n\n\n\n\n\nFigure 2.11: The cycle of developing a specific type of model. EDA standard for exploratory data analysis.\n\n\n\n\n\nFor novel data sets, it is a challenge to know a priori what good choices are for tuning parameters, feature engineering methods, and predictors. The modeler must iterate using choices informed by their experience and knowledge of the problem.\n\n\nPredictor Representations\nThis example revolves around a small set of predictors. In most situations, there will be numerous predictors of different types (e.g., quantitative, qualitative, etc.). It is unclear which predictors should be included in the model for new modeling projects and how they should be represented.\nWhen considering how to approach a modeling project, the stereotype is that the user focuses on which new, fancy model to apply to the data. The reality is that many of the most effective decisions that users must confront are which predictors to use and in what format. Thoughtfully adding the right feature representations often obviates the need (and overhead and risks) for complex models. That is one of the lessons from this particular data set.\nWe’ll put the feature engineering process on equal footing with model training For this reason, Figure 2.11 shows that the choice of model features and parameters are both important.\nWhile Cubist and the neural network were able to internally determine the more elaborate patterns in the data, our discovery of these same motifs has a higher value. We learned something about the nature of the data and can relate them to the model (via feature engineering) and the end-users17. Later, in Section 8.1, we’ll see how a black-box model can be used to suggests additional interactions to add to our linear regression model. This has the effect of eliminating some of the large residuals seen in Figure 2.4.\n\n\nModel Selection\nAt some point in the process, a specific model pipeline must be chosen. This chapter demonstrated two types of selection. First, we chose some models over others: the linear regression model did as well as more complex models and was the final selection. In this case, we decided between model pipelines. There was also a second type of model selection shown. The number of hidden units for the neural network was determined by trying different values and assessing the results. This was also model selection, where we decided on the configuration of a neural network (as defined by the number of hidden units). In this case, we selected within different neural networks.\nIn either case, we relied on data separate from the training set to produce quantitative efficacy assessments to help choose. This book aims to help the user gain intuition regarding the strengths and weaknesses of different models to make informed decisions.\n\n\nMeasuring Performance\nBefore using the test set, two techniques were used to determine the model’s effectiveness. First, quantitative assessments of statistics (i.e., MAE) from the validation set help the user understand how each technique would perform on new data. The other tool was to create simple visualizations of a model, such as plotting the observed and predicted values, to discover areas of the data where the model does particularly well or poorly. This qualitative information is lost when the model is gauged only using summary statistics and is critical for improving models. There are many more visualization methods for displaying the outcomes, their predictions, and other data. The figures shown here are pretty simplistic.\nNote that almost all of our approaches for model evaluation are data-driven in a way where we determine if the predicted values are “close” to the actual values. This focus on empirical validation is critical in proving that a model pipeline is fit for purpose. Our general mantra is\n\nAlways have a separate piece of data that can contradict what you believe.\n\nFor models that require specific probabilistic assumptions, it is a good idea to check these too, primarily if a model will also be used for inference. However, before considering questions regarding theoretical assumptions, empirical validation should be used to ensure that the model can explain the data well enough to be worth our inferences. For example, if our validation set MAE was around 15 minutes, the model predictions would not show much fidelity to the actual times. Any inferences generated from such a model might be suspect.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#sec-summary-whole-game",
    "href": "chapters/whole-game.html#sec-summary-whole-game",
    "title": "2  The Whole Game",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nAt face value, model building appears straightforward: pick a modeling technique, plug in the data, and generate a prediction. While this approach will yield a predictive model, it unlikely to generate a reliable, trustworthy model. To achieve this we must first understand the data and the objective of the modeling. We finally proceed to building, evaluating, optimizing, and selecting models after these steps.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#chapter-references",
    "href": "chapters/whole-game.html#chapter-references",
    "title": "2  The Whole Game",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nPerkins, D. 2010. Making Learning Whole. Wiley.\n\n\nWolpert, D. 1996. “The Lack of a Priori Distinctions Between Learning Algorithms.” Neural Computation 8 (7): 1341–90.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/whole-game.html#footnotes",
    "href": "chapters/whole-game.html#footnotes",
    "title": "2  The Whole Game",
    "section": "",
    "text": "For event time data, it is possible to get incomplete data due to censoring. Suppose that, after 10 minutes you don’t have your order yet. You don’t know the exact delivery time but you do know that it is at least 10 minutes. There are specialized tools for analyzing and modeling censored data. Thankfully, all of our data are complete since we know the exact time of delivery.↩︎\nOverall, we advise looking at the other columns in the data after the splitting.↩︎\nThese are described more in ?sec-outcome-transformation.↩︎\nThe two strategies for creating flexible, smooth fits are splines and generalized additive models. These are described in detail in Sections 8.3 and ?sec-cls-gam, respectively. If you have used the ggplot2 function geom_smooth() function, you’ve used a smoother. ↩︎\nDiscussed in ?sec-reg-metrics↩︎\nIf we want to make inferences on the parameters, such as p-values and confidence intervals, we we’ll need to make more specific assumptions (such as normality of the errors).↩︎\nDifferent ways to convert categorical predictors to numeric predictors are detailed in Chapter 6.↩︎\nSection 8.3↩︎\nWe will re-examine the linear regression model for these data in Section 8.1.2, where we discover (spoilers!) an important term left out of this analysis.↩︎\nThe details for regression trees and Cubist are in ?sec-reg-trees↩︎\nSee Sections ?sec-cls-nnet and ?sec-reg-nnet for discussions and details.↩︎\nThis model description is deliberately vague (so as not to confuse readers who are new to this model). Other details: each model could be trained for up to 5,000 epochs. However, a small amount of the training set was randomly set aside to monitor the sums of squared errors at each epoch. Early stopping was triggered when this out-of-sample loss degraded for 10 consecutive epochs. A rectified linear unit activation function was used between the predictor and hidden layers. A cyclic learning rate scheduler was used with a maximum rate of 0.1 and a weight decay value of 0.01 was applied. In a more complete analysis, all of these values would also be tuned.↩︎\nThe noisy profile suggests that our validation set might not be large enough to differentiate between subtle differences within or between models. If the problem required more precision in the performance statistics, a more thorough resampling method would be appropriate, such as cross-validation. These are described in Chapter Chapter 10.↩︎\nTo some degree, this is an artificially optimistic estimate since we are using the same data to pick the best result and estimate the model’s overall performance. This is called optimization bias and it is discussed more in Section 11.4. For the characteristics of these data, the optimism is likely to be small, especially compared to the noise in the MAE results.↩︎\nWe will revisit this statement in ?sec-comparing-boot, where methods for estimating confidence intervals for validation and test results are presented.↩︎\nWhen resampling is used in place of a validation set, Chapter 10 has a similar diagram that reflects the subtle differences in data usage.↩︎\nIn fact, the day-to-day users of the data are the best resources for feature engineering.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html",
    "href": "chapters/initial-data-splitting.html",
    "title": "3  Initial Data Splitting",
    "section": "",
    "text": "3.1 The Ames Housing Data\nIn the previous chapter, Figures 2.10 and 2.11 described various operations for the development and evaluation of ML models. We’ve also emphasized that “the right data should be used at the right time.” If the same samples were used for many different purposes, we run the risk of overfitting. Illustrated in Chapter 9, this occurs when the model over-interprets irreproducible patterns in the modeling data that don’t happen in any other data set. As a result, the model performance statistics are likely to be very optimistic and give us a false sense of how well the model works. If the model were evaluated on a separate set of data (that does not have abnormal patterns), performance would look considerably worse. Because of potential overfitting, the modeler must decide how to best utilize their data across different operations.\nThis chapter will examine how we can appropriately utilize our data. Except in Section 3.8, we’ll assume that each data set row is statistically independent of the others. Before proceeding further, we’ll introduce an example data set used in multiple chapters.\nThese data, originally published by De Cock (2011), are an excellent teaching example. Data were collected for 2,930 houses in Ames, Iowa, via the local assessor’s office. A variety of different characteristics of the houses were measured. Chapter 4 of Kuhn and Silge (2022) contains a detailed examination of these data. For illustration, we will focus on a smaller set of predictors, summarized in Tables 3.1 and 3.2. The geographic locations of the properties are shown in Figure 3.2.\nColumn\n      Min\n      Median\n      Max\n      Std. Dev.\n      Skewness\n      Distribution\n    \n  \n  \n    Baths\n     0.0\n      2.0\n      5.0\n     0.64\n 0.3\n\n    Gross Living Area\n   334.0\n  1,442.0\n  5,642.0\n   505.51\n 1.3\n\n    Latitude\n    42.0\n     42.0\n     42.1\n     0.02\n-0.5\n\n    Longitude\n   -93.7\n    -93.6\n    -93.6\n     0.03\n-0.3\n\n    Lot Area\n 1,300.0\n  9,436.5\n215,245.0\n 7,880.02\n12.8\n\n    Sale Price\n12,789.0\n160,000.0\n755,000.0\n79,886.69\n 1.7\n\n    Year Built\n 1,872.0\n  1,973.0\n  2,010.0\n    30.25\n-0.6\n\n    Year Sold\n 2,006.0\n  2,008.0\n  2,010.0\n     1.32\n 0.1\n\n  \n  \n  \n\n\n\n\n\nTable 3.1: A summary of numeric predictors in the Ames housing data.\nColumn\n      # Values\n      Most Frequent (n)\n      Least Frequent (n)\n      Distribution\n    \n  \n  \n    Building Type\n5\nSingle-Family Detached (2425)\nTwo-Family Conversion (62)\n\n    Central Air\n2\nYes (2734)\nNo (196)\n\n    Neighborhood\n28\nNorth Ames (443)\nLandmark (1)\n\n  \n  \n  \n\n\n\n\n\nTable 3.2: A summary of categorical predictors in the Ames housing data.\nAs shown in Table 3.1, the sale price distribution is fairly right-skewed. For this reason, and because we do not want to be able to predict negative prices, the outcome is analyzed on the log (base-10) scale.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-train-test",
    "href": "chapters/initial-data-splitting.html#sec-train-test",
    "title": "3  Initial Data Splitting",
    "section": "3.2 Training and Testing Sets",
    "text": "3.2 Training and Testing Sets\nOne of the first decisions is to decide which samples will be used to evaluate performance. We should evaluate the model with samples that were not used to build or fine-tune it. An “external sample” will help us obtain an unbiased sense of model effectiveness. A selection of samples can be set aside to evaluate the final model. The training data set is the general term for the samples used to create the model. The remaining samples, or a subset of them, are placed in the testing data set. The testing data set is exclusively used to quantify how well the model works on an independent set of data. It should only be accessed once to validate the final model candidate.\nHow much data should be allocated to the training and testing sets? This depends on several characteristics, such as the total number of samples, the distribution of the response, and the type of model to be built. For example, suppose the outcome is binary and one class has far fewer samples than the other. In that case, the number of samples selected for training will depend on the number of samples in the minority class. Finally, the more tuning parameters required for a model, the larger the training set sample size will need to be. In general, a decent rule of thumb is that 75% could be used from training.\nWhen the initial data pool is small, a strong case can be made that a test set should be avoided because every sample may be needed for model building. Additionally, the size of the test set may not have sufficient power or precision to make reasonable judgments. Several researchers (J. Martin and Hirschberg 1996; Hawkins, Basak, and Mills 2003; Molinaro 2005) show that validation using a single test set can be a poor choice. Hawkins, Basak, and Mills (2003) concisely summarizes this point:\n\n“hold-out samples of tolerable size […] do not match the cross-validation itself for reliability in assessing model fit and are hard to motivate”.\n\nResampling methods (Chapter 10), such as cross-validation, are an effective tool that indicates if overfitting is occurring. Although resampling techniques can be misapplied, such as the example shown in Ambroise and McLachlan (2002), they often produce performance estimates superior to a single test set because they evaluate many alternate versions of the data.\n\nOverfitting is the greatest danger in predictive modeling. It can occur subtly and silently. You cannot be too paranoid about overfitting.\n\nFor this reason, it is crucial to have a systematic plan for using the data during modeling and ensure that everyone sticks to the program. This can be particularly important in cases where the modeling efforts are collaborations between multiple people or institutions. We have had experiences where a well-meaning person included the test set during model training and showed stakeholders artificially good results. For these situations, it might be a good idea to have a third party split the data and blind the outcomes of the test set. In this way, we minimize the possibility of accidentally using the test set (or people peeking at the test set results).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-leakage",
    "href": "chapters/initial-data-splitting.html#sec-leakage",
    "title": "3  Initial Data Splitting",
    "section": "3.3 Information Leakage",
    "text": "3.3 Information Leakage\nInformation leakage (a.k.a data leakage) is another aspect of data handling to consider at the onset of a modeling project. This occurs when the model has access to data that it should not. For example,\n\nUsing the distribution of the predictor data in the test set (or other future data) to inform the model.\nIncluding identical or statistically related data in training and test sets.\nExploiting inadvertent features that are situationally confounded with the outcome.\n\nAn example of the last item we experienced may be familiar to some readers. A laboratory was producing experimental results to evaluate the difference between two treatments for a particular disorder. The laboratory was under time constraints due to an impending move to another building. They prioritized samples corresponding to the new treatment since these were more interesting. Once finished, they moved to their new home and processed the samples from the standard treatment.\nOnce the data were examined, there was an enormous difference between the two treatment sets. Fortuitously, one sample was processed twice: before and after they moved. The two replicate data points for this biological sample also showed a large difference. This means that the signal seen in the data was potentially driven by the changes incurred by the laboratory move and not due to the treatment type.\nThis type of issue can frequently occur. See, for example, Baggerly, Morris, and Coombes (2004), Kaufman et al. (2012), or Kapoor and Narayanan (2023).\nAnother example occurs in the Ames housing data set. These data were produced by the local assessor’s office, whose job is to appraise the house and estimate the property’s value. The data set contains several quality fields for things like the heating system, kitchen, fireplace, garage, and so on. These are subjective results based on the assessor’s experience. These variables are in a qualitative, ordinal format: “poor”, “fair”, “good”, etc. While these variables correlate well with the sale price, they are actually outcomes and not predictors. For this reason, it is inappropriate to use them as independent variables.\nFinally, the test set must emulate the data that will be seen “in the wild”, i.e., in future samples. We have had experiences where the person in charge of the initial data split had a strong interest in putting the “most difficult” samples in the test set. The prevalence of such samples should be consistent with their prevalence in the population that the model is predicting.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "href": "chapters/initial-data-splitting.html#sec-basic-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.4 Simple Data Splitting",
    "text": "3.4 Simple Data Splitting\nWhen splitting the data, it is vital to think about the model’s purpose and how the predictions will be used. The most important issue is whether the model will predict the same population found in the current data collection. For example, for the Ames data, the purpose is to predict new houses in the town. This definition implies a measure of interpolation since we are primarily concerned with what is happening in Ames. The existing data capture the types of properties that might be seen in the future.\nAs a counter-example, Chapter 4 of Kuhn and Johnson (2019) highlights a prediction problem in which a model is used to predict the future ridership of commuters on the Chicago elevated trains. This data set has daily records of how many commuters ride the train, and temporal factors highly affect the patterns. In this case, the population we will predict is future ridership. Given the heavy influence of time on the outcome, this implies that we will be extrapolating outside the range of existing data.\nIn cases of temporal extrapolation, the most common approach to creating the training and testing set is to keep the most recent data in the test set. In general, it is crucial to have the data used to evaluate the model be as close to the population to be predicted. For times series data, a deterministic split is best for partitioning the data.\nWhen interpolation is the focus, the simplest way to split the data into a training and test set is to take a simple random sample. If we desire the test set to contain 25% of the data, we randomly generate an appropriately sized selection of row numbers to allocate sales to the test set. The remainder is placed in the training set.\nWhat is the appropriate percentage? Like many other problems, this depends on the characteristics of the data (e.g., size) and the modeling context. Our general rule of thumb is that one-fourth of the data can go into testing. The criticality of this choice is driven by how much data are available. The split size is not terribly important if a massive amount of data is available. When data are limited, deciding how much data to withhold from training can be challenging.\nT. Martin et al. (2012) compares different methods of splitting data, including random sampling, dissimilarity sampling, and other methods.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "href": "chapters/initial-data-splitting.html#sec-split-with-outcome",
    "title": "3  Initial Data Splitting",
    "section": "3.5 Using the Outcome",
    "text": "3.5 Using the Outcome\nSimple random sampling does not control for any data attributes, such as the percentage of data in the classes. When one class has a disproportionately small frequency compared to the others (discussed in ?sec-imbalances), the distribution of the outcomes may be substantially different between the training and test sets.\nWhen splitting the data, stratified random sampling (Kohavi 1995) applies random sampling within sub-groups (such as the classes) to account for the outcome. In this way, there is a higher likelihood that the outcome distributions will match. When an outcome is a number, we use a similar strategy; the numeric values are broken into similar groups (e.g., low, medium, and high) and execute the randomization within these groups.\nLet’s use the Ames data to demonstrate stratification. The outcome is the sale price of a house. Figure 3.1(a) shows the distribution of the outcomes with vertical lines that separate 20% partitions of the data. Panel (b) shows that the outcome distributions are nearly identical after partitioning into training and testing sets.\n\n\n\n\n\n\n\n\nFigure 3.1: (a) A density plot of the sale price of houses in Ames with vertical lines that indicate regions that cover 20% of the data. The ‘rug’ on the axis shows the individual data points. (b) Density plots of the training set outcomes (solid red) and test set outcomes (dashed blue) for the Ames data.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "href": "chapters/initial-data-splitting.html#sec-split-with-predictors",
    "title": "3  Initial Data Splitting",
    "section": "3.6 Using the Predictors",
    "text": "3.6 Using the Predictors\nAlternatively, we can split the data based on the predictor values. Willett (1999) and Clark (1997) proposed data splitting based on maximum dissimilarity sampling. The dissimilarity between two samples can be measured in several ways. The simplest method uses the distance between the predictor values for two samples. If the distance is small, the points are nearby. Larger distances between points are indicative of dissimilarity. To use dissimilarity as a tool for data splitting, we should initialize the training set with a single sample. We calculate the dissimilarity between this initial sample and the unallocated samples. The unallocated sample that is most dissimilar is added to the training set. A method is needed to allocate more instances to the training set to determine the dissimilarities between groups of points (i.e., the two in the training set and the unallocated points). One approach is to use the average or minimum of the dissimilarities. For example, to measure the dissimilarities between the two samples in the training set and a single unallocated point, we can determine the two dissimilarities and average them. The third point added to the training is chosen as having the maximum average dissimilarity to the existing set. This process continues until we achieve the targeted training set size.\nFigure 3.2 illustrates this process for the Ames housing data. Starting with a data point near the middle of the town, dissimilarity sampling selected 25 data points using scaled longitude and latitude as predictors. As the sampling proceeds, the algorithm initially chooses samples near the outskirts of the data, especially if they are outliers. Overall, the selected data points cover the space with no redundancy.\n\n\n\n\n\n\n\n\nFigure 3.2: Maximum dissimilarity sampling of 25 points in the Ames data. The small black circles are individual properties. Larger, lighter colors indidicate earlier selection.\n\n\n\n\n\nFor this example, the two predictors used for splitting were numeric. In this case, we typically use simple distance functions to define dissimilarity. Many other functions are possible. The Gower distance (Gower 1971) is a good alternative when a data set has non-numeric predictors. ?sec-cls-knn discusses this metric in more detail.\n\nWhile this analysis nicely illustrates the dissimilarity sampling process, it is flawed since it ignores the issue of spatial autocorrelation. This is the idea that things close to one another act more similarly than objects farther away and will be discussed more in Section 3.9 below.\n\nThere are various other methods to split the data using the predictor set. For example, Kennard and Stone (1969) describes an algorithm that attempts to sequentially select points to be uniformly distributed in the space defined by the splitting variables. Similarly, Vakayil and Joseph (2022) proposed a data splitting method called twinning, where a split of the data is sought that minimizes an aggregate distance between points in the training and testing set. Twinning uses the energy distance of Székely and Rizzo (2013), which measures the equality of distributions, to make the two data sets similar. Any variables can be used in the distance calculations.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-three-way-split",
    "href": "chapters/initial-data-splitting.html#sec-three-way-split",
    "title": "3  Initial Data Splitting",
    "section": "3.7 Validation Sets",
    "text": "3.7 Validation Sets\nAs previously discussed, validation sets are a separate partition of the data that function as a precursor for the testing set. It allows us to obtain performance estimates on our model(s) during the development cycle. These are commonly used in deep learning and other domains where the initial data sizes range from very large to massive. This additional partition is often created simultaneously with the training and testing sets.\nValidation sets serve the same purpose as resampling methods described in Section 10.3 and we can consider them single resamples of the training data. Methods like bootstrapping or cross-validation use many alternative versions of the training set to compute performance statistics. When our data are extensive, multiple resamples are computationally expensive without significantly improving the precision of our estimates.\nWithout loss of generalization, we will treat the validation set as a particular case of resampling where there is a single resample of the training set. This difference is not substantive and allows us to have a common framework for measuring model efficacy (before the testing set).\nWe’ll see validation sets discussed in Section 10.3 and used in Sections TODO and TODO.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "href": "chapters/initial-data-splitting.html#sec-multilevel-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.8 Multi-Level Data",
    "text": "3.8 Multi-Level Data\nThere are cases where the rows of a data set may not be statistically independent. This often occurs when multiple data points are collected on individual people, such as\n\nPatients in medical studies may have data collected over time.\nPurchase histories of individual customers in a retail database.\n\nIn these and other situations, the data within a person tend to be correlated. This means that the data from a specific person have a higher correlation than data between people. There are many names for this type of data: multi-level data, hierarchical data, longitudinal data, random effect data, profile data, functional data, and so on. In some cases, there are multiple layers of data hierarchies.\nNote that the variable that indicates the person is generally not a predictor; we would not be making predictions about individual people. People, in this example, are sampled from the broader population. In this case, we are more concerned with the population rather than the individuals sampled from that population.\nThis aspect of the data differentiates it from the neighborhood predictor in the Ames data. The houses within each neighborhood may be more similar to one another than houses between neighborhoods. However, the difference is that we want to make predictions using information from these specific neighborhoods. Therefore, we will include neighborhood as a predictor since the individual neighborhoods are not a selected subset of those in the town; instead, the data contain all of the neighborhoods currently in the city.1\nChapter 9 of Kuhn and Johnson (2019) has a broad discussion on this topic with an illustrative example.\nWhen splitting multi-level data into a training and test set, the data are split at the subject level (as opposed to the row level). Each subject would have multiple rows in the data, and all of the subject’s rows must be allocated to either the training or the test set. In essence, we conduct random sampling on the subject identifiers to partition the data, and all of their data are added to either the training or test set.\nIf stratification is required, the process becomes more complicated. Often, the outcome data can vary within a subject. To stratify to balance the outcome distribution, we need a way to quantify the outcome per subject. For regression models, the mean of each subject’s outcome might be an excellent choice to summarize them. Analogously, the mode of categorical outcomes may suffice as an input into the stratification procedure.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#sec-spatial-splitting",
    "href": "chapters/initial-data-splitting.html#sec-spatial-splitting",
    "title": "3  Initial Data Splitting",
    "section": "3.9 Splitting Spatial Data",
    "text": "3.9 Splitting Spatial Data\nSpatial data reflect information that corresponds to specific geographic locations, such as longitude and latitude values on the earth2. Spatial data can have autocorrelation, with objects being more similar to closer objects than to objects further away. Tobler’s First Law of Geography (Bjorholm et al. 2008) is:\n\n“Everything is related to everything else, but near things are more related than distant things.”\n\nThink of an overhead color satellite image of a forest. Different types of foliage—or lack thereof—have different colors. If we focus on the color green, the distribution of “greeness” is not a random pattern of the x/y axes (random distribution would resemble static). In other words, pixels cannot be assumed to be independent of one another.\nSpatial autocorrelation will generally affect how we partition the data; we want to avoid having very similar data points in both the training and testing sets.\nThe example data that we’ll use is from the US Forest Service, who send experts to specific locations to make a determination about whether an area is forested. Couch, White, and Frick (2024) state:\n\nThe U.S. Department of Agriculture, Forest Service, Forest Inventory and Analysis (FIA) Program provides all sorts of estimates of forest attributes for uses in research, legislation, and land management. The FIA uses a set of criteria to classify a plot of land as “forested” or “non-forested,” and that classification is a central data point in many decision-making contexts. A small subset of plots in Washington State are sampled and assessed “on-the-ground” as forested or non-forested, but the FIA has access to remotely sensed data for all land in the state.\n\nSee Frescino et al. (2023) and White et al. (2024) for similar data sets and background on the original problem. The Washington State data contains 7,107 locations. The data are shown in Figure 3.3 where the points are colored by the outcome class (green representing forestation). This figure shows that locations across the state are not spread uniformly. Bechtold and Patterson (2015) describes the sampling protocols and related issues. Finley, Banerjee, and McRoberts (2008) and May, Finley, and Dubayah (2024) show examples of how models have been applied to US forestry data in other geographic regions.\nThe rate for forestation in these data is nearly even at 54.8%.\n\n\n\n\n\n\n\n\nFigure 3.3: Area locations in Washington State sampled for determining forestation. Green points reflect locations that were determined to be forested.\n\n\n\n\nHow should we split these data? A simple random sample is inadequate; we need to ensure that adjacent/nearby locations are not allocated to the training and testing sets. There are a few ways to compensate for this issue.\nFirst, we can group the data into local areas and iterate by removing one or more group at a time. If there is no rational, objective process for creating groups, we can use clustering methods or create a regular grid across the data range (Roberts et al. 2017). For example, Figure 3.4(a) shows a collection of data grouped by a grid of hexagons. We could randomly sample some groups, often referred to as blocks, to allocate to the testing set.\nHowever, notice that some points, while in different hexagonal blocks, are still close to one another. Buffering (Brenning 2005; Le Rest et al. 2014) is a method that excludes data around a specific data point or group. Panel (b) shows a buffer for one block. If this region was selected for the testing set, we would discard the points that are within the buffer (but are in different blocks) from being allocated to the training set.\n\n\n\n\n\n\n\n\nFigure 3.4: A plot of 65 spatial data points where (a) shows how they can be blocked into one of seven hexagonal regions. Panel (b) shows the same data with a circular buffer around the center of a single block.\n\n\n\n\n\nJ Pohjankukka T Pahikkala and Heikkonen (2017), Lyons et al. (2018), Meyer et al. (2019) Karasiak et al. (2022), and Mahoney et al. (2023) have good overviews of the importance of countering spatial autocorrelation and appropriate sampling methods.\nFor our example, we used blocking and buffering to allocate roughly 20% of the forestry data to the testing set. First, the locations were allocated to a 25 x 25 grid of hexagons across the state. After this, a buffer was created around the hexagon such that outside points within the buffer would be excluded from being used in the training or testing sets. Finally, roughly 20% of the locations were selected for the test set by moving through the space and selecting hexagons sequentially. Figure 3.5 shows the results of this process.\nIn the end, a training set of NA locations and a test set of NA locations (roughly a NA% holdout) were assembled. The buffer contained NA locations which was about NA% of the original pool of data points. The event rates were similar between the data sets: 55.4% for the training set and 55.3% for the testing set.\n\n\n\n\n\n\n\n\nFigure 3.5: A split of the forestation data into training (magenta) and testing (purple) sets. Some locations, in black, correspond to the buffer and are not allocated to either set. Clicking on individual points will show their allocation results.\n\n\n\n\nFrom this map, we can see that the hexagons selected for the testing set cover the geographic area well. While the training set dominates some spaces, this should be sufficient to reduce the effects of spatial autocorrelation on how we measure model performance.\nWe will revisit these data in Section 10.8 to demonstrate how to resample them appropriately.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#chapter-references",
    "href": "chapters/initial-data-splitting.html#chapter-references",
    "title": "3  Initial Data Splitting",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBaggerly, K, J Morris, and K Coombes. 2004. “Reproducibility of SELDI-TOF Protein Patterns in Serum: Comparing Datasets from Different Experiments.” Bioinformatics 20 (5): 777–85.\n\n\nBechtold, W, and P Patterson. 2015. The Enhanced Forest Inventory and Analysis Program - National Sampling Design and Estimation Procedures. U.S. Department of Agriculture, Forest Service, Southern Research Station.\n\n\nBjorholm, S, JC Svenning, F Skov, and H Balslev. 2008. “To What Extent Does Tobler’s 1 St Law of Geography Apply to Macroecology? A Case Study Using American Palms (Arecaceae).” BMC Ecology 8: 1–10.\n\n\nBrenning, A. 2005. “Spatial Prediction Models for Landslide Hazards: Review, Comparison and Evaluation.” Natural Hazards and Earth System Sciences 5 (6): 853–62.\n\n\nClark, R. 1997. “OptiSim: An Extended Dissimilarity Delection Method for Finding Diverse Representative Subsets.” Journal of Chemical Information and Computer Sciences 37 (6): 1181–88.\n\n\nCouch, S, G White, and H Frick. 2024. Forested: Forest Attributes in Washington State. https://github.com/simonpcouch/forested.\n\n\nDe Cock, D. 2011. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19 (3).\n\n\nFinley, A, S Banerjee, and R McRoberts. 2008. “A Bayesian Approach to Multi-Source Forest Area Estimation.” Environmental and Ecological Statistics 15: 241–58.\n\n\nFrescino, T, G. Moisen, P Patterson, C Toney, and G White. 2023. “‘FIESTA’: A Forest Inventory Estimation and Analysis R Package’.” Ecography 2023 (7).\n\n\nGower, J. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71.\n\n\nHawkins, D, S Basak, and D Mills. 2003. “Assessing Model Fit by Cross-Validation.” Journal of Chemical Information and Computer Sciences 43 (2): 579–86.\n\n\nJ Pohjankukka T Pahikkala, P Nevalainen, and J Heikkonen. 2017. “Estimating the Prediction Performance of Spatial Models via Spatial k-Fold Cross Validation.” International Journal of Geographical Information Science 31 (10): 2001–19.\n\n\nKapoor, S, and A Narayanan. 2023. “Leakage and the Reproducibility Crisis in Machine-Learning-Based Science.” Patterns 4 (9).\n\n\nKarasiak, N, JF Dejoux, C Monteil, and D Sheeren. 2022. “Spatial Dependence Between Training and Test Sets: Another Pitfall of Classification Accuracy Assessment in Remote Sensing.” Machine Learning 111 (7): 2715–40.\n\n\nKaufman, S, S Rosset, C Perlich, and O Stitelman. 2012. “Leakage in Data Mining: Formulation, Detection, and Avoidance.” ACM Transactions on Knowledge Discovery from Data 6 (4): 1–21.\n\n\nKennard, R W, and L A Stone. 1969. “Computer Aided Design of Experiments.” Technometrics 11 (1): 137–48.\n\n\nKohavi, R. 1995. “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.” International Joint Conference on Artificial Intelligence 14: 1137–45.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nKuhn, M, and J Silge. 2022. Tidy Modeling with R. O’Reilly Media, Inc.\n\n\nLe Rest, K, D Pinaud, P Monestiez, J Chadoeuf, and V Bretagnolle. 2014. “Spatial Leave-One-Out Cross-Validation for Variable Selection in the Presence of Spatial Autocorrelation.” Global Ecology and Biogeography 23 (7): 811–20.\n\n\nLyons, M, D Keith, S Phinn, T Mason, and J Elith. 2018. “A Comparison of Resampling Methods for Remote Sensing Classification and Accuracy Assessment.” Remote Sensing of Environment 208: 145–53.\n\n\nMahoney, MJ, LK Johnson, J Silge, H Frick, M Kuhn, and CM Beier. 2023. “Assessing the Performance of Spatial Cross-Validation Approaches for Models of Spatially Structured Data.” arXiv.\n\n\nMartin, J, and D Hirschberg. 1996. “Small Sample Statistics for Classification Error Rates I: Error Rate Measurements.” Department of Informatics and Computer Science Technical Report.\n\n\nMartin, T, P Harten, D Young, E Muratov, A Golbraikh, H Zhu, and A Tropsha. 2012. “Does Rational Selection of Training and Test Sets Improve the Outcome of QSAR Modeling?” Journal of Chemical Information and Modeling 52 (10): 2570–78.\n\n\nMay, P, A Finley, and R Dubayah. 2024. “A Spatial Mixture Model for Spaceborne Lidar Observations over Mixed Forest and Non-Forest Land Types.” Journal of Agricultural, Biological and Environmental Statistics, 1–24.\n\n\nMeyer, H, C Reudenbach, S Wöllauer, and T Nauss. 2019. “Importance of Spatial Predictor Variable Selection in Machine Learning Applications–Moving from Data Reproduction to Spatial Prediction.” Ecological Modelling 411: 108815.\n\n\nMolinaro, A. 2005. “Prediction Error Estimation: A Comparison of Resampling Methods.” Bioinformatics 21 (15): 3301–7.\n\n\nRoberts, DR, V Bahn, S Ciuti, MS Boyce, J Elith, G Guillera-Arroita, S Hauenstein, et al. 2017. “Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.” Ecography 40 (8): 913–29.\n\n\nSzékely, G J, and M L Rizzo. 2013. “Energy Statistics: A Class of Statistics Based on Distances.” Journal of Statistical Planning and Inference 143 (8): 1249–72.\n\n\nVakayil, A, and V R Joseph. 2022. “Data Twinning.” Statistical Analysis and Data Mining: The ASA Data Science Journal 15 (5): 598–610.\n\n\nWhite, G, J Yamamoto, D Elsyad, J Schmitt, N Korsgaard, J Hu, G Gaines III, T Frescino, and K McConville. 2024. “Small Area Estimation of Forest Biomass via a Two-Stage Model for Continuous Zero-Inflated Data.” arXiv.\n\n\nWillett, P. 1999. “Dissimilarity-Based Algorithms for Selecting Structurally Diverse Sets of Compounds.” Journal of Computational Biology 6 (3): 447–57.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/initial-data-splitting.html#footnotes",
    "href": "chapters/initial-data-splitting.html#footnotes",
    "title": "3  Initial Data Splitting",
    "section": "",
    "text": "If you are familiar with non-Bayesian approaches to multi-level data, such as mixed effects models, this is the same as the difference between random and fixed effects. ↩︎\nChapters 11 and 12 will also look at spatial effects unassociated with real geographies.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Initial Data Splitting</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html",
    "href": "chapters/missing-data.html",
    "title": "4  Missing Data",
    "section": "",
    "text": "4.1 Root Causes\nIt is not uncommon for some predictor values to be unknown. There can be a multitude of reasons. To illustrate, let’s consider a laboratory test for a respiratory disease. One or more results may be missing due to a failed control or an inappropriate database join. It is also possible that the test itself failed to produce a result. For example, the test might fail for diagnostics that use throat swabs because an interfering substance, such as food coloring from a lozenge, is on the swab.\nOne of our illustrative data, the Ames housing data, has missing values in 22 predictors. Figure 4.1 illustrates the occurrence of missingness for the these data. This figure presents the full rectangular data matrix where each property (sorted alphabetically by neighborhood) is on the x-axis, and each predictor is on the y-axis. Red indicates that the value was missing in the data.\nThe figure highlights several characteristics. The most noteworthy: missing data affects every property (i.e., row) in the data. Pool quality is missing for nearly every property, while the columns for types of alleys, basements, and fences as well as the quality of fireplaces are missing for the vast majority of properties. Second, there are specific patterns of missing information that are visually apparent. For example, when missing data occurs in one garage variable, it likely occurs across other garage variables (condition, finish, quality, type, and year built). The same is true for basement variables (condition, exposure, quality, type 1, and type 2). Other variables, like fence, fireplace quality, and lot frontage do not appear to have any visual structure to their missingness.\nAn “upset plot” is a method for visualizing high-dimensional Venn diagrams (Lex et al. 2014). We can use this to explore potential patterns of missingness across predictors. For example, Figure 4.2 shows that the majority of the properties in Ames are missing values for ally, fence, or pool.\nThe problem with missing data is that many models are not naturally equipped to deal with this lack of information. As a simple example, consider the multiple linear regression model. Deriving the regression coefficients depends on operations on the predictor values (e.g., computing the covariance matrix). These calculations cannot be performed if the predictors contain any missing values; this is also true for the majority of models. Consequently, it is crucial to address the presence of missing data to build any of these predictive methods1. In addition to addressing missing data, the pattern and nature of missingness can sometimes serve as a significant predictor of the response. In this chapter, we will explore the root causes of missing data, examine approaches for resolving the problem, and understand when it should be addressed in the modeling process.\nAllison (2002) has an excellent and succinct summary of relevant statistical concepts and methods. Emmanuel et al. (2021) and Hasan et al. (2021) provide literature surveys on methods for missing data, while Nijman et al. (2022) describes a survey on just how badly these topics are described/documented in specific studies in the literature.\nWhen encountering missing data, the primary question is, “Why are these values missing?” Knowing is a good idea, and the answer can substantially affect how we compensate for the problem. In some instances, the answer might be apparent or can be inferred from the data. In the example of the laboratory test, suppose a random defect in the test kit was to blame for missing measurements. In this instance, the value was missing completely at random (MCAR). MCAR means that the mechanisms that affect missingness in the predictor are known and unrelated to the columns in the data set. When this is the case, our complete case sample (achieved by using rows with no missing values) corresponds to sampling from the same distribution as the entire data set (i.e., includes the missing rows).\nWith MCAR, we have significant latitude in terms of how to handle the situation. A complete case analysis will not induce any systematic bias in our results but, due to the smaller sample size, the model will have increased variability.\nAs a counter-example, suppose that patients with more severe respiratory illnesses were more likely to deal with their symptoms by using a lozenge, possibly leading to missing values. If the likelihood of a missing predictor value is a function of some other variable(s) contained in the data, this is called missing at random (MAR). You can think of this as conditional MCAR: each each level of a column describing lozenge usage (yes/no), the data are MCAR. Bhaskaran and Smeeth (2014) contains practical examples that help distinguish MAR and MCAR.\nAnother example data set will be introduced in Section 6.1, where we try to predict the cost of a hotel room (known as the “average daily rate” or ADR). The data set includes data on the method of booking the room, such as the name of the travel agent, the travel company, and the customer’s country of origin. The agent variable in the hotel rate data was missing for 22.7% of the data. We don’t always know if the agent is missing because no agent was used or because the value was not recorded. Treating the missingness of the agent as a binary outcome, we can use a simple recursive partitioning model (?sec-cart-cls) to understand potential relationships with other predictors. This might suggest the mechanism(s) influencing the occurrence of missing data. Figure 4.3 illustrates the predictors that partition the missing agent data into increasing homogeneous groups. For example, non-missing company values are highly associated with missing values. This might mean that the company making the reservation used automated systems instead of a specific person in the company being associated with the reservation. Lead time also appears to be related to missingness, with shorter lead times occurring more frequently with missing agent values. We don’t know whether lead time influences missingness or vice-versus. However, this analysis would provide a direction to begin an investigation.\nFigure 4.3: A classification tree to predict the missing category of agent in the hotel rate dataset. “TA” stands for travel agent and “TO” means tour operators.\nWith some simple exploration, as demonstrated with the Ames and hotel rate data sets, we can understand the causes or potential reasons for missing data. However, determining the cause of missing data may be more challenging for many other data sets. We need a framework to understand and manage missing data in such cases.\nOne helpful framework involves examining the mechanisms behind missing data, including structural deficiencies, random occurrences, and specific causes.\nFirst, there can be structural deficiencies. This type of missing data arises when necessary information about a predictor is omitted. Moreover, it is often the easiest to address once the missing information has been identified. The agent variable in the hotel rate data is one example of a structural deficiency. As a second example, we saw earlier in the Ames housing data that several of the garage predictors were simultaneously missing across 5% of homes. The missing information was because these homes did not have a garage2. Some or all of the variables of finish, quality, type, and year built for garages will likely be important for determining a home’s value. Therefore, we will need an approach to address these structural deficiencies.\nThere can also be random occurrences. Missing values often occur at random with no defined root cause. For example, a patient in a clinical trial may miss a scheduled visit due to a scheduling mishap. As another example, sporadically occurring severe weather can create missing data for collection devices that depend on continuous power. Generally, missing data due to random occurrences can happen a small percentage of the time and can be remedied. However, if randomly missing data occurs a large percentage of the time, the measurement system should be evaluated, and the collected variables should be scrutinized before being included in the modeling process. For a detailed understanding of this type of problem, see Little and Rubin (2019).\nWe might be able to identify specific causes for missingness. As we will see later in this chapter (Section 4.2.4), several basement variables in the original Ames data have missing values. This information was not missing because there was some failure in recording the data. The explanation was much simpler: these homes had no basements. This type of missing data is the most challenging to manage, and the appropriateness of techniques can vary. Hence, understanding the nature of the missing data is crucial before applying any methods.\nThe worst situation is missing not at random (MNAR)3, where we do not know the factors that influence the probability that values are missing. The best approach to MNAR data is not to have MNAR data; it is essential to determine why the data are missing and, hopefully, relate that to columns in the data. Otherwise, we can use imputations and other methods described below to solve the issues. However, there will likely be significant ambiguity about the analysis and how well it works. With models built in the MNAR assumption, it would behoove us to conduct extensive sensitivity analyses to assess the robustness of our approach to the missing data problem.\nAs an example, Chapters 12 - 15 of Kuhn and Johnson (2013) showed multiple classification models to predict the probability that a grant would be accepted. Two categorical predictors contained encoded values of “unknown” and had very high empirical associated with the outcome classes:\nThis is not a good place to be: we cannot explain the existence of two of our main predictors. It would be that the populations of grants that had missing data for these variables correspond to very successful results or that the process that assembled the data contains a systematic flaw. We do know that we can’t induce success in a grant application by just labeling the value as unknown. Clearly, we would want to investigate this situation to determine what is happening.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-root-causes",
    "href": "chapters/missing-data.html#sec-root-causes",
    "title": "4  Missing Data",
    "section": "",
    "text": "Informative missingness seemed to occur in these data; unknown values of the contract value band and sponsor code were heavily used in many models. This itself is a likely surrogate or signal for some other piece of information.\n\n\n\nUnderstanding these mechanisms will guide us in choosing appropriate techniques for handling missing data appropriately.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-resolve-missing-data",
    "href": "chapters/missing-data.html#sec-resolve-missing-data",
    "title": "4  Missing Data",
    "section": "4.2 Approaches for Resolving Missing Data",
    "text": "4.2 Approaches for Resolving Missing Data\nThere are three general ways to resolve missing values: removal, imputation, or encoding. Each approach has advantages and disadvantages, and which we choose should depend on the specific problem context and data set. We will review these approaches in the subsections below and provide guidance on when each would be appropriate.\n\n4.2.1 Models that Tolerate Missing Data\nInstead of directly addressing the missing values, we could sidestep it by using models that tolerate missingness. Let’s examine a few.\nThe CART decision tree model (Breiman et al. 1984) recursively identifies variables and split points that optimally partition the data into subsets whose outcome frequency distributions are more homogeneous. For each selected variable and split point, additional variables and split points are identified with the next best ability to partition the data into more pure subsets. These additional variables are called surrogate splits. When a sample has a missing value for a predictor in the tree, the surrogate predictors are then used to direct the sample toward the appropriate terminal node.\nWhile C5.0 (Quinlan 1993; Kuhn and Johnson 2013) is also a decision tree, it adopts a unique approach to addressing missing values. This method utilizes fractional counts in subsequent splits based on the frequency distribution of missing data for a predictor. This approach enables the model to estimate where the missing values might fall within the partitioning.\nBoosted trees, such as xgboost (Chen and Guestrin 2016), are also based on a recursive partitioning framework. However, xgboost’s approach to addressing missing data is more complex and is called sparsity-aware split finding. In the model-building process, the algorithm determines which direction would be more optimal for each node in the tree if a sample had a missing value for that predictor.\nRandom forest (Breiman 2001) has several approaches to handling missing values. The naive approach internally imputes using the median of non-missing values for continuous predictors or the most frequent value for categorical predictors. A more advanced approach identifies the nearest non-missing samples to the sample with the missing value and imputes the value based on a weighted distance score.\nThe primary benefit of these models is that we can use the original data as-is. They are eliminate the propagation of errors that can occur when imputation tools are used. The drawback is that the number of models that can be applied to the data is very limited. As we know from the No Free Lunch theorem, no one model will be optimal for all problems. Therefore, we must address the missing data problem head-on to adequately explore the predictive ability of a wide range of models.\n\nThese models only solve the logistical aspect of missing data. If the nature of missingness in your data causes a systematic bias, the models above will not correct this bias.\n\nFor example, naive Bayes (Webb, Keogh, and Miikkulainen 2010) builds models by analyzing each predictor independently of the others. When a predictor contains a missing value, this missing sample’s information is omitted from probability calculations for only the affected predictors. If a distribution is systematically affected by missingness, the probabilities computed during prediction will be biased and may result in poor results in new samples. If missing data informs our understanding of the response, omitting the missing samples will be detrimental to the model.\nFigure 4.4 shows a two-class classification example where one predictor is used. The top panel shows the probability of a missing predictor value is related to its own value (i.e., MCAR). The predictor value is less likely to be complete was its value increases.\nNaive Bayes would use the predictor data to compute conditional densities of the predictor for each outcome class. The middle plot shows what these densities should resemble if the dataset were complete. There is a small overlap between the predictor densities; decent model performance is achievable.\n\n\n\n\n\n\n\n\nFigure 4.4: An example of how missingness can cause bias in the training data and impair a model fit.\n\n\n\n\n\nThe bottom set of densities reflects the observed data. The density for class level B is more affected and would appear to have a tighter distribution. This induces more overlap in the densities, making the classification problem more difficult.\nThis demonstrates that a model being agnostic to missing values does not mean that the missing value problem does not go away.\n\n\n4.2.2 Removal\nThe simplest method for managing missing data is to eliminate the predictors or samples that contain them. However, the deletion of data requires careful consideration of several factors within the dataset. When deleting data, the order in which we assess the proportion of missing values (in columns or rows) is important to consider. In some scenarios, there are many more predictors than samples; samples are often difficult or expensive to collect. In this case, it would be wise to first identify predictors that should be removed due to excessive missing values, then proceed to identify and possibly remove samples that have excessive missing values. If, on the other hand, the data contain many more samples than predictors, then the removal procedure could be reversed. In any case, however, we need to remember that the samples are our currency for tuning models and assessing model performance. Therefore, we will often place a higher priority on preserving samples over predictors.\n\n\n\n\n\n\n\n\nFigure 4.5: The distribution of percent missing values across predictors for the Ames housing data set.\n\n\n\n\n\nLet’s return to the Ames data set. In this example, there are many more samples (\\(n\\)=2930) than predictors (\\(p\\)=73). While this is true, Figure 4.1 clearly illustrates that a few of the predictors were missing for the vast majority of the samples. The proportion of missing values across the predictors is shown in Figure 4.5. This figure reveals that 4 predictors have more than 20% missing sample values. Because the percentage of missing values is large, these predictors would be candidates for removal. In the hotel rate data, the agent and company predictors had missing values of 22.7% and 91%, respectively. In our experience, we have tended to remove predictors that have more than 20%-30% missing values. This percentage is simply a point of guidance and not a hard rule to be applied to every data set. After removing these predictors, the Ames data set now has 0.7% missing values and the hotel rate data set had 0.05% missing values.\nAfter removing predictors with excessive missing data, we can then consider removing samples with excessive missing data. No sample in the Ames data had more than 12.1% of missing predictors. The samples with the greatest percentage of missing predictors were those that had missing basement information. No sample in the hotel rate data had more than 3.6% of missing predictors. Neither of these percentages is large enough to merit the removal of these samples. Therefore, we will keep these samples and utilize an imputation or encoding procedure to fill in these gaps.\nThere are a couple of caveats to removing predictors or samples based on missing information. First, the missing information in a predictor may be informative for predicting the response. For the hotel rate data, the missing status of the agent variable is associated with the response. Therefore, it may be better to encode the missing status for this variable rather than eliminate the variable altogether. This approach will be discussed below. Second, as seen in Figure 4.4, removing samples may create a bias in the remaining data which would impact the predictive ability of the model on future samples.\n \n\n\n4.2.3 Imputation\nImputation is the process of using other existing information (i.e., the predictors) to estimate what each missing value might have been. In other words, we will build an imputation model to fill in the missing column(s) so that we can run the primary machine-learning model. A separate imputation model is required for each column that contains (or could contain) missing data.\nImputation is a well-researched statistical methodology. This technique has traditionally been used for inferential models, focusing on maintaining the validity of test statistics to support hypothesis testing. As mentioned in earlier chapters, there is an important distinction between the objectives of statistical inference and prediction accuracy. See Sperrin et al. (2020) for a discussion related to imputation.\nIn the statistical literature (D’Agostino McGowan, Lotspeich, and Hepler 2024), two terms can help us understand the distinction:\n\nDeterministic imputation creates a single model to estimate the missing predictor’s value using one or more predictors in the training set4. After imputation, the new values are treated as known (i.e., not random variables).\nStochastic imputation involves creating many imputation models with the goal of creating a distribution of possible values for the missing data. It is used primarily to conduct proper inferential analyses and includes traditional multiple imputation techniques.\n\nOne important concept heavily featured in subsequent chapters is resampling, where we’ll train our preprocessors and supervised models using slightly different data sets. The goal of resampling is to estimate model performance accurately. Although resampling will involve repeated re-imputation of missing data, it is not a stochastic imputation method.\nLet’s highlight a couple of key distinctions when considering imputation for the purpose of inference versus prediction. One is that inferential models generally make assumptions about the statistical distributions of the predictors. Conversely, many machine learning models, like support vector machines, tree-based models, and neural networks, do not make such assumptions. Therefore, stochastic imputation is less relevant for predictive models. A second difference is that multiple imputation methods focus on understanding relationships within the existing data, while predictive models aim for generalizable relationships to unseen samples. This difference has implications on when imputation should be applied to the data, which will be discussed later in Section 4.4. Finally, in some cases, including the outcome data in the imputation model might make sense. This is inappropriate for our goals here (i.e., deterministic imputation). While D’Agostino McGowan, Lotspeich, and Hepler (2024) explains the theoretical reasons why this is the case, our objections can be viewed as purely functional: if we require the outcome to make decisions, we cannot predict new samples where the outcome is unknown.\nWhat are the important characteristics of an imputation technique that will be used in a prediction model? There are a few that we will focus on:\n\nTolerate other missing data: as we saw in the Ames data, multiple variables within a sample may be missing. Therefore, an imputation technique should be feasible in the presence of other missing data.\nHandle different predictor types: many data sets have different variables, such as numeric, categorical, and ordinal. The method should be able to accommodate numeric and qualitative predictors seamlessly and without changing the nature of the data (e.g., categorical predictors should be be converted to indicator columns).\nProduce efficient prediction equations: each predictor with missing data will require an equation. Imputation for large data sets, when used with resampling approaches during model training, will increase computation time as well as the size of the imputation equation. Therefore, the more efficient the imputation approach, the less computation time will be needed to obtain the final model.\nEnsure robustness: the method should be stable and not overly affected by outliers.\nBe accurate: the results should be close to what the actual value would have been5.\n\nBy considering each of these characteristics, imputation can effectively alleviate missing data problems, enhancing models’ quality and predictive performance. There are several imputation methods that meet most or all of the above characteristics. These imputation techniques fall into two general categories: most likely value and model-based (i.e., “regression imputation”).\nTo illustrate how these methods work, we will use two simple, simulated two-predictor classification data sets. Figure 4.6 displays the simulated data. For these data sets, the optimal class separation is defined by the black lines. We will initially focus on the nonlinear data set. From these data set, 10% of the samples will be randomly selected. Of these samples, half will have the value of first predictor deleted, while the other half will have the second predictor deleted.\n\n\n\n\n\n\n\n\nFigure 4.6: Two complete two-class simulated datasets with 200 data points, one with a linear relationship between the predictors and the other nonlinear. The optimal partitions of the classes are represented by the black lines.\n\n\n\n\n\n\n\n\n4.2.4 Encoding Missing Data\nWhen a predictor takes categorical values, an alternative approach to imputation is encoding. Consider the basement exposure variable in the original Ames data set. Possible exposure values are: “good,” “average,” “minimum,” and “no exposure” and contains 83 missing values. The most likely value approach would impute the missing values with the “no exposure” category since this is the most frequent category. A model-based approach would utilize information across the rest of the variables to predict which of the 4 available categories would be best. Would imputing with one of the 4 available categories be a good approach for the missing samples? In this case, the answer is “no!”. With a little more investigation, we can see that the reason the exposure variable contains missing values for these houses is because these houses do not have basements.\nWhen a value is missing for a specific reason like we see with the basement exposure variable, encoding the missing information will be more informative to the models. In an encoding procedure, we simply acknowledge that the value is missing by creating a new category such as “unknown,” “unspecified,” or “not applicable.” In the case of basement exposure, a more appropriate categorization would be “no basement.”\nThe encoding procedure is mostly used for categorical variables. However, there are times when encoding can be applied to continuous variables. In many analytical procedures, an instrument cannot reliably provide measurements below a specified limit. Measurements of samples above the limit are returned as numeric values, but measurements below the limit are returned as either “BLQ” (below lower limit of quantitation) or “&lt; 3.2”, where 3.2 is the lower limit of quantitation. These values cannot be included with the continuous values. What can we do in this situation? One approach would be to impute the BLQ values with a reasonable numeric value less than lower limit of quantitation6. This information could also be encoded by creating a new variable that contains two values, “ALQ” and “BLQ”, which would identify samples that could and could not be measured. If this variable is related to the outcome, then the ability to measure the quantity may be predictively informative.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-imputation-methods",
    "href": "chapters/missing-data.html#sec-imputation-methods",
    "title": "4  Missing Data",
    "section": "4.3 Specific Imputation Methods",
    "text": "4.3 Specific Imputation Methods\nTechnically, almost any model that can make a prediction is a potential candidate for imputing data. However, some are better than others and we summarize a few of the most used below.\n\n4.3.1 Most Likely Value\nThe simplest approach to imputation is to replace a missing value with its most likely value based on that predictor’s non-missing values. For numeric data, the most likely value can be summarized by the mean or median. For categorical data, the most likely value is the mode (or most frequent value). Most likely value imputation meets many of the desirable characteristics for imputation. They can tolerate missing values from other predictors (because they operate one-predictor-at-a-time) and can handle different predictor types while also producing efficient prediction equations. Furthermore, achieving a robust imputation can be done using either the median or the trimmed mean (Barnett and Lewis 1994). The trimmed mean is the mean of the middle-most samples. For example, we could compute the mean of the samples with 5% of the the most extreme (smallest and largest) samples removed. Doing this would minimize the impact of any extreme sample values on the computation of the mean. However, imputing with the most likely value for a single predictor may not produce an imputed value that is close to the true value.\nTo demonstrate, we have imputed the missing values in the simulated data using the mean and median imputation techniques for both data sets seen in Figure 4.6. Figure 4.7 shows the values of the original samples that were selected to have missing values along with the imputed values based on mean and median imputation.\nFor both data sets, the imputed values are towards the center over the overall plot and away from the parabolic/linear regions where we know the actual data are. The median imputed values are less affected by extreme values and are closer to the region of actual data. Both techniques, however, are unable to place most of the missing data near their true values.\nFor many data sets, and especially for sets with a minimal number of missing values, most likely value imputation may be sufficiently good for what we need. The procedure is fast but is terribly inaccurate. But there may be occasions when when we need the imputations to be closer to the true (yet unknown) values.\n\n\n\n\n#| label: shiny-imputation-exploration\n#| out-width: \"80%\"\n#| viewerHeight: 425\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(bslib)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-imputation-exploration.R\")\n\napp\n\n\n\nFigure 4.7: A visualization of the four new features for different linear embedding methods. The data shown are the validation set results from the two simulations shown in Figure 4.6.\n\n\n\n\n\n4.3.2 Linear Methods\nWhen a data set has many predictors and the predictors have some correlation with each other, then multiple linear regression can be a more effective imputation technique than the most likely value methods. That is, multiple linear regression will utilize the information contained in other predictors to estimate an imputed value that will be closer to what the actual value would have been. Multiple linear regression also produces a straightforward, compact prediction equation for each predictor with missing values which is computationally efficient when applied to new samples. However, this technique cannot naturally handle missing values in the predictors used to develop the imputation model. In practice, therefore, the imputation approach utilizes a complete case analysis.\nReturning to the simulation illustration, the linear imputation method creates two simple linear regression models, where one model uses predictor 1 as the response and predictor 2 as the predictor, and the other model reverses the roles of the predictors.\nSelecting the nonlinear data set in Figure 4.7 illustrates the impact of this technique. Assuming that we have conducted some exploratory data analysis, we would be aware of the nonlinear relationship between the predictors. As such, the imputation model for the second predictor included an additional squared term. This greatly improves the imputation quality. Unfortunately, the same approach is not possible for imputing the first predictor (which is poorly predicted).\nHowever, if these predictors were linearly related, then this technique would generate values closer to the actual values. This can be seen by choosing the linear data set in Figure 4.7.\nSince this imputation method is basically linear regression, we can use it for slightly more sophisticated results. For example, if we want to impute a predictor’s values based on some other predictors categories, linear regression can achieve this. There is also the possibility of interactions, splines, and so on (as long as the “predictors” in the imputation model are not missing themselves).\nWhen the predictor that requires imputation is categorical, then logistic or multinomial regression can be used to generate an imputed category.\nFigure 4.8 provides another visual comparison of the imputation techniques. In this figure, each point represents the Euclidean distance from the imputed sample to its known location. The mean, median, and linear methods have similar distributions of distances.\n\n\n\n\n\n\n\n\nFigure 4.8: A comparison of the distribution of distances between the actual and imputed samples across imputation techniques for the nonlinear data shown in Figure 4.6. The mean, median, and linear techniques perform similarly, while K-NN and bagging generate imputed values that are modestly closer to the actual values.\n\n\n\n\n\n\n\n4.3.3 Nearest Neighbors\n\\(K\\)-nearest neighbor calculations will be discussed in Section 7.3, where it will be used in some multidimensional scaling methods, and in ?sec-knn-cls in the context of a supervised model. In short, when imputing a missing predictor value for a new sample, the \\(K\\) most similar samples from the training set are determined (using all complete predictor values). The \\(K\\) predictor values for the predictor of interest are summarized via the mean (for numeric outcomes) or the mode. This summary statistic is used to fill in the missing value. This is a localized version of the most likely value approach.\nFigure 4.7 shows that a nearest-neighbor approach generally increases the accuracy of the imputation, especially for the first predictor. For the linear case, both predictors have high-quality imputations.\n\n\n4.3.4 Trees\nAs mentioned earlier, tree-based models are a reasonable choice for imputation techniques because many types of trees do not require complete data themselves. They generally provide good accuracy and do not extrapolate values beyond the bounds of the training data. While a single tree can be used for imputation, it will likely have low bias but high variance. Ideally, we would like imputed values to have low bias as well as low variance.\nTree ensembles, such as bagging and random forests (Chapters ?sec-trees-cls and ?sec-trees-reg), help solve this issue since they blend the predictors from many individual tree models. However, these methods can be computationally taxing for moderate- to large-sized data sets. Specifically, random forests require many trees (hundreds to thousands) to achieve a stable and reliable imputation model. This comes at the cost of a large computational footprint, which may become a challenge as the number of predictors with missing data increases; a separate model must be trained and retained for each predictor. As discussed later, bagged tree models tend to be smaller and faster than their random forest counterparts. Typically, there is a marginal loss in accuracy from using bagging instead of random forests.\nLike nearest-neighbor imputation, the bagged tree places most of the imputed values close to the original data distribution, as seen in Figure 4.7. The distribution of distances (Figure 4.8) for bagged imputation is similar to that of nearest neighbors. Figure 4.7 provides a comparison of the imputation techniques discussed here with the nonlinear simulated data and a data set in which the two groups are optimally separated by a linear boundary. In this comparison, the percentage of missing data can be adjusted to demonstrate how each imputation method performs.\nWhich imputation technique we choose depends on the problem. If a few predictors have missing values, then using \\(K\\)-NN or bagged imputation may not add much computational burden to the modeling process. However, if many predictors have missing values and the data set is large, then the model-based imputation techniques may become cumbersome. In this case, beginning with a most likely value imputation approach may be prudent. If the predictive performance of the optimally tuned machine learning technique is high, then the imputation approach was sufficient. However, if predictive performance lags, we could implement a different imputation technique while training models.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#sec-when-to-address-missing-data",
    "href": "chapters/missing-data.html#sec-when-to-address-missing-data",
    "title": "4  Missing Data",
    "section": "4.4 When to Address Missing Data",
    "text": "4.4 When to Address Missing Data\nWhen missing values are present, it may be tempting to immediately address this problem and create a complete data set prior to beginning the modeling or data splitting processes. We recommend that imputation be done as part of the model development process as discussed in Chapter 2. In the diagram presented in Figure 2.10, imputation would occur as part of the training process. Recall, the model based imputation techniques discussed earlier have parameters that can be tuned. Understanding how these parameters affect model performance would be done during the training process, and an optimal value can be selected.\nIt is also important to understand that imputation is fundamentally a preprocessing step–we must do this before commencing model building. Therefore, we must think about where imputation should be located in the order of preprocessing steps. Specifically, imputation should occur as the first step. It is advisable to impute qualitative predictors before creating indicator variables to maintain the binary nature of the resulting data. Moreover, imputation should precede parameter estimation steps. For example, if centering and scaling are done before imputation, the resulting means and standard deviations will reflect biases and issues from the missing data. This approach ensures the integrity and accuracy of subsequent data processing and analysis stages.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#chapter-references",
    "href": "chapters/missing-data.html#chapter-references",
    "title": "4  Missing Data",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAllison, P. 2002. Missing Data. SAGE Publications, Inc.\n\n\nBarnett, V, and T Lewis. 1994. Outliers in Statistical Data. Vol. 3. 1. Wiley New York.\n\n\nBhaskaran, K, and L Smeeth. 2014. “What Is the Difference Between Missing Completely at Random and Missing at Random?” International Journal of Epidemiology 43 (4): 1336–39.\n\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nBreiman, L, J Friedman, C Stone, and RA Olshen. 1984. Classification and Regression Trees. CRC Press.\n\n\nChen, T, and C Guestrin. 2016. “Xgboost: A Scalable Tree Boosting System.” In Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining, 785–94.\n\n\nD’Agostino McGowan, L, S Lotspeich, and S Hepler. 2024. “The \"Why\" Behind Including \"Y\" in Your Imputation Model.” Statistical Methods in Medical Research 33 (6): 996–1020.\n\n\nEmmanuel, T, T Maupong, D Mpoeleng, T Semong, B Mphago, and O Tabona. 2021. “A Survey on Missing Data in Machine Learning.” Journal of Big Data 8: 1–37.\n\n\nHasan, K, A Alam, S Roy, A Dutta, T Jawad, and S Das. 2021. “Missing Value Imputation Affects the Performance of Machine Learning: A Review and Analysis of the Literature (2010-2021).” Informatics in Medicine Unlocked 27: 100799.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\nLex, A, N Gehlenborg, H Strobelt, R Vuillemot, and H Pfister. 2014. “UpSet: Visualization of Intersecting Sets.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983–92.\n\n\nLittle, R, and D Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley & Sons.\n\n\nNijman, SWJ, AM Leeuwenberg, I Beekers, I Verkouter, JJL Jacobs, ML Bots, FW Asselbergs, KGM Moons, and TPA Debray. 2022. “Missing Data Is Poorly Handled and Reported in Prediction Model Studies Using Machine Learning: A Literature Review.” Journal of Clinical Epidemiology 142: 218–29.\n\n\nQuinlan, R. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers.\n\n\nSisk, R, M Sperrin, N Peek, M van Smeden, and G Martin. 2023. “Imputation and Missing Indicators for Handling Missing Data in the Development and Deployment of Clinical Prediction Models: A Simulation Study.” Statistical Methods in Medical Research 32 (8): 1461–77.\n\n\nSperrin, M, G Martin, R Sisk, and N Peek. 2020. “Missing Data Should Be Handled Differently for Prediction Than for Description or Causal Explanation.” Journal of Clinical Epidemiology 125: 183–87.\n\n\nWebb, G, E Keogh, and R Miikkulainen. 2010. “Naı̈ve Bayes.” Encyclopedia of Machine Learning 15 (1): 713–14.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-data.html#footnotes",
    "href": "chapters/missing-data.html#footnotes",
    "title": "4  Missing Data",
    "section": "",
    "text": "Exceptions are listed below.↩︎\nHowever, note that these properties have complete values for their size column (i.e., a zero ft2 garage). ↩︎\nAlso called not missing at random (NMAR), informative missingness, or nonignorable missing data.↩︎\nSisk et al. (2023) calls this approach, when using multiple predictors for imputation, as “regression imputation.” Their definition would exclude simple “most likely value” imputations.↩︎\nUnfortunately, we may not be able to judge the accuracy of the imputation for most data sets since the missing data will never be known.↩︎\nOne approach is to generate a random uniform predictor value between zero and the limit of quantitation).↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing Data</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html",
    "href": "chapters/numeric-predictors.html",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "5.1 What are Problematic Characteristics, and When Should Transformations be Applied?\nData that are available for modeling are often collected passively without the specific purpose of being used for building a predictive model. As an example, the Ames Housing data contains a wealth of information on houses in Ames, Iowa. But this available data may not contain the most relevant measurements for predicting house price. This may be due to the fact that important predictors were not measured. Or, it may be because the predictors we have collected are not in the best form to allow models to uncover the relationship between the predictors and the response.\nAs mentioned previously, feature engineering is the process of representing your predictor data so that the model has to do the least amount of work to explain the outcome effectively. A tool of feature engineering is predictor transformations. Some models also need predictors to be transformed to meet the model’s mathematical requirements (i.e., pre-processing). In this chapter we will review transformations for quantitative predictors.\nWe will begin by describing transformations that are applied to one predictor at a time that yield a revised for of the predictor (one in, one out). After these, an example of a group transformation is described called the spatial sign. Later, Chapter 7 will describe different types of many-to-many transformations such as principal component analysis (PCA) and multidimensional scaling (MDS). Additionally, in Chapter 8, we will examine techniques for expanding a single numeric predictor to many predictors (one in, many out).\nLet’s begin by understanding some general data characteristics that need to be addressed via feature engineering and when transformations should be applied.\nCommon problematic characteristics that occur across individual predictors are:\nSome models, like those that are tree-based, are able to tolerate these characteristics. However, these characteristics can detrimentally affect most other models. Techniques used to address these problems generally involve transformation parameters. For example, to place the predictors on the same scale, we would subtract the mean of a predictor from a sample and then divide by the standard deviation. This is know as standardizing and will be discussed in the next section.\nWhat data should be used to estimate the mean and standard deviation? Recall, the training data set was used to estimate model parameters. Similarly, we will use the training data to estimate transformation parameters. When the test set or any future data set are standardized, the process will use the estimates from the training data set. Any model fit that uses these standardized predictors would want new samples being predicted to have the same reference distribution.\nSuppose that a predictor column had an underlying Gaussian distribution with a sample mean estimate of 5.0 and a sample standard deviation of 1.0. Suppose a new sample has a predictor value of 3.7. For the training set, this new value lands around the 10th percentile and would be standardized to a value of -1.3. The new value is relative to the training set distribution. Also note that, in this scenario, it would be impossible to standardize using a recomputed standard deviation for the new sample (which means we try to divide with a zero standard deviation).\nMany transformations that involve a single predictor change the data distribution. Most predictive models do not place specific parametric assumptions on the predictor variables (e.g., require normality), but some distributions might facilitate better predictive performance than others.\nTODO some based on convention or scientific knowledge. Others like the arc-sin (ref The arcsine is asinine: the analysis of proportions in ecology) or logit? See issue #10.\nThe next two sections will consider two classes of transformations for individual predictors: those that resolve distributional skewness and those that convert each predictor to a common distribution (or scale).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-general-transformation",
    "href": "chapters/numeric-predictors.html#sec-general-transformation",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "skewed or unusually shaped distributions,\nsample(s) that have extremely large or small values, and\nvastly disparate scales.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-skewness",
    "href": "chapters/numeric-predictors.html#sec-skewness",
    "title": "5  Transforming Numeric Predictors",
    "section": "5.2 Resolving Asymmetry and Skewness",
    "text": "5.2 Resolving Asymmetry and Skewness\nAn asymmetric statistical distribution is one in which the probability of a sample occurring is not symmetric around the center of the distribution (e.g., the mean). For example, Figure 5.1 (panel a) shows the training set distribution of the lot area of houses in Ames. There is a much higher likelihood of the lot area being lower than the mean (or median) lot size. There are fewer large lots than there are proportionally smaller lots. And, in a few cases, the lot sizes can be extremely large.\nThe skew of a distribution indicates the direction and magnitude of the asymmetry. It can be quantified using the skewness statistic:\n\\[\\begin{align}\n  skewness &= \\frac{1}{(n-1)v^{3/2}} \\sum_{i=1}^n (x_i-\\overline{x})^3 \\notag \\\\\n  \\text{where}\\quad  v &= \\frac{1}{(n-1)}\\sum_{i=1}^n (x_i-\\overline{x})^2 \\notag\n\\end{align}\n\\]\nwhere values near zero indicate a symmetric distribution, positive values correspond a right skew, and negative values left skew. The lot size data are significantly right-skewed (with a skewness value of 13.5). As previously mentioned, there are 2 samples in the training set that sit far beyond the mainstream of the data.\n\n\n\n\n\n\n\n\nFigure 5.1: Lot area for houses in Ames, IA. The raw data (a) are shown along with transformed versions using the Yeo-Johnson (b), percentile (c), and ordered quantile normalization (d) transformations.\n\n\n\n\n\nOne might infer that “samples far beyond the mainstream of the data” is synonymous with the term “outlier”; The Cambridge dictionary defines an outlier as\n\na person, thing, or fact that is very different from other people, things, or facts […]\n\nor\n\na place that is far from the main part of something\n\nThese statements imply that outliers belong to a different distribution than the bulk of the data. For example, a typographical error or an incorrect merging of data sources could be the cause.\nThe Croarkin et al. (2012) describes them as\n\nan observation that lies an abnormal distance from other values in a random sample from a population\n\nIn our experience, researchers are quick to label (and discard) extreme data points as outliers. Often, especially when the sample size is not large, these data points are not abnormal but belong to a highly skewed distribution. They are ordinary in a distributional sense. That is the most likely case here; some houses in Ames have very large lot areas, but they certainly fall under the definition of “houses in Ames, Iowa.” These values are genuine, just extreme.\nThis, by itself, is okay. However, suppose that this column is used in a calculation that involves squaring values, such as Euclidean distance or the sample variance. Extreme values in a skewed distribution can influence some predictive models and cause them to place more emphasis on these predictors1. When the predictor is left in its original form, the extreme samples can end up degrading a model’s predictive performance.\nOne way to resolve skewness is to apply a transformation that makes the data more symmetric. There are several methods to do this. The first is to use a standard transformation, such as logarithmic or the square root, the latter being a better choice when the skewness is not drastic, and the data contains zeros. A simple visualization of the data can be enough to make this choice. The problem is when there are many numeric predictors; it may be inefficient to visually inspect each predictor to make a subjective judgment on what if any, transformation function to apply.\nBox and Cox (1964) defined a power family of transformations that use a single parameter, \\(\\lambda\\), for different methods:\n\n\n\n\n\nno transformation via \\(\\lambda = 1.0\\)\nsquare (\\(x^2\\)) via \\(\\lambda = 2.0\\)\nsquare root (\\(\\sqrt{x}\\)) via \\(\\lambda = 0.5\\)\n\n\n\nlogarithmic (\\(\\log{x}\\)) via \\(\\lambda = 0.0\\)\ninverse square root (\\(1/\\sqrt{x}\\)) via \\(\\lambda = -0.5\\)\ninverse (\\(1/x\\)) via \\(\\lambda = -1.0\\)\n\n\n\n\n\nand others in between. The transformed version of the variable is:\n\\[\nx^* =\n\\begin{cases} \\lambda^{-1}(x^\\lambda-1) & \\text{if $\\lambda \\ne 0$,}\n\\\\[3pt]\nlog(x) &\\text{if $\\lambda = 0$.}\n\\end{cases}\n\\]\nTheir paper defines this as a supervised transformation of a non-negative outcome (\\(y\\)) in a linear regression model. They find a value of \\(\\lambda\\) that minimizes the residual sums of squared errors. In our case, we can co-opt this method to use for unsupervised transformations of non-negative predictors (in a similar manner as Asar, Ilk, and Dag (2017)). Yeo and Johnson (2000) extend this method by allowing the data to be negative via a slightly different transformation:\n\\[\nx^* =\n\\begin{cases}\n\\lambda^{-1}\\left[(x + 1)^\\lambda-1\\right] & \\text{if $\\lambda \\ne 0$ and $x \\ge 0$,} \\\\[3pt]\nlog(x + 1) &\\text{if $\\lambda = 0$ and $x \\ge 0$.} \\\\[3pt]\n-(2 - \\lambda)^{-1}\\left[(-x + 1)^{2 - \\lambda}-1\\right] & \\text{if $\\lambda \\ne 2$ and $x &lt; 0$,} \\\\[3pt]\n-log(-x + 1) &\\text{if $\\lambda = 2$ and $x &lt; 0$.}\n\\end{cases}\n\\]\nIn either case, maximum likelihood is also used to estimate the \\(\\lambda\\) parameter.\nIn practice, these two transformations might be limited to predictors with acceptable density. For example, the transformation may not be appropriate for a predictor with a few unique values. A threshold of five or so unique values might be a proper rule of thumb (see the discussion in Section 13.4). On occasion the maximum likelihood estimates of \\(\\lambda\\) diverge to huge values; it is also sensible to use values within a suitable range. Also, the estimate will never be absolute zero. Implementations usually apply a log transformation when the \\(\\hat{\\lambda}\\) is within some range of zero (say between \\(\\pm 0.01\\))2.\nFor the lot area predictor, the Box-Cox and Yeo-Johnson techniques both produce an estimate of \\(\\hat{\\lambda} = 0.15\\). The results are shown in Figure 5.1 (panel b). There is undoubtedly less right-skew, and the data are more symmetric with a new skewness value of 0.114 (much closer to zero). However, there are still outlying points.\nThere are numerous other transformations that attempt to make the distribution of a variable more Gaussian. Table 5.1 shows several more, most of which are indexed by a transformation parameter \\(\\lambda\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nEquation\nSource\n\n\n\n\nModulus\n\\[x^* = \\begin{cases} sign(x)\\lambda^{-1}\\left[(|x|+1)^\\lambda-1\\right] & \\text{if $\\lambda \\neq 0$,}\\\\[3pt]\nsign(x) \\log{(|x|+1)} &\\text{if $\\lambda = 0$}\n\\end{cases}\\]\nJohn and Draper (1980)\n\n\nBickel-Docksum\n\\[x^* = \\lambda^{-1}\\left[sign(x)|x| - 1\\right]\\quad\\text{if $\\lambda \\neq 0$}\\]\nBickel and Doksum (1981)\n\n\nGlog / Gpower\n\\[x^* = \\begin{cases} \\lambda^{-1}\\left[({x+ \\sqrt{x^2+1}})^\\lambda-1\\right]  & \\text{if $\\lambda \\neq 0$,}\\\\[3pt]\n\\log({x+ \\sqrt{x^2+1}}) &\\text{if $\\lambda = 0$}\n\\end{cases}\\]\nDurbin et al. (2002), Kelmansky, Martínez, and Leiva (2013)\n\n\nNeglog\n\\[x^* = sign(x) \\log{(|x|+1)}\\]\nWhittaker, Whitehead, and Somers (2005)\n\n\nDual\n\\[x^* = (2\\lambda)^{-1}\\left[x^\\lambda - x^{-\\lambda}\\right]\\quad\\text{if $\\lambda \\neq 0$}\\]\nYang (2006)\n\n\n\n\n\nTable 5.1: Examples of other families of transformations for dense numeric predictors.\n\n\n\n\n\n\n\nSkewness can also be resolved using techniques related to distributional percentiles. A percentile is a value with a specific proportion of data below it. For example, for the original lot area data, the 0.1 percentile is 4,726 square feet, which means that 10% of the training set has lot areas less than 4,726 square feet. The minimum, median, and maximum are the 0, 50th and 100th percentiles, respectively.\nNumeric predictors can be converted to their percentiles, and these data, inherently between zero and one, are used in their place. Probability theory tells us that the distribution of the percentiles should resemble a uniform distribution. This results from the transformed version of the lot area shown in Figure 5.1 (panel c). For new data, values beyond the range of the original predictor data can be truncated to values of zero or one, as appropriate.\nAdditionally, the original predictor data can be coerced to a specific probability distribution. Peterson and Cavanaugh (2020) define the Ordered Quantile (ORQ) normalization procedure. It estimates a transformation of the data to emulate the true normalizing function where “normalization” literally maps the data to a standard normal distribution. In other words, we can coerce the original distribution to a near exact replica of a standard normal. Figure 5.1 (panel d) illustrates the result for the lot area. In this instance, the resulting distribution is precisely what would be seen if the true distribution was Gaussian with zero mean and a standard deviation of one.\nIn Section 5.4 below, another tool for attenuating outliers in groups of predictors is discussed.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-common-scale",
    "href": "chapters/numeric-predictors.html#sec-common-scale",
    "title": "5  Transforming Numeric Predictors",
    "section": "5.3 Standardizing to a Common Scale",
    "text": "5.3 Standardizing to a Common Scale\nAnother goal for transforming individual predictors is to convert them to a common scale. This is a pre-processing requirement for some models. For example, a K-nearest neighbors model computes the distances between data points. Suppose Euclidean distance is used with the Ames data. One predictor, the year a house was built, has training set values ranging between 1872 and 2010. Another, the number of bathrooms, ranges from 0 to 5. If these raw data were used to compute the distance, the value would be inappropriately dominated by the year variable simply because its values were large. See TODO appendix for a summary of which models require a common scale.\nThe previous section discussed two transformations that automatically convert predictors to a common distribution. The percentile transformation generates values roughly uniformly distributed on the [0, 1] scale, and the ORQ transformation results in predictors with standard normal distributions. However, two other standardization methods are commonly used.\nFirst is centering and scaling (as previously mentioned). To convert to a common scale, the mean (\\(\\bar{x}\\)) and standard deviation (\\(\\hat{s}\\)) are computed from the training data and the standardized version of the data are \\(x^* = (x - \\bar{x}) / \\hat{s}\\). The shape of the original distribution is preserved; only the location and scale are modified to be zero and one, respectively.\nIn Section 6.2, methods are discussed to convert categorical predictors to a numeric format. The standard tool is to create a set of columns consisting of zeros and ones called indicator or dummy variables. When centering and scaling, what should we do with these binary features? These should be treated the same as the dense numeric predictors. The result is that a binary column will still have two unique values, one positive and one negative. The values will depend on the prevalence of the zeros and ones in the training data. While this seems awkward, it is required to ensure each predictor has the same mean and standard deviation. Note that if the predictor set is only scaled, Gelman (2008) suggests that the indicator variables be divided by two standard deviations instead of one.\nFigure 5.2(b) shows the results of centering and scaling the gross living area predictor from the Ames data. Note that the shape of the distribution does not change; only the magnitude of the values is different.\n\n\n\n\n\n\n\n\nFigure 5.2: The original gross living area data and two standardized versions.\n\n\n\n\n\nAnother common approach is range standardization. Based on the training set, a predictor’s minimum and maximum values are computed, and the data are transformed to a [0, 1] scale via\n\\[\nx^* = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\]\nWhen new data are outside the training set range, they can either be clipped to zero/one or allowed to go slightly beyond the intended range. The nice feature of this approach is that the range of the raw numeric predictors matches the range of any indicator variables created from previously categorical predictors. However, this does not imply that the distributional properties are the same (e.g., mean and variance) across predictors. Whether this is an issue depends on the model being used downstream. Figure 5.2(c) shows the result when the gross living predictor is range transformed. Notice that the shape of the distributions across panels (a), (b), and (c) are the same — only the scale of the x-axis changes.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#sec-spatial-sign",
    "href": "chapters/numeric-predictors.html#sec-spatial-sign",
    "title": "5  Transforming Numeric Predictors",
    "section": "5.4 Spatial Sign",
    "text": "5.4 Spatial Sign\nSome transformations involve multiple predictors. An upcoming chapter describes a specific class of simultaneous feature extraction transformations. Here, we will focus on the spatial sign transformation (Serneels, Nolf, and Espen 2006). This method, which requires \\(p\\) standardized predictors as inputs, projects the data points onto a \\(p\\) dimensional unit hypersphere. This makes all of the data points equally distant from the center of the hypersphere, thereby eliminating all potential outliers. The equation is:\n\\[\nx^*_{ij}=\\frac{x_{ij}}{\\sum\\limits^{p}_{j=1} x_{ij}^2}\n\\]\nNotice that all of the predictors are simultaneously modified and that the calculations occur in a row-wise pattern. Because of this, the individual predictor columns become combinations of the other columns and now reflect more than the individual contribution of the original predictors. In other words, after this transformation is applied, if any individual predictor is considered important, its significance should be attributed to all of the predictors used in the transformation.\nFigure 5.3 shows predictors from the Ames data. In these data, we somewhat arbitrarily labeled 29 samples as being “far away” from most of the data in either lot area and/or gross living area. Each of these predictors may follow a right-skewed distribution, or there is some other characteristic that is associated with these samples. Regardless, we would like to transform these predictors simultaneously.\nThe second panel of the data shows the same predictors after an orderNorm transformation. Note that, after this operation, the outlying values appear less extreme.\n\n\n\n\n\n\n\n\nFigure 5.3: Lot area (x) versus gross living area (y) in raw format as well as with order-norm and spatial sign transformations.\n\n\n\n\n\nThe panel on the right shows the data after applying the spatial sign. The data now form a circle centered at (0, 0) where the previously flagged instances are no longer distributionally abnormal. The resulting bivariate distribution is quite jarring when compared to the original. However, these new versions of the predictors can still be important components in a machine-learning model.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#chapter-references",
    "href": "chapters/numeric-predictors.html#chapter-references",
    "title": "5  Transforming Numeric Predictors",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAsar, O, O Ilk, and O Dag. 2017. “Estimating Box-Cox Power Transformation Parameter via Goodness-of-Fit Tests.” Communications in Statistics-Simulation and Computation 46 (1): 91–105.\n\n\nBickel, P, and K Doksum. 1981. “An Analysis of Transformations Revisited.” Journal of the American Statistical Association 76 (374): 296–311.\n\n\nBox, GEP, and D Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society. Series B (Methodological), 211–52.\n\n\nCroarkin, C, P Tobias, J Filliben, B Hembree, W Guthrie, L Trutna, and J Prins, eds. 2012. NIST/SEMATECH e-Handbook of Statistical Methods. NIST/SEMATECH. http://www.itl.nist.gov/div898/handbook/.\n\n\nDurbin, B, J Hardin, D Hawkins, and D Rocke. 2002. “A Variance-Stabilizing Transformation for Gene-Expression Microarray Data.” Bioinformatics 18.\n\n\nGelman, A. 2008. “Scaling Regression Inputs by Dividing by Two Standard Deviations.” Statistics in Medicine 27 (15): 2865–73.\n\n\nJohn, J, and N Draper. 1980. “An Alternative Family of Transformations.” Journal of the Royal Statistical Society Series C: Applied Statistics 29 (2): 190–97.\n\n\nKelmansky, D, E Martínez, and V Leiva. 2013. “A New Variance Stabilizing Transformation for Gene Expression Data Analysis.” Statistical Applications in Genetics and Molecular Biology 12 (6): 653–66.\n\n\nPeterson, R, and J Cavanaugh. 2020. “Ordered Quantile Normalization: A Semiparametric Transformation Built for the Cross-Validation Era.” Journal of Applied Statistics 47 (13-15): 2312–27.\n\n\nSerneels, S, E De Nolf, and P Van Espen. 2006. “Spatial Sign Preprocessing: A Simple Way to Impart Moderate Robustness to Multivariate Estimators.” Journal of Chemical Information and Modeling 46 (3): 1402–9.\n\n\nWhittaker, J, C Whitehead, and M Somers. 2005. “The Neglog Transformation and Quantile Regression for the Analysis of a Large Credit Scoring Database.” Journal of the Royal Statistical Society Series C: Applied Statistics 54 (5): 863–78.\n\n\nYang, Z. 2006. “A Modified Family of Power Transformations.” Economics Letters 92 (1): 14–19.\n\n\nYeo, I, and R Johnson. 2000. “A New Family of Power Transformations to Improve Normality or Symmetry.” Biometrika 87 (4): 954–59.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/numeric-predictors.html#footnotes",
    "href": "chapters/numeric-predictors.html#footnotes",
    "title": "5  Transforming Numeric Predictors",
    "section": "",
    "text": "The field of robust techniques is predicated on making statistical calculations insensitive to these types of data points.↩︎\nIf you’ve never seen it, the “hat” notation (e.g. \\(\\hat{\\lambda}\\)) indicates an estimate of some unknown parameter.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming Numeric Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html",
    "href": "chapters/categorical-predictors.html",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "6.1 Example Data: Daily Hotel Rates\nThis chapter will discuss tools for handling predictor variables that are not measured on a strict numeric scale. For example, the building type and neighborhood variables are qualitative in the Ames housing data. The possible values of qualitative predictors are often called categories or levels (which will be used interchangeably here).\nWhy would we need to modify this type of predictor? First, most mathematical models require information to be numbers. We’ll need a method to convert qualitative data effectively to some quantitative format. This is an encoding problem; we are finding another representation of our predictor data. There are various tools for converting qualitative data, ranging from simple indicator variables (a.k.a. dummy variables) to more statistically sophisticated tools. These are the focus of this chapter.\nSecond, there are other circumstances where simplifying or transforming categorical predictors might be advantageous. These are usually related to exceptional situations, such as when there are many possible values (i.e., high cardinality) or infrequently occurring values. Techniques to address these issues are also discussed.\nAs implied above, some modeling techniques can take the categorical predictors in their natural form. These include tree- or rule-based models, naive Bayes models, and others. In this book’s third and fourth parts, we’ll discuss how certain models naturally handle categorical variables in their respective sections.\nTo begin, the primary data set used in this chapter is introduced.\nAntonio, de Almeida, and Nunes (2019) describe a data set with booking records from two Portuguese hotels. There are numerous variables in the data related to the\nIn addition, there are several qualitative variables: customer type, market segment, room type, etc.\nWe’ll analyze these data in various ways (especially in ?sec-reg-linear to ?sec-reg-summary where they are used as a case study). Our goal is to predict the average daily rate (ADR, i.e., the average cost in euros) from one of the two hotels in the original data.\nThere were 15,402 bookings for the resort hotel. An initial two-way split was used:\nThis chapter will use these data to demonstrate how qualitative data, such as the booking agent’s name or company, can be converted into quantitative information that many models require.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-hotel-rates",
    "href": "chapters/categorical-predictors.html#sec-hotel-rates",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "Reservation: room type, date, meal plan, etc.,\nBooking method: agent, distribution channel, etc.,\nCustomer: type, number of children staying, country of origin, etc.\n\n\n\n\n\nThe training set contains 11,551 (75%) bookings whose arrivals are between 2016-07-02 and 2017-05-15.\nThe remaining 25% were used as the test set with 3,851 bookings (between 2017-05-15 to 2017-08-31).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-indicators",
    "href": "chapters/categorical-predictors.html#sec-indicators",
    "title": "6  Working with Categorical Predictors",
    "section": "6.2 Simple Indicator Variables",
    "text": "6.2 Simple Indicator Variables\nAs a simple example, consider the customer type predictor with categories: “contract”, “group”, “transient”, and “transient (party)”. What would we do if a model required these data to be numeric? There is no natural ordering to these values, so giving the models integer values (e.g., contract = 1.0, group = 2.0, etc.) would be erroneous since that implies a scale for these values. Instead, it is common to create multiple binary columns (numeric with zeros or ones) representing the occurrence of specific values. These are called indicator columns or sometimes dummy variables.\nTable 6.1 shows how this works for the customer type. The rows depict the possible values in the data, while the columns are the resulting features used in place of the original column. This table uses the most common indicator encoding method called reference cell parameterization (also called a treatment contrast). First, a category is chosen as the reference value. In Table 6.1, the first alpha-numeric value is used (\"contract\"), but this is an arbitrary choice. After this, we create separate columns for all possible values except for the reference value. Each of these columns has a value of one when the data matches the column for that value (and is zero otherwise).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer type\n\nIndicator Columns\n\n\n\ngroup\ntransient\ntransient party\n\n\n\n\ncontract\n0\n0\n0\n\n\ngroup\n1\n0\n0\n\n\ntransient\n0\n1\n0\n\n\ntransient party\n0\n0\n1\n\n\n\n\n\n\n\n\nTable 6.1: Indicator columns produced from a categorical column using a reference cell parameterization.\n\n\n\n\nThe rationale for excluding one column is that you can infer the reference value if you know the values of all of the existing indicator columns1. Including all possible indicator columns embeds a redundancy in the data. As we will see shortly, data that contain this type of redundancy pose problems for some models like linear regression.\nBefore moving on, let’s examine how indicator columns shown in Table 6.1 affect an example data analysis. Suppose that we are modeling a numeric outcome using a linear regression model (discussed in ?sec-reg-linear) of the form:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_4x_{i4} + \\epsilon_i\n\\tag{6.1}\\]\nwhere the \\(\\beta\\) values are the parameters to be estimated and, in this application, the \\(x_{ij}\\) are the indicator columns shown in Table 6.1. The intercept term (\\(\\beta_0\\)) corresponds to a column of all ones. It is almost always the case that we include the intercept term in the model.\nTable 6.2 shows the encoding in Table 6.1 and adds columns for a numeric outcome and the intercept term. The outcome column shows the average daily rate (in €). Using a standard estimation procedure (called ordinary least squares), the bottom row of Table 6.2 shows the 4 parameter estimates. Since all of the indicators for the contract customer row are zero, the intercept column estimates the mean value for that level (\\(\\widehat{\\beta}_0\\) = 73). The variable \\(x_{i1}\\) only has an indicator for the “group” customers. Hence, its estimate corresponds to the difference in the average group outcome values (63.8) minus the effect of the reference cell: 73 - 63.8. From this, the resulting estimate (\\(\\widehat{\\beta}_1\\) = -9.23) is the effect of the group customers above and beyond the impact of the contract customers. The parameter estimates for the other possible values follow analogous interpretations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADR\n\nModel Terms\n\n\n\n(Intercept)\ngroup\ntransient\ntransient_party\n\n\n\n\ncontract\n73.0\n1\n0\n0\n0\n\n\ngroup\n63.8\n1\n1\n0\n0\n\n\ntransient\n94.6\n1\n0\n1\n0\n\n\ntransient party\n76.4\n1\n0\n0\n1\n\n\nestimate (€):\n\n\n73.0\n-9.2\n21.6\n3.4\n\n\n\n\n\n\n\n\nTable 6.2: An example of linear regression parameter estimates corresponding to a reference cell parameterization.\n\n\n\n\nAnother popular method for making indicator variables is called one-hot encoding (also known as a cell means encoding). This technique, shown in Table 6.3, makes indicators for all possible levels of the predictor and does not show an intercept column (for reasons described shortly). In this model parameterization, indicators are specific to each value in the data, and the linear regression estimates are the average response values for each customer type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADR\n\nModel Terms\n\n\n\ncontract\ngroup\ntransient\ntransient_party\n\n\n\n\ncontract\n73.0\n1\n0\n0\n0\n\n\ngroup\n63.8\n0\n1\n0\n0\n\n\ntransient\n94.6\n0\n0\n1\n0\n\n\ntransient party\n76.4\n0\n0\n0\n1\n\n\nestimate (€):\n\n\n73.0\n63.8\n94.6\n76.4\n\n\n\n\n\n\n\n\nTable 6.3: One-hot encoded indicator variables from a categorical column of data.\n\n\n\n\nOne-hot encodings are often used in nonlinear models, especially in neural networks and tree-based models. Indicators are not generally required for the latter but can be used2.\nOne issue with creating indicator variables from categorical predictors is the potential for linear dependencies. This occurs when two or more sets of predictors have a mathematically linear relationship with one another. For example, suppose that each agent only worked with a single customer type. If we were to create indicator variables for the agent names and the customer type, there would be a redundancy: if we know the agent, we know the customer type. Mathematically, the row-wise sum of the agent name indicators would equal the row-wise sum of the position indicators. For some types of predictive models, including linear or logistic regression, the linear dependencies would cause the mathematical estimation of the model parameters to fail.\nAn example of this is the one-hot encoding method shown in Table 6.3. If we were to include an intercept term with a one-hot encoding, the row-wise sum of the indicator columns equals the intercept column. The implication is that one of the model terms is not “estimable.” For this reason, this encoding method is mainly used with models that do not use specific linear algebra operations (e.g., matrix inversion) to estimate parameters. Some tools can identify linear combinations (Elswick et al. 1991), and from this information, the smallest possible set of columns can be found to delete and thereby eliminate the linear dependencies.\nNote that linear dependencies are exact linear relationships between predictors. There are other circumstances where two or more predictors are highly correlated but not perfectly related. This situation, called multicollinearity, is discussed throughout the book but mostly in ?sec-colinearity. Multicollinearity can be a problem for many models and needs to be resolved prior to modeling.\nIn addition to reference cell and one-hot encodings, there are various tools for creating indicator columns. Some, called contrast methods, are associated with more niche strategies, and many produce fractional values instead of binary indicators. For example, Helmert contrasts (Ruberg 1989) are configured so that each category is compared to the average of the preceding categories. No encoding method is uniformly better than the others; the choice is often driven by how the user will interpret the parameter estimates.\nThe situation becomes slightly more complex when a model contains multiple categorical predictors. The reference value combines levels across predictors for reference value parameterizations. For example, if a model included the customer’s country of residence, the reference would include that factor. Suppose Canadians were the reference nationality. Then the overall reference cell would be Canadian contract customers.\nNote that these encoding techniques are basically look-up tables that map individual categories known in the training set to specific numeric values. As discussed in the next section, mapping categories can be problematic when novel values are seen in new data (such as the test set).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-novel-categories",
    "href": "chapters/categorical-predictors.html#sec-novel-categories",
    "title": "6  Working with Categorical Predictors",
    "section": "6.3 Novel Categories",
    "text": "6.3 Novel Categories\nWhen creating indicator variables, there is the assumption that the categories in the training data reflect all possible categories. This may not be the case. For example, the agents observed in our data are not the only agents that will be booking rooms in the future. After we create a model with a specific set of agents, what happens when we predict future data that involves new agents?\nOne approach we could take is to prospectively create a new value for the predictor (let’s give it a value of \"new\"). When a future sample has an agent that was not in the original data, we could assign this sample to the general category so that the model can anticipate this new level. In general, this won’t help since the model estimation process will never have any “new” values on hand when the model is fit. For some models, this is innocuous and has no real harm (but offers no benefit either).\nLet’s revisit linear regression results from the reference cell parameterization again. Suppose there is a new customer type of “influencer.” Our reference cell encoding method could add an indicator column for that customer type, but since no influencers currently exist in the training data, it will be a column filled with zeros. We can call this a zero-variance predictor since it has a single unique value. It is entirely non-informative. For linear regression, this column of zeros creates a linear dependency with the intercept column because all rows have the same numeric value. Most regression software would detect zero-variance columns and remove them before the model is fit.\nSince there would be no influencer column in the data, all of the indicator columns would be zero when an influencer was observed in new data being predicted. However, from Table 6.2 we know that such a pattern would map to the reference level (which is the \"contract\" customer in our data). The new levels would be mapped to the reference level for this parameterization. This would occur more by happenstance and would not be a deliberate strategy.\nTraditional indicator encoding methods should be avoided if new predictor categories are anticipated. Instead, the techniques used in the next section (effect encodings or feature hashing) will likely be more effective at dealing with this situation.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-high-cardinality",
    "href": "chapters/categorical-predictors.html#sec-high-cardinality",
    "title": "6  Working with Categorical Predictors",
    "section": "6.4 Predictors with Many Values",
    "text": "6.4 Predictors with Many Values\nSome predictors have high cardinality; they can take many possible values relative to the number of data points. For example, in the entirety of the hotel rate data, there are 122 unique agents (see Figure 6.1). Should we create so many indicator variables for this predictor? In the data, 49 agents had fewer than six bookings. This means there will be many sparse columns with almost all zero entries. Additionally, 71 of the agents have no data in the training set; they only occur in the test set. Their indicator columns would contain all zeros.\n\n\n\n\n\n\n\n\nFigure 6.1: The frequency distribution of agents in the training set.\n\n\n\n\n\nThis section will consider several methods for effectively representing high cardinality predictors. The first two techniques are unsupervised (i.e., they do not use the outcome), while the others are supervised. Cerda and Varoquaux (2022) surveys different methods for dealing with these types of predictors.\n\n6.4.1 “Othering”\nIt is possible to isolate infrequently occurring categories into a new “other” category. For example, there are 49 agents with fewer bookings than 0.05% of the training set. It may make sense to group these agents into their own categories as long as the predictive ability of the variable is maintained. There is no magic number for the training set frequency cutoff that decides which categories to pool. This pre-processing value requires optimization since each data set will have its own nuances with infrequent categories.\n\n\n\n6.4.2 Feature Hashing\nFeature hashing (Weinberger et al. 2009) is a technique where the user specifies how many indicator columns should be used to represent the data. Using cryptographic principles, an algorithm assigns each row to an indicator column. This assignment only uses the value of the category in the calculations and, unlike the traditional methods discussed in Section 6.2, does not require a dictionary that maps pre-defined categories to indicator columns.\nSuppose our agent column had a value of \"nobody owens\". The algorithm translates this value to a fixed-length string using a hash function. Hash functions are designed to be non-random and, effectively, irreversible. Once the original value is converted to the hash value, it is very difficult to reverse engineer. Also, the frequency distributions of the hash values are designed to appear uniform in distribution, as seen below.\nThe hashed value of the predictor’s level can be converted to an integer, and, using modular arithmetic, the integer is mapped to a feature column. Suppose that we create 256 hashing features and nobody owens maps to an integer of 123,456. To assign this agent to one of 256 feature columns, we compute 123456 mod 256 = 64. For this row of data, we put a value of one in column 64.\nPresumably, the number of possible categories is larger than the number of indicator columns we request. This means multiple predictor values will map to the same feature column. In cryptography, this is called a collision, the same as aliasing in statistical experimental design (Box, Hunter, and Hunter 1978). There is a trade-off between the number of feature columns and the number of collisions. As the number of columns increases, the probability of collisions decreases. However, the time required to train a model increases as the number of columns increases. Also, it is entirely possible that there are predictor columns where no corresponding categories, yielding all zeros.\nTo reduce collisions, the hashing function can produce integers that can be both positive and negative. When assigning the integer to a feature column, we can assign a value of -1 when the hashed integer value is negative. In this way, columns can have values of -1, 0, or 1. This means fewer collisions.\nTable 6.4 shows the results for several agents3 when ten signed feature hash columns are requested. The last row of the table shows the rate of non-zero values in each column. The average density is close to 10%, demonstrating that the hash function produces frequencies that emulate a uniform distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgent\n\nFeatures from Hashing\n\n\n\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n\n\n\n\nAaron Marquez\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n\n\nAlexander Drake\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n\n\nAllen Her\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n\n\nAnas El Bashir\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n\n\n:\n:\n:\n:\n:\n:\n:\n:\n:\n:\n:\n\n\nYa Eesh El Sadiq\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nYoonus Al Sultana\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nYoujia Tsuchiya\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nZachary Leedholm\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nDensity (%)\n10.7\n13.1\n7.4\n13.1\n7.4\n6.6\n9.8\n10.7\n9.8\n11.5\n\n\n\n\n\n\n\n\nTable 6.4: Signed indicators for agent via feature hashing.\n\n\n\n\nThe main downside of this method is that the use of hash values makes it impossible to explain the model. If the tenth feature column is critical, we can’t explain why this is the case for new data (since the hash function is practically non-reversible and may include collisions). This may be fine if the primary objective is prediction rather than interpretation. When the goal is to optimize predictive performance, then the number of hashing columns to use can be included as a tuning parameter. The model tuning process can then determine an optimal value of the number of hashing columns.\nOne unanticipated side effect of using feature hashing is that it might remove linear dependencies with other columns. This is because hashing distributes the predictor levels to hashing columns roughly uniformly. There is a high probability that this eliminates any existing relationship between other predictor values.\n\n\n\n6.4.3 Effect Encodings\nWe can also use a different supervised tool called effect encoding4 (Micci-Barreca 2001; Pargent et al. 2022). Recall the discussion of one-hot encodings for indicator variables. Table 6.3 showed that the model terms for such an encoding estimate the cell means for each category (when the outcome is numeric). For example, the effect on the outcome for contract customers in that table was 73€.\nEffect encodings use a simple, separate model to estimate the effect of each predictor category on the outcome and then use that value in place of the original data. For the example in Table 6.3, the rows corresponding to the group customers would be replaced with 63.8, transient customers with 94.6, and so on. Instead of making multiple binary columns out of the categorical predictor, a single numeric column would take its place.\nThis may seem somewhat problematic though; we are using the training data to estimate the effect of a predictor, putting the resulting effects back into the data, and using our primary model to (re)estimate the effect of the predictor. Since the same data are used, is this a circular argument? Also, there should be a worry about data quality. As previously mentioned, the number of bookings per agent can vary greatly. For example:\n\nAgent Chelsea Wilson had a single booking with an associated ADR of 193.6€. Using this single data point as the effect estimate is unsatisfactory since we suspect that the estimated ADR would not stay the same if the number of total bookings were larger5.\nAgent Alexander Drake had 967 bookings with a simple mean ADR estimate of 120.3€. We would judge the quality of this agent’s data to be better than cases with a handful of bookings.\n\nHow well can we estimate an effect if there are only one or two bookings for an agent in the data set?\nBoth of these concerns can be addressed by how we estimate the effect of the predictor. Recall that Table 6.3 showed how simple linear regression was used to calculate the mean for each customer type. The way that the indicator variables impact the parameter estimates is such that the rows for each customer type are the only data used to estimate that customer type’s effect. For example, the contract data do not directly affect the parameter estimate for the transient customers, and so on. This is often called a no pooling method of estimating parameters. In this context, “pooling” means combining data across multiple categories to estimate parameters.\nWe could compute the naive ADR estimate using sample means and produce a similar table. However, we can create better mean estimates for each agent by considering the data from all agents using a technique called partial pooling (Gelman et al. 2013; Johnson, Ott, and Dogucu 2022, chap. 15).\n\nIn the following two sub-sections, we’ll describe this technique in detail for cases where the outcome is a numeric value assumed to be normally distributed or or non-Gaussian. These sections will rely more on probability theory and statistical details than most others.\n\n\nNumeric Outcomes\nFor our ADR analysis, the outcome is a quantitative value. A linear regression model (see ?sec-linear-regression) is the most commonly used approach to model this type of outcome. Linear regression models the outcome as a function of a slope and intercept model. For our example, where we only consider estimating the effect of the agents using indicator columns, the model is:\n\\[\ny_{ij} =\\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_{121}x_{i 121} + \\epsilon_{ij} = \\mu_i  + \\epsilon_{ij}\n\\]\nwhere \\(y_{ij}\\) is the outcome (in this case, the ADR of booking \\(i\\) by agent \\(j\\)), \\(x_{ij}\\) is an indicator for reservation \\(i\\) from agent \\(j\\), \\(\\beta_0\\) is the estimated outcome of the reference cell, and we assume \\(\\epsilon_i \\sim N(0, \\sigma^2_\\epsilon)\\). This leads to the assumption that \\(y_{ij} \\sim N(\\mu_i, \\sigma^2_\\epsilon)\\). The \\(\\hat{\\beta}_{j}\\) parameters are the effect of the agents above and beyond the reference cell effect. The index \\(j\\) is from 1 to 121 since there were 122 agents in the training set with at least one booking.\nAgain, this approach is called no pooling since it only uses each agent’s data to estimate their effect. It treats the agent as a known set of values and cannot generalize to agents outside of those in the training set6.\nPartial pooling takes the viewpoint that the agents in the training set are a sample of a population of agents. From this, we might posit a probability distribution should be assigned to the agent-to-agent effects. Since the range of the \\(\\beta\\) parameters is the real line, we might propose a random intercept model:\n\\[\ny_{ij} = { \\color{darkseagreen} \\underbrace{\\color{black} (\\beta_0 + \\beta_{0j})}_{\\text{intercept(s)}} }  + \\epsilon_{ij} = \\mu_j + \\epsilon_{ij}\n\\tag{6.2}\\]\nwhere the \\(\\beta_{0j}\\) are per-agent differences from the population effect (\\(\\beta_{0}\\)) (\\(j = 1 \\ldots 122\\) this time). Note that the model has no \\(x_{ij}\\) terms. This is simply a notation difference between the models since most methods that estimate this model do not treat that variable as a traditional regression covariate.\nSeveral techniques can fit this model. For a linear mixed model approach to fitting a random intercept model (Laird and Ware 1982; Demidenko 2013), we assume that the model parameters have normal distributions with \\(\\beta_{0} \\sim N(\\beta, \\sigma^2)\\) and \\(\\beta_{0j} \\sim N(0, \\sigma_0^2)\\) and that residuals (\\(\\epsilon_{ij}\\)) follow a Gaussian distribution. When used in Bayesian analysis7, such distributions on parameters are referred to as prior distributions. We’ll stick with this terminology for Bayesian and non-Bayesian techniques. There are many strategies for estimating this model type, including maximum likelihood, restricted maximum likelihood, or Bayesian methods. See Stroup (2012) or Gelman et al. (2013) for more specifics.\nHowever, we might gain some intuition if we pause and consider a less complex version of this model. For simplicity, we’ll also make the completely unrealistic assumptions that all agents had the same number of bookings (\\(n_j = n\\)) and that we know that \\(\\sigma^2_\\epsilon = 1\\). From this, we can also say that sample means of the outcome data for each category are also normal with \\(\\bar{y}_{j} \\sim N(\\mu_i, 1/n)\\). These assumptions greatly simplify the math required to produce a formula for the estimates of the per-category means:\n\\[\n\\hat{y}_j = \\mu_0 + (\\bar{y}_j - \\mu_0) \\frac{\\sigma^2_0}{\\frac{1}{n} + \\sigma^2_0}\n\\tag{6.3}\\]\nThere are a few aspects of this equation8 to note. First, the effect for a specific category is estimated by a combination of the sample mean for each category (\\(\\bar{y}_j\\)) as well as our prior mean (\\(\\mu_0\\)). The difference demonstrates the possible shrinkage towards the prior mean.\nAlso, the shrinkage coefficient depends on \\(\\sigma^2_0\\) and the amount of data in the category. Suppose we choose a non-informative prior by making it extremely wide9 via \\(\\sigma^2_0 = 100\\). No matter how much data are observed in the category, the fraction will be close to 1, and our partial pooling estimate is essentially \\(\\bar{y}_j\\). If we had a highly opinionated prior10, \\(\\sigma^2_0 = 1\\), the weighting factor becomes \\(n / (n + 1)\\). For \\(n = 1\\), the partial pooling estimate has an even share of the sample mean and the prior mean. The effect of the prior mean decreases as \\(n\\) increases.\nThese example computations assumed that each category has the same number of rows in the training set (i.e., \\(n_j = n\\)). In practice, this is unrealistic. However, the insights into how the sample size changes the amount of shrinkage demonstrate the previous idea of how “poor data” can affect the estimates with partial pooling.\nWhile Equation 6.3 helps illustrate how partial pooling works, we should understand that the specific results hinge on knowing \\(\\sigma^2_\\epsilon = 1\\). We would seldom make such a critical assumption in practice.\nWe’ll use the non-Bayesian mixed model approach to estimate effects for our data since it is more computationally efficient. Using the training set data, maximum likelihood estimation produced \\(\\hat{\\beta}_0\\) = 76.9. Figure 6.2(a) shows the distribution of the 122 random intercept estimates \\(\\hat{\\beta}_{0j}\\). The estimated standard deviation of this distribution was \\(\\hat{\\sigma_0}\\) = 22.9€. Even though we assume that the prior distribution of the individual effects was symmetric, the histogram of the estimates shows a lack of symmetry. This may be due to a few outliers or indicate that our prior should not be symmetric.\n\n\n\n\n\n\n\n\nFigure 6.2: The effect encoding results for the hotel rate data. Panel (a) shows the distribution of the estimated per-agent effects. Panel (b) compares the two estimation methods for all unique data patterns in the training set.\n\n\n\n\n\nThe benefit of using this model, called a hierarchical model, is that it views the per-agent effects as a collection of values from a distribution. The mathematical consequence of this formulation is that the resulting parameter estimates use the data from all agents to estimate each agent’s estimates (hence the use of the phrase “partial pooling”). The intuition for that statistical method applies here: the estimates for agents with “poor data quality” have their results shrunken towards the overall population mean across all agents. Agents with “better” data are shrunken less during estimation. What does “poor data quality” mean? It primarily means high variance, which is usually a function of sample size and some baseline, the irreducible noise level in the system.\nRecall the previous discussion regarding agents Chelsea Wilson and Alexander Drake. Using the partial pooling approach:\n\nAgent Chelsea Wilson: partial pooling shrinks the single value (193.6€) back towards the estimated population mean (76.9€) to produce a more modest estimated effect of 95.3€. Because there is a single value for this agent, the prior distribution greatly influences the final estimate.\nAgent Alexander Drake: partial pooling only slightly adjusts the original sample mean (120.3€) to the final estimate (120.1€) since there are more data available. The prior has almost no influence on this estimate since the agent had 967 bookings in the training set.\n\nThe amount of shrinkage was driven mainly by the number of bookings per agent. Figure 6.2(b) shows the collection of effect estimates for each unique data pattern in the training set. The dotted line indicates the overall ADR sample mean, and the size of the points indicates the total number of bookings by an agent. The distribution of the shrunken effects (y-axis) is significantly attenuated compared to the no pooling effects estimates (x-axis). However, note that the points with large numbers of bookings line up along the diagonal green line (i.e., minimal shrinkage to the center); regardless of the value of their agent-specific ADR estimate (\\(\\bar{y}_j\\)).\nTo reiterate how these values are used for pre-processing this type of predictor, Table 6.5 shows the linear mixed model analysis results. The numeric column is our primary model’s data for representing the agent names. This avoids creating a large number of indicator variables for this predictor.\n\n\n\n\n\n\n\n\n\n\n\nAgent\nEffect Estimate (€)\n\n\n\n\nAaron Marquez\n92.2\n\n\nAlexander Drake\n120.1\n\n\nAllen Her\n73.7\n\n\n:\n:\n\n\nYoonus Al Sultana\n58.8\n\n\nYoujia Tsuchiya\n74.8\n\n\nZachary Leedholm\n85.6\n\n\n\n\n\n\n\n\nTable 6.5: Examples of the numeric values that are used in place of each agent’s data when effect encodings are used.\n\n\n\n\nAs an alternative to generalized linear mixed models to estimate the effects, a more general approach uses Bayesian analysis (Kruschke 2014; McElreath 2015). Here, we give all of the model parameters prior probability distributions, which reflect our prior knowledge of the parameters’ shape and range. “Priors” can be pretty vague or highly opinionated, depending on what we feel we know about the situation11. For example, to continue with the random intercept formulation, we might specify the following:\n\\[\\begin{align}\n\\beta_0 &\\sim N(0, \\sigma_0^2) \\notag \\\\\n\\beta_{0j} &\\sim t(2) \\notag \\\\\n\\sigma_0 &\\sim U(0, \\infty)\n\\end{align}\\]\nThe uniform prior in the standard deviation is incredibly vague and is considered a non-informative prior. Conversely, the t distribution (with two degrees of freedom) for the random effects reflects that we might expect that the distribution is symmetric but has heavy tails (e.g., potential outliers such as those seen in Figure 6.2(a)). Priors can be vague or highly opinionated.\nWhen estimating the Bayesian model, the prior distributions are combined with the observed data to produce a posterior distribution for each parameter. Like the previous mixed-model technique, the posterior mimics the prior when there is very little (or noisy) data. The posterior distribution favors the observed results when high-quality data are abundant. Using Bayesian methods to estimate the effects enables much more versatility in specifying the model but is often more computationally intensive.\n\n\nNon-Gaussian Outcomes\nIn many of these cases, effect encodings can be computed using Bayesian methods or via the generalized linear mixed model approach (Bolker et al. 2009; Stroup 2012) approach fitting a random intercept model.\nThe latter model has a similar right-hand side as Equation 6.2. For example, when the outcome (\\(y_{ij}\\)) is a count, the standard Poisson model would be\n\\[\n\\log(\\mu_{ij}) = \\beta_0 + \\beta_{0j}\n\\] where \\(\\mu_{ij}\\) is the theoretical effect in the original units.\nSuppose the data are binomial proportions or Bernoulli (i.e., binary 0/1 values). The canonical effects model is:\n\\[\n\\log\\left(\\frac{\\pi_{ij}}{1-\\pi_{ij}}\\right) = \\beta_0 + \\beta_{0j}\n\\] with \\(\\pi_{ij}\\) being the parameter value for the effects. This is a basic logistic regression formulation of the problem.\nSimilar Bayesian approaches can be used. However, a completely different technique for binary outcomes is to assume again that the original data are binomial with rate parameter \\(\\pi\\) but, instead of the logistic regression formulation, assume that the actual rates come from a Beta distribution with parameters \\(\\alpha\\) and \\(\\gamma\\). With the Beta distribution as the prior, users can specify distributions with different shapes, such as uniform, bell-shaped, or “U” shaped, depending on the values chosen for \\(\\alpha\\) and \\(\\gamma\\). See Chapter 3 of Johnson, Ott, and Dogucu (2022). The Bayesian approach can be very flexible.\n\n\nConsequences of using Effect Encodings\nAs with feature hashing, effect encoding methods can naturally handle novel categories in future data. Without knowing anything about the new category, the most logical choice for this method is to assign the overall population effect (e.g., the mode of the posterior distribution) as the value.\nAs with feature hashing, effect encodings can eliminate linear dependencies between sets of indicators. Since the categorical data are being replaced with numeric summaries of the outcome data, it may be likely that there is no longer a redundancy in the data. Indicator columns reflect the identities of the predictor categories, while the effect encoding column measures the effectiveness of the categories. For example, a hypothetical scenario was mentioned where the agent was aliased with the customer type, yielding a linear dependency. Once the agent name column is converted to effect estimates, there would no longer be a relationship between the two predictors.\nLastly, applying effect encodings requires a robust data usage strategy. The risk of embedding over-fitted statistics into the model could cause problems that may not be observed until a new data set is predicted (see Chapter 9). Using resampling methods or a validation set (Chapter 10) is critical for using this tool. Also, when there is a temporal aspect to the data, we should try to ensure that historical data are not being predicted with future data (i.e., information leakage). For example, when an agent has multiple bookings, we kept the oldest data for an agent in the training set and newer bookings in the test set. Simple random sampling may result in the opposite occurring.\n\n\n\n\n6.4.4 Supervised Combining of Categories\nAnother approach for dealing with predictors with large numbers of categories is to collapse them into smaller groups that similarly affect the outcome data. For example, a cluster of agents may have similar performance characteristics (i.e., mean ADRs). If we can derive clusters of categories from the training set, a smaller group of cluster-level indicators might be a better approach than many indicator columns.\nThis approach closely mirrors the splitting process in tree-based models (discussed in Sections ?sec-cart-cls and ?sec-cart-reg), and we can use these models to find suitable groupings of predictor values. In summary, the categories are ordered by their estimated effects and then partitioned into two groups by optimizing some performance measures. For classification models, this is usually a measure of the purity of the classes in the resulting partition12. A measure of error is used for numeric outcomes, such as RMSE. Once the initial partition is made, the process repeats recursively within each division until no more splits can be made, perhaps due to insufficient data.\nFor the agent data, Figure 6.3 shows this process. The 122 agent are categorized into 12 groups. The top panel shows the ordered mean ADR values for each agent. The vertical lines indicate how the different agents are grouped at each step of the splitting process. The bottom panel shows a scaled estimate of the RMSE that uses the group-level mean ADR to predict the individual outcome results.\n\n\n\n\n\n\n\n\nFigure 6.3: The progression of using a tree-based method to collapse predictor categories into smaller sets.\n\n\n\n\n\nFor these data, there is no rational grouping or clustering of the agents (regarding their ADR) for the tree-based model to discover. The process produces increasingly granular groups, and the stopping point is somewhat arbitrary. For these reasons, it is probably not the best approach for this predictor in this particular data set.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#sec-ordinal-encodings",
    "href": "chapters/categorical-predictors.html#sec-ordinal-encodings",
    "title": "6  Working with Categorical Predictors",
    "section": "6.5 Encodings for Ordinal Predictors",
    "text": "6.5 Encodings for Ordinal Predictors\nIn some cases, a categorical predictor will have levels with an inherent order. For example, in the Ames data, some house amenities were ranked in terms of quality with values ‘excellent’, ‘fair’, ‘good’, ‘typical’, and ‘poor’. These values do not imply a specific numerical translation (e.g., ‘excellent’ = 2 times ‘poor’). For this reason, there are different options for converting them to numeric features.\nFirst, the analysis could treat them as unordered categories and allow the model to try to determine how the values relate to the outcome. For example, if there is the belief that, as the quality of the heating system improves, the house price should increase. Using unordered categories would be agnostic to this assumption and the model would estimate whatever trend it saw in the data. This approach would essentially be treating the quality variable as a categorical variable with 5 levels.\nAnother choice is to use domain-specific knowledge to imply relationships between levels. For example, the quality values could be replaced with integers one through five. This does bake the assumed relationship into the features, and that is advantageous when the assumed pattern is correct.\nThere are also contrast methods, similar to the one discussed in Section 6.2, that can automatically create potential patterns using polynomial expansions (more on these in Section 8.2). For our quality example, there are five possible levels. Mathematically, we can create up to the number of unique levels (minus one) polynomial function. In this example, linear, quadratic, cubic, and quartic (\\(x^4\\)) polynomial functions can be used. Figure 6.4 shows these patterns using orthogonal polynomial values13.\n\n\n\n\n\n\n\n\nFigure 6.4: Polynomial contrasts for an ordered categorical predictor with five possible levels.\n\n\n\n\n\nIn many applications, higher-degree polynomials (&gt; 2) are unlikely to be effective predictors. For example, the cubic pattern in Figure 6.4 (“Feature 3”) implies that houses in Ames with ‘excellent’ quality heating systems would have lower sale prices than those with ‘fair’ or ‘poor’ systems. Utilizing higher order polynomials usually requires some specific knowledge about the problem at hand.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#chapter-references",
    "href": "chapters/categorical-predictors.html#chapter-references",
    "title": "6  Working with Categorical Predictors",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAntonio, N, A de Almeida, and L Nunes. 2019. “Hotel Booking Demand Datasets.” Data in Brief 22: 41–49.\n\n\nBolker, B, M Brooks, C Clark, S Geange, J Poulsen, H Stevens, and JS White. 2009. “Generalized Linear Mixed Models: A Practical Guide for Ecology and Evolution.” Trends in Ecology and Evolution 24 (3): 127–35.\n\n\nBox, GEP, W Hunter, and J. Hunter. 1978. Statistics for Experimenters. New York: Wiley.\n\n\nCerda, P, and G Varoquaux. 2022. “Encoding High-Cardinality String Categorical Variables.” IEEE Transactions on Knowledge and Data Engineering 34 (3): 1164–76.\n\n\nDemidenko, E. 2013. Mixed Models: Theory and Applications with R. John Wiley & Sons.\n\n\nEfron, B, and T Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.\n\n\nElswick, RK, C Gennings, V Chinchilli, and K Dawson. 1991. “A Simple Approach for Finding Estimable Functions in Linear Models.” The American Statistician 45 (1): 51–53.\n\n\nGelman, A, J Carlin, H Stern, and D Rubin. 2013. Bayesian Data Analysis. Chapman; Hall/CRC.\n\n\nJohnson, A, M Ott, and M Dogucu. 2022. Bayes Rules!: An Introduction to Applied Bayesian Modeling. Chapman; Hall/CRC.\n\n\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press.\n\n\nLaird, N, and J Ware. 1982. “Random-Effects Models for Longitudinal Data.” Biometrics, 963–74.\n\n\nMcElreath, R. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman; Hall/CRC.\n\n\nMicci-Barreca, D. 2001. “A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems.” ACM SIGKDD Explorations Newsletter 3 (1): 27–32.\n\n\nPargent, F, F Pfisterer, J Thomas, and B Bischl. 2022. “Regularized Target Encoding Outperforms Traditional Methods in Supervised Machine Learning with High Cardinality Features.” Computational Statistics, 1–22.\n\n\nRuberg, S. 1989. “Contrasts for Identifying the Minimum Effective Dose.” Journal of the American Statistical Association 84 (407): 816–22.\n\n\nStroup, W. 2012. Generalized Linear Mixed Models: Modern Concepts, Methods and Applications. CRC press.\n\n\nWeinberger, K, A Dasgupta, J Langford, A Smola, and J Attenberg. 2009. “Feature Hashing for Large Scale Multitask Learning.” In Proceedings of the 26th Annual International Conference on Machine Learning, 1113–20. ACM.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-predictors.html#footnotes",
    "href": "chapters/categorical-predictors.html#footnotes",
    "title": "6  Working with Categorical Predictors",
    "section": "",
    "text": "In other words, we know that the vector of indicators (0, 0, 0) must represent the contract customers.↩︎\nThis is discussed in greater detail for one of the case studies in ?sec-reg-summary.↩︎\nThe original data uses codes to anonymize the agent names. The names shown here are randomly fabricated. The same is true of the company names.↩︎\nAlso called target encoding or likelihood encoding.↩︎\nThis demonstrates the concept of overfitting, discussed in Chapter 9 and subsequent chapters.↩︎\nIn the language of mixed effects models, the agent is a fixed effect in this parameterization.↩︎\nThere is a more general discussion of Bayesian methods in Section 12.5.↩︎\nEquation 6.3 is related to the well-known James-Stein estimator (Efron and Hastie 2016, chap. 7).↩︎\nMaking the range of the prior exceedingly wide indicates that almost any value is possible.↩︎\nIn other words, a very narrow distribution. There are other ways to create opinionated priors though. For instance, a skewed prior distribution for one of the \\(\\beta\\) regression slopes would be considered opinionated.↩︎\nThe linear mixed model approach mentioned above is often considered as an empirical Bayes approach with more restrictive options for the prior distributions. For empirical Bayes, one might say that you make your priors for your regression parameters any distribution you want, as long as they are Gaussians.↩︎\nThe notion of “purity” can mean a very tight distribution of numerical data (for regression models) or when the frequency distribution of qualitative has a very high probability for one category.↩︎\nThese features have characteristics that their values sum to zero and there is zero correlation between features.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html",
    "href": "chapters/embeddings.html",
    "title": "7  Embeddings",
    "section": "",
    "text": "7.1 Example: Predicting Barley Amounts\nWhen there are a multitude of predictors, it might be advantageous to condense them into a smaller number of artificial features. To be useful, this smaller set should represent what is essential in the original data. This process is often called feature extraction, dimension reduction, or manifold learning. We’ll use a more general term currently en vogue: embeddings. While this chapter focuses on feature extraction, embeddings can be used for other purposes, such as converting non-numeric data (e.g., text) into a more usable numeric format.\nThis chapter will examine several primary classes of embedding methods that can achieve multiple purposes. First, we’ll consider linear methods that take a numeric input matrix \\(\\boldsymbol{X}\\) that is \\(n \\times p\\) and create a different, probably smaller set of features \\(\\boldsymbol{X}^*\\) (\\(n \\times m\\)) using the transformation \\(\\boldsymbol{X}^* = \\boldsymbol{X}\\boldsymbol{Q}\\). We hope that we can find appropriate embeddings so that \\(m &lt;&lt; p\\).\nAfter describing linear methods, we will consider a different class of transformations that focuses on the distances between data points called multidimensional scaling (MDS). MDS creates a new set of \\(m\\) features that are not necessarily linear combinations of the original features but often use some of the same math as the linear techniques.\nFinally, some embedding techniques specific to classification are discussed. These are based on class centroids.\nBefore beginning, we’ll introduce another data set that will be used here and in forthcoming chapters.\nLarsen and Clemmensen (2019) and Fernández P et al. (2020) describe a data set where laboratory measurements are used to predict what percentage of a liquid was lucerne, soy oil, or barley oil1. An instrument is used to measure how much of particular wavelengths of light are absorbed by the mixture to help determine chemical composition. We will focus on using the lab measurements to predict the percentage of barley oil in the mixture. The distribution of these values is shown in Figure 7.1(a).\nFigure 7.1: (a) The distribution of the outcome for the entire data set. The bar colors reflect the percent barley distribution and are used in subsequent sections. (b) Selected training set spectra for four barley samples. Each line represents the set of 550 predictors in the data.\nNote that most of the data have very little barley oil. About 27% of the data are less than 1%, and the median barley oil percentage is 5.96%.\nThe 550 predictors are the light absorbance for sequential values in the light region of interest (believed to be from wavelengths between 1300 and 2398 nm). Figure 7.1(b) shows a selection of four samples from the data. The darker lines represent samples with lower barley content.\nThese predictor values, called spectra, have a very high serial correlation between predictors; median correlation between the predictors was 0.98. The high degree of between-predictor correlation can be a major complication for some models and can degrade predictive performance. Therefore, we need methods that will simultaneously decorrelate predictors while extracting useful predictive information for the outcome.\nAnalyses of similar data sets can be found in Section 9.1 of Kuhn and Johnson (2019) and Johnson and Kuhn (2024).\nIn the following computations, each predictor was standardized using the orderNorm transformation mentioned earlier (unless otherwise noted).\nThe data originated from a modeling competition to find the most accurate model and specific samples were allocated to training and test sets. However, there were no public outcome values for the test set; our analysis will treat the 6,915 samples in their training set as the overall pool of samples. This is enough data to split into separate training (\\(n_{tr} =\\) 4,839), validation (\\(n_{val} =\\) 1,035), and test sets (\\(n_{te} =\\) 1,041). The allocation of samples to each of the three data sets utilized stratified sampling based on the outcome data.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-linear-embed",
    "href": "chapters/embeddings.html#sec-linear-embed",
    "title": "7  Embeddings",
    "section": "7.2 Linear Transformations",
    "text": "7.2 Linear Transformations\nThe barley data set presents two common challenges for many machine learning techniques:\n\nThe number of original predictors (550) is fairly large .\nThe features are highly correlated. Techniques that require inverting the covariance matrix of the features, such as linear regression, will become numerically unstable or may not be able to be fit when features are highly correlated.\n\nWhat can we do if we desire to use a modeling technique that is adversely affected by either (or both) of these characteristics?\nDimensionality reduction is one technique that can help with the first issue (an abundance of columns). As we’ll see below, we might be able to extract new features (\\(X^*\\)) such that the new features optimally summarize information from the original data (\\(X\\)).\nFor the problem of highly correlated predictors, some embedding methods can additionally decorrelated the predictors by deriving embedded features with minimal correlation.\nThe three embedding methods we’ll discuss first are linear in a mathematical sense because they transform a table of numeric features into new features that are linear combinations of the original features. These new linear combinations are often called scores. This transformation uses the equation.\n\\[ \\underset{n\\times m}{\\boldsymbol{X}^*} = \\underset{n\\times p}{\\boldsymbol{X}}\\ \\underset{p\\times m}{\\boldsymbol{Q}} \\]\nwhere \\(\\boldsymbol{Q}\\) is a matrix that translates or embeds the original features into a potentially lower dimensional space (\\(m &lt; p\\)) without losing much information. It is easy to see why this matrix operation is linear when the elements of the \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Q}\\) matrices are expanded. For example, the first score for the \\(i^{th}\\) sample would be:\n\\[\nx^*_{i1} = q_{11} x_{i1} + q_{21}x_{i2} + \\ldots +  q_{p1}x_{ip}\n\\tag{7.1}\\]\nThe \\(q_{ij}\\) values are often referred to as the loadings for each predictor in the linear combination.\nIn this section, we will review principal components analysis (PCA), independent component analysis (ICA), and partial least squares (PLS), which are fundamental linear embedding techniques that are effective at identifying meaningful embeddings but estimate \\(A\\) in different ways. PCA and ICA extract embedded features using only information from the original features; they are unsupervised. Conversely, PLS uses the predictors and outcome; it is a supervised technique.\nThere are many other linear embedding methods that can be used. For example, non-negative matrix factorization (Lee and Seung 1999, 2000)is an embedding method that is useful when the data in \\(X\\) are integer counts or other values that cannot be negative.\nTo start, we’ll focus on the most often used embedding method: PCA.\n\n7.2.1 Principal Component Analysis\nKarl Pearson introduced PCA over a century ago, yet it remains a fundamental dimension reduction method (Jolliffe and Cadima 2016). Its relevance stems from the underlying objective of the technique: to find linear combinations of the original predictors that maximize amount of variation in the new features2.\n\nFrom a statistical perspective, variation is synonymous with information.\n\nSimultaneously, the scores produced by PCA (i.e., \\(X^*\\)) are required to be orthogonal to each other. The important side benefit of this technique is that PCA scores are uncorrelated. This is very useful for modeling techniques that need the predictors to be relatively uncorrelated: multiple linear regression, neural networks, support vector machines, and others.\nIn a way, PCA components have an order or hierarchy. The first principal component has a higher variance than any other component. The second component has a higher variance than subsequent components, and so on. We can think of this process as one of deflation; the first component extracts the largest sources of information/variation in the predictors set. The second component extracts as much as possible for whatever is left behind by the first, and so on. For this reason, we can track how much variation in the original data each component accounts for. We often use the phrase that “PCA chases variation” to achieve its goals.\nSuppose we start with \\(p\\) columns3 in our data. In that case, we can produce up to \\(p\\) PCA components, and their accumulated variation will eventually add up to the total variation in the original \\(X\\).\nWhile PCA can deliver new features with the desirable characteristics, the technique must be used with care. Specifically, we must understand that PCA seeks to find embeddings without regard to any further understanding of the original features (such as the response). Hence, PCA may generate embeddings that are ignorant of the modeling objective (i.e., they lack predictive information).\n\nPreparing for PCA\nBecause PCA seeks linear combinations of predictors that maximize variability, it will naturally first be drawn to summarizing predictors with more variation. If the original predictors are on measurement scales that differ in magnitude, then the first components will focus on summarizing the higher magnitude predictors. This means that the PCA scores will focus on the scales of the measurements (rather than their intrinsic value). If, by chance, the most relevant feature for predicting the response also has the largest numerical values, the new features created by PCA will retain the essential information for predicting the response. However, if the most important features are in the smallest units, then the top features created by PCA will be much less effective.\nIn most practical machine learning applications, features are on vastly different scales. We suggest using the tools from Section 5.3 to transform the data so that they have the same units. This prevents PCA from focusing dimension reduction simply on the measurement scale.\nAdditionally, the distributions of each feature may show considerable skewness or outliers. While there are no specific distributional requirements to use PCA, it is focused on variance and variance calculations often involve squared terms (e.g., \\((x_i - \\bar{x})^2\\)). This type of computation can be very sensitive to outliers and skewness; resolving these issues prior to PCA is recommended. See the methods previously described in Section 5.2. When outliers are a specific concern, the spatial-sign transformation might be a good idea (Visuri, Koivunen, and Oja 2000; Croux, Ollila, and Oja 2002).\n\n\nHow Does PCA Work?\nPrincipal component analysis is focused on understanding the relationships between the predictor columns. PCA is only effective when there are correlations between predictors. Otherwise, each new PCA feature would only highlight a single predictor, and you would need the full set of PCA features to approximate the original columns. Conversely, data sets with a handful of intense between-predictor relationships require very few predictors to represent the source columns.\nThe barley data set has abnormally high correlations between each predictor. These tend to be autoregressive; the correlations between predictors at adjacent wavelengths are very high, and the correlation between a pair of predictors diminishes as you move away from their locations on the spectrum. For example, the correlation between the first and second predictor (in their raw form) is essentially 1 while the corresponding correlation between the first and fiftieth columns is 0.67. We’ll explore the latter pair of columns below.\nBefore proceeding, let’s go on a small “side-quest” to talk about the mathematics of PCA.\n\nThe remainder of this subsection discussed a mathematical operation called the singular value decomposition (SVD). PCA and many other computations rely on on the SVD. It is a fundamental linear algebra calculation.\nWe’ll describe it loosely with a focus on how it is used. The concept of eigenvalues will come up again in the next section and in subsequent chapters. Banerjee and Roy (2014) and Aggarwal (2020) are thorough but helpful resources for learning more.\n\nThe SVD process is intended to find a way to rotate the original data in a way that the resulting columns are orthogonal to one another. It takes a square matrix as an input and can decompose it into several pieces:\n\\[\\underset{p\\times p}{A} = \\underset{p\\times p}{Q}\\quad\\underset{p\\times p}{\\Lambda}\\quad \\underset{p\\times p}{Q'}\\]\nThe results of this computation are the \\(p\\) eigenvectors \\(\\mathbf{q}_j\\) and eigenvalues \\(\\lambda_j\\). The eigenvectors tell you about the directions in which values of \\(A\\) are moving and the eigenvalues describe the corresponding magnitudes (e.g. how far they go in their corresponding direction).\nThe transformation works with PCA by using the covariance matrix as the input \\(A\\). We are trying to find trends in the relationships between variables and the SVD is designed to translate the original matrix to one that is orthogonal (that is, uncorrelated). This aspect of the SVD is how it connects to PCA. If we want to approximate our original data with new features that are uncorrelated, we need a transformation to orthogonality.\nThe matrix \\(Q\\) houses the \\(p\\) possible eigenvectors. These are the values by which you multiply the original matrix \\(A\\) to achieve an orthogonal version. For PCA, they are the loading values shown in Equation 7.1. The matrix \\(\\Lambda\\) is a diagonal matrix whose \\(p\\) values are the eigenvalues. It turns out that the eigenvalues represent the amount of variation (i.e., information) captured by each component.\nIf we conduct the SVD (and PCA) on the covariance matrix of the data, we are again subject to issues around potentially different units and scales of the predictors. Previously, we suggested to center and scale the data prior to PCA. The equivalent approach here is to conduct the SVD on the correlation matrix of the data. Similar to our recommendations on standardization, we recommend using the correlation as the input to PCA.\nOne interesting side-effect of using the covariance or correlation matrix as the input to PCA is related to missing data (discussed in more detail in Chapter 4). The conventional sample covariance (or correlation) matrices can be computed for each matrix element (i.e., without using matrix multiplication). Each covariance or correlation can be computed with whatever rows of the two predictors have pairwise-complete results. This means that we do not have to globally drop specific rows of the data when a small number of columns contain missing data.\n\n\nA Two Dimensional Example\nTo demonstrate, we chose two predictors in the barley data. They correspond to wavelengths4 1,300 and 1,398 shown in Figure 7.1(b). In the data, these two predictors were preprocessed using the ORQ procedure (Section 5.2). The standardized versions of the predictors have a correlation of 0.68 and are shown in Figure 7.2(a).\nPCA was performed on these data, and the results are animated in Figure 7.2(b). This visualization shows the original data (colored by their outcome data) and then rotates it around the center position. The PCA transformation corresponds to the rotation where the x-axis has the largest spread. The variance is largest at two angles (135\\(^{\\circ}\\) and 315\\(^{\\circ}\\)) since the solution for PCA is unique up to sign. Both possible solutions are correct, and we usually pick one. For one solution, the PCA loading coefficients are:\n\\[\\begin{align}\nx^*_{i1} &= -0.71 x_{i1} -0.71 x_{i2} \\notag \\\\\nx^*_{i2} &= -0.71 x_{i1} + 0.71 x_{i2} \\notag\n\\end{align}\\]\nBecause of this illustrative example’s low-dimensional aspect, this solution is excessively simple. Ordinarily, the loading values take a great many values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: A demonstration that PCA is just a rotation of the data. The original predictors (those at the first and fiftieth wavelengths) are in Panel (a). The animation in (b) shows the rotation of two of the predictors, which stops at the two optimal rotations corresponding to the principal components and the original data.\n\n\n\nAgain,the remarkable characteristic of this specific set of linear combinations is that they are orthogonal (i.e., uncorrelated) with one another.\n\nUsing PCA to transform the data into a full set of uncorrelated variables is often called “whitening the data.” It retains all of the original information and is more pliable to many computations. It can be used as a precursor to other algorithms.\n\nNow that we’ve seen PCA on a small scale, let’s look at an analysis of the full set of predictors\n\n\nPCA on the Barley Data\nWe’ll repeat the analysis in the preceding section; PCA was calculated on the training set using all predictors. There are 550 feature columns and the same number of possible principal components. The number of new features to retain is a parameter that must be determined. In practice, the number of new features is often selected based on the percentage of variability that the new features summarize relative to the original features. Figure 7.3 displays the new PCA feature number (x-axis) versus the percent of total variability across the original features (commonly called a “scree plot”). In this graph, three components summarize 99.4% of the total variability. We often select the smallest number of new features that summarize the greatest variability. Again, this is an unsupervised optimization. To understand how the number of components affects supervised predictors (e.g., RMSE), the number of features can be treated as a tuning parameter .\n\n\n\n\n\n\n\n\nFigure 7.3: A scree plot of the PCA component number versus the percent of variance explained for the first 10 components of the barley data.\n\n\n\n\n\nNote that the first component alone captured 92.3% of the variation in the 550 training data columns. This reflects the extreme correlation seen in the predictors.\nFigure 7.4 shows a scatterplot of the principal components colored by the percentage of barley oil. The figure reveals that the first PCA component delineates the higher barley oil samples from those with less oil. There also appear to be three different clusters of samples with very low (if any) barley oil. It might be helpful to investigate these samples to ascertain if they are artifacts or caused by some systematic, underlying factors not in the current set of columns. The pairing of the second and fourth components appears to help differentiate lower barely samples from the broader set. The pairing of the third and fourth components doesn’t offer much predictive power on their own.\n\n\n\n\n#| label: shiny-linear-scores\n#| out-width: \"80%\"\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(ggforce)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-linear-scores.R\")\n\napp\n\n\n\nFigure 7.4: A visualization of the four new features for different linear embedding methods. The data shown are the validation set results.\n\n\n\nOne note about this visualization. The axis scales are not common across panels (for ease of illustration). If we were to keep a common scale, the later components would appear to have very little effect due to the flatness of the resulting figures. When looking at the scores, we suggest keeping a common scale, which will most likely be the scale of the first component. This will help avoid over-interpreting patterns in the later components.\nFor PCA, it can be very instructive to visualize the loadings for each component. This can help in several different ways. First, it can tell which predictors dominate each specific new PCA component. If a particular component ends up having a strong relationship with the outcome, this can aid our explanation of how the model works. Second, we can examine the magnitude of the predictors to determine some of the relationships between predictors. If a group of predictors have approximately the same loading value, this implies that they have a common relationship with one another. This, on its own, can be a significant aid when conducting exploratory data analysis on a high-dimensional data set. Figure 7.5 shows the relationship between the PCA loadings and each predictor’s position on the spectrum.\n\n\n\n\n#| label: shiny-linear-loadings\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(dplyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\n  \"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-linear-loadings.R\"\n)\n\napp\n\n\n\nFigure 7.5: The loadings for the first four components of each linear embedding method as a function of wavelength.\n\n\n\nFor the first component, wavelengths at the low ends of the spectrum have a relatively small impact that increases as you get to about 1,400 nm. At this point, the predictors have a constant effect on that component. The second PCA component is almost the opposite; it emphasizes the smaller wavelengths while predictors above 1,400 nm fluctuate around zero. The third and fourth components emphasize different areas of the spectrum and are generally different than zero.\n\n\nHow to Incorporate PCA into the Model\nThe barley data demonstrates that PCA can effectively capture the training set information in a smaller predictor set when there is a correlation between predictors. The number of components to retain depends on the data. It is a good idea to optimize the number of components to retain via model tuning.\n\nNote that “dimension reduction” is not the same as feature selection. The PCA components described above are functions of all of the predictors. Even if you only need a handful of PCA components to approximate the training set, the original predictor set is still required when predicting new samples.\n\nAnother aspect of using PCA for feature extraction is the scale of the resulting scores. Figure 7.4 demonstrates that the first component has the largest variance, resulting in a wider range of training set values. For the validation set scores, the variance of PC1 is 14-fold larger than the variance of PC2. Even though the scores are unitless, there may be issues for models that expect the predictors to have a common scale, similar to the requirement for PCA itself. In these cases, it may be advisable to standardize the PCA components to have the same range before serving them to the supervised ML model.\n\n\nNon-Standard PCA\nVarious methods exist for estimating the PCA loadings. Some are well-suited to different dimensions of the predictor’s data. For example, “small \\(n\\), large \\(p\\)” data sizes can make PCA computations difficult. Wu, Massart, and De Jong (1997) and others describe a kernel method more appropriate for very wide data sets. Schölkopf, Smola, and Müller (1998) extended this even further but using nonlinear kernel methods5. Still, others use modified objective functions to compensate for some potential deficiencies of the canonical technique.\nFor example, the standard loading matrix (i.e., the eigenvectors) are dense. It is possible that the loadings for some predictors shouldn’t affect specific components. The SVD might estimate these to be close to zero but not exactly zero. Sparse PCA techniques can, through various means, estimate some loading values to be exactly zero, indicating that the predictor has no functional effect on the embedding.\nFor example, Shen and Huang (2008) take a penalization approach that puts a high price for loadings to have very large values. This regularization method can shrink the values towards zero and make some absolute zero. This is similar to the regularization approach seen in Section 6.4.3 and is directly related to techniques discussed in ?sec-logistic-reg, ?sec-nnet-cls, and ?sec-linear-reg. This option can coerce values across the loadings to be zero. It is unlikely to force a particular predictor’s values to be zero across all components. In other words, it may not completely erase the effect of a predictor from the embedding altogether.\nAnother method is based on a Bayesian approach (Ning and Ning 2021), where the loading values are assumed to come from a mixture of two different distributions. One has a sharp “spike” around zero, and the other is a flat, wide distribution that encompasses a wide range of values (called the “slab”). The method then estimates the parameters to favor one distribution or the other and sets some proportion of the loadings to zero.\nAdditionally, there are PCA variants for non-numeric data, such as categorical predictors. Probabilistic PCA (Tipping and Bishop 1999; Schein, Saul, and Ungar 2003) uses a PCA generalization to reduce the dimensions of qualitative data.\n\n\n\n\n7.2.2 Independent Component Analysis\nICA has roots in signal processing, where the observed signal that is measured is a combination of several different input signals.\nFor example, electroencephalography (EEG) uses a set of electrodes on a patient’s scalp to noninvasively measure the brain’s electrical activity over time. The electrodes are placed on specific parts of the scalp and measure different brain regions (e.g., prefrontal cortex etc.). However, since the electrodes are near others, they can often measure mixtures of multiple underlying signals.\nThe objective of independent component analysis is to identify linear combinations of the original features that are statistically independent of each other (Hyvärinen and Oja 2000; Nordhausen and Oja 2018).\n\nTwo variables can be uncorrelated but still have a relationship (most likely nonlinear). For example, suppose we have a variable \\(z\\) that is a sequence of evenly spaced values between -2 and 2. The correlation between \\(z\\) and \\(z^2\\) is very close to zero but there is a clear and precise relationship between them, therefore the variables are not independent.\n\nIn practice, satisfying the statistically independence requirement of ICA can be done several ways. One approach maximizes the “non-Gaussianity” of the resulting components. As shown by Hyvärinen and Oja (2000),\n\nThe fundamental restriction in ICA is that the independent components must be nongaussian for ICA to be possible.\n\nThere are different ways to measure non-Gaussianity. The fastICA algorithm uses an information theory statistic called negentropy (Hyvärinen and Oja 2000). Another approach called InfoMax uses neural networks to estimate a different information theory statistic.\nBecause ICA’s objective requires statistical independence among components, it will generate features different from those of PCA. Unlike PCA, which orders new components based on summarized variance, ICA’s components have no natural ordering. This means that ICA may require more components than PCA to find the ones related to the outcome.\nThe predictors should be centered (and perhaps scaled) before estimating the ICA loadings. Additionally, some ICA algorithms internally add a PCA step to “whiten” the data before computing the ICA loadings (using all possible PCA components). Since independent component analysis thrives on non-Gaussian data, we should avoid transformations that induce a more symmetric distribution. For this reason, we will not use the ORD transformation to preprocess the predictors. Finally, the ICA loadings are often initialized before training using random values. If this randomness is not controlled, different results will likely occur from training run to training run.\nFigure 7.4 contains a scatterplot matrix of the first 4 ICA components colored by the outcome. In this figure, Two components that appear to differentiate levels of barley by themselves (components two and three). Unsurprisingly, their interaction appears to be the one that visually differentiates the different amounts of barley oil in the validation set.\nThe loadings are shown in Figure 7.5 and tell an interesting story. The pattern of loadings one and three have similar patterns over the wavelengths. The same situation is the case for components two and four. These two pairs of trends in the loadings are somewhat oppositional to one another. Also, the patterns have little in common with the PCA loadings. For example, the first PCA loading was relatively constant across wavelengths. None of the ICA components show a constant pattern.\n\n\n\n7.2.3 Partial Least Squares\nBoth PCA and ICA focus strictly on summarizing information based on the features exclusively. When the outcome is related to the way that the unsupervised techniques extract the embedded features, then PCA and/or ICA can be effective dimension reduction tools. However, when the qualities of variance summarization or statistical independence are not related to the outcome, then these techniques may struggle to identify appropriate embedded features that are useful for predicting the outcome.\nPartial least squares (PLS) (Wold, Martens, and Wold 1983) is a dimension reduction technique that identifies embedded features that are optimally linearly related to the outcome. While the objective of PCA is to find linear combinations of the original features that best summarize variability, the objective of PLS is to do the same and to find the linear combinations that are correlated with the outcome (Stone and Brooks 1990). This process can be thought of as a constrained optimization problem in which PLS is forced to compromise between maximizing information from the predictors and simultaneously maximizing the prediction of the outcome. This means that the outcome is used to focus the dimension reduction process such that the new features have the greatest correlation with the outcome. For more detailed information, we suggest Geladi and Kowalski (1986) and Esposito Vinzi and Russolillo (2013).\nBecause one of the objectives of PLS is to summarize variance among the original features, the features should be pre-processed in the same way as PCA.\nFor the barley data, the first few sets of PLS loads are fairly similar to those generated by PCA. The first three components have almost identical values as their PCA analogs. The fourth component has different loadings, and the correlation between the fourth PCA and PLS scores is -0.63. The similarities in the first three scores and loadings between the methods can be seen in Figure 7.4 and Figure 7.5, respectively.\nIf our goal is to maximize predictive performance, fewer features are necessary when using PLS. As we’ll see below, we can sometimes achieve similar performance using PCA or ICA but we will need more features to match what PLS can do with less.\nLike principal component analysis, there are many modified versions of partial least squares. For example, Lê Cao et al. (2008) and Lê Cao et al. (2008) describe a sparse estimation routine that can set some loadings equal to absolute zero. Also, Barker and Rayens (2003) and Y. Liu and Rayens (2007) adapt the algorithm for classification problems (i.e., qualitative outcomes).\n\n\n\n7.2.4 Overall Comparisons\nHow do these three approaches differ in terms of predictive performance? To evaluate this, these embeddings were computed with up to 25 new features and used as the inputs for a linear regression model. After the embeddings and regression parameters were estimated, the validation set RMSE was computed. Figure 7.6 has the results. The bands around each curve are 90% confidence intervals for the RMSE.\n\n\n\n\n\n\n\n\nFigure 7.6: Results for linear regressions using three different linear embeddings.\n\n\n\n\n\nOverall, the patterns are similar. By the time 25 components are added, there is parity between the methods. However, PLS appears to be more efficient at finding predictive features since its curve has uniformly smaller RMSE values than the others . This isn’t surprising since it is the only supervised embedding of the three.\nPCA and PLS will be discussed in more detail in ?sec-colinearity.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-mds",
    "href": "chapters/embeddings.html#sec-mds",
    "title": "7  Embeddings",
    "section": "7.3 Multidimensional Scaling",
    "text": "7.3 Multidimensional Scaling\nMultidimensional scaling (Torgerson 1952) is a feature extraction tool that creates embeddings that try to preserve the geometric distances between training set points. In other words, the distances between points in the smaller dimensions should be comparable to those in the original dimensions.\nMany different distance metrics can be used, but we’ll postpone that discussion until ?sec-cls-knn. That said, we’ll denote a distance value between two vectors of predictors as \\(d(x_i, x_j)\\) and we’ll assume basic Euclidean distance unless otherwise noted. With distance metrics, the predictors should be standardized to equivalent units before those computations. As with PCA, we also recommend transformations to resolve skewness.\nTake Figure 7.7(a) as an example. There are ten points in two dimensions (colored by three outcome classes). If we were to project these points down to a single dimension, we’d like points that are close in the original two dimensions to remain close when projected down to a single dimension. Panel (c) shows two such solutions. Each does reasonably well with some exceptions (i.e., points six and nine are too close for non-Metric MDS).\n\n\n\n\n\n\n\n\nFigure 7.7: (a) A collection of samples associated with three outcome classes. (b) A diagram of the two nearest neighbors for each data point. (c) A one-dimensional projection using two different methods: non-metric MDS and Isomap.\n\n\n\n\n\nHere we present a few MDS methods, but there are many more. Ghojogh et al. (2023) has an excellent review of an assortment of methods and their nuances.\nSome MDS methods compute all pairwise distances between points and use this as the input to the embedding algorithm. This is similar to how PCA can be estimated using the covariance or correlation matrices. One technique, Non-Metric MDS (Kruskal 1964a, 1964b; Sammon 1969), finds embeddings that minimize an objective function called “stress”:\n\\[\n\\text{Stress} = \\sqrt{\\frac{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}\\left(d(x_i, x_j) - d(x^*_i, x^*_j)\\right)^2}{\\sum\\limits^{n_{tr}}_{i = 1}\\;\\sum\\limits^{n_{tr}}_{j = i+1}d(x_i, x_j)^2}}\n\\]\nThe numerator uses the squared difference between the pairwise distances in the original values (\\(x\\)) and the smaller embedded dimension (\\(x^*\\)). The summations only move along the upper triangle of the distance matrices to reduce redundant computations. Figure 7.7(c, top row) has the resulting one dimensional projection of our two-dimensional data.\nThis can be an effective dimension reduction procedure, although there are a few issues. First, the entire matrix of distances is required (with \\(n_{tr}(n_{tr}-1)/2\\) entries). For large training sets, this can be unwieldy and time-consuming. Second, like PCA, it is a global method that uses all data in the computations. We might be able to achieve more nuanced embeddings by focusing on local structures. Finally, it is challenging to apply metric MDS to project new data onto the space in which the original data was projected.\nFor these reasons, let’s take a look at more modern versions of multidimensional scaling.\n\n7.3.1 Isomap\nTo start, we’ll focus on Isomap (Tenenbaum, Silva, and Langford 2000). This nonlinear MDS method uses a specialized distance function to find the embedded features. First, the K nearest neighbors are determined for each training set point using standard functions, such as Euclidean distance. Figure 7.7(b) shows the K = 2 nearest neighbors for our example data. Many nearest-neighbor algorithms can be very computationally efficient and their use eliminates the need to compute all of the pairwise distances.\nThe connections between neighbors form a graph structure that qualitatively defines which data points are closely related to one another. From this, a new metric called geodesic distance can be approximated. For a graph, we can compute the approximate geodesic distance using the shortest path between two points on the graph. With our example data, the Euclidean distance between points four and five is not large. However, its approximate geodesic distance is greater because the shortest path is through points nine, eight, and seven. Ghojogh et al. (2023) use a wonderful analogy:\n\nA real-world example is the distance between Toronto and Athens. The Euclidean distance is to dig the Earth from Toronto to reach Athens directly. The geodesic distance is to move from Toronto to Athens on the curvy Earth by the shortest path between two cities. The approximated geodesic distance is to dig the Earth from Toronto to London in the UK, then dig from London to Frankfurt in Germany, then dig from Frankfurt to Rome in Italy, then dig from Rome to Athens.\n\nThe Isomap embeddings are a function of the eigenvalues computed on the geodesic distance matrix. The \\(m\\) embedded features are functions of the first \\(m\\) eigenvectors. Although eigenvalues are associated with linear embeddings (e.g., PCA), nonlinear geodesic distance results in a local nonlinear embedding. Figure 7.7(c, bottom row) shows the 1D results for the example data set. For a new data point, its nearest-neighbors in the training set are determined so that the approximate geodesic distance can be computed. The estimated eigenvectors and eigenvalues are used to project the new point into the embedding space.\nFor Isomap, the number of nearest neighbors and the number of embeddings are commonly optimized. Figure 7.8 shows a two-dimensional Isomap embedding for the barley data with varying numbers of neighbors. In each configuration, the higher barley values are differentiated from the small (mostly zero) barley samples. We again see the two or three clusters of data associated with small outcome values. The new features become more densely packed as the number of neighbors increases.\n\n\n\n\n\n\n\n\nFigure 7.8: Isomap for the barley data for different numbers of nearest neighbors. The training set was used to fit the model and these results show the projections on the validation set. Lighter colors indicate larger values of the outcome.\n\n\n\n\n\n\n\n\n7.3.2 Laplacian Eigenmaps\nThere are many other approaches to preserve local distances. One is Laplacian eigenmaps (Belkin and Niyogi 2001). Like Isomap, it uses nearest neighbors to define a graph of connected training set points. For each connected point, a weight between graph nodes is computed that becomes smaller as the distance between points in the input space increases. The radial basis kernel (also referred to as the “heat kernel”) is a good choice for the weighting function6:\n\\[\nw_{ij} = \\exp\\left(\\frac{-||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2}{\\sigma}\\right)\n\\]\nwhere \\(\\sigma\\) is a scaling parameter that can be tuned. If two points are not neighbors, or if \\(i = j\\), then \\(w_{ij} = 0\\). Note that the equation above uses Euclidean distance. For the 2-nearest neighbor graph shown in Figure 7.7(b) and \\(\\sigma = 1 / 2\\), the weight matrix is roughly\n\n\\[\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\n\\boldsymbol{W} = \\begin{bmatrix}\n\\0 & 0.1 & \\0 & 0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n& \\0 & 0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  & \\0 & 0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  &  & \\0 & \\0 & \\0 & 0.1 & \\0 & \\0 & \\0\\\\\n&  &  &  & \\0 & 0.4 & \\0 & \\0 & 0.1 & \\0\\\\\n&  & sym &  &  & \\0 & \\0 & \\0 & 0.1 & \\0\\\\\n&  &  &  &  &  & \\0 & 0.4 & \\0 & 0.1\\\\\n&  &  &  &  &  &  & \\0 & 0.1 & 0.3\\\\\n&  &  &  &  &  &  &  & \\0 & \\0\\\\\n&  &  &  &  &  &  &  &  & \\0\n\\end{bmatrix}\n\\]\n\nThe use of nearest neighbors means that the matrix can be very sparse and the zero values help define locality for each data point. Recall that samples 2 and 3 are fairly close to one another, while samples 1 and 2 are farther away. The weighting scheme gives the former pair a 4-fold larger weight in the graph than the latter pair.\nLaplacian eigenmaps rely heavily on graph theory. This method computes a graph Laplacian matrix, defined as \\(\\boldsymbol{L} = \\boldsymbol{D} - \\boldsymbol{W}\\) where the matrix \\(\\boldsymbol{D}\\) has zero non-diagonal entries and diagonals equal to the sum of the weights for each row. For our example data:\n\n\\[\n\\newcommand{\\0}{{\\color{lightgray} 0.0}}\nL = \\begin{bmatrix}\n0.3 & -0.1 & \\0 & -0.2 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  0.5 & -0.4 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  &  0.5 & -0.1 & \\0 & \\0 & \\0 & \\0 & \\0 & \\0\\\\\n&  &  &  0.4 & \\0 & \\0 & -0.1 & \\0 & \\0 & \\0\\\\\n&  &  &  &  0.5 & -0.4 & \\0 & \\0 & -0.1 & \\0\\\\\n&  & sym &  &  &  0.5 & \\0 & \\0 & -0.1 & \\0\\\\\n&  &  &  &  &  &  0.6 & -0.4 & \\0 & -0.1\\\\\n&  &  &  &  &  &  &  0.8 & -0.1 & -0.3\\\\\n&  &  &  &  &  &  &  &  0.3 & \\0\\\\\n&  &  &  &  &  &  &  &  &  0.4\n\\end{bmatrix}\n\\]\n\nThe eigenvalues and eigenvectors of this matrix are used as the main ingredients for the embeddings. Bengio et al. (2003) shows that, since these methods eventually use eigenvalues in the embeddings, they can be easily used to project new data.\n\n\n7.3.3 UMAP\nThe Uniform Manifold Approximation and Projection (UMAP) (Sainburg, McInnes, and Gentner 2020) technique is one of the most popular distance-based methods. Its precursors, stochastic neighbor embedding (SNE) (Hinton and Roweis 2002) and Student’s t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton 2008), redefined feature extraction, particularly for visualizations. UMAP borrows significantly from Laplacian eigenmaps and t-SNE but has a more theoretically sound motivation.\nAs with Laplacian eigenmaps, UMAP converts the training data points to a sparse graph structure. Given a set of K nearest neighbors, it computes values similar to the previously shown weights (\\(\\boldsymbol{W}\\) matrix), which we will think of as the probability that point \\(j\\) is a neighbor of point \\(i\\):\n\\[\np_{j|i} = \\exp\\left(\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i}\\right)\n\\]\nwhere \\(\\rho_i\\) is the distance from \\(\\boldsymbol{x}_i\\) to its closest neighbor, and \\(\\sigma_i\\) is a scale parameter that now varies with each sample (\\(i\\)). To compute \\(\\sigma_i\\), we can solve the equation\n\\[\n\\sum_{i=1}^K  \\exp\\left(\\frac{-\\left(||\\boldsymbol{x}_i - \\boldsymbol{x}_j||^2 - \\rho_i\\right)}{\\sigma_i}\\right) = \\log_2(K)\n\\]\nUnlike the previous weighting system, the resulting \\(n \\times n\\) matrix may not be symmetric, so the final weights are computed using \\(p_{ij} = p_{j|i} + p_{i|j} - p_{j|i}p_{i|j}\\).\nUMAP performs a similar weight calculation for the embedded values \\(\\boldsymbol{x}^*\\). We’ll denote the probability that embedded points \\(\\boldsymbol{x}_i^*\\) and \\(\\boldsymbol{x}_j^*\\) are connected as \\(p_{ij}^*\\). We’d like the algorithm to estimate these values such that \\(p_{ij} \\approx p_{ij}^*\\).\nNumerical optimization methods7 used to estimate the \\(n \\times m\\) values \\(x^*_{ij}\\). The process is initialized using a very sparse Laplacian eigenmap, the first few PCA components, or random uniform numbers. The objective function is based on cross-entropy and attempts to make the graphs in the input and embedded dimensions as similar as possible by minimizing:\n\\[\nCE = \\sum_{i=1}^{n_{tr}}\\sum_{j=i+1}^{n_{tr}} \\left[p_{ij}\\, \\log\\frac{p_{ij}}{p_{ij}^*} + (1 - p_{ij})\\log\\frac{1-p_{ij}}{1-p_{ij}^*}\\right]\n\\]\nUnlike the other embedding methods shown in this section, UMAP can also create supervised embeddings so that the resulting features are more predictive of a qualitative or quantitative outcome value. See Sainburg, McInnes, and Gentner (2020).\nBesides the number of neighbors and embedding dimensions, several more tuning parameters exist. The optimization process’s number of optimization iterations (i.e., epochs) and the learning rate can significantly affect the final results. A distance-based tuning parameter, often called min-dist, specifies how “packed” points should be in the reduced dimensions. Values typically range from zero to one. However, the original authors state:\n\nWe view min-dist as an essentially aesthetic parameter governing the appearance of the embedding, and thus is more important when using UMAP for visualization.\n\nAs will be seen below, the initialization scheme is an important tuning parameter.\nFor supervised UMAP, there is an additional weighting parameter (between zero and one) that is used to balance the importance of the supervised and unsupervised aspects of the results. A value of zero specifies a completely unsupervised embedding.\nFigure 7.9 shows an interactive visualization of how UMAP can change with different tuning parameters. Each combination was trained for 1,000 epochs and used a learning rate of 1.0. For illustrative purposes, the resulting embeddings were scaled to a common range.\n\n\n\n\n#| label: shiny-umap\n#| viewerHeight: 550\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(dplyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-umap.R\")\n\napp\n\n\n\nFigure 7.9: A visualization of UMAP results for the barley data using different values for several tuning parameters. The points are the validation set values.\n\n\n\nThere are a few notable patterns in these results:\n\nThe initialization method can heavily impact the patterns in the embeddings.\nAs with Isomap, there are two or three clusters of data points with small barley values.\nWhen the amount of supervision increases, one or more circular structures form that are associated with small outcome values.\nThe minimum distance parameter can drastically change the results.\n\nt-SNE and UMAP have become very popular tools for visualizing complex data. Visually, they often show interesting patterns that linear methods such as PCA cannot. However, they are computationally slow and unstable over different tuning parameter values. Also, it is easy to believe that the UMAP distances between embedding points are important or quantitatively predictive. That is not the case; the distances can be easily manipulated using the tuning parameters (especially the minimum distance).",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-centroids",
    "href": "chapters/embeddings.html#sec-centroids",
    "title": "7  Embeddings",
    "section": "7.4 Centroid-Based Methods",
    "text": "7.4 Centroid-Based Methods\nAnother common approach to creating features is to calculate distances between landmarks or pairs of predictors. For example, the distance to desirable grade schools (or the university campus) could affect house prices in the Ames housing data. Potential predictors can be created that measure how close each home is to these points of interest. To calculate distance, a simple Euclidean metric can be used. However, for spatial data across longer distances, the Haversine metric (Sinnott 1984) is a better alternative because it takes into account the curvature of the earth.\nNote that distance-based features are often right skewed. When a metric produces a right-skewed distribution, the log-transformation often helps improve predictive performance when the predictor is truly informative.\nDistance-based features can also be effective for classification models. A centroid is another name for the multivariate mean of a collection of data. It is possible to compute class-specific centroids using only the data from each class. When a new sample is predicted, the distance to each class centroid can be used as a predictor. These features would be helpful when a model could be better at detecting/emulating linear class boundaries. An example of this would be tree-based models; these models have to work hard to approximate linear trends in the data. Supplementing the data with simple centroid features might improve performance.\nAgain, there are several choices for the distance metric. Mahalanobis distance is a good choice when there is not an overwhelming number of predictors:\n\\[\nD_c(\\boldsymbol{x}_0) = (\\boldsymbol{x}_0 - \\boldsymbol{\\bar{x}}_{c})' \\boldsymbol{S}^{-1}_c (\\boldsymbol{x}_0 - \\boldsymbol{\\bar{x}}_{c})\n\\]\nwhere \\(\\boldsymbol{x}_0\\) is the new data point being predicted, \\(\\boldsymbol{\\bar{x}}\\) is a vector of sample means, \\(\\boldsymbol{S}\\) is the estimated covariance matrix (the subscript of \\(c\\) denotes the class-specific statistics). This metric requires more data points within each class than the number of predictors being used. It also assumes that there are no linear dependencies between the predictors.\nWhen the model has many features, regularizing the centroid distances can be a good approach. This approach, similar to the tools described in Section 6.4.3, will shrink the class-specific centroids towards the overall (class-nonspecific) centroid at different rates. If a predictor does have any discriminative ability in the training set, its contribution to the class-specific centroids can be removed.\nWe’ll let \\(x_{ij}\\) denote sample \\(i\\) (\\(i=1\\ldots n_{tr}\\)) for predictor \\(j\\) (\\(j=1\\ldots p\\)). The approach by Tibshirani et al. (2003) estimates the standardized difference between the class-specific centroid and the global centroid using the following:\n\\[\\begin{align}\n\\delta_{jc} &= \\frac{\\bar{x}_{jc} - \\bar{x}_j}{w_c s_j} &&\\text{ where } \\notag \\\\\n\\bar{x}_{jc} &= \\frac{1}{{n_{tr}^c}}\\sum_{i=1}^{{n_{tr}^c}} x_{ij}\\, I(y_i = c) && \\text{\\textcolor{grey}{(class-specific centroid elements)}} \\notag \\\\\n\\bar{x}_{j} &= \\frac{1}{n_{tr}}\\sum_{i=1}^{n_{tr}} x_{ij}  && \\text{\\textcolor{grey}{(global centroid elements)}}\\notag \\\\\nw_c &= \\sqrt{\\frac{1}{{n_{tr}^c}}  - \\frac{1}{n_{tr}}}  && \\text{\\textcolor{grey}{{(weights)}}} \\notag \\\\\ns_j &= \\frac{1}{n_{tr}-C}\\sum_{c=1}^C\\sum_{i=1}^{n_{tr}} \\left(x_{ij} - \\bar{x}_{jc}\\right)^2 I(y_i = c)  && \\text{\\textcolor{grey}{(pooled standard deviation for predictor $j$)}}\\notag\n\\end{align}\\]\nwhere \\(n_{tr}^c\\) is the number of training set points for class \\(c\\) and \\(I(x)\\) is a function that returns a value of one when \\(x\\) is true.\nTo shrink this difference towards zero, a tuning parameter \\(\\lambda\\) is used to create a modified version of each predictor’s contribution to the difference:\n\\[\\begin{equation}\n\\delta^*_{jc} = sign(\\delta_{jc})\\,h(|\\delta_{jc}| - \\lambda)\n\\end{equation}\\]\nwhere \\(h(x) = x\\) when \\(x &gt; 0\\) and zero otherwise. If the difference between the class-specific and global centroid is small (relative to \\(\\lambda\\)), \\(\\delta^*_{jc} = 0\\) and predictor \\(j\\) does not functionally affect the calculations for class \\(c\\). The class-specific shrunken centroid is then\n\\[\\begin{equation}\n\\bar{x}^*_{jc}= \\bar{x}_j + w_c\\, s_j\\, \\delta^*_{jc}\n\\end{equation}\\]\nNew features are added to the model based on the distance between \\(\\boldsymbol{x}_0\\) and \\(\\boldsymbol{\\bar{x}}^*_{c}\\). The amount of shrinkage is best optimized using the tuning methods described in later chapters. There are several variations of this specific procedure. Wang and Zhu (2007) describe several different approaches and Efron (2009) demonstrates the connection to Bayesian methods.\n\n\n\n\n\n\n\n\nFigure 7.10: An example of shrunken, class-specific centroids. Panel (a) shows data where there are three classes. Panel (b) demonstrates how, with increasing regularization, the centorids converge towards the global centroid (the black open circle).\n\n\n\n\n\nFigure 7.10 shows the impact of regularization for a simple data set with two predictors and three classes. The data are shown in Panel (a), while Panel (b) displays the raw, class-specific centroids. As regularization is added, the lines indicate the path each takes toward the global centroid (in black). Notice that the centroid for the first class eventually does not use predictor \\(x_1\\); the path becomes completely vertical when that predictor is removed from the calculations. As a counter-example, the centroid for the third class moves in a straight line towards the center. This indicates that both predictors showed a strong signal, and neither was removed as regularization increased.\nLike distance-based methods, predictors based on data depth (R. Liu, Parelius, and Singh 1999; Ghosh and Chaudhuri 2005; Mozharovskyi, Mosler, and Lange 2015) can be helpful for separating classes. The depth of the data was initially defined by Tukey (1975), and it can be roughly understood as the inverse of the distance to a centroid. For example, Mahalanobis depth would be:\n\\[\nDepth_c(\\boldsymbol{x}_0) = \\left[1 + D_c(\\boldsymbol{x}_0)\\right]^{-1}\n\\]\nThere are more complex depth methods, and some are known for their robustness to outliers. Again, like distances, class-specific depth features can be used as features in a model.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#sec-other-embeddings",
    "href": "chapters/embeddings.html#sec-other-embeddings",
    "title": "7  Embeddings",
    "section": "7.5 Other Methods",
    "text": "7.5 Other Methods\nThere are numerous other methods to create embeddings such as autoencoders (Borisov et al. 2022; Michelucci 2022). Boykis (2023) is an excellent survey and discussion of modern embedding methods, highlighting methods for text and deep learning.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#chapter-references",
    "href": "chapters/embeddings.html#chapter-references",
    "title": "7  Embeddings",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAggarwal, C. 2020. Linear Algebra and Optimization for Machine Learning. Vol. 156. Springer.\n\n\nBanerjee, S, and A Roy. 2014. Linear Algebra and Matrix Analysis for Statistics. Vol. 181. CRC Press.\n\n\nBarker, M, and W Rayens. 2003. “Partial Least Squares for Discrimination.” Journal of Chemometrics: A Journal of the Chemometrics Society 17 (3): 166–73.\n\n\nBelkin, M, and P Niyogi. 2001. “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering.” Advances in Neural Information Processing Systems 14.\n\n\nBengio, Y, JF Paiement, P Vincent, O Delalleau, N Roux, and M Ouimet. 2003. “Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering.” In Advances in Neural Information Processing Systems, edited by S. Thrun, L. Saul, and B. Schölkopf. Vol. 16. MIT Press.\n\n\nBorisov, V, T Leemann, K Seßler, J Haug, M Pawelczyk, and G Kasneci. 2022. “Deep Neural Networks and Tabular Data: A Survey.” IEEE Transactions on Neural Networks and Learning Systems.\n\n\nBoykis, V. 2023. “What are embeddings?” https://doi.org/10.5281/zenodo.8015029.\n\n\nCroux, C, E Ollila, and H Oja. 2002. “Sign and Rank Covariance Matrices: Statistical Properties and Application to Principal Components Analysis.” In Statistical Data Analysis Based on the L1-Norm and Related Methods, 257–69. Springer.\n\n\nEfron, B. 2009. “Empirical Bayes Estimates for Large-Scale Prediction Problems.” Journal of the American Statistical Association 104 (487): 1015–28.\n\n\nEsposito Vinzi, V, and G Russolillo. 2013. “Partial Least Squares Algorithms and Methods.” Wiley Interdisciplinary Reviews: Computational Statistics 5 (1): 1–19.\n\n\nFernández P, J. A., A. Laborde, L. Lakhal, M. Lesnoff, M. Martin, Y. Roggo, and P. Dardenne. 2020. “The Applicability of Vibrational Spectroscopy and Multivariate Analysis for the Characterization of Animal Feed Where the Reference Values Do Not Follow a Normal Distribution: A New Chemometric Challenge Posed at the ’Chimiométrie 2019’ Congress.” Chemometrics and Intelligent Laboratory Systems 202: 104026.\n\n\nGeladi, P., and B Kowalski. 1986. “Partial Least-Squares Regression: A Tutorial.” Analytica Chimica Acta 185: 1–17.\n\n\nGhojogh, B, M Crowley, F Karray, and A Ghodsi. 2023. “Elements of Dimensionality Reduction and Manifold Learning.” In, 185–205. Springer International Publishing.\n\n\nGhosh, A K, and P Chaudhuri. 2005. “On Data Depth and Distribution-Free Discriminant Analysis Using Separating Surfaces.” Bernoulli 11 (1): 1–27.\n\n\nHinton, G, and S Roweis. 2002. “Stochastic Neighbor Embedding.” Advances in Neural Information Processing Systems 15.\n\n\nHyvärinen, Aapo, and Erkki Oja. 2000. “Independent Component Analysis: Algorithms and Applications.” Neural Networks 13 (4-5): 411–30.\n\n\nJohnson, K, and M Kuhn. 2024. “What They Forgot to Tell You about Machine Learning with an Application to Pharmaceutical Manufacturing.” Pharmaceutical Statistics n/a (n/a).\n\n\nJolliffe, I, and J Cadima. 2016. “Principal Component Analysis: A Review and Recent Developments.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 374 (2065): 20150202.\n\n\nKruskal, J. 1964a. “Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.” Psychometrika 29 (1): 1–27.\n\n\nKruskal, J. 1964b. “Nonmetric Multidimensional Scaling: A Numerical Method.” Psychometrika 29 (2): 115–29.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nLarsen, J, and L Clemmensen. 2019. “Deep Learning for Chemometric and Non-Translational Data.” https://arxiv.org/abs/1910.00391.\n\n\nLê Cao, KA, D Rossouw, C Robert-Granié, and P Besse. 2008. “A Sparse PLS for Variable Selection When Integrating Omics Data.” Statistical Applications in Genetics and Molecular Biology 7 (1).\n\n\nLee, D, and S Seung. 1999. “Learning the Parts of Objects by Non-Negative Matrix Factorization.” Nature 401 (6755): 788–91.\n\n\nLee, D, and S Seung. 2000. “Algorithms for Non-Negative Matrix Factorization.” Advances in Neural Information Processing Systems 13.\n\n\nLiu, R, J Parelius, and K Singh. 1999. “Multivariate Analysis by Data Depth: Descriptive Statistics, Graphics and Inference.” The Annals of Statistics 27 (3): 783–858.\n\n\nLiu, Y, and W Rayens. 2007. “PLS and Dimension Reduction for Classification.” Computational Statistics, 189–208.\n\n\nMichelucci, I. 2022. “An Introduction to Autoencoders.” arXiv.\n\n\nMozharovskyi, P, K Mosler, and T Lange. 2015. “Classifying Real-World Data with the DD\\(\\alpha\\)-Procedure.” Advances in Data Analysis and Classification 9 (3): 287–314.\n\n\nNing, B, and N Ning. 2021. “Spike and Slab Bayesian Sparse Principal Component Analysis.” arXiv.\n\n\nNordhausen, K, and H Oja. 2018. “Independent Component Analysis: A Statistical Perspective.” Wiley Interdisciplinary Reviews: Computational Statistics 10 (5): e1440.\n\n\nSainburg, T, L McInnes, and TQ Gentner. 2020. “Parametric UMAP: Learning Embeddings with Deep Neural Networks for Representation and Semi-Supervised Learning.” arXiv.\n\n\nSammon, J. 1969. “A Nonlinear Mapping for Data Structure Analysis.” IEEE Transactions on Computers 100 (5): 401–9.\n\n\nSchein, A, L Saul, and L Ungar. 2003. “A Generalized Linear Model for Principal Component Analysis of Binary Data.” In Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics, edited by C Bishop and B Frey, R4:240–47. Proceedings of Machine Learning Research.\n\n\nSchölkopf, B, A Smola, and KR Müller. 1998. “Nonlinear Component Analysis as a Kernel Eigenvalue Problem.” Neural Computation 10 (5): 1299–319.\n\n\nShen, H, and JZ Huang. 2008. “Sparse Principal Component Analysis via Regularized Low Rank Matrix Approximation.” Journal of Multivariate Analysis 99 (6): 1015–34.\n\n\nSinnott, R. 1984. “Virtues of the Haversine.” Sky and Telescope 68 (2): 158.\n\n\nStone, M, and R Brooks. 1990. “Continuum Regression: Cross-Validated Sequentially Constructed Prediction Embracing Ordinary Least Squares, Partial Least Squares and Principal Components Regression.” Journal of the Royal Statistical Society: Series B (Methodological) 52 (2): 237–58.\n\n\nTenenbaum, J, V Silva, and J Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290 (5500): 2319–23.\n\n\nTibshirani, R, T Hastie, B Narasimhan, and G Chu. 2003. “Class Prediction by Nearest Shrunken Centroids, with Applications to DNA Microarrays.” Statistical Science, 104–17.\n\n\nTipping, M, and C Bishop. 1999. “Probabilistic Principal Component Analysis.” Journal of the Royal Statistical Society Series B: Statistical Methodology 61 (3): 611–22.\n\n\nTorgerson, W. 1952. “Multidimensional Scaling: I. Theory and Method.” Psychometrika 17 (4): 401–19.\n\n\nTukey, J. 1975. “Mathematics and the Picturing of Data.” In Proceedings of the International Congress of Mathematicians, 2:523–31.\n\n\nVan der Maaten, L, and G Hinton. 2008. “Visualizing Data Using t-SNE.” Journal of Machine Learning Research 9 (11).\n\n\nVisuri, S, V Koivunen, and H Oja. 2000. “Sign and Rank Covariance Matrices.” Journal of Statistical Planning and Inference 91 (2): 557–75.\n\n\nWang, S, and J Zhu. 2007. “Improved Centroids Estimation for the Nearest Shrunken Centroid Classifier.” Bioinformatics 23 (8): 972–79.\n\n\nWold, S, H Martens, and H Wold. 1983. “The Multivariate Calibration Problem in Chemistry Solved by the PLS Method.” In Proceedings from the Conference on Matrix Pencils. Heidelberg: Springer-Verlag.\n\n\nWu, W, DL Massart, and S De Jong. 1997. “The Kernel PCA Algorithms for Wide Data. Part I: Theory and Algorithms.” Chemometrics and Intelligent Laboratory Systems 36 (2): 165–72.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/embeddings.html#footnotes",
    "href": "chapters/embeddings.html#footnotes",
    "title": "7  Embeddings",
    "section": "",
    "text": "Retrieved from https://chemom2019.sciencesconf.org/resource/page/id/13.html↩︎\nAs is, this is far too general. We could maximize the variance by making every value of \\(Q\\) equal to \\(\\infty\\). PCA, and other methods, impose an implicit constraint to limit the elements of \\(Q\\). In this case, PCA constrains the loading vectors to have unit length.↩︎\nAssuming that the \\(p\\) columns are linearly independent.↩︎\nThese wavelengths correspond to the 1st and 50th predictors.↩︎\nWe will see similar methods in ?sec-svm-cls.↩︎\nA note about some notation… We commonly think of the norm notation as \\(\\|\\boldsymbol{x}\\|_p = \\left(|x_1|^p + |x_2|^p + \\ldots + |x_n|^p\\right)^{1/p}\\). So what does the lack of a subscript in \\(||\\boldsymbol{x}||^2\\) mean? The convention is the sum of squares: \\(||\\boldsymbol{x}||^2 = x_1^2 + x_2^2 + \\ldots + x_n^2\\).↩︎\nSpecifically gradient descent with a user-defined learning rate.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Embeddings</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html",
    "href": "chapters/interactions-nonlinear.html",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "8.1 Interactions\nWe saw in previous chapters that preprocessing is necessary because models can be sensitive to specific predictor characteristics. For instance, some models:\nPreprocessing methods address aspects of the predictors that place them in a form so that models can be built. This is what the model needs to function.\nWhile preprocessing techniques allow models to be built, they do not necessarily transform them in ways that help the model to identify predictive relationships with the outcome. This is the fundamental concept of feature engineering which addresses the question: what can we do to make it easier for the model to understand and predict the outcome.\nIn this chapter we will discuss techniques that address this question. For example, if the relationship between a predictor is nonlinear, how can we represent that predictor so that the relationship can be modeled? Or if two predictors work in conjunction with each other, then how can this information be engineered for a model? While feature engineering is not necessary for models to be built, it contains a crucial set of tools for improving model performance.\nLet’s look at a simple example. Occasionally, predictor pairs work better in unison rather than as main effects. For example, consider the data in Figure 8.1(a), where two predictors are:\nIn the predictors’ original form, there is a significant overlap between the two classes of samples. However, when the ratio of the predictors is used, the newly derived predictor better discriminates between classes (shown in Figure 8.1(b)). While not a general rule, the three data characteristics above suggest that the modeler attempts to form ratios from two or more predictors 1.\nIt is essential to understand that these data require transformation. We can put the original predictors into a model as-is. The model won’t produce an error but won’t have good predictive performance. We can induce a separation of the classes when a joint feature is used.\nThis aspect of feature engineering depends on the data set so it is difficult to enumerate all possible techniques. In this chapter, we’ll describe a few commonly used methods: splines, interactions, and discretization (a.k.a. binning).\nWhen building models for prediction, the majority of variation in the outcome is generally explained by the cumulative effect of the important individual predictors. For many problems, additional variation in the response can be explained by the effect of two or more predictors working in conjunction with each other.\nThe healthcare industry has long understood the concept of interactions among drugs for treating specific diseases Altorki et al. (2021). As an example of an interaction, consider treatment for the disease non-small cell lung cancer (NSCLC). In a recent study, patients with an advanced stage of NSCLC with an EGFR mutation were given either osimertinib alone or osimertinib in combination with traditional chemotherapy (Planchard et al. 2023). Patients taking the combination treatment had a significantly longer progression-free survival time than patients taking osimertinib alone. Hence, the interaction of the treatments is more effective than the single treatment. To summarize, two (or more) predictors interact if their combined impact is different (less or greater) than what would be expected from the added impact of each predictor alone.\nWe’ve already encountered an example of an interaction in Section 2.3 where the relationship between delivery time and the time of order differed across days of the week. This trend is reproduced in Figure 8.2(a). The telltale sign of the interaction is that the trendlines are not parallel with one another; they have different rates of increase and cross.\nHow would this plot change if there was no interaction between the order time and day? To illustrate, we estimated trendlines in a way that coerced the nonlinear trend for order time to be the same. Figure 8.2(b) shows the results: parallel lines for each day of the week.\nFigure 8.2: Delivery times versus the time of the order, colored by the day of the week. (a) A duplicate of a panel from Figure 2.2. (b) The same data with trendlines where the underlying model did not allow an interaction between the delivery day and hour.\nThis example demonstrates an interaction between a numeric predictor (hour of order) and a categorical predictor (day of the week). Interactions can also occur between two (or more) numeric or categorical predictors. Using the delivery data, let’s examine potential interactions solely between categorical predictor columns.\nFor regression problems, one visualization technique is to compute the means (or medians) of the outcome for all combinations of variables and then plot these means in a manner similar to the previous figure. Let’s look at the 27 predictors columns for whether a specific item was included in the order. The original column contains counts, but the data are mostly zero or one. We’ll look at two variables at a time and plot the four combinations of whether the item was ordered at all (versus not at all). Figure 8.3 shows two potential sets of interactions. The x-axis indicates whether Item 1 was in the order or not. The y-axis is the mean delivery time with 90% confidence intervals2.\nThe left panel shows the joint effect of items 1 and 9. The lines connecting the means are parallel. This indicates that each of these two predictors affects the outcome independently of one another. Specifically, the incremental change in delivery time is the same when item 9 is or is not included with item 1. The right panel has means for items 1 and 10. The mean delivery times are very similar when neither is contained in the order. Also, when only one of the two items is in the order, the average time is similarly small. However, when both are included, the delivery time becomes much larger. This means that you cannot consider the effect or either item 1 or 10 alone; their effect on the outcome occurs jointly.\nFigure 8.3: Interaction examples with two categorical predictors. Items 1 and 9 (left panel) do not interaction, while items 1 and 10 appear to have a strong interaction (right panel).\nAs a third example, interactions may occur between continuous predictors. It can be difficult to discover the interaction via visualization for two numeric predictors without converting one of the predictors to categorical bins. To illustrate the concept of an interaction between two numeric predictors, \\(x_1\\) and \\(x_2\\), let’s use a simple linear equation:\n\\[\ny = \\beta_0 + \\beta_{1}x_1 + \\beta_{2}x_2 + \\beta_{3}x_{1}x_{2} + \\epsilon\n\\tag{8.1}\\]\nThe \\(\\beta\\) coefficients represent the overall average response (\\(\\beta_0\\)), the average rate of change for each individual predictor (\\(\\beta_1\\) and \\(\\beta_2\\)), and the incremental rate of change due to the combined effect of \\(x_1\\) and \\(x_2\\) (\\(\\beta_3\\)) that goes beyond what \\(x_1\\) and \\(x_2\\) can explain alone. The parameters for this equation can be estimated using a technique such as ordinary least squares (REF). The sign and magnitude of \\(\\beta_3\\) indicate how and the extent to which the two predictors interact:\nTo understand this better, Figure 8.4 shows a contour plot of a predicted linear regression model with various combinations of the model slope parameters. The two predictors are centered at zero with values ranging within \\(x_j \\pm 4.0\\)). The default setting shows a moderate synergistic interaction effect since all of the \\(\\beta_j = 1.0\\)). In the plot, darker values indicate smaller predicted values.\nInteraction effects between predictors are sometimes confused with correlation between predictors. They are not the same thing. Interactions are defined by their relationship to the outcome while between-predictors correlations are unrelated to it. An interaction could occur independent of the amount of correlation between predictors.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-interactions",
    "href": "chapters/interactions-nonlinear.html#sec-interactions",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "When \\(\\beta_3\\) is positive, the interaction is synergistic since the response increases beyond the effect of either predictor alone.\n\nAlternatively, when \\(\\beta_3\\) is negative, the interaction is antagonistic since the response decreases beyond the effect of either predictor alone.\n\nA third scenario is when \\(\\beta_3\\) is essentially zero. In this case, there is no interaction between the predictors and the relationship between the predictors is additive.\nFinally, for some data sets, we may find that neither predictor is important individually (i.e., \\(\\beta_1\\) and \\(\\beta_2\\) are zero). However, the coefficient on the interaction term is not zero. Because this case occurs very infrequently, it is called atypical.\n\n\n\n\n\n\n#| label: shiny-interaction-contours\n#| viewerHeight: 600\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(viridis)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-interaction-contours.R\")\n\napp\n\n\n\nFigure 8.4: Prediction contours for a linear regression model with Equation 8.1 with \\(\\beta_0 = 0\\). Darker values indicate smaller predictions.\n\n\n\n\n\n\n8.1.1 How Likely are Interactions?\nWithin the context of statistical experimental design, Hamada and Wu (1992) discuss some probabilistic aspects of predictor importance. The effect sparsity principle is that there are often few predictors that are relevant for predicting the outcome. Similarly, the effect hierarchy principle states that “main effects” (i.e. a feature involving only predictor) are more likely to occur than interactions. Also, as more predictors are involved in the interaction, the less likely they become. Finally, the heredity principle conjectures that if an interaction is important, it is very likely that the corresponding main effects are likely too. Chipman (1996) further expands this principle.\nThe original context of these principles was envisioned for screening large numbers of predictors in experimental designs; they are still very relevant for the analysis of tabular data.\n\n\n8.1.2 Detecting Interactions\nAn ideal scenario would be that we would know which predictors interact before modeling the data. If this would be the case, then these terms could be included in a model. This would be ideal because the model could more easily find the relationship with the response, thus leading to better predictive performance. Unfortunately, knowledge of which predictors interact is usually not available prior to initiating the modeling process.\nIf meaningful interactions are unknown before modeling, can models still discover and utilize these potentially important features? Recent studies using various advanced modeling techniques have shown that some methods can inherently detect interactions. For example, tree-based models (Elith, Leathwick, and Hastie 2008), random forests (García-Magariños et al. 2009), boosted trees (Lampa et al. 2014), and support vector machines (Chen et al. 2008) are effective at uncovering them.\nIf modern modeling techniques can naturally find and utilize interactions, then why is it necessary to spend any time uncovering these relationships? The first reason is to improve interpretability. Recall the trade-off between prediction and interpretation that was discussed in Section TODO. More complex models are less interpretable and generally more predictive, while simpler models are more interpretable and less predictive. Therefore, if we know which interaction(s) are important, we can include these in a simpler model to enable a better interpretation. A second reason to spend time uncovering interactions is to help improve the predictive performance of models.\nWhat should we do if we have a candidate set of interactions? If we are using a more formal statistical model, such as linear or logistic regression, the traditional tool for evaluating whether additional model terms in a set of nested models are worth including is the conventional analysis of variance (ANOVA). This uses the statistical likelihood value as the objective function, which is equivalent to comparing the RMSE values between models for linear regression 3. The error reduction and how many additional terms are responsible for the improvement are computed. Using these results and some probability assumptions about the data, we can formally test the null hypothesis that the additional parameters all have coefficient values of zero.\nLooking at the delivery data, our model in Section 2.4 included a single set of interactions (e.g., hour-by-day). What if we included one more interaction: item 1 \\(\\times\\) item 10? Table 8.1 shows the ANOVA results. The RMSE computed on the training set is listed in the first and second columns. The reduction by including this additional model term is 0.18 (decimal minutes). Assuming normality of the model residuals, the p-value for this test is exceedingly small. This indicates that there is no evidence that this parameter is truly zero. In other words, there is strong evidence that the inclusion of the interaction helps explain the response. This statistical difference may not make much of a practical difference. However, machine learning models often behave like “a game of inches” where every small improvement adds up to an overall improvement that matters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Set RMSE\n\n\n\nValidation Set\n\n\n\nInteractions\nDecimal\nTime\nDeg. Free.\np-Value\nMAE\nRMSE\n\n\n\n\nHour x Day\n2.30\n(2m, 17s)\n5,890\n\n1.61\n2.29\n\n\nAdd One Item Interaction\n2.12\n(2m, 6s)\n5,889\n10-219.8\n1.56\n2.10\n\n\nAdd All Item Interactions\n2.06\n(2m, 3s)\n5,539\n10-18.4\n1.60\n2.13\n\n\n\n\n\n\n\n\nTable 8.1: ANOVA table and validation set statistics for three models with different interaction sets.\n\n\n\n\n\n\n\nWhat about the other two-way interactions between the other item predictors? Since the training set is fairly large (compared to the current number of model parameters), it is feasible to include the remaining 350 pairwise interactions and use the ANOVA method to validate this choice. These results are contained in the third row of Table 8.1 where the RMSE dropped further by 0.05 minutes. The p-value is also very small, indicating that there is no evidence that all of the 350 parameter estimates are zero. More extensive testing would be required to determine which actually are zero. This can be tedious and a potentially dangerous “fishing expedition” that could result in serious bias creeping into the modeling process.\nOne problem with the ANOVA method is that it calculates the model error using the training set, which we know may not indicate what would occur with previously unseen data. In fact, it is well known that, for linear regression via ordinary least squares estimation, it is impossible for the training set error to ever increase when adding new model terms. Therefore, was the drop in RMSE when all interactions were included due to this fact? Or was it due to other important interactions that were included? To understand this we can turn to the validation set to provide confirmation. You can see this in Table 8.1 where the validation set RMSE and MAE values are included. Note that, when the large set of interactions were added, these metrics both increase in the validation set, a result contrary to what the ANOVA results tell us.\nThe observed and predicted visualizations for each model in Table 8.1 are shown in Figure 8.5. Adding the additional interaction yielded a slight numerical improvement. However, the visualization in the middle panel shows fewer very large residuals. The figure also shows the model results that include the full set of 351 two-way interactions; this panel shows no significant reduction in large residuals, further casting doubt on using the entire set.\n\n\n\n\n\n\n\n\nFigure 8.5: Validation set predicted vs. observed delivery times for linear regression models with and without additional interactions.\n\n\n\n\n\nSo far, we have used visualizations to unearth potential interactions. This can be an effective strategy when the number of potential interactions is large, but the visual nature of this process is subjective. There are some specialized quantitative tools for identifying interactions. For example, Lim and Hastie (2015) used regularized generalized linear models (sections TODO) to estimate all possible two-way interactions and use a penalization method to determine which should be retained. Miller (1984) and Kuhn and Johnson (2019) (Section 7.4.3) describe the feasible solution algorithm, an iterative search method for interaction discovery. Another, which we will now describe in more detail, is Friedman’s H-statistic (Friedman and Popescu 2008).\nWe can estimate the joint effect of a set of predictors on the outcome as well as the effect of individual predictors. If we thought that only main effects and two-factor interactions were possible, we could factor out the individual effects from the joint effect. The leftover predictive ability would then be due to interactions. Consider the linear model in Equation 8.1. The joint effect would include all possible model terms associated with a predictor. We can also create main effects too:\n\\[\\begin{align}\nf(x_1, x_2) &= \\beta_{1}x_1 + \\beta_{2}x_2 + \\beta_{3}x_{1}x_{2} \\notag \\\\\nf(x_1) &= \\beta_{1}x_1  \\notag \\\\\nf(x_2) &= \\beta_{2}x_2 \\notag\n\\end{align}\\]\nTo isolate the potential interaction effect:\n\\[\nf(x_1\\times x_2) = f(x_1, x_2) - f(x_1) - f(x_2) = \\beta_{3}x_{1}x_{2}\n\\tag{8.2}\\]\nThis shows that, for this situation, we can isolate the effect of an interaction by removing any other systematic effects in the data4. Equation 8.2 is based on a simple parametric linear model. For models with more complex prediction equations, we can’t analytically pick out which model parameters should be used to investigate potential interactions.\nHowever, for any model, the joint and marginal effects can be quantified using partial dependence profiles (PDP) (Molnar (2020), Section 8.1). First, we determine a sequence of values covering the observed range of the predictor(s) of interest. Then we randomly sample a data point, perhaps from the training set, and over-ride the value of the predictor of interest with values from the grid. This produces a prediction profile over the grid. We can repeat this process many times to approximate \\(f(\\cdot)\\) by averaging the multiple prediction values for each grid point.\nFigure 8.6 visualizes the PDP data derived from using the ensembles of regression trees for \\(f(hour)\\), \\(f(day)\\), and \\(f(hour, day)\\). The first panel shows the random realizations of the relationship between delivery time and order hour from 1,000 randomly sampled rows of the training set. The results are unsurprising; we can see a similarity between these results and the initial visualizations in Figure 2.2. The single trend line in panel (b) is the average of these profiles for each value of the predictor (delivery hour). There appears to be, on average, a nonlinear effect of the delivery hour. The day of the week is an informative predictor and the joint effect profile in panel (d) shows that its effect induces different patterns in the delivery hour. If this were not the case, the patterns in panels (b) and (c) would, when combined, approximate the pattern in panel (d).\n\n\n\n\n\n\n\n\nFigure 8.6: Partial dependence profiles for the food delivery hour and day, derived for the tree-based ensemble. Panel (a) shows the individual grid predictions from 1,000 randomly sampled points in the training set. The other panels show the average trends.\n\n\n\n\n\nFriedman’s H-statistic can quantify the effect of interaction terms using partial dependence profiles. When investigating two-factor interactions between predictors \\(j\\) and \\(j'\\), the statistic is\n\\[\nH^2_{jj'}=\\frac{\\sum_\\limits{i=1}^B\\left[\\hat{f}(x_{ij},x_{ij'})-\\hat{f}(x_{ij})-\\hat{f}(x_{ij'})\\right]}{ \\sum_\\limits{i=1}^B\\hat{f}^2(x_{ij}, x_{ij'})}\n\\]\nwhere the \\(\\hat{f}(x_{ij})\\) term represents the partial dependence profile for predictor \\(j\\) for sample point \\(i\\) along the grid for that predictor and \\(B\\) is the number of training set samples. The denominator captures the total joint effect of predictors \\(j\\) and \\(j'\\) so that \\(H^2\\) can be interpreted as the fraction of the joint effect explained by the potential interaction.\nFor a set of \\(p\\) predictors, we could compute all \\(p(p-1)/2\\) pairwise interactions and rank potential interactions by their statistic values. This can become computationally intractable at some point. One potential shortcut suggested by the heredity principle is to quantify the importance of the \\(p\\) predictors and look at all pairwise combinations of the \\(p^*\\) most important values. For \\(p^* = 10\\), Figure 8.7 shows the top five interactions detected by this procedure. We’ve already visually identified the hour \\(\\times\\) day interaction, so seeing this in the rankings is comforting. However, another large interaction effect corresponds to the variables for items #1 and #10. Discovering this led us to visually confirm the effect back in Figure 8.3.\n\n\n\n\n\n\n\n\nFigure 8.7: \\(H^2\\) statistics for the top ten pairwise interaction terms.\n\n\n\n\n\nNote that the overall importance of the features are being quantified. From this analysis alone, we are not informed that the relationships between the hour and distance predictors and the outcome are nonlinear. It also doesn’t give any sense of the direction of the interaction (e.g., synergistic or antagonistic). We suggest using this tool early in the exploratory data analysis process to help focus visualizations on specific variables to understand how they relate to the outcome and other predictors.\nThere is a version of the statistic that can compute how much a specific predictor is interacting with any other predictor. It is also possible to compute a statistical test to understand if the H statistic is different than zero. In our opinion, it is more helpful to use these values as diagnostics rather than the significant/insignificant thinking that often accompanies formal statistical hypothesis testing results.\nInglis, Parnell, and Hurley (2022) discuss weak spots in the usage of the H-statistic, notably that correlated predictors can cause abnormally large values. This is an issue inherited from the use of partial dependence profiles (Apley and Zhu 2020).\nAlternatively, Greenwell, Boehmke, and McCarthy (2018) measures the potential for variables to interact by assessing the “flatness” over regions of the partial dependence profile. The importance of a predictor, or a pair of predictors, is determined by computing the standard deviation of the flatness scores over regions. For example, Figure 8.3 shows a flat profile for the item 1 \\(\\times\\) item 9 interaction while the item 1 \\(\\times\\) item 10 interaction has different ranges of means across values of item 1. Figure 8.8 shows ten interactions with the largest flatness importance statistics. As with the \\(H^2\\) results, the hour-by-day interaction. The two top ten lists have 6 other interactions in common.\n\n\n\n\n\n\n\n\nFigure 8.8: Importance statistics for the top ten pairwise interaction terms.\n\n\n\n\n\nOther alternatives can be found in Hooker (2004), Herbinger, Bischl, and Casalicchio (2022), and Oh (2022).\nThe two PDP approaches we have described apply to any model capable of estimating interactions. There are also model-specific procedures for describing which joint effects are driving the model (if any). One method useful for tree-based models is understanding if two predictors are used in consecutive splits in the data. For example, Figure 2.5 showed a shallow regression tree for the delivery time data. The terminal nodes had splits with orderings \\(\\text{hour}\\rightarrow\\text{hour}\\) and \\(\\text{hour}\\rightarrow\\text{day} \\rightarrow\\text{distance}\\) (twice). This implies that these three predictors have a higher potential to be involved in interactions with one another.\nKapelner and Bleich (2013) describe an algorithm that counts how many times pairs of variables are used in consecutive splits in a tree. They demonstrate this process with Bayesian Adaptive Regression Trees (BART, ?sec-bart-cls) which creates an ensemble of fairly shallow trees. Figure 8.9 shows the top ten pairwise interactions produced using this technique. Note that the BART implementation split categorical predictors via single categories. For the delivery data, this means that the tree would split on a specific day of the week (.i.e., Friday or not) instead of splitting all of the categories at once. This makes a comparison between the other two methods difficult. However, it can be seen that the hour \\(\\times\\) day interaction has shown to be very important in all three methods.\n\n\n\n\n\n\n\n\nFigure 8.9: BART model statistics identifying interactions by how often they occur in the tree rules.\n\n\n\n\n\nCaution should be exercised with this method. First, some tree ensembles randomly sample a subset of predictors for each split (commonly known as \\(m_{try}\\)). For example, if \\(m_{try} = 4\\), a random set of four predictors are the only ones considered for that split point. This deliberately coerces non-informative splits; we are not using the best possible predictors in a split. This can dilute the effectiveness of counting predictors in successive splits. Second, as discussed in ?sec-cls-trees, many tree-based splitting procedures disadvantage predictors with fewer unique values. For example, in the delivery data, the distance and hour predictors are more likely to be chosen for splitting than the item predictors even if they are equally informative. There are splitting procedures that correct for this bias, but users should be aware of the potential impartiality.\nThe H-statistic and its alternatives can be an incredibly valuable tool for learning about the data early on as well as suggesting new features that should be included in the model. They are, however, computationally expensive.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-polynomials",
    "href": "chapters/interactions-nonlinear.html#sec-polynomials",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.2 Polynomial Basis Expansions",
    "text": "8.2 Polynomial Basis Expansions\nIn the food delivery data, we have thoroughly demonstrated that there are quantitative predictors that have nonlinear relationships with the outcome. In Section 2.4, two of the three models for these data could naturally estimate nonlinear trends. Linear regression, however, could not. To fix this, the hour predictor spawned multiple features called spline terms and adding these to the model enables it to fit nonlinear patterns. This approach is called a basis expansion. In this section, we’ll consider (global) polynomial basis expansions.\nThe most traditional basis function is the polynomial expansion. If we start with a simple linear regression model with a single predictor:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\epsilon_i\n\\]\nthe expansion would add additional terms with values of the predictor exponentiated to the \\(p^{th}\\) power5. For a cubic polynomial model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_{i}^3 + \\epsilon_i\n\\tag{8.3}\\]\nThis is a linear model in the sense that it is linear in the statistical parameters (the \\(\\beta\\) values), so we can use standard parameter estimation procedures (e.g., least squares).\nFigure 8.10 shows data from Bralower et al. (1997) which was collected to understand the relationship between the age of a fossil and the ratio of two radioactive isotopes. The data set contains 106 data points for these two variables. The relationship is highly nonlinear, with several regions where the data have significant convex or concave patterns.\nFor simplicity, a fit from a cubic model Equation 8.3 is shown as the default. While the pattern is nonlinear, the line does not conform well to the data. The lines above the plot show the three basis functions that, for \\(p=3\\), correspond to linear (\\(x\\)), quadratic (\\(x^2\\)), and cubic (\\(x^3\\)) terms. The effects of these terms are blended using their estimated slopes (\\(\\hat{\\beta}_j\\)) to produce the fitted line. Notice that the cubic fit is very similar to the cubic basis shown at the top of the figure. The estimated coefficient for the cubic term is much larger than the linear or quadratic coefficients\n\n\n\n\n#| label: shiny-global-polynomial\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(splines2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(aspline)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\n# source(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-polynomial.R\")\n\n# Requires the sources in shiny-setup.R\n\ndata(fossil)\n\nui &lt;- page_fillable(\n  # theme = grid_theme,\n  padding = \"1rem\",\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-3, 6, -3)),\n    sliderInput(\n      \"global_deg\",\n      label = \"Polynomial Degree\",\n      min = 1L,\n      max = 20L,\n      step = 1L,\n      value = 3L,\n      ticks = TRUE\n    ) # sliderInput\n  ), # layout_columns\n  layout_columns(\n    fill = FALSE,\n    col_widths = breakpoints(sm = c(-1, 10, -1)),\n    as_fill_carrier(plotOutput('global'))\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  maybe_lm &lt;- function(x) {\n    try(lm(y ~ poly(x, input$piecewise_deg), data = x), silent = TRUE)\n  }\n  \n  names_zero_padded &lt;- function(num, prefix = \"x\", call = rlang::caller_env()) {\n    rlang:::check_number_whole(num, min = 1, call = call)\n    ind &lt;- format(seq_len(num))\n    ind &lt;- gsub(\" \", \"0\", ind)\n    paste0(prefix, ind)\n  }\n  \n  expansion_to_tibble &lt;- function(x, original, prefix = \"term \") {\n    cls &lt;- class(x)[1]\n    nms &lt;- names_zero_padded(ncol(x), prefix)\n    colnames(x) &lt;- nms\n    x &lt;- as_tibble(x)\n    x$variable &lt;- original\n    res &lt;- tidyr::pivot_longer(x, cols = c(-variable))\n    if (cls != \"poly\") {\n      res &lt;- res[res$value &gt; .Machine$double.eps, ]\n    }\n    res\n  }\n  \n  mult_poly &lt;- function(dat, degree = 4) {\n    rng &lt;- extendrange(dat$x, f = .025)\n    grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n    grid_df &lt;- tibble(x = grid)\n    feat &lt;- poly(grid_df$x, degree)\n    res &lt;- expansion_to_tibble(feat, grid_df$x)\n    \n    # make some random names so that we can plot the features with distinct colors\n    rand_names &lt;- lapply(\n      1:degree,\n      function(x) paste0(sample(letters)[1:10], collapse = \"\")\n    )\n    rand_names &lt;- unlist(rand_names)\n    rand_names &lt;- tibble(name = unique(res$name), name2 = rand_names)\n    res &lt;- dplyr::inner_join(res, rand_names, by = dplyr::join_by(name)) %&gt;%\n      dplyr::select(-name) %&gt;%\n      dplyr::rename(name = name2)\n    res\n  }\n  \n  col_rect &lt;- ggplot2::element_rect(fill = \"#fcfefe\", colour = \"#fcfefe\")\n  theme_light_bl &lt;- function() {\n    ggplot2::theme(\n      panel.background = col_rect,\n      panel.border = col_rect,\n      legend.background = col_rect,\n      legend.key = col_rect,\n      plot.background = col_rect\n    )\n  }\n  \n  # ------------------------------------------------------------------------------\n  \n  spline_example &lt;- tibble(x = fossil$age, y = fossil$strontium.ratio)\n  rng &lt;- extendrange(fossil$age, f = .025)\n  grid &lt;- seq(rng[1], rng[2], length.out = 1000)\n  grid_df &lt;- tibble(x = grid)\n  alphas &lt;- 1 / 4\n  line_wd &lt;- 1.0\n  \n  base_p &lt;- spline_example %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point(alpha = 3 / 4, pch = 1, cex = 3) +\n    labs(x = \"Age\", y = \"Isotope Ratio\") +\n    lims(x = rng) +\n    theme_bw()\n  \n  output$global &lt;- renderPlot({\n    poly_fit &lt;- lm(y ~ poly(x, input$global_deg), data = spline_example)\n    poly_pred &lt;- predict(\n      poly_fit,\n      grid_df,\n      interval = \"confidence\",\n      level = .90\n    ) %&gt;%\n      bind_cols(grid_df)\n    \n    global_p &lt;- base_p\n    \n    if (input$global_deg &gt; 0) {\n      global_p &lt;- global_p +\n        geom_ribbon(\n          data = poly_pred,\n          aes(y = NULL, ymin = lwr, ymax = upr),\n          alpha = 1 / 15\n        ) +\n        geom_line(\n          data = poly_pred,\n          aes(y = fit),\n          col = \"black\",\n          linewidth = line_wd\n        ) +\n        theme(\n          plot.margin = margin(t = -20, r = 0, b = 0, l = 0),\n          panel.background = col_rect,\n          plot.background = col_rect,\n          legend.background = col_rect,\n          legend.key = col_rect\n        )\n      \n      feature_p &lt;- poly(grid_df$x, input$global_deg) %&gt;%\n        expansion_to_tibble(grid_df$x) %&gt;%\n        ggplot(aes(variable, y = value, group = name, col = name)) +\n        geom_line(show.legend = FALSE) + # , linewidth = 1, alpha = 1 / 2\n        theme_void() +\n        theme(\n          plot.margin = margin(t = 0, r = 0, b = -20, l = 0),\n          panel.background = col_rect,\n          plot.background = col_rect,\n          legend.background = col_rect,\n          legend.key = col_rect\n        ) +\n        scale_color_viridis(discrete = TRUE, option = \"turbo\")\n      \n      p &lt;- (feature_p / global_p) + plot_layout(heights = c(1.5, 4))\n    }\n    \n    print(p)\n  })\n}\n\napp &lt;- shinyApp(ui, server)\n\napp\n\n\n\nFigure 8.10: A global polynomial approach to modeling data from Bralower et al. (1997) along with a representation of the features (above the data plot). The black dashed line is the true function and the shaded region is the 90% confidence intervals around the mean.\n\n\n\nWe might improve the fit by increasing the model complexity, i.e., adding additional polynomial terms. An eight-degree polynomial seems to fit better, especially in the middle of the data where the pattern is most dynamic. However, there are two issues.\nFirst, we can see from the 90% confidence bands around the line that the variance can be very large, especially at the ends. The fitted line and confidence bands go slightly beyond the range of the data (i.e., extrapolation). Polynomial functions become erratic when they are not close to observed data points. This becomes increasingly pathological as the polynomial degree increases. For example, in Figure 8.10, choosing degrees greater than 14 will show extreme uncertainty in the variance of the mean fit (the solid line). This is a good example of the variance-bias tradeoff discussed in Section 8.4 below.\nDecreasing the polynomial degree will result in less complex patterns that tend to underfit the data. Increasing the degree will result in the fitted line being closer to the data, but, as evidenced by the confidence interval in the shaded region, the uncertainty in the model explodes (especially near or outside of the range range). This is due to severe overfitting. Eventually, you can increase the degree until the curve passes through every data point. However, the fit for any other values will be wildly inaccurate and unstable.\nSecond, the trend is concave for age values greater than 121 when it probably should be flat. The issue here is that a global polynomial pattern is stipulated. The effect of each model term is the same across the entire data range6. We increased the polynomial degree to eight to improve the fit via additional nonlinearity. Unfortunately, this means that some parts of the data range will be too nonlinear than required, resulting in poor fit.\nA global polynomial is often insufficient because the nonlinear relationships are different in different data sections. There may be steep increases in one area and gradual increases in others. Rather than using a global polynomial, what if we used different basis expansions in regions with more consistent trends rather than a global polynomial?\nFigure 8.11 shows a crude approach where three different regions have different polynomial fits with the same degree. The pre-set degree and data ranges are probably as good as can be (after much manual fiddling) but the approach isn’t terribly effective. The curves do not connect. This discontinuity is visually discordant and most likely inconsistent with the true, underlying pattern we are attempting to estimate. Also, there are large jumps in uncertainty when predicting values at the extremes of each region’s range.\n\n\n\n\n#| label: shiny-piecewise-polynomials\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(splines2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(aspline)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-piecewise.R\")\n\napp\n\n\n\nFigure 8.11: A piecewise polynomial approach to model the data from Figure 8.10.\n\n\n\nIt turns out that we can make sure that the different fitted curves can connect at the ends and that they do that smoothly. This is done via a technique called splines which yield better results and are more mathematically elegant.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-splines",
    "href": "chapters/interactions-nonlinear.html#sec-splines",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.3 Spline Functions",
    "text": "8.3 Spline Functions\nLet’s assume we use piecewise cubic polynomials in the regional curve fits. To make the curves connect, we can add additional features that isolate local areas of \\(x\\) via a new feature column such as \\(h(x - \\xi)^3\\) where\n\\[\nh(u)  =\n\\begin{cases} x & \\text{if $u &gt; 0$,}\n\\\\\n0 & \\text{if $u \\le 0$.}\n\\end{cases}\n\\]\nand \\(\\xi\\) defines one of the boundary points.\nThis zeros out parts of the predictor’s range that are greater than \\(\\xi\\). A smooth, continuous model would include a global polynomial expansion (i.e., \\(x\\), \\(x^2\\), and \\(x^3\\)) and these partial features for each value that defines the regions (these breakpoints, denoted with \\(\\xi\\), are called knots7). For example, if a model has three regions, the basis expansion has 3 + 2 = 5 feature columns. For the fossil data, a three region model with knots at \\(\\xi_1 = 107\\) and \\(\\xi_2 = 114\\) would have model terms\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x^2 + \\beta_3x^3 + \\beta_4h(x_i - 107)^3 + \\beta_5h(x_i - 114)^3 + \\epsilon_i\n\\tag{8.4}\\]\nThe model fit, with the default interior knots, shown in Figure 8.12 looks acceptable. As with global polynomials, there is a significant increase in the confidence interval width as the model begins to extrapolate slightly beyond the data used to fit the model.\n\n\n\n\n#| label: shiny-simple-spline\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(bslib)\nlibrary(aspline)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-spline-manual-knots.R\")\n\napp\n\n\n\nFigure 8.12: A simple cubic spline function with three regions. By default, the regions are partitions at age values of 107 and 114. The fitted curve has the same form as Equation 8.4. The two curves above the data plot indicate the range and values of the partial featured truncated using the function \\(h(\\cdot)\\).\n\n\n\nLet’s consider how each model parameter affects the predictor’s space by labeling the regions A, B, and C (from left to right). For region A, the model is a standard cubic function (using \\(\\beta_0\\) though \\(\\beta_3\\)). Region B adds the effect of the first partial function by including \\(\\beta_4\\). Finally, region C is a function of all five model parameters. This representation of a spline is called a truncated power series and is mostly used to demonstrate how spline functions can flexibly model local trends.\n\nThe word “spline” is used in many different contexts. It’s confusing, but we’ll try to clear it up.\nWe’ll define a “spline” or “spline function” as a collection of features that represent continuous and smooth piece-wise polynomial features of a single-number predictor (with some specific properties).\n“Regression splines” usually describe the use of splines of one or more predictors in a model fit such as ordinary least squares (Wood 2006; Arnold, Kane, and Lewis 2019).\nThe term “smoothing splines” refers to a procedure that encapsulates both a spline function as well as an estimation method to fit it to the data (usually via regularization). There is typically a knot for each unique data point in \\(x\\) (Wang 2011).\nAlso, there are many types of spline functions; we’ll focus on one but describe a few others at the end of this section.\n\nA different method for representing spline basis expansions is the B-spline (De Boor 2003). The terms in the model are parameterized in a completely different manner. Instead of the regions using increasingly more model parameters to define the regional fit, B-spline terms have more localized features such that only a few are “active” (i.e., non-zero) at a particular point \\(x_0\\). This can be seen in Figure 8.13 which contains the basis function values for the truncated power series and B-spines\n\n\n\n\n\n\n\n\nFigure 8.13: Two representations of a cubic spline for the fossil data with knots at 107 and 114. The top panel illustrates the model terms defined by Equation 8.4. The bottom panel shows the B-spline representation. All lines have been scaled to the same y-axis range for easy visualization.\n\n\n\n\n\nB-spline parameterizations are used primarily for their numerical properties and, for this reason, we’ll use them to visualize different types of splines.\nBefore considering another type of spline function, let’s look at one practical aspect of splines: choosing the knots.\n\n8.3.1 Defining the Knots\nHow do we choose the knots? We can select them manually, and some authors advocate for this point of view. As stated by Breiman (1988):\n\nMy impression, after much experimentation, is the same - that few knots suffice providing that they are in the right place.\n\nA good example is seen in Figure 8.12 with the fossil data. Using the knot positioning widget at the top, we can find good and bad choices for \\(\\xi_1\\) and \\(\\xi_2\\); knot placement can matter. However, hand-curating each feature becomes practically infeasible as the number of knots/features increases. Otherwise, they can be set algorithmically.\nThere are two main choices for automatic knot selection. The first uses a sequence of equally-spaced values encompassing the predictor space. The second is to estimate percentiles of the predictor data so that regions have about the same number of values they capture. For example, a fourth degree of freedom spline would have three split points within the data arranged at the 25th, 50th, and 75th percentiles (with the minimum and maximum values bracketing the outer regions).\nIt would be unwise to place knots as evenly spaced values between the minimum and maximum values. Without taking the distribution of the predictor into account, it is possible that the region between some knots might not contain any training data. Since there are usually several overlapping spline features for a given predictor value, we can still estimate the model despite empty regions (but it should be avoided).\nMany, but not all, splines specify how complex the fit should be in the area between the knots. However, cubic splines are a common choice because they allow for greater flexibility than linear or quadratic fits, but are not overly flexible, which could lead to over-fitting. Increasing the polynomial degree beyond three has more disadvantages than advantages. Recall that, in Figure 8.10, variance exploded at the ends of the predictor distribution when the polynomial degree was very large.\n\n\n8.3.2 Natural Cubic Splines\nThe type of spline that we suggest using by default is the natural cubic spline8 (Stone and Koo 1985; Arnold, Kane, and Lewis 2019; Gauthier, Wu, and Gooley 2020). It uses cubic fits in the interior regions and linear fits in regions on either end of the predictor distribution. For a simple two-knot model similar to Equation 8.4, there are only three slope parameters (the global quadratic and cubic terms are not used):\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2h(x_i - \\xi_1)^3 + \\beta_3h(x_i - \\xi_2)^3 + \\epsilon_i\n\\tag{8.5}\\]\nFor the fossil data, Figure 8.14 shows how this model performs with different numbers of knots chosen using quantiles. The 90% confidence bands show an increase in uncertainty when extrapolating, but the increase is orders of magnitude smaller than a global polynomial with the same number of model terms.\n\n\n\n\n#| label: shiny-natural-cubic-spline\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n\nlibrary(shiny)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(splines2)\nlibrary(bslib)\nlibrary(viridis)\nlibrary(aspline)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-spline-natural.R\")\n\napp\n\n\n\nFigure 8.14: A natural cubic spline fit to the data previously shown in Figure 8.12. Good fits occur when approximately nine degrees of freedom are used. The top curves are the B-spline representation of the model terms.\n\n\n\n\nGiven the simplicity of this spline function, the fixed polynomial degree, and its stability in the tails of the predictor’s distribution, we suggest choosing natural cubic splines for most problems.\n\nSplines can be configured in different ways. Two details to specify are how many regions should be separately modeled and how they are apportioned. The number of regions is related to how many features are used in the basis expansion. As the number of regions increases, so does the ability of the spline to adapt to whatever trend might be in the data. However, the risk of overfitting the model to individual data points increases as the flexibility increases.\nFor this reason, we often tune the amount of complexity that the spline will accommodate. Generally, since the number of regions is related to the number of features, we’ll refer to the complexity of the basis function via the number of degrees of freedom afforded by the spline. We don’t necessarily know how many degrees of freedom to use. There are two ways to determine this. First, we can treat the complexity of the spline as a tuning parameter and optimize it with the tuning methods mentioned in Chapters 11 and 12. Another option is to over-specify the number of degrees of freedom and let specialized training methods solve the potential issue of over-fitting. From Wood (2006):\n\nAn alternative to controlling smoothness by altering the basis dimension, is to keep the basis dimension fixed, at a size a little larger than it is believed it could reasonably be necessary, but to control the model’s smoothness by adding a “wiggliness” penalty to the least squares fitting objective.\n\nThis approach is advantageous when there are separate basis expansions for multiple predictors. The overall smoothness can be estimated along with the parameter estimates in the model. The process could be used with general linear models (Chapters ?sec-ols and ?sec-ordinary-logistic-regression) or other parametric linear models.\nSplines are extremely useful and are especially handy when we want to encourage a simple model (such as linear regression) to approximate the predictive performance of a much more complex black-box model (e.g., a neural network or tree ensemble). We’ll also see splines and spline-like features used within different modeling techniques, such as generalized additive models (Sections ?sec-reg-gam and ?sec-cls-gam), multivariate adaptive regression splines (?sec-mars), and a few others.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#sec-variance-bias",
    "href": "chapters/interactions-nonlinear.html#sec-variance-bias",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.4 Sidebar: The Variance-Bias Tradeoff",
    "text": "8.4 Sidebar: The Variance-Bias Tradeoff\nOur discussion of basis expansions presents an excellent time for a segue to discuss an essential idea in statistics and modeling: the variance-bias tradeoff. This idea will be relevant in upcoming sections for resampling and specific models. At the end of this section, we’ll also connect it to Section 6.4.3.\nWhat does variance mean in terms of a machine learning model? In this context, it would quantify how much the fitted model changes if we slightly change the data. For example, would the curve change much if we repeat the data collection that produces the values in Section 8.2 and fit the sample model (for a fixed sample size and polynomial degree)? We can also look at the variance of prediction: for a specific new data point \\(x_0\\), how much intrinsic uncertainty is there? Similar to the discussion regarding extrapolation of global polynomials, we might want to compare how much the uncertainty changes as we move outside the training set’s range.\nBias is the difference between some estimate, like an average or a model prediction, and its true value. The true value is not the same as the data we collect; it is the unknowable theoretical value. For this reason, bias is often difficult to compute directly. For machine learning, the most pertinent idea of bias relates to how well a model can conform to the patterns we see in the data (hoping that the observed data are a good representation of the true values). Back in Figure 8.10, we saw that a linear model didn’t fit the data well. This is due to simple linear regression being a high-bias model because, without additional feature engineering, it cannot replicate nonlinear trends. Adding polynomial or spline terms decreased the model’s bias since it was more flexible.\n\nWe can think of bias in terms of the question: “How close are we aiming to the center of the target?” and variance as: “How much do our results vary when shooting at the target?”\n\nThe concepts of variance and bias are paired because they are often at odds with one another. For models that predict, we’d like a low variance, low bias model. In many cases, that can be exceedingly difficult to achieve. We can often lower bias by adding model complexity. However, increased complexity usually comes at the expense of stability (i.e., high variance). We will often be in the position of trying to find an acceptable compromise between the two. This was discussed with global polynomials; linear models were bad for our data, but adding too many polynomial terms adds unnecessary complexity and an explosion of variance (reflected in the confidence bands).\nThis leads us to the mean squared error (MSE). We’ve seen the root mean squared error already where we used it as a measure of accuracy for regression models. More generally, we can write it in terms of some unknown parameter \\(\\theta\\) and some estimate \\(\\hat{\\theta}\\) based on statistical estimation from data: \\[MSE = E\\left[(\\theta - \\hat{\\theta})^2\\right]\\]\nIt turns out that the MSE is a combination of model variance and (squared) bias (\\(\\theta - \\hat{\\theta}\\)):\n\\[MSE = E\\left[(\\theta - \\hat{\\theta})^2\\right] = Var[\\hat{\\theta}] + (\\theta - \\hat{\\theta})^2 + \\sigma^2\\]\nwhere \\(\\sigma^2\\) represents some unknown amount of “irreducible noise.” Because of this, MSE offers a statistic to minimize that accounts for both properties. This can offer a compromise between the two.\nTo illustrate this, we simulated a simple nonlinear model:\n\\[y_i = x_i^3 + 2\\exp\\left[-6(x_i - 0.3)^2\\right] + \\epsilon_i\\]\nwhere the error terms are \\(\\epsilon_i \\sim N(0, 0.1)\\) and the predictor values were uniformly spaced across [-1.2, 1.2]. Since we know the true values, we can compute the model bias.\nTo create a simulated data set, 31 samples were generated. Of these, 30 were used for the training set and one was reserved for the estimating the variance and bias. The training set values were roughly equally spaced across the range of the predictor. One simulated data set is shown in Figure 8.15, along with the true underlying pattern.\n\n\n\n\n\n\n\n\nFigure 8.15: A simulated data set with a nonlinear function. The test set point is the location where our simulation estimates the bias and variance.\n\n\n\n\n\n\n#&gt; Warning: executing %dopar% sequentially: no parallel backend registered\n\nA linear model was estimated using ordinary least squares and a polynomial expansion for each simulated data set. The test set values were predicted, and the bias, variance, and root mean squared error statistics were computed. This was repeated 1,000 times for polynomial degrees ranging from one to twenty.\nFigure 8.16 shows the average statistic values across model complexity (i.e., polynomial degree). A linear model performs poorly for the bias due to underfitting (as expected). In panel (a), adding more nonlinearity results in a substantial decrease in bias because the model fit is closer to the true equation. This improvement plateaus at a sixth-degree polynomial and stays low (nearly zero). The variance begins low; even though the linear model is ineffective, it is stable. Once additional terms are added, the variance of the model steadily increases then explodes around a 20th degree polynomial9. This shows the tradeoff; as the model becomes more complex, it fits the data better but eventually becomes unstable.\n\n\n\n\n\n\n\n\nFigure 8.16: Simulation results showing variances, squared biases, and mean squared errors.\n\n\n\n\n\nThe mean squared error in panel (b) shows the combined effect of variance and bias. It shows a steep decline as we add more terms, then plateaus between seven and fifteenth-degree models. After that, the rapidly escalating variance dominates the MSE as it rapidly increases.\nThe variance-bias tradeoff is the idea that we can exploit one for the other. Let’s go back to the basic sample mean statistic. If the data being averaged are normally distributed, the simple average is an unbiased estimator: it is always “aiming” for the true theoretical value. That’s a great property to have. The issue is that many unbiased estimators can have very high variance. In some cases, a slight increase in bias might result in a drastic decrease in variability. The variance-bias tradeoff helps us when one of those two quantities is more important.\nFigure 8.17 shows a simple example. Suppose we have some alternative method for estimating the mean of a group that adds some bias while reducing variance. That tradeoff might be worthwhile if some bias will greatly reduce the variance. In our figure, suppose the true population mean is zero. The green curve represents the sample mean for a fixed sample size. It is centered at zero but has significant uncertainty. The other curve might be an alternative estimator that produces a slightly pessimistic estimate (its mean is slightly smaller than zero) but has 3-fold smaller variation. When estimating the location of the mean of the population, the biased estimate will have a smaller confidence interval for a given sample size than the unbiased estimate. While the biased estimate may slightly miss the target, the window of the location will be much smaller than the unbiased estimate. This may be a good idea depending on how the estimate will be used 10.\n\n\n\n\n\n\n\n\nFigure 8.17: A simple example of the variance-bias tradeoff.\n\n\n\n\n\nThe categorical encoding approach shown in Equation 6.3 from Section 6.4.3 is another example. Recall that \\(\\bar{y}_j\\) was the average daily rate for agent \\(j\\). That is an unbiased estimator but has high variance, especially for agents with few bookings. The more complex estimator \\(\\hat{y}_j\\) in Equation 6.3 is better because it is more reliable. That reliability is bought by biasing the estimator towards the overall mean (\\(\\mu_0\\)).\nAs mentioned, we’ll return to this topic several times in upcoming sections.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#discretization",
    "href": "chapters/interactions-nonlinear.html#discretization",
    "title": "8  Interactions and Nonlinear Features",
    "section": "8.5 Discretization",
    "text": "8.5 Discretization\nDiscretization11 is the process of converting quantitative data into a set of qualitative groups (a.k.a “bins”). The model uses these values instead of the original predictor column (perhaps requiring an additional step to convert them into binary indicator columns). Sadly, Figure 8.18 illustrates numerous analyses that we have witnessed. This example uses the food delivery data and breaks the order hour and distance predictors into six and three groups, respectively. It also converts the delivery time outcome into three groups. This visualization, colloquially known as the “Wall of Pie,” tries to explain how the two predictors affect the outcome categories, often with substantial subjectivity.\n\n\n\n\n\n\n\n\nFigure 8.18: An unfortunate visualization of two predictors and the outcome in the food delivery data. The colors of the pie chart reflect the binned delivery times at cut points of 20 and 30 minutes with lighter blue indicating earlier times.\n\n\n\n\n\nOur general advice, described in more detail in Kuhn and Johnson (2019), is that the first inclination should never be to engineer continuous predictors via discretization12. Other tools, such as splines, are both theoretically and practically superior to converting quantitative data to qualitative data. In addition, if a predictor can be split into regions that are predictive of the response, then methods that use recursive partitioning will be less arbitrary and more effective.\nThe literature supporting this is extensive, such as: Cohen (1983), Altman (1991), Maxwell and Delaney (1993), Altman et al. (1994), Buettner, Garbe, and Guggenmoos-Holzmann (1997), Altman (1998), Taylor and Yu (2002), MacCallum et al. (2002), Irwin and McClelland (2003), Owen and Froman (2005), Altman and Royston (2006), Royston, Altman, and Sauerbrei (2006), van Walraven and Hart (2008), Fedorov, Mannino, and Zhang (2009), Naggara et al. (2011), Bennette and Vickers (2012), Kuss (2013), Kenny and Montanari (2013), Barnwell-Menard, Li, and Cohen (2015), Fernandes et al. (2019), as well as the references shown in Harrell (2015). These articles identify the main problems of discretization as follows:\n\nArbitrary (non-methodological) choice of breaks for binning can lead to significant bias.\nThe predictor suffers a significant loss of information, making it less effective. Moreover, there is reduced statistical power to detect differences between groups when they exist.\nThe number of features are increased, thus exacerbating the challenge of feature selection.\nCorrelations between predictors are inflated due to the unrealistic reduction in the variance of predictors.\n\nPettersson et al. (2016) shows differences in analyses with and without discretization. Their Fig. 1 shows a common binning analysis: a continuous outcome and one or more predictors are converted to qualitative formats and a grid of pie charts is created. Inferences are made from this visualization. One main problem is related to uncertainty. The noise in the continuous data is squashed so that any visual signal that is seen appears more factual than it is in reality13. Also, the pie charts do not show measures of uncertainty; how do we know when two pie charts are “significantly different”?\nAlternatively, Figs. 4 and 5 of their paper shows the results of a logistic regression model where all predictors were left as-is and splines were used to model the probability of the outcome. This has a much simpler interpretation and confidence bands give the reader a sense that the differences are real.\nWhile it is not advisable to discretize most predictors, there are some cases when discretization can be helpful. As a counter-example, one type of measurement that is often appropriate to discretize is date. For example, Kuhn and Johnson (2019) show a data set where daily ridership data was collected for the Chicago elevated train system. The primary trend in the data was whether or not the day was a weekday. Ridership was significantly higher when people commute to work. A simple indicator for Saturday/Sunday (as well as major holiday indicators) was the driving force behind many regression models on those data. In this case, making qualitative versions of the date was rational, non-arbitrary, and driven by data analysis.\nNote that several models, such as classification/regression trees and multivariate adaptive regression splines, estimate cut points in the model-building process. The difference between these methodologies and manual binning is that the models use all the predictors to derive bins based on a single objective (such as maximizing accuracy). They evaluate many variables simultaneously and are usually based on statistically sound methodologies.\nIf it is the last resort, how should one go about discretizing predictors? First, topic specific expertise of the problem can be used to create appropriate categories when categories are truly merited as in the example of creating an indicator for weekend day in the Chicago ridership data. Second, and most important, any methodology should be well validated using data that were not used to build the model (or choose the cut points for binning). To convert data to a qualitative format, there are both supervised and unsupervised methods.\nThe most reliable unsupervised approach is to choose the number of new features and use an appropriate number of percentiles to bin the data. For example, if four new features are required, the 0, 25%, 50%, 75%, and 100% quantiles would be used. This ensures that each resulting bin contains about the same number of samples from the training set.\nIf you noticed that this is basically the same approach suggested for choosing spline knots in the discussion earlier in this chapter, you are correct. This process is very similar to using a zero-order polynomial spline, the minor difference being the placement of the knots. A zero-order model is a simple constant value, usually estimated by the mean. This is theoretically interesting but also enables users to contrast discretization directly with traditional spline basis expansions. For example, if a B-spline was used, the modeler could tune over the number of model terms (i.e., the number of knots) and the spline polynomial degree. If binning is the superior approach, the tuning process would select that approach as optimal. In other words, we can let the data decide if discretization is a good idea14.\nA supervised approach would, given a specific number of new features to create, determine the breakpoints by optimizing a performance measure (e.g., RMSE, classification accuracy, etc.). A good example is a tree-based model (very similar to the process shown in Figure 6.3). After fitting a single tree or, better yet, an ensemble of trees, the split values in the trees can be used as the breakpoints.\nLet’s again use the fossil data from Bralower et al. (1997) illustrated in previous section on basis functions. We can fit a linear regression with qualitative terms for the age derived using:\n\nAn unsupervised approach using percentiles at cut-points.\nA supervised approach where a regression tree model is used to set the breakpoints for the bins.\n\nIn each case, the number of new features requires tuning. Using the basic grid search tools described in ?sec-grid, the number of required terms was set for each method (ranging from 2 to 10 terms) by minimizing the RMSE from a simple linear regression model. The results are that both approaches required the same number of new features and produced about the same level of performance; the unsupervised approach required 6 breaks to achieve an RMSE of 0.0000355 and the supervised model has an RMSE of 0.0000302 with 6 cut points. Figure 8.19 shows the fitted model using the unsupervised terms.\n\n\n\n\n\n\n\n\nFigure 8.19: The estimated relationship between fossil age and the isotope ratio previously shown in Figure 8.11, now using discretization methods.\n\n\n\n\n\nThe results are remarkably similar to one another. The blocky nature of the fitted trend reflects that, within each bin, a simple mean is used to estimate the sale price.\nAgain, we want to emphasize that arbitrary or subjective discretization is almost always suboptimal.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#chapter-references",
    "href": "chapters/interactions-nonlinear.html#chapter-references",
    "title": "8  Interactions and Nonlinear Features",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAltman, D G. 1991. “Categorising Continuous Variables.” British Journal of Cancer 64 (5): 975.\n\n\nAltman, D G. 1998. “Suboptimal Analysis Using ’Optimal’ Cutpoints.” British Journal of Cancer 78 (4): 556–57.\n\n\nAltman, D G, B Lausen, W Sauerbrei, and M Schumacher. 1994. “Dangers of Using \"Optimal\" Cutpoints in the Evaluation of Prognostic Factors.” Journal of the National Cancer Institute 86 (11): 829.\n\n\nAltman, D G, and P Royston. 2006. “The Cost of Dichotomising Continuous Variables.” BMJ 332 (7549): 1080.\n\n\nAltorki, Nasser K, Timothy E McGraw, Alain C Borczuk, Ashish Saxena, Jeffrey L Port, Brendon M Stiles, Benjamin E Lee, et al. 2021. “Neoadjuvant Durvalumab with or Without Stereotactic Body Radiotherapy in Patients with Early-Stage Non-Small-Cell Lung Cancer: A Single-Centre, Randomised Phase 2 Trial.” The Lancet Oncology 22 (6): 824–35.\n\n\nApley, D, and J Zhu. 2020. “Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.” Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (4): 1059–86.\n\n\nArnold, T, M Kane, and B Lewis. 2019. A Computational Approach to Statistical Learning. Chapman; Hall/CRC.\n\n\nBarnwell-Menard, JL, Q Li, and A Cohen. 2015. “Effects of Categorization Method, Regression Type, and Variable Distribution on the Inflation of Type-I Error Rate When Categorizing a Confounding Variable.” Statistics in Medicine 34 (6): 936–49.\n\n\nBennette, C, and A Vickers. 2012. “Against Quantiles: Categorization of Continuous Variables in Epidemiologic Research, and Its Discontents.” BMC Medical Research Methodology 12: 21.\n\n\nBralower, T, P Fullagar, C Paull, G Dwyer, and R Leckie. 1997. “Mid-Cretaceous Strontium-Isotope Stratigraphy of Deep-Sea Sections.” Geological Society of America Bulletin 109 (11): 1421–42.\n\n\nBreiman, L. 1988. “Monotone Regression Splines in Action (Comment).” Statistical Science 3 (4): 442–45.\n\n\nBuettner, P, C Garbe, and I Guggenmoos-Holzmann. 1997. “Problems in Defining Cutoff Points of Continuous Prognostic Factors: Example of Tumor Thickness in Primary Cutaneous Melanoma.” Journal of Clinical Epidemiology 50 (11): 1201–10.\n\n\nChen, SH, J Sun, L Dimitrov, A Turner, T Adams, D Meyers, BL Chang, et al. 2008. “A Support Vector Machine Approach for Detecting Gene-Gene Interaction.” Genetic Epidemiology 32 (2): 152–67.\n\n\nChipman, H. 1996. “Bayesian Variable Selection with Related Predictors.” Canadian Journal of Statistics 24 (1): 17–36.\n\n\nCohen, J. 1983. “The Cost of Dichotomization.” Applied Psychological Measurement 7 (3): 249–53.\n\n\nDe Boor, C. 2003. A Practical Guide to Splines. Springer-Verlag.\n\n\nElith, J, J Leathwick, and T Hastie. 2008. “A Working Guide to Boosted Regression Trees.” Journal of Animal Ecology 77 (4): 802–13.\n\n\nFedorov, V, F Mannino, and R Zhang. 2009. “Consequences of Dichotomization.” Pharmaceutical Statistics 8 (1): 50–61.\n\n\nFernandes, A, C Malaquias, D Figueiredo, E da Rocha, and R Lins. 2019. “Why Quantitative Variables Should Not Be Recoded as Categorical.” Journal of Applied Mathematics and Physics 7 (7): 1519–30.\n\n\nFriedman, J, and B Popescu. 2008. “Predictive Learning via Rule Ensembles.” The Annals of Applied Statistics 2 (3): 916–54.\n\n\nGarcía-Magariños, M, I López-de-Ullibarri, R Cao, and A Salas. 2009. “Evaluating the Ability of Tree-Based Methods and Logistic Regression for the Detection of SNP-SNP Interaction.” Annals of Human Genetics 73 (3): 360–69.\n\n\nGauthier, J, QV Wu, and TA Gooley. 2020. “Cubic Splines to Model Relationships Between Continuous Variables and Outcomes: A Guide for Clinicians.” Bone Marrow Transplantation 55 (4): 675–80.\n\n\nGreenwell, B, B Boehmke, and A J McCarthy. 2018. “A Simple and Effective Model-Based Variable Importance Measure.” arXiv.\n\n\nHamada, M, and CF Wu. 1992. “Analysis of Designed Experiments with Complex Aliasing.” Journal of Quality Technology 24 (3): 130–37.\n\n\nHarrel, F, K Lee, and B Pollock. 1988. “Regression Models in Clinical Studies: Determining Relationships Between Predictors and Response.” JNCI: Journal of the National Cancer Institute 80 (15): 1198–1202.\n\n\nHarrell, F. 2015. Regression Modeling Strategies. Springer.\n\n\nHerbinger, J, B Bischl, and G Casalicchio. 2022. “REPID: Regional Effect Plots with Implicit Interaction Detection.” In International Conference on Artificial Intelligence and Statistics, 10209–33.\n\n\nHooker, G. 2004. “Discovering Additive Structure in Black Box Functions.” In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 575–80.\n\n\nInglis, A, A Parnell, and C Hurley. 2022. “Visualizing Variable Importance and Variable Interaction Effects in Machine Learning Models.” Journal of Computational and Graphical Statistics 31 (3): 766–78.\n\n\nIrwin, J R, and G H McClelland. 2003. “Negative Consequences of Dichotomizing Continuous Predictor Variables.” Journal of Marketing Research 40 (3): 366–71.\n\n\nKapelner, A, and J Bleich. 2013. “bartMachine: Machine Learning with Bayesian Additive Regression Trees.” arXiv.\n\n\nKenny, P W, and C A Montanari. 2013. “Inflation of Correlation in the Pursuit of Drug-Likeness.” Journal of Computer-Aided Molecular Design 27 (1): 1–13.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nKuss, O. 2013. “The Danger of Dichotomizing Continuous Variables: A Visualization.” Teaching Statistics 35 (2): 78–79.\n\n\nLampa, E, L Lind, P Lind, and A Bornefalk-Hermansson. 2014. “The Identification of Complex Interactions in Epidemiology and Toxicology: A Simulation Study of Boosted Regression Trees.” Environmental Health 13 (1): 57.\n\n\nLim, M, and T Hastie. 2015. “Learning Interactions via Hierarchical Group-Lasso Regularization.” Journal of Computational and Graphical Statistics 24 (3): 627–54.\n\n\nMacCallum, R C, S Zhang, K J Preacher, and D Rucker. 2002. “On the Practice of Dichotomization of Quantitative Variables.” Psychological Methods 7 (1): 19–40.\n\n\nMaxwell, S, and H Delaney. 1993. “Bivariate Median Splits and Spurious Statistical Significance.” Psychological Bulletin 113 (1): 181–90.\n\n\nMiller, A. 1984. “Selection of Subsets of Regression Variables.” Journal of the Royal Statistical Society. Series A (General), 389–425.\n\n\nMokhtari, Reza Bayat, Tina S Homayouni, Narges Baluch, Evgeniya Morgatskaya, Sushil Kumar, Bikul Das, and Herman Yeger. 2017. “Combination Therapy in Combating Cancer.” Oncotarget 8 (23): 38022–43.\n\n\nMolnar, C. 2020. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. Lulu.com.\n\n\nNaggara, O, J Raymond, F Guilbert, D Roy, A Weill, and D G Altman. 2011. “Analysis by Categorizing or Dichotomizing Continuous Variables Is Inadvisable: An Example from the Natural History of Unruptured Aneurysms.” AJNR. American Journal of Neuroradiology 32 (3): 437–40.\n\n\nOh, S. 2022. “Predictive Case-Based Feature Importance and Interaction.” Information Sciences 593: 155–76.\n\n\nOwen, S, and R Froman. 2005. “Why Carve up Your Continuous Data?” Research in Nursing and Health 28 (6): 496–503.\n\n\nPettersson, M, XiXnjun Hou, M Kuhn, T T Wager, G W Kauffman, and P R Verhoest. 2016. “Quantitative Assessment of the Impact of Fluorine Substitution on P-Glycoprotein (P-gp) Mediated Efflux, Permeability, Lipophilicity, and Metabolic Stability.” Journal of Medicinal Chemistry 59 (11): 5284–96.\n\n\nPlanchard, D, P Jänne, Y Cheng, J Yang, N Yanagitani, SW Kim, S Sugawara, et al. 2023. “Osimertinib with or Without Chemotherapy in EGFR-Mutated Advanced NSCLC.” New England Journal of Medicine 389 (21): 1935–48.\n\n\nRoyston, P, D G Altman, and W Sauerbrei. 2006. “Dichotomizing Continuous Predictors in Multiple Regression: A Bad Idea.” Statistics in Medicine 25 (1): 127–41.\n\n\nSingh, Nina, and Pamela J Yeh. 2017. “Suppressive Drug Combinations and Their Potential to Combat Antibiotic Resistance.” The Journal of Antibiotics 70 (11): 1033–42.\n\n\nStone, C, and CY Koo. 1985. “Additive Splines in Statistics.” Proceedings of the American Statistical Association 45: 48.\n\n\nTaylor, J M G, and M Yu. 2002. “Bias and Efficiency Loss Due to Categorizing an Explanatory Variable.” Journal of Multivariate Analysis 83 (1): 248–63.\n\n\nvan Walraven, C, and R Hart. 2008. “Leave ’Em Alone - Why Continuous Variables Should Be Analyzed as Such.” Neuroepidemiology 30 (3): 138–39.\n\n\nWang, Y. 2011. Smoothing Splines: Methods and Applications. CRC Press.\n\n\nWood, S. 2006. Generalized Additive Models: An Introduction with R. Chapman; Hall/CRC.",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/interactions-nonlinear.html#footnotes",
    "href": "chapters/interactions-nonlinear.html#footnotes",
    "title": "8  Interactions and Nonlinear Features",
    "section": "",
    "text": "Alternatively, PCA after skewness-correcting transformations may be another good option.↩︎\nThese were computed using the bootstrap, as in Section 2.3.↩︎\nIt so happens that the Gaussian likelihood function is equivalent to the sums of squared errors (SSE). That, in turn, is equivalent to the RMSE. That simplification does not automatically occur for other probability distributions.↩︎\nThis approach works because the model in Equation 8.1 is capable of estimating the interaction. There are many models that do not have the ability to measure interaction effects, and, for this case, it would be impossible to isolate the interaction term(s). However, tree-based ensembles are good at estimating interactions, as are other complex black-box models such as neural networks and support vector machines. The tools described below only work with “interaction capable” models.↩︎\nIn actuality, a more computationally preferred method is to use orthogonal polynomials which rescale the variables before we exponentiate them. We’ll denote these modified versions of the predictor in equation Equation 8.3 as just \\(x\\) to reduce mathematical clutter. We’ve seen these before in Figure 6.4↩︎\nThis can be seen in the basis functions above the plot: each line covers the entire range of the data.↩︎\nActually, they are the interior knots. The full set of knots includes the minimum and maximum values of \\(x\\) in the training set.↩︎\nAlso called a restricted cubic spline in Harrel, Lee, and Pollock (1988).↩︎\nIn this particular case, the variance becomes very large since the number of parameters is nearly the same as the number of training set points (30). This makes the underlying mathematical operation (matrix inversion) numerically unstable. Even so, it is the result of excessive model complexity.↩︎\nThis specific example will be referenced again when discussing resampling in Chapter 10.↩︎\nAlso known as binning or dichotomization.↩︎\nIn the immortal words12 of Lucy D’Agostino McGowan and Nick Strayer: “Live free or dichotomize.”↩︎\nKenny and Montanari (2013) does an excellent job illustrating this issue.↩︎\nBut is most likely not a good idea.↩︎",
    "crumbs": [
      "Preparation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interactions and Nonlinear Features</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html",
    "href": "chapters/overfitting.html",
    "title": "9  Overfitting",
    "section": "",
    "text": "9.1 Model Complexity and Overfitting\nThis chapter describes the most crucial concept in predictive modeling: overfitting. When a model fails, it is almost always due to overfitting of some sort. The problem is that you may only realize that there is an issue once you finish the development phase of modeling and are exposed to completely new data.\nTo get started, we’ll discuss model complexity and how it can be both good and bad.\nMany predictive models have parameters. For example, the linear regression model initially shown in Equation 8.3 contains slopes and intercepts (the \\(\\beta\\) values) estimated during model training using the predictor and outcome data. In other words, it is possible to write the equations to solve to estimate these values.\nHowever, in that same equation, a series of polynomial terms were used. The number of additional features is also a parameter but, unlike the \\(\\beta\\) coefficients, it cannot be directly estimated from the data. Equation 8.3 arbitrarily used three terms for the predictor \\(x\\).\nTwo other models were discussed in Chapter 2. First, the tree-based model shown in Figure 2.5 had five splits of the data. This led to a model fit that resulted in six regions of the \\(x\\) space, each with its specific predicted value. We could have continued to split the training set into finer and finer partitions so that the fitted curve more effectively modeled the observed data. As such, the depth of the tree was a parameter that needed to be set before the model fit could be estimated.\nSimilarly, the neural network required the number of hidden units to be defined (among other values). We weren’t sure what that should be, so a set of candidate values were assessed, and the one with the best results was used for the final model fit (see Figure 2.7). We’ve also seen specific tuning parameters in Section 6.4.2, Section 6.4.4, and also for the embedding methods in Chapter 7.\nIn each of these cases, there was a tuning parameter (a.k.a., hyperparameter) that defined the structure of the model and could not be directly estimated from the data. Increasing the parameter’s value for each model made the model’s potential structure more elaborate. The effect of this increase in the model’s complexity is additional capability to adapt to any pattern seen in the training data. The straight line fit associated with a linear regression model without polynomial terms would have done an abysmal job on such a nonlinear data set. Adding additional complexity allowed it to better conform to the training set data.\nThe problem is that added complexity can also add risk. Being more capable of adapting to subtle trends in the training data is only a good thing when such trends generalize to other data. Recall in Section 6.4.3 where we were estimating the effect of an agent on the price of a hotel room. One agent had a single booking. The naive estimate of the ADR wouldn’t generalize well. As they accrue more and more bookings, their mean would undoubtedly change to a more realistic value. Using the raw ADR value would “overfit to the training set data.”\nFor many sophisticated models, the complexity can be modulated via one or more tuning parameters that detect inconsequential patterns in the training data. The problem is that they may go too far and over-optimize the model fit.\nAs an example, Figure 9.1 shows a scenario where there are two predictors (A and B) and each point belongs to one of two classes (vermilion circles or blue triangles). These data were simulated and panel (a) shows a thick grey line representing the true class boundary where there is a 50% chance that a data point belongs to either class. Points to the left of the grey curve are more likely to be circles than points to the right. The curve itself has a smooth, parabolic shape.\nWe can fit a model to these data and estimate the class boundary. The model we used1 has a tuning parameter called the cost value. For now, you can think of this parameter as one that discourages the model from making an incorrect prediction (i.e., placing it on the wrong side of the estimated boundary). In other words, as the cost parameter increases, the model is more and more incentivized to be accurate on the training data.\nWhen the cost is low, the results are often boundaries with low complexity. Figure 9.1(b) shows the result where the fitted boundary is in black; points on the left of the boundary would be classified as circles. There is some curvature and, while correctly classifying the points in the middle of the training set, it could do a better job of emulating the true boundary. This model is slightly underfit since more complexity would make it better.\nFigure 9.1: Examples of how a model can overfit the training data across different levels of complexity.\nWhen the cost is increased, the complexity also increases (Figure 9.1(c)). The boundary is more parabolic, and a few more training set points are on the correct side of the estimated line.\nFigure 9.1(d) shows what occurs when the model is instructed that it is incredibly bad when incorrectly predicting a data point in the training set. Its boundary contorts in a manner that attempts to gain a few more points of accuracy. Two specific points, identified by black arrows, appear to drive much of the additional structure in the black curve. Since we know the actual boundary, it’s easy to see that these contortions will not reproduce with new data. For example, it is implausible that a small island of vermilion circles exists in the mainstream of blue triangles.\nAnother sign that a model probably has too much complexity is the additional component of the decision boundary at the bottom of Figure 9.1(d) (red arrow). Points inside this part of the boundary would be classified as triangles even though no triangles are near it. This is a sort of “blowback” of an overly complex model where choices the model makes in one part of the predictor space add complexity in a completely different part of the space. This blowback is often seen in areas where there are no data. It’s unnecessary and points to the idea that the model is trying to do too much.\nThis boundary demonstrates how added complexity can be a bad thing. A test set of data was simulated and is shown in Figure 9.2. The performance for this overly complex model is far from optimal since the extra areas induced by the high cost value do not contain the right class of points.\nFigure 9.2: The high complexity fit from Figure 9.1(d) superimposed over a test set of data.\nOur goal is to find tuning parameter values that are just right: complex enough to accurately represent the data but not complex enough to over-interpret it.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-how-to-overfit",
    "href": "chapters/overfitting.html#sec-how-to-overfit",
    "title": "9  Overfitting",
    "section": "9.2 The Ways Models Overfit",
    "text": "9.2 The Ways Models Overfit\nThis leads us to a formal definition of overfitting:\n\nOverfitting is the situation where a model over-interprets trends and patterns in the training set that do not generalize to new data.\nThese irreproducible data trends result in models that do very well on the training set but greatly underperform on the validation set.\n\nWe can overinterpret the training set data in a few different ways. For example, ?sec-imbalances will discuss class imbalance problems that occur when one or more outcome classes have a very low probability of occurrence. As models are allowed more complexity, they will often maximize accuracy (or some proxy for it) by simply declaring that all data points belong to the majority class. If there is a 1% event rate, 99% accuracy is easily achievable by overfitting to the majority class.\nAnother example is overfitting the predictor set. The “low \\(N\\), high \\(P\\)” problem (Johnstone and Titterington 2009) occurs when data points are expensive to obtain, but each comes with abundant information. For example, in high-dimensional biology data, it is common to have dozens of data points and hundreds of thousands of predictors. Many models cannot be estimated with this imbalance of dimensions, and often, the first step is to filter out uninformative predictors.\nSuppose some statistical inferential method, such as a t-test, is used to determine if a predictor can differentiate between two classes. Small sample sizes make the chance of uninformative predictors falsely passing the filter large. If the same data are then used to build and evaluate the model, there is a significant probability that an ineffective model will appear to be very predictive (Ambroise and McLachlan 2002; Kuhn and Johnson 2019). Again, using a data set external to the model training and development process shows you when this occurs.\nFinally, it is possible to overfit a model via sample filtering. For example, in some machine learning competitions, participants might sub-sample the training set to be as similar to the unlabelled test set as possible. This will most likely improve their test set performance, but it generally handicaps a model when predicting a new yet-to-be-seen data set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-external-validation",
    "href": "chapters/overfitting.html#sec-external-validation",
    "title": "9  Overfitting",
    "section": "9.3 External Data to Measure Effectiveness",
    "text": "9.3 External Data to Measure Effectiveness\nBy now, it should seem clear that the best way of avoiding overfitting is to quantify the model performance using a separate data set. How do we do that? We’ve said that most modeling projects create an initial partition of the data into training and testing sets. Can the testing set detect overfitting?\nWe’ll again use the classification data from Figure 9.1 to demonstrate, this time with a different model. A boosted tree2 is an ensemble model. The model creates a sequence of decision trees, each depending on the previous, to create a large ensemble of individual trees. For a new sample, each tree is used to predict the outcome, and a weighted average is used to create the predicted class probabilities.\nThe model has tuning parameters related to the tree (e.g., tree depth) and ones related to the process of building the ensemble (e.g., the number of trees in the ensemble). We’ll use 100 trees to create our ensemble. However, we don’t know how deep the trees should be. One method of controlling the number of splits in a tree is to control how much data should be in a partition to continue splitting. For example, in Figure 2.5, the left-most terminal node in the tree contained \\(n_{tr}\\) = 971 data points after all splits are applied. If we keep splitting with no constraint, at some point, the amount of data in the node is too small to continue.\nThis tuning parameter, we’ll call it \\(n_{min}\\), is used to moderate the tree-growing process. Small values increase the risk of overfitting; it’s possible to have a single data point left in the terminal node. Large values prevent the tree from accurately modeling the data, and underfitting occurs. What does the trade-off between complexity and performance look like for these data?\nWe took the training data and fit models using values \\(n_{min}\\) = 3, 4, …, 80. To understand performance, the Brier score (Brier 1950; Steyerberg 2009) is used. It is an error metric for classification models (similar to RMSE) and measures how close the predicted probabilities are to their binary (i.e., 0/1) values. A Brier score of zero is a perfect model, and ineffectual models tend to have values \\(\\ge\\) 0.25.\nFigure 9.3 shows the results. The yellow curve illustrates the Brier score when the training set is re-predicted. There is a sharp increase in error as the tuning parameter values increase. This line suggests that we use very small values. The smallest Brier score is associated with \\(n_{min}\\) = 3 (producing an effectively zero Brier score).\n\n\n\n\n\n\n\n\nFigure 9.3: An example to show how complexity and model error, measured two ways, interact. The tuning parameter on the x-axis is \\(n_{min}\\) used for moderating tree growth.\n\n\n\n\n\nThe purple line shows the Brier score trend on the test set. This tells a different story where small \\(n_{min}\\) values overfit, and the model error improves as larger values are investigated. The Brier score is lowest around \\(n_{min}\\) = 39 with a corresponding Brier score estimate of 0.0928. From here, increasing the tuning parameter results in shallow trees that underfit. As a result the Brier score slowly increases.\nContrasting these two curves, data that are external to the training set gives a much more accurate picture of performance. However:\n\nDo not use the test set to develop your model.\n\nWe’ve only used it here to show that an external data set is needed to effectively optimize model complexity. If we only have training and testing data, what should we do?\nThere are two ways to solve this problem:\n\nUse a validation set.\nResample the training set Chapter 10.\n\nBoth of these options are described in detail in the next chapter.\nAs previously discussed, validation sets are a third split of the data, created at the same time as the training and test sets. They can be used as an external data set during model development (as in Chapter 2). This approach is most applicable when the project starts with abundant data.\nResampling methods take the training set and make multiple variations of it, usually by subsampling the rows of the data. For example, 10-fold cross-validation would make ten versions of the training set, each with a different 90% of the training data. We would say that, for this method, there are ten “resamples” or “folds.” To measure performance, we would fit ten models on the majority of the data in each fold, then predict the separate 10% that was held out. This would generate ten different Brier scores, which are averaged to produce the final resampling estimate of the model. If we are trying to optimize complexity, we will apply this process for every candidate tuning parameter value.\nFigure 9.4 shows the results when 10-fold cross-validation is used to pick \\(n_{min}\\). The points on the additional curve are the averages of 10 Brier scores from each fold for each value of \\(n_{min}\\). The resampling curve doesn’t precisely mimic the test set results but does have the same general pattern: there is a real improvement in model error as \\(n_{min}\\) is increased but it begins to worsen due to underfitting. In this case, resampling finds that \\(n_{min}\\) = 56 is numerically best and the resampling estimate of the Brier score was 0.104, slightly more pessimistic when compared to the test set result.\n\n\n\n\n\n\n\n\nFigure 9.4: Model complexity versus model error with external validation via resampling.\n\n\n\n\n\nWe suggest using either of these two approaches (validation set or resampling) to measure performance during model development. Before going to the next chapter, let’s give a preview of Chapters 11 and 12.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-optimizing-complexity",
    "href": "chapters/overfitting.html#sec-optimizing-complexity",
    "title": "9  Overfitting",
    "section": "9.4 How Should We Optimize Complexity?",
    "text": "9.4 How Should We Optimize Complexity?\nNow that we know how to appropriately estimate metrics for evaluating effectiveness, how do we choose tuning parameter values? Do we pick them at random?\nOperationally, there are two main classes of search routines for optimizing model complexity via tuning parameters. The oldest is grid search. For each tuning parameter in the model, we define a reasonable set of specific candidate values to investigate. When there is more than one tuning parameter, there are various ways to create a multidimensional grid (these are discussed in Chapter 11). Each candidate in the grid is evaluated and the “best” combination of parameter values is chosen as the one to use (if we think the model works well enough).\nOne tuning parameter used in tree-based ensembles and neural networks is the learning rate. This parameter typically governs how quickly a model adapts during iterations of model fitting (i.e., training). In gradient descent, the learning rate specifies how far we proceed in the optimal direction. For machine learning models, the parameter must be greater than zero and usually has a maximum value around 0.1 to 1.0. When evaluating different learning rates, it is common to think of them in logarithmic units (typically, base 10). This is because the effect of the parameter on model performance is often nonlinear; a per-unit change of 0.05 has different effects on where in the total range that it occurs.\nSuppose that the performance goal is to minimize some notion of error (e.g., RMSE, the Brier score, etc.). Figure 9.5 shows an example of how the learning rate can affect the model error3. At low rates, the error is high. Increasing the rate results in a drop in error that reaches a trough of optimal performance. However, increasing the learning rate at some point causes a different type of underfitting4, increasing the error.\nThe solid points illustrate a very simple five-point grid of errors whose y-axis value is the resampled error. While this grid does not pick the absolute best value, it does result in a model with good performance (relatively speaking) very quickly. If a larger grid were used, we would have placed a grid point much closer to the optimal value.\n\n\n\n\n\n\n\n\nFigure 9.5: An example of grid search containing five candidates for a learning rate parameter.\n\n\n\n\n\nGrid search pre-defines the grid points and all of them are tested before the results can be analyzed. There are good and bad aspects of this method, and it has been (unjustly) criticized for being inefficient. For most models, it can be very efficient and there are additional tools and tricks to make it even faster. See Chapter 11 for more details.\nThe other type of search method is iterative. These tools start with one or more initial candidate points and conduct analyses that predict which tuning parameter value(s) should be evaluated next. The most widely used iterative tuning method is Bayesian optimization (Močkus 1975; Gramacy 2020; Garnett 2023). After each candidate is evaluated, a Bayesian model is used to suggest the next value and this process repeats until a pre-defined number of iterations is reached. Any search method could be used, such as simulated annealing, genetic algorithms, and others.\nFigure 9.6 has an animation to demonstrate. First, three initial points were sampled (shown as open circles). A Bayesian model is fit to these data points and is used to predict the probability distribution of the metric (e.g. RMSE, Brier score, etc.). A tool called an acquisition function is utilized to choose which learning rate value to evaluate on the next search iteration based on the mean and variance of the predicted distribution. In Figure 9.6, this process repeats for a total of 9 iterations.\n\n\n\n\n\n\n\n\nFigure 9.6: An example of iterative optimization. The open circles represent the three initial points and the solid circles show the progress of the Bayesian optimization of the learning rate.\n\n\n\n\n\nThe animation shows that the search evaluated very disparate values, including the lower and upper limits that were defined. However, after a few iterations, the search focuses on points near the optimal value. Again, this method discussed in more detail in Chapter 12.\nAll of our optimization methods assume that a reasonable range of tuning parameters is known. The range is naturally bounded in some cases, such as the number of PCA components. Otherwise, you (and/or the ML community) will develop a sense of appropriate ranges of these parameters. We will suggest basic ranges here. For example, for the learning rate, we know it has to be greater than zero but the fuzzy upper bound of about 0.1 is a convention learned by trial and error.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#sec-permutations",
    "href": "chapters/overfitting.html#sec-permutations",
    "title": "9  Overfitting",
    "section": "9.5 Sidebar: How Bad Can the Model Be?",
    "text": "9.5 Sidebar: How Bad Can the Model Be?\nWhen a model is overfit, the results can be unduly optimistic. Let’s take a minute to consider the opposite: ff we compute some performance measure, what’s the worst value we can achieve?\nFor some metrics, the possible range of values is well-defined. For example, for the coefficient of determination (a.k.a. R2), a value of zero means that the model explains none of the variation in the outcome. In classification, when computing the area under the ROC curve ?sec-roc, a value of 0.5 indicates that the model has no ability to discriminate between the classes. In these instances, the range of metric values is not dependent on the outcome data distribution.\nAs a counter-example, the mean absolute deviation (MAE) used in Chapter 2 to measure the effectiveness of our time-to-delivery models does depend on the outcome data. We know a value of zero is best, but what is the worst it can be?\nThe worst performance occurs when no relationship exists between the observed and predicted outcomes. To translate this to a worst-case metric value, we can use a permutation strategy where we break the relationship by randomly shuffling the outcome data and then compute the metric.\nSince it is possible to get “a bad shuffle” that doesn’t completely break the relationship, we can do this multiple times and then take the average of these values.\nFor example, we’ve said above that the best Brier score is zero and that poor models “have values \\(\\ge\\) 0.25.” We used the predictions from the high complexity fit from Figure 9.1 and shuffled the outcome 50 times. The average permuted Brier score for this data set was 0.321.\nPermuted performance values will show up several times in subsequent chapters.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#chapter-references",
    "href": "chapters/overfitting.html#chapter-references",
    "title": "9  Overfitting",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBrier, G. 1950. “Verification of Forecasts Expressed in Terms of Probability.” Monthly Weather Review 78 (1): 1–3.\n\n\nGarnett, R. 2023. Bayesian Optimization. Cambridge University Press.\n\n\nGramacy, R. 2020. Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.\n\n\nJohnstone, I, and M Titterington. 2009. “Statistical Challenges of High-Dimensional Data.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 367 (1906): 4237–53.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nMočkus, J. 1975. “On Bayesian Methods for Seeking the Extremum.” In Optimization Techniques IFIP Technical Conference Novosibirsk, edited by G Marchuk, 400–404. Springer Berlin Heidelberg.\n\n\nSteyerberg, E. 2009. Clinical Prediction Models. Springer.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/overfitting.html#footnotes",
    "href": "chapters/overfitting.html#footnotes",
    "title": "9  Overfitting",
    "section": "",
    "text": "A support vector machine. See ?sec-cls-svm.↩︎\nThese are described in more detail in ?sec-cls-boosting↩︎\nThis was taken from a real example.↩︎\nA very high learning rate for tree-based ensembles can result in models that produce near-constant predicted values (as opposed to many unique predicted values that are inaccurate).↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html",
    "href": "chapters/resampling.html",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "10.1 The Resampling Process and Nomenclature\nIn Section 9.3, it was shown that using the same data to estimate and evaluate our model can produce inaccurate estimates of how well it functions to predict new samples. It also described using separate partitions of the data to fit and evaluate the model.\nThere are two general approaches for using external data for model evaluation. The first is a validation set, which we’ve seen before in Section 3.7. This is a good idea if you have a lot of data on hand. The second approach is to resample the training set. Resampling is an iterative approach that reuses the training data multiple times based on sound statistical methodology. We’ll discuss both validation sets and resampling in this chapter.\nFigure 10.1 shows a standard data usage scheme that incorporates resampling. After an initial partition, resampling creates multiple versions of the training set. We’ll use special terminology1 for the data partitions within each resample which serve the same function as the training and test sets:\nLike the training and test sets, the analysis and assessment sets are mutually exclusive data partitions and are different for each of the B iterations of resampling.\nFigure 10.1: A general data usage scheme that includes resampling. The colors denote the data used to train the model (in tan) and the separate data sets for evaluating the model (colored periwinkle).\nThe resampling process fits a model to an analysis set and predicts the corresponding assessment set. One or more performance statistics are calculated from these held-out predictions and saved. This process continues for B iterations, and, in the end, there is a collection of B statistics of efficacy. These are averaged to produce the overall resampling estimate of performance. To formalize this, we’ll consider a mapping function \\(M(\\mathfrak{D}^{tr}, B)\\) that takes the training set as input and can output \\(B\\) sets of analysis and assessment sets.\nAlgorithm 10.1 describes this process.\nThere are many different resampling methods, such as cross-validation and the bootstrap. They differ in how the analysis and assessment sets are created in line 7 of Algorithm 10.1. Subsequent sections below discuss a number of mapping functions.\nTwo primary aspects of resampling make it an effective tool. First, the separation of data used to create and appraise the model avoids the data reuse problem described in Section 9.3.\nSecond, using multiple iterations (B &gt; 1) means that you are evaluating your model under slightly different conditions (because the analysis and assessment sets are different). This is a bit like a “multiversal” science fiction story: what would have happened if the situation were slightly different before I fit the model? This lets us directly observe the model variance; how stable is it when the inputs are slightly modified? An unstable model (or one that overfits) will have a high variance.\nIn this chapter, we’ll describe some conceptual aspects of resampling. Then, we’ll define and discuss various methods. Finally, the last section lists frequently asked questions we often hear when teaching these tools.\nUp until Section 10.7, we will assume that each row of the data are independent. We’ll see examples of non-independent in the later sections.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampling-basics",
    "href": "chapters/resampling.html#sec-resampling-basics",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "The analysis set (of size \\(n_{fit}\\)) estimates quantities associated with preprocessing, model training, and postprocessing.\nThe assessment set (\\(n_{pred}\\)) is only used for prediction so that we can compute measures of model effectiveness (e.g., RMSE, classification accuracy, etc).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{algorithm} \\begin{algorithmic} \\State $\\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$ \\State $B$: number of resamples \\State $M(\\mathfrak{D}^{tr}, B)$: a mapping function to split $\\mathfrak{D}^{tr}$ for each of $B$ iterations. \\State $f()$: model pipeline \\Procedure{Resample}{$\\mathfrak{D}^{tr}, f, M(\\mathfrak{D}^{tr}, B)$} \\For{$b =1$ \\To $B$} \\State Partition $\\mathfrak{D}^{tr}$ into $\\{\\mathfrak{D}_b^{fit}, \\mathfrak{D}_b^{pred}\\}$ using $M_b(\\mathfrak{D}^{tr}, B)$. \\State Train model pipeline $f$ on the analysis set to produce $\\hat{f}_{b}(\\mathfrak{D}_b^{fit})$. \\State Generate assessment set predictions $\\hat{y}_b$ by applying model $\\hat{f}_{b}$ to $\\mathfrak{D}_b^{pred}$. \\State Estimate performance statistic $\\hat{Q}_{b}$. \\EndFor \\State Compute resampling estimate $\\hat{Q} = \\sum_{b=1}^B \\hat{Q}_{b}$. \\Return $\\hat{Q}$. \\Endprocedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nAlgorithm 10.1: Resampling models to estimate performance.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Algorithm 10.1, \\(f()\\) is the model pipeline described in Section 1.5. Any estimation methods, before, during, or after the supervised model, are executed on the analysis set B times during resampling.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampling-goals",
    "href": "chapters/resampling.html#sec-resampling-goals",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.2 What is Resampling Trying to Do?",
    "text": "10.2 What is Resampling Trying to Do?\nIt’s important to understand some of the philosophical aspects of resampling. It is a statistical procedure that tries to estimate some true, unknowable performance value (let’s call it \\(Q\\)) associated with the same population of data represented by the training set. The quantity \\(Q\\) could be RMSE, the Brier score, or any other performance statistic.\nThere have been ongoing efforts to understand what resampling methods do on a theoretical level. Bates, Hastie, and Tibshirani (2023) is an interesting work focused on linear regression models with ordinary least squares. Their conclusion regarding resampling estimates is that they\n\n… cannot be viewed as estimates of the prediction error of the final model fit on the whole data. Rather, the estimate of prediction error is an estimate of the average prediction error of the final model across other hypothetical data sets from the same distribution.\n\nIt is a good idea to think that this will also be true for more complex machine learning models. The important idea is that resampling is measuring performance on training sets similar to ours (and the same size).\nTo understand different methods, let’s examine some theoretical properties of resampling estimates that matter to applied data analysis.\nLet’s start with analogy; consider the usual sample mean estimator used in basic statistics (\\(\\bar{x}\\)). Based on assumptions about the data, we can derive an equation that optimally estimates the true mean value based on some criterion. Often, the statistical theory focuses on our estimators’ theoretical mean and variance. For example, if the data are independent and follow the same Gaussian distribution, the sample mean attempts to estimate the actual population mean (i.e., it is an unbiased estimator). Under the same assumptions we can also derive what the estimator’s theoretical variance. These properties, or some combination of them, such as mean squared error2, help guide us to the best estimator.\nThe same is true for resampling methods. We can understand how their bias and variance change under different circumstances and choose an appropriate technique. To demonstrate resampling methods, we’ll focus on bias and variance as the main properties of interest.\nFirst is bias: how accurately does a resampling technique estimate the true population value \\(Q\\)? We’d like it to be unbiased, but to our knowledge, that is nearly impossible. However, some things we do know. Figure 10.1 shows that the training set (of size \\(n_{tr}\\)) is split into B resampling partitions which are then split into analysis and assessment sets of size \\(n_{fit}\\) and \\(n_{pred}\\), respectively. It turns out that, as \\(n_{pred}\\) becomes larger, the bias increases in a pessimistic direction3. In other words, if our training set has 1,000 samples, a resampling method where the assessment set has \\(n_{pred} = 100\\) data points has a smaller bias than one using \\(n_{pred} = 500\\). Another thing that we know is that increasing the number of resamples (B) can’t significantly reduce the bias. Therefore, we must think carefully about the way the data is partitioned in the resampling process to reduce potential bias.\nThe variance (often called precision) of resampling is also important. We want to get a performance estimate that gives us consistent results if we repeat it. The resampling precision is driven mainly by B and \\(n_{tr}\\). With some resampling techniques, if your results are too noisy, you can resample more and stabilize or reduce the estimated variance (at the cost of increased computational time). We’ll examine the bias and precision of different methods as we discuss each method.\nLet’s examine a few specific resampling methods, starting with the most simple: a single validation set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-validation",
    "href": "chapters/resampling.html#sec-validation",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.3 Validation Sets",
    "text": "10.3 Validation Sets\nWe’ve already described a validation set; it is typically created via a three-way initial split4 of the data (as shown in Figure 10.2). For time series data, it is common to use the most recent data for the test set. Similarly, the validation set would include the most recent data (once the test set partition is created). The training set would include everything else.\n\n\n\n\n\n\n\n\nFigure 10.2: An initial data splitting scheme that incorporates a validation set.\n\n\n\n\n\nWhile not precisely the same, a validation set is extremely similar to an approach described below called Monte Carlo cross-validation (MCCV). If we used a single MCCV resample, the results would be effectively the same as those of a validation set; the difference is not substantive.\nOne aspect of using the validation set is what to do with it after you’ve made your final decision about the model pipeline. Ordinarily, the entire training set is used for the final model fit.\nData can be scarce and, in some cases, an argument can be made that the final model fit could include the training and validation set. Doing this does add some risk; your validation set statistics are no longer completely valid since they measured how well the model works with similar training sets of size \\(n_{tr}\\). If you have an abundance of data, the risk is low, but, at the same time, the model fit won’t change much by adding \\(n_{val}\\) data points. However, if your training data set is not large, adding more data could have a profound impact, and you risk using the wrong model for your data5.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-cv-mc",
    "href": "chapters/resampling.html#sec-cv-mc",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.4 Monte Carlo Cross-Validation",
    "text": "10.4 Monte Carlo Cross-Validation\nMonte Carlo cross-validation emulates the initial train/test partition. For each one of B resamples, it takes a random sample of the training set (say about 75%) to use as the analysis set6. The remainder is used for model assessment. Each of the B resamples is created independently of the others, so some of the training set points are included in multiple assessment sets. Figure 10.3 shows the results of \\(M(\\mathfrak{D}^{tr}, 3)\\) for MCCV with a data set with \\(n_{tr} = 30\\) data points and 80% of the data are allocated to the analysis set. Note that samples 16 and 17 are in multiple assessment sets.\n\n\n\n\n\n\n\n\nFigure 10.3: A schematic of three Monte Carlo cross-validation resamples created from an initial pool of 30 data points.\n\n\n\n\n\nHow many resamples should we use, and what proportion of the data should be used for the analysis set? These choices are partly driven by computing power and training set size. Clearly, using more resamples means better precision.\nA thought experiment can be useful for any resampling method. Based on the proportion of data going into the assessment set, how confident would you feel in a performance metric being computed for this much data? Suppose that we are computing the RMSE for a regression model. If our assessment set contained \\(n_{pred} = 10\\) (on average), we might think our metric’s mean is excessively noisy. In that case, we could either increase the proportion held out (better precision, worse bias) or resample more (better precision, same bias, long computational time).\nEach resampling method has different trade-offs between bias and variance. Let’s look at some simulation results to help understand the trade-off for MCCV.\n\nAnother Simulation Study\nSection 9.1 described a simulated data set that included 200 training set points. In that section, Figure 9.1 illustrates the effect of overfitting using a particular model (KNN). This chapter will use the same training set but with a simple logistic regression model where a four-degree-of-freedom spline was used with each of the two predictors.\nSince these are simulated data, an additional, very large data set was simulated and used to approximate the model’s true performance, once again evaluated using a Brier score. Our logistic model has a Brier score value of \\(Q \\approx\\) 0.0898, which demonstrates a good fit. Using this estimate, the model bias can be computed by subtracting the resampling estimate from 0.0898.\nThe simulation created 500 realizations of this 200 sample training set7. We resampled the logistic model for each and then computed the corresponding Brier score estimate from MCCV. This process was repeated using the different analysis set proportions and values of B shown in Figure 10.4. The left panel shows that the bias8 decreases as the proportion of data in the analysis set becomes closer to the amount in the training set. It is also apparent that the bias is unaffected by the number of resamples (B). The panel on the right shows the precision, estimated using the standard error, of the resampled estimates. This decreases nonlinearly as B increases; the cost-benefit ratio of adding more resamples shows eventual diminishing returns. The amount retained for the analysis set shows that values close to one have higher precision than the others. Regarding computational costs, the time to resample a model increases with both parameters (amount retained and B).\n\n\n\n\n\n\n\n\nFigure 10.4: Variance and bias statistics for simulated data using different configurations for Monte Carlo cross-validation. The range of the y-axes are common across similar plots below for different resampling techniques.\n\n\n\n\n\nThe results of this simulation indicate that, in terms of precision, there is little benefit in including more than 70% of the training set in the assessment set. Also, while more resamples always help, there is incremental benefit in using more than 50 or 60 resamples (for this size training set). Regarding bias, the decrease somewhat slows down near 80% of the training set retained. Holdout proportions of around 75% - 80% might be a reasonable rule of thumb (these are, of course, subjective and based on this simulation).",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-cv",
    "href": "chapters/resampling.html#sec-cv",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.5 V-Fold Cross-Validation",
    "text": "10.5 V-Fold Cross-Validation\nV-fold cross-validation (sometimes called K-fold cross-validation) (Stone 1974) is the most well-known resampling method9. It randomly allocates the training data to one of V groups of about equal size, called a “fold” (a stratified allocation can also be used). There are B = V iterations where the analysis set comprises V -1 of the folds, and the remaining fold defines the assessment set. For example, for 10-fold cross-validation, the ten analysis sets consist of 90% of the data, and the ten assessment sets contain the remaining 10% of the data and are used to quantify performance. As a visual illustration, Figure 10.5 shows the process for V = 3 (for brevity), for a data set with 30 samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.5: An example mapping with \\(V = 3\\)-fold cross-validation.\n\n\n\nArlot and Celisse (2010) is a comprehensive survey of cross-validation. Let’s look at two special cases.\n\nLeave-One-Out Cross-Validation\nLeave-one-out cross-validation (LOOCV, also called the jackknife) sets \\(V = n_{tr}\\). A single sample is withheld and, over \\(n_{tr}\\) iterations, a set of models is created, each with an analysis set of \\(n_{tr} - 1\\) samples. The resampling estimate is created by applying the metric function to the resulting \\(n_{tr}\\) predicted values. There are no replicate performance values; only a single \\(\\hat{Q}\\). As one might expect, the bias is nearly zero for this method. Although it would be difficult to quantify, the standard error of the estimator should be very large.\nThis method is extremely computationally expensive10 and is not often used in practice unless computational shortcuts are available.\n\n\nRepeated Cross-Validation\nOne way to improve precision of this method is to use repeated V-fold cross-validation. In this case, V-fold CV is repeated multiple times using different random number seeds. For example, two repeats of 10-fold cross-validation11 create two sets of V folds, which are treated as a collection of twenty resamples (e.g., B = V \\(\\times\\) R for R repeats). Again, increasing the number of resamples does not generally change the bias.\n\n\nVariance and Bias for v-Fold Cross-Validation\nThe discussions of LOO and repeated cross-validation beg the question: what value of V should we use? Sometimes the choice of V is driven by computational costs; smaller values of V require less computation time. A second consideration for choice of V is metric performance in terms of bias and variance. As we will see later in this chapter, the degree of bias is determined by how much data are retained in the assessment sets (as defined by V). As V increases, bias decreases; V=5 has substantially more bias than V=10. Fushiki (2011) describes post hoc methods for reducing the bias for this resampling method.\nCompared to other resampling methods, V-fold cross-validation generally has a smaller bias, but relatively poor precision (that would be improved via repeats).\nTo understand the trade-off, the same simulation approach from the previous section is used. Figure 10.6 shows bias and variance results, where the x-axis is the total number of resamples \\(V\\times R\\). As expected, the bias decreases with V and is constant over the number of resamples. Five-fold cross-validation stands out as particularly bad with a percent bias of roughly 3.9%. Using ten folds decreases this to about 2.2%. The precision values show that smaller values of V have better precision; 10-fold CV needs substantially more resamples to have the same standard error than V = 5.\n\n\n\n\n\n\n\n\nFigure 10.6: Variance and bias statistics for simulated data using different configurations for \\(V\\)-fold cross-validation.\n\n\n\n\n\nIs there any advantage to using V &gt; 10? Not much. The decrease in bias is small and many more replicates are required to reach the same precision as V = 10. For example, twice as many resamples are required for 20-fold CV to match the variance of 10-fold CV.\nFor this simulation, the properties are about the same when MCCV and 10-fold CV are matched in terms of number of resamples and the amount allocated to the assessment set.\nOur recommendation is to almost always use V = 10. The bias and variance improvements are both good with 10 folds. Reducing bias in 5-fold cross-validation is difficult but the precision for 10-fold can be improved by increased replication.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-bootstrap",
    "href": "chapters/resampling.html#sec-bootstrap",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.6 The Bootstrap",
    "text": "10.6 The Bootstrap\nThe bootstrap (Efron 1979, 2003; Davison and Hinkley 1997; Efron and Hastie 2016) is a resampling methodology originally created to compute the sampling distribution of statistics using minimal probabilistic assumptions12. In this chapter, we’ll define a bootstrap sample and show how it can be used to measure fit quality for predictive models.\nFor a training set with \\(n_{tr}\\) data points, a bootstrap resample takes a random sample of the training set that is also size \\(n_{tr}\\). It does this by sampling with replacement; when each of the \\(n_{tr}\\) samples is drawn, it has no memory of the prior selections. This means that each row can be randomly selected again. For example, Figure 10.7 shows another schematic for three bootstrap samples. In this figure, the thirteenth training set sample was selected to go into the first analysis set three times. For this reason, a bootstrap resample will contain multiple replicates of some training set points, while others will not be selected at all. The data that were never selected are used to create the assessment set. This means that while the training set will have the same number of data points, the assessment sets will have varying numbers of data points.\nFor the bootstrap, we’ll refer to the mean estimate (line 12 in Algorithm 10.1) as the “ordinary” estimator (others will follow shortly).\n\n\n\n\n\n\n\n\nFigure 10.7: A schematic of three bootstraps resamples created from an initial pool of 30 data points.\n\n\n\n\n\nThe probability that a data point will be picked is \\(1 / n_{tr}\\) and, from this, the probability that a training set point is not selected at all is\n\\[\\prod_{i=1}^{n_{tr}} \\left(1 - \\frac{1}{n_{tr}}\\right) = \\left(1 - \\frac{1}{n_{tr}}\\right)^{n_{tr}}\\approx e^{-1} = 0.368\\]\nSince the bootstrap sample contains the selected data, each training point is selected with probability \\(1 - e^{-1} \\approx 0.632\\). The implication is that, on average, the analysis set, contains about 63.2% unique training set points and the assessment set includes, on average, 36.8% of the training data. When comparing the number of unique training set points excluded by the bootstrap method to those excluded by V-fold cross-validation, the bootstrap method is roughly equivalent to using \\(V=3\\) in cross-validation.\nFigure 10.8 shows bias and variance for the simulated data. As one might expect, there is considerable bias in the bootstrap estimate of performance. In comparison to the corresponding plot for MCCV, the bootstrap bias is worse than the MCCV curve where 60% were held out. The curve is also flat; the bias doesn’t go away by increasing B.\nHowever, the precision is extremely good, even with very few resamples.\n\n\n\n\n\n\n\n\nFigure 10.8: Variance and bias statistics for simulated data using different configurations for the bootstrap.\n\n\n\n\n\n\nCorrecting for Bias\nThere have been some attempts to de-bias the ordinary bootstrap estimate. The “632 estimator” (Efron 1983) uses the ordinary bootstrap estimate (\\(\\hat{Q}_{bt}\\)) and the resubstitution estimate (\\(\\hat{Q}_{rsub}\\)) together:\n\\[\\hat{Q}_{632} = e^{-1}\\, \\hat{Q}_{rsub} + (1 - e^{-1})\\, \\hat{Q}_{bt} = 0.368\\,\\hat{Q}_{rsub} + 0.632\\,\\hat{Q}_{bt}\\] Figure 10.8 shows that there is a significant drop in the bias when using this correction. The 632 estimator combines two different statistical estimates, and we only know the standard error of \\(\\hat{Q}_{bt}\\). Therefore, the right-hand panel does not show a standard error curve for the 632 estimator.\nLet’s look at an average example from the simulated data sets with B = 100. Table 10.1 has the estimator values and their intermediate values. Let’s first focus on the values for the column labeled “Logistic.” The ordinary bootstrap estimate was \\(\\hat{Q}_{bt}\\) = 0.101 and repredicting the training set produced \\(\\hat{Q}_{rsub}\\) = 0.0767. The 632 estimate shifts the Brier score downward to \\(\\hat{Q}_{632}\\) = 0.0919. This reduces the pessimistic bias; our large sample estimate of the true Brier score is 0.0898 so we are closer to that value.\nAnother technique, the 632+ estimator (Efron and Tibshirani 1997), uses the same blending strategy but uses dynamic weights based on how much the model overfits (if at all). It factors in a model’s “no-information rate”: the metric value if the predicted and true outcome values were independent. The author gives a formula for this, but it can also be estimated using a permutation approach where we repeatedly shuffle the outcome values and compute the metric (Section 9.5). We will denote this value as \\(\\hat{Q}_{nir}\\). For our simulated data set \\(\\hat{Q}_{nir}\\) = 0.427; we believe that this is the worst case value for our metric.\nWe then compute the relative overfitting rate (ROR) as\n\\[ ROR = \\frac{\\hat{Q}_{bt}-\\hat{Q}_{rsub}}{\\hat{Q}_{nir} -\\hat{Q}_{rsub}} \\]\nThe denominator measures the range of the metric (from most optimistic to most pessimistic). The numerator measures the optimism of our ordinary estimator. A value of zero implies that the model does not overfit, and values of one indicate the opposite. For our logistic model, the ratio is 0.024 / 0.35 = 0.0686, indicating that the logistic model is not overinterpeting the training set.\nThe final 632+ estimator uses a different weighting system to combine estimates:\n\\[\n\\begin{align}\n\\hat{Q}_{632+} &= (1 - \\hat{w})\\, \\hat{Q}_{rsub} + \\hat{w}\\, \\hat{Q}_{bt} \\quad \\text{with}\\notag \\\\\n\\hat{w} &= \\frac{0.632}{1 - 0.368ROR} \\notag\n\\end{align}\n\\] Plugging in our values, the final weight on the ordinary estimator (\\(w\\) = 0.648) is very close to what is used by the regular 632 estimator (\\(w\\) = 0.632). The final estimate is also similar: \\(\\hat{Q}_{632+}\\) = 0.0923.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrier Score\n\n\n\n\nLogistic\n1 NN\n\n\n\n\nEstimates\n\n\n(truth)\n0.090\n0.170\n\n\nresubstitution\n0.077\n0.000\n\n\nsimple mean\n0.101\n0.174\n\n\nIntermediates\n\n\nno information rate\n0.427\n0.496\n\n\nrelative overfitting rate\n0.069\n0.351\n\n\nweights\n0.648\n0.726\n\n\nFinal Estimates\n\n\n632\n0.092\n0.110\n\n\n632+\n0.092\n0.126\n\n\n\n\n\n\n\n\n\n\n\nTable 10.1: Bias correction values for two models on a simulated data set.\n\n\n\nThe simulations studies shown here in Figure 10.8 replicated what Molinaro (2005) found; the 632+ estimator has bias properties about the same as 10-fold cross-validation.\nHow do these bias corrected estimators work when there is extreme overfitting? One way to tell is to consider a 1-nearest neighbor model. In this case, re-predicting the training set predicts every data point with its outcome value (i.e., a “perfect” model). For the Brier score, this means \\(\\hat{Q}_{rsub}\\) is zero. Table 10.1 shows the rest of the computations. For this model \\(Q\\approx\\) 0.17 and the ordinary estimate comes close: \\(\\hat{Q}_{bt}\\) = 0.174.\nThe 632 estimate is \\(\\hat{Q}_{632} = 0.632\\,\\hat{Q}_{bt} =\\) 0.11; this is too much bias reduction as it overshoots the true value by a large margin. The 632+ estimator is slightly better. The ROR value is higher (0.351) leading to a higher weight on the ordinary estimator to produce \\(\\hat{Q}_{632+}\\) = 0.126.\nShould we use the bootstrap to compare models? Its small variance is enticing, but the bias remains an issue. The bias is likely to change as a function of the magnitude of the metric. For example, the bootstrap’s bias is probably different for models with 60% and 95% accuracy. If we think our models will have about the same performance, the bootstrap (with a bias correction) may be a good choice. Note that Molinaro (2005) found that the 632+ estimator may not perform well when the training set is small (and especially of the number of predictors is large).\nLet’s take a look at a few specialized resampling methods.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-time-series-resampling",
    "href": "chapters/resampling.html#sec-time-series-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.7 Time Series Data",
    "text": "10.7 Time Series Data\nTime series data (Hyndman and Athanasopoulos 2024) are sequential observations ordered over time. The most common example is daily stock prices. These data are a special case due to autocorrelation: rows of the data set are not independent. This can occur for many different reasons (e.g. underlying seasonal trends, etc.), and this dependency complicates the resampling process in two primary ways. We need to think more carefully about how analysis and assessment sets are created.\n\nRandomly allocating samples to these sets would deteriorate the relationships among the samples due to autocorrelation. If the autocorrelation is important information related to the response, then random sampling will negatively impact a model’s ability to uncover the predictive relationship with the response.\nPlacing newer data in the analysis set and older data in the assessment set would not make sense because of the time trends in the data.\n\nRolling forecasting origin splitting (Tashman 2000) is a resampling scheme that mimics the train/test splitting pattern from Section 3.4 where the most recent data are used in the assessment set. We would choose how much data (or what range of time) to use for the analysis and assessment sets, then slide this pattern over the whole data set. Figure 10.9 illustrates this process where each block could represent some unit of time. For example, we might build the model on six months of data and then predict the next month. The next resample iteration could bump that forward by a month or some other period (so that the assessment sets can be distinct or overlapping).\n\n\n\n\n\n\n\n\nFigure 10.9: A schematic of rolling origin resampling for time series data. Each box corresponds to a unit of time (e.g., day, month, year, etc.).\n\n\n\n\n\nWe also have the choice to cumulatively expand the analysis data from the start of the training set to the end of the current slice. For example, in Resample 2 of Figure 10.9, the model would be fit using samples 1-9; in Resample 3, the model would be fit using samples 1-10; and so on. This approach may be useful when the number of periods of time are smaller and we would like to include as much of the historical data as possible in each resample.\nWe’ll use rolling forecasting origin resampling in one of the upcoming case studies using the hotel rate data previously discussed in Section 6.1.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-spatial-resampling",
    "href": "chapters/resampling.html#sec-spatial-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.8 Spatial Data",
    "text": "10.8 Spatial Data\nSection 3.9 described the problem of spatial correlation between locations and a method for creating an initial split for the forestation data.\nLike time-series data, the resampling scheme for this type of data emulates the initial splitting process. We could create groupings for the training set using the following methods:\n\nClustering methods: These methods create partitions of the data where points with spatial coordinates are closer to each other within the same cluster than to points in other clusters.\nGrid-based methods: A grid of equally sized blocks, either rectangular or hexagonal, is created to encompass the training set points.\n\nFrom these groupings, we can apply procedures similar to previous cross-validation variations:\n\nLeave-one-group-out resampling: This method creates as many resamples as there are groups. In each resample, data points from one group are used as the assessment set, while the remaining groups are used to train the model. This process repeats for each group.\nV-fold cross-validation: This method assigns each group to one of V meta-groups (e.g., folds). In each of the V iterations, one fold is left out as the assessment set, while the remaining folds are used to train the model.\nRepeated V-fold cross-validation: This method involves restarting the grouping process with a different random number seed, allowing for multiple rounds of V-fold cross-validation.\n\nEach of these approaches can be augmented using a buffer to exclude very close neighbors from being used.\nGoing back to our forestry data, we previously used hexagonal blocks to split the data (see Figure 3.5). The same blocking/buffering scheme was re-applied to the training set in order to create a 10-fold cross-validation. Recall that a 25 x 25 grid was used. For the training set, some of these hexagons are missing all of their data (since they were previously added to the test set). Of the remaining grid points, we systematically assign them to the ten folds. Figure 10.10 shows this process for the first fold. The figure shows the blocks of data, along with the corresponding buffer for this iteration of cross-validation. The purple blocks are, for the most part, not adjacent to one another. These are combined into the first assessment set. The other locations that are not in the buffers are pooled into the first analysis set.\n\n\n\n\n\n\n\n\nFigure 10.10: An example of one split produced by block cross-validation on the forestation data. The analysis (magenta) and assessment (purple) sets are shown with locations in the buffer in black. The empty spaces represent the data reserved for the test set (see Figure 3.5).\n\n\n\n\nOur training set contains 4,832 locations. Across each of the ten folds, every location appears once in the analysis and assessment sets.\nJust as with standard V-fold cross-validation, the number of folds can be chosen based on bias and variance concerns. Unlike standard resampling, any investigation into this topic would depend on a specific spatial autocorrelation structure. As such, we have decided to follow the recommendations for non-spatial data and use V=10. If we find that the variation in our statistics estimated from ten resamples is too large, we can employ the same process of repeating the block resampling process using different random number seeds. For our data, the average number of locations in the assessment sets is 483 and this appears large enough to produce sufficiently precise performance statistics.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-multilevel-resampling",
    "href": "chapters/resampling.html#sec-multilevel-resampling",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.9 Grouped or Multi-Level Data",
    "text": "10.9 Grouped or Multi-Level Data\nWhen rows of data are correlated for other reasons, such as those discussed in Section 3.8, we can extend the techniques described above. Consider the previously mentioned example of having the purchase histories of many users from a retail database. If we had a training set of 10,000 instances that included 100 customers, we could conduct V-fold cross-validation to determine which customers would go into the analysis and assessment sets. As with the initial split, any rows associated with specific customers would be placed into the same partition (i.e., analysis or assessment).",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#sec-resampling-faq",
    "href": "chapters/resampling.html#sec-resampling-faq",
    "title": "10  Measuring Performance with Resampling",
    "section": "10.10 Frequently Asked Questions",
    "text": "10.10 Frequently Asked Questions\nThe general concept of resampling seems clear and straightforward. However, we regularly encounter confusion and commonly asked questions about this process. In this section we will address these common questions and misconceptions.\n\nIs it Bad to Get Different Results from Each Resample?\nThe point of resampling is to see how the model pipeline changes when the data changes. There is a good chance that some important estimates differ from resample to resample. For example, when algorithmically filtering predictors, you might get B different predictor lists, and that’s okay. It does give you a sense of how much noise is in the part of your model pipeline.\nIt can be good to examine the distribution of resampling performance. Occasionally, we may find a resample that has unusual performance with respect to the rest of the resamples. This may merit further investigation as to why the resample is unusual.\n\n\nCan/Should I Use the Individual Resampling Results to Change the Model Pipeline?\nWe often hear questions about how to use the cross-validated results to define the model. For example, using the top X% of predictors selected during resampling as the final predictor set in a new model pipeline seems intuitive.\nOne important thing to consider is that you now have a different model pipeline than what was just resampled. To understand the performance of your newly informed model, you need to resample a model pipeline that algorithmically selects the X% of predictors and resample it.\nNote that resampling the specific set of top predictors determined from the resamples is not appropriate. We have seen this done many times before in the hopes of improving predictive performance. Performance often improves for the assessment sets. But the performance cannot be generalized to the test set or for other new samples. The rule to determine the top set should be resampled, not the specific output of that rule.\nFor the original pipeline, the final predictor set is determined when you fit that pipeline on the training set. The resampling results tell you about what could have happened; the training set results show what did happen.\n\n\nWhat Happens to the b Trained Models? Which One Should I Keep?\nThe B models fit within resampling are only used to estimate how well the model fits the data. You don’t need them after that. You can retain them for diagnostic purposes though. As mentioned above, using the replicate models to look into how parts of the model change can be helpful.\nAlso, if you are not using a single validation set, recall that none of these models are trained with the unaltered training set, so none would be the final model.\n\n\nIs this some Sort of Ensemble?\nNo. We’ll talk more about ensembles in ?sec-ensembles and how the assessment set predictions can be used to create an ensemble.\n\n\nCan I Accidentally Resample Incorrectly?\nUnfortunately, yes. As previously mentioned, improper data usage is one of the most frequent ways that machine learning models silently fail; that is, you won’t know that there is a problem until the next set of labeled data. A good example discussed in ?sec-removing-predictors is Ambroise and McLachlan (2002).\nIf you are\n\ntraining every part of your model pipeline after the initial data split and using only the training set and,\nevaluating performance with external data\n\nthe risk of error is low.\n\n\nWhat is Nested Resampling?\nThis is a version of where an additional layer of resampling occurs. For example, suppose you are using 10-fold cross-validation. Within each of the 10 iterations, you might add 20 bootstrapping iterations (that resamples the analysis set). That ends up training 200 distinct models13.\nFor a single model, that’s not very useful. However, as discussed in Section 11.4, there are situations where the analysis set is being used for too many purposes. For example, during model tuning, you might use resampling to find the best model and measure its performance. In certain cases, that can lead to bad results for the latter task.\nAnother example is recursive feature selection, where we are trying to rank predictors, sequentially remove them, and determine how many to remove. In this case, it’s a good idea to use an outer resampling loop to determine where to stop and an inner loop for the other tasks.\nWe will see nested resampling in the next chapter.\n\n\nIs Resampling Related to Permutation Tests?\nNot really. Permutation methods (Section 9.5) are similar to resampling only in that they perform multiple calculations on different versions of the training set.\nWe used permutation methods in the bootstrapping section to estimate the no-information rate. Otherwise, they won’t be used to evaluate model performance.\n\n\nAre Sub-Sampling or Down-Sampling the Same as Resampling?\nNo. These are techniques to modify your training set (or analysis set) to rebalance the data when a classification problem has rare events. They’ll be discussed in ?sec-imbalance-sampling.\n\n\nWhy Do I Mostly Hear About Validation Sets?\nAs discussed in Section 1.6, stereotypical deep learning models are trained on very large sets of data. There is very little reason to use multiple resamples in these instances, and deep learning takes up a lot of space in the media and social media.\nGenerally, every data set that is not massive could benefit from multiple resamples.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#chapter-references",
    "href": "chapters/resampling.html#chapter-references",
    "title": "10  Measuring Performance with Resampling",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nArlot, S, and A Celisse. 2010. “A Survey of Cross-Validation Procedures for Model Selection.” Statistics Surveys 4: 40–79.\n\n\nBates, S, T Hastie, and R Tibshirani. 2023. “Cross-Validation: What Does It Estimate and How Well Does It Do It?” Journal of the American Statistical Association, 1–12.\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nEfron, B. 1983. “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.” Journal of the American Statistical Association, 316–31.\n\n\nEfron, B. 2003. “Second Thoughts on the Bootstrap.” Statistical Science, 135–40.\n\n\nEfron, B, and T Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.\n\n\nEfron, B, and R Tibshirani. 1997. “Improvements on Cross-Validation: The 632+ Bootstrap Method.” Journal of the American Statistical Association 92 (438): 548–60.\n\n\nFushiki, T. 2011. “Estimation of Prediction Error by Using k-Fold Cross-Validation.” Statistics and Computing 21: 137–46.\n\n\nHyndman, RJ, and G Athanasopoulos. 2024. Forecasting: Principles and Practice, 3rd Edition. Otexts.\n\n\nMolinaro, A. 2005. “Prediction Error Estimation: A Comparison of Resampling Methods.” Bioinformatics 21 (15): 3301–7.\n\n\nStone, M. 1974. “Cross-Validatory Choice and Assessment of Statistical Predictions.” Journal of the Royal Statistical Society: Series B (Methodological) 36 (2): 111–33.\n\n\nTashman, L. 2000. “Out-of-Sample Tests of Forecasting Accuracy: An Analysis and Review.” International Journal of Forecasting 16 (4): 437–50.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/resampling.html#footnotes",
    "href": "chapters/resampling.html#footnotes",
    "title": "10  Measuring Performance with Resampling",
    "section": "",
    "text": "These names are not universally used. We only invent new terminology to avoid confusion; people often refer to the data in our analysis set as the “training set” because it has the same purpose.↩︎\nThis discussion is very similar to the variance-bias tradeoff discussed in Section 8.4. The context here differs, but the themes are the same.↩︎\nMeaning that performance looks worse than it should. For example, smaller \\(R^2\\) or inflated Brier score.↩︎\nThe split can be made using the same tools already discussed, such as completely random selection, stratified random sampling, etc.↩︎\nAlso, if \\(n_{tr}\\) is not “large,” you shouldn’t use a validation set anyway.↩︎\nAgain, this could be accomplished using any of the splitting techniques described in Chapter 3.↩︎\nThe simulation details, sources, and results can be found at https://github.com/topepo/resampling_sim.↩︎\nThe percent bias is calculated as \\(100(Q_{true} - \\hat{Q})/Q_{true}\\) for metrics where smaller is better.↩︎\nArlot and Celisse (2010) provides a comprehensive survey of cross-validation↩︎\nHowever, for linear regression, the LOOCV predictions can be quickly computed without refitting a large number of models.↩︎\nThis isn’t the same as using V = 20. That scheme has very different bias properties than two repetitions of V = 10.↩︎\nThis application will be discussed in ?sec-boot-intervals.↩︎\nWe will refer to the first resample (10-fold CV) as the outer resample and the bootstrap as the inner resample.↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Measuring Performance with Resampling</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html",
    "href": "chapters/grid-search.html",
    "title": "11  Grid Search",
    "section": "",
    "text": "11.1 Regular Grids\nGrid search is a method to optimize a model pipeline’s tuning parameters. It creates a pre-defined set of candidate values and computes performance for each. From there, the numerically best candidate could be chosen, or the relationship between the tuning parameter(s) and model performance can be inspected to see if the model might benefit from additional optimization.\nSuppose there is a single tuning parameter, as in the \\(n_{min}\\) example of Section 9.3; each candidate takes a scalar value (quantitative or qualitative). For other models, there are multiple tuning parameters. For example, support vector machine models can have two or three tuning parameters, the Elastic Net model has two tuning parameters, and the boosted tree model has multiple tuning parameters. We’ll look at two of the boosted tree parameters in detail below.\nThe previous chapter demonstrated that using external data to evaluate the model is crucial (be it resampling or a validation set). Grid search has no free lunch: we cannot simply fit the model to each candidate set and evaluate them by simply re-predicting the same data.\nAlgorithm 11.1 formally describes the grid search process. For a model with \\(m\\) tuning parameters, we let \\(\\Theta\\) represent the collection of \\(s\\) candidate values. For each specific combination of parameters (\\(\\theta_j\\)), we resample the model to produce some measure of efficacy (e.g., \\(R^2\\), accuracy, etc.)1. From there, the best value is chosen, or additional work is carried out to find a suitable candidate.\nTo demonstrate, this chapter will initially focus on grids for a boosted tree model with two tuning parameters. Recall that a boosted tree is a collection of individual decision trees created sequentially. One parameter related to the process of creating the ensemble is the learning rate: the next tree uses information from the last tree to improve it. An important parameter is how much, or how fast it learns. It could be that some data sets need a large number of trees that evolve slowly (i.e., low learning rate) to find an optimal value of the performance metric. Alternatively, other data sets require fewer trees that change rapidly for optimal predictive performance. The learning rate parameter must be greater than zero, and is typically in the range of 10-5 to 10-1. Because the range of this parameter is very large, it is best conceptualized in log units.\nWe’ll illustrate different strategies of grid search using this model setup with varying configurations of the learning rate and the number of trees in the ensemble.\nThere are two main classes of grids: regular and irregular. We can view generating a collection of candidate models as a statistical design of experiments (DOE) problem (Box, Hunter, and Hunter 1978). There is a long history of DOE, and we’ll invoke relevant methods for each grid type. Santner, Williams, and Notz (2018) and Gramacy (2020) have excellent overviews of the DOE methods discussed in this chapter.\nThe following two sections describe different methods for creating the candidate set \\(\\Theta\\). Subsequent sections describe strategies for making grid search efficient using tools such as parallel processing and model racing.\nA regular grid starts with a sequence or set of candidate values for each tuning parameter and then creates all combinations. In statistics, this is referred to as a factorial design. The number of values per tuning parameter does not have to be the same.\nTo illustrate a regular grid, we’ll use five values of the number of trees (1, 500, 1000, 1500, and 2000) and three values of the learning rate (10-3, 10-2, and 10-1). This grid of 15 candidates is shown in Figure 11.1. The grid covers the entire space with significant lacuna in between.\nFigure 11.1: Three types of grids (in columns) are illustrated with tuning parameters from a boosting model. Each grid contains 15 candidates. The space-filling design was created using the Audze-Eglais method described in Section 11.2.\nWe’ll use the simulated training set in Section 9.1 with the same 10-fold cross-validation scheme described there. Once again, Brier scores were used to measure how well each of the 15 configurations of boosted tree model predicted the data. Figure 11.2 shows the results: a single tree is a poor choice (due to underfitting), and there are several learning rate values that work well. Even though this is not a diverse set of candidate values, we can probably pick out a reasonable candidate with a small Brier score, such as 500 trees and a learning rate of 10-2 (although there are a few other candidates that would be good choices).\nFigure 11.2: The boosted tree tuning results for the regular grid shown in Figure 11.1.\nThe pattern shown in this visualization is interesting: the trajectory for the number of trees is different for different values of the learning rate. The two larger learning rates are similar, showing that the optimal number of trees is in the low- to mid-range of our grid. The results for the smallest learning rate indicate that better performance might be found using more trees than were evaluated. This indicates that there is an interaction effect in our tuning parameters (just as we saw for predictors in Section 8.1). This might not affect how we select the best candidate value for the grid, but it does help build some intuition for this particular model that might come in handy when tuning future models.\nGaining intuition is much easier for regular grids than other designs since we have a full set of combinations. As we’ll see shortly, this interaction is very hard to see for a space-filling design.\nThe primary downside to regular grids is that, as the number of tuning parameters increases, the number of points required to fill the space becomes extremely large (due to the curse of dimensionality). However, for some models and pre-processing methods, regular grids can be very efficient despite the number of tuning parameters (see the following section that describes the “submodel trick”).",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-irregular-grid",
    "href": "chapters/grid-search.html#sec-irregular-grid",
    "title": "11  Grid Search",
    "section": "11.2 Irregular Grids",
    "text": "11.2 Irregular Grids\nIrregular grids are not factorial in nature. A simple example is a random grid where points are randomly placed using a uniform distribution on an appropriate range for each tuning parameter. A design of size 15 is shown in Figure 11.1. There are some gaps and clustering of the points, but the space for each individual dimension is covered well. The tuning results are shown in the top panels of Figure 11.3. Because it is an irregular design, we can’t use the same visualization as in Figure 11.2. Instead, a “marginal” plot is shown, where each numeric tuning parameter is plotted against performance in a separate panel.\nThe top left panel suggests that a very small learning rate is bad, but otherwise, there is not a strong trend. For the number of trees (in the top right panel), a small number of trees should be avoided, and perhaps the maximum number tested would be a good choice. From this plot, it is impossible to discover the interaction effect seen with the regular grid. However, this marginal plot is the main technique for visualizing the results when there are a moderate to large number of tuning parameters. The numerically best candidate was not too dissimilar from the regular grid: 1,898 trees and a learning rate of 10-2.7\n\n\n\n\n\n\n\n\nFigure 11.3: The tuning results for the irregular grids shown in Figure 11.1. The lines are spline smooths.\n\n\n\n\n\nAnother type of irregular grid is a space-filling design (Joseph 2016), where the goal is to make sure that the tuning parameter space is covered and that there is minimal redundancy in the candidate values. There are a variety of methods for achieving this goal.\nFor example, Figure 11.1 shows a 15 point Audze-Eglais space-filling design. The space is covered more compactly than the regular design of the same size and is much more uniform than the 15 point random design. The lower panels of Figure 11.3 show the tuning results. The results are somewhat cleaner than the random grid results but show similar trends. Here, the numerically best candidate was: 286 trees and a learning rate of 10-2.\nThere are many types of space-filling designs, and it is worth taking the time to take a quick tour of some of them.\nThe Latin hypercube design (LHD) is the most popular method for constructing space-filling designs (Husslage et al. 2011; Viana 2016). These designs have a simple definition. Suppose our hyperparameter space is rectangular and partitioned into smaller (hyper)cubes. From this, a LHD is a set of distinct points where no one dimension has multiple values in any bins2. We desire candidates that fill the space of each parameter and are not close to one another.\nThe most basic approach to creating a Latin hypercube design is random sampling (Mckay, Beckman, and Conover 2000). If there are \\(m\\) parameters and we request \\(s\\) candidate values, the parameter space is initially divided into \\(s^m\\) hypercubes of equal size. For each tuning parameter, \\(s\\) regions in its dimension are selected at random, and a value is placed in this box (also at random). This process repeats for each dimension. Suppose there are ten bins for a parameter that ranges between zero and one. If the first design point selects bin two, a random uniform value is created in the range [0.1 0.2). Figure 11.4 shows three such designs, each generated with different random numbers.\n\n\n\n\n\n\n\n\nFigure 11.4: Three replicate Latin hypercube sampling designs of size ten for two parameters. Each design uses different random numbers. The grid lines illustrate the 100 hypercubes used to generate the values.\n\n\n\n\n\nTechnically, these designs cover the space of each predictor uniformly. However, large multivariate regions can be empty. For example, one design only samples combinations along the diagonal. Additional constraints can make the design more consistent with our desires.\nFor example, we could choose points to maximize the minimum pairwise distances between the candidates. These designs are usually referred to as MaxiMin designs (Pronzato 2017). Comparing the two irregular designs in Figure 11.1, the random grid has a maximum minimum distance between candidates of 0.09. In contrast, the corresponding space-filling design’s value (0.26) is 2.9-fold larger3. The latter design was optimized for coverage, and we can see far less redundancy in this design.\nA similar method, initially proposed by Audze and Eglais (1977), maximizes a function of the inverse distances between \\(s\\) candidate points:\n\\[\ncriterion = \\sum_{i=1}^s \\sum_{j=1,\\;i\\ne j}^s\\frac{1}{dist(\\theta_i, \\theta_j)^2}\n\\]\nBates, Sienz, and Toropov (2004) devised search methods to find optimal designs for this criterion.\nSome other space-filling designs of note:\n\nMaximum entropy sampling selects points based on assumptions related to the distributions of the tuning parameters and their covariance matrix (Shewry and Wynn 1987; Joseph, Gul, and Ba 2015).\nUniform designs (Fang et al. 2000; Wang, Sun, and Xu 2022) optimally allocate points so that they are uniformly distributed in the space.\n\nWhile these methods can be generally constructed by sampling random points and using a search method to optimize a specific criterion, there has been scholarship that has pre-optimized designs for some combination of the number of tuning parameters and the requested grid size.\nThe advantage of space-filling designs over random designs is that, for smaller designs, the candidates do a better job covering the space and have a low probability of producing redundant points. Also, it is possible to create a space-filling design so that the candidates for each numerical parameter are nearly equally spaced (as was done in Figure 11.1).\nMany designs assume that all the tuning parameter values are quantitative. That may not always be the case. For example, K-nearest neighbors can adjust its predictions by considering how far the cost are from a new point; more distant points should not have the same influence as close cost. A weighting function can be used for this purpose. For example, weights based on the inverse of the distance between neighbors may produce better results. Equal weighting is often called a “rectangular weighting” function. The type of algebraic function used for weighting is a qualitative tuning parameter.\nA simple workaround for creating a space-filling design is to repeat the unique parameter values as if they were \\(s\\) distinct values when making the design. This allows them to be used with a Latin hypercube design and any space-filling design that uses this approach is not technically optimal. Practically speaking, this is an effective approach for tuning models. However, there are sliced LHD that can accomplish the same goal (Qian 2012; Ba, Myers, and Brenneman 2015).\nWe recommend space-filling designs since they are more efficient than regular designs. Regular designs have a lot of benefits when using an unfamiliar modeling methodology since you will learn a lot more about the nuances of how the tuning parameters affect one another.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-efficient-grid",
    "href": "chapters/grid-search.html#sec-efficient-grid",
    "title": "11  Grid Search",
    "section": "11.3 Efficient Computations for Conventional Grid Search",
    "text": "11.3 Efficient Computations for Conventional Grid Search\nThe computational cost of grid search can become large, depending on the resampling strategy and the number of candidates under consideration. In our example, a total of 150 boosted tree models are evaluated (5 values of the number of trees, 3 values of the learning rate, and 10-fold cross-validation) before determining which candidates are most favorable to our data.\nWe’ll look at three approaches to making grid search more efficient. One method (submodels) happens automatically for some types of models, another approach (parallel processing) uses software engineering tools, and the last tool (racing) is a statistical solution.\nTo illustrate these more efficient search approaches, a 100 grid point space-filling design will be used with a large parameter range (1 to 3000 trees) and learning rates between 10-5 and 10-1.\n\n11.3.1 Submodels\nLet’s begin by understanding the submodel trick. For some models a single training model can be used to predict many tuning parameter candidates. For example, the boosted tree model that we have been using creates a sequential ensemble of decision trees, each depending on the previous. Suppose that a boosting model with 3,000 trees is created. Most implementations of boosted trees can use this model to predict the outcomes of any smaller ensemble size. Therefore, we only need to build the boosted tree model using the largest number of trees for each specific combination of the remaining tuning parameters. For a regular grid, this can effectively drop a dimension of the computations. Depending on the model and grid, the speed-up for using this approach can be well into double digits.\nSome different models and pre-processors can have this quality, including the glmnet model (?sec-penalized-logistic-regression), partial least squares (Section 7.2.3), principal component feature extraction (Section 7.2.1), and others.\nUnfortunately, irregular designs cannot exploit the submodel trick since they are not factorial in nature. A hybrid design could be used where a dense sequence of \\(s_1\\) parameter values is created for the tuning parameter associated with submodels and a separate space-filling design of size \\(s_2\\) for the other parameters. These two grids can be crossed so that the space-filling design is replicated for each value of the submodel parameter. This produces \\(s_1\\times s_2\\) grid points but, effectively, only \\(s_2\\) models are fit.\nFor our larger grid, we created a similar regular grid of 100 points with the same expanded ranges of parameters. There were 10 unique, evenly spaced values for both parameters. To evaluate this grid, we only need to train 10 models. Compared to the analogous space-filling design, the regular grid was 3.3 times faster to evaluate.\n\n\n11.3.2 Parallel Processing\nFortunately, none of the 1,000 models in our large grid depend on one another and can be computed separately. Since almost all modern computers have GPUs and multiple CPUs, we can break the computations into different “chunks” of work and execute them simultaneously on distinct processors (or separate computers entirely). The parallel processing of models can significantly reduce the time it takes to tune using grid search.\nFor example, consider the space-filling designs with 15 candidates evaluated across 10 resamples. We spread the 150 model fits across 10 worker processes (on the same computer). Despite the meager computation costs of training each boosted tree, there was still a 5.7-fold speedup (35.5s versus 6.2s) when run in parallel.\nHowever, there are some important nuances that we should consider before initiating parallel processing.\nFirst, it is an excellent idea to parallelize the “longest loop” (literally or figuratively). Looking back at Algorithm 11.1, there is a loop across the \\(s\\) candidates in line 6. Line 7 contains the resampling loop across \\(B\\) resamples. If the data set is large, it may be optimal to invert the loop where we parallel process across the \\(B\\) resamples and execute the \\(s\\) models within each. Keeping each resample on a single worker means less input/output traffic across the workers. See Kuhn and Silge (2022) Section 13.5.2. Alternatively, if the data are not large, it could be best to “flatten” the two loops into a single loop with \\(s\\times B\\) iterations (with as many workers as possible).\nAdditionally, if expensive preprocessing is used, a naive approach where each pipeline is processed in parallel might be counterproductive because we unnecessarily repeat the same preprocessing. For example, suppose we are tuning a supervised model along with UMAP preprocessing. By sending each of the \\(s\\) candidates to a worker, the same UMAP training occurs for each set of candidates that correspond to the supervised model. Instead, a conditional process can be used where we loop across the UMAP tuning parameter combinations and, for each, run another loop across the parameters associated with the supervised model.\nAnother consideration is memory. If the data used to train and evaluate the model are copied for each worker, the number of workers can be restricted to fit within system memory. A well thought out process can avoid this unnecessary restriction.\nWhen we understand the computational aspects of parallel processing, we can almost always use it to greatly reduce the time required for model tuning.\n\n\n\n11.3.3 Racing\nA third way we can improve the model tuning process is through Racing (Maron and Moore 1997). This technique adaptively resamples the data during grid search. The goal is to cull tuning parameter combinations that have no real hope of being optimal before they are completely resampled. For example, in our previous tuning of the boosting model using a regular grid, it is clear that a single tree performs very poorly. Unfortunately, we will not know this until all the computations are finished.\nRacing tries to circumvent this issue by doing an interim statistical analysis of the tuning results. From this, we can compute a probability (or score) that measures how likely each candidate will be the best (for some single metric). If a candidate is exceedingly poor, we usually can tell that after just a few resamples4.\nThere are a variety of ways to do this. See Kuhn (2014), Krueger, Panknin, and Braun (2015), and Bergman, Purucker, and Hutter (2024). The general process is shown in Algorithm 11.2. The resampling process starts normally for the first \\(B_{min}\\) resamples. Lines 7-12 can be conducted in parallel for additional computation speed. After the initial resampling for all candidates, the interim analysis at iteration \\(b\\) results in a potentially smaller set of \\(s_b\\) candidates. Note that parallel processing can be incorporated into this process for each loop and the speed-up from using submodels also occurs automatically.\nAt each resampling estimate beyond the first \\(B_{min}\\) iterations, the current candidate set is evaluated for a single resample, and the interim analysis potentially prunes tuning parameter combinations.\n\n\n\n\n\n\n\n\n\n\\begin{algorithm} \\begin{algorithmic} \\State $\\mathfrak{D}^{tr}$: training set of predictors $X$ and outcome $y$ \\State $B$: number of resamples \\State Initial number of resamples $1 \\lt B_{min} \\lt B$ executed prior to analysis \\State $M(\\mathfrak{D}^{tr}, B)$: a mapping function to split $\\mathfrak{D}^{tr}$ for each of $B$ iterations. \\State $f()$: model pipeline \\State $\\Theta$: Parameter set ($s \\times m$) with candidates $\\theta_j$ \\For{$j=1$ \\To $s$} \\For{$b=1$ \\To $B_{min}$} \\State Generate $\\hat{Q}_{jb} =$ \\Call{Resample}{$\\mathfrak{D}^{tr}, f(\\cdot;\\theta_j), M_b(\\mathfrak{D}^{tr}, B)$} \\EndFor \\State Compute $\\hat{Q}_{j} = 1/B_{min}\\sum_b \\hat{Q}_{jb}$. \\EndFor \\State Eliminate candidates to produce $\\Theta^b$ ($s_b \\times m$) \\For{$b = B_{min} + 1$ \\To $B$} \\For{$j=1$ \\To $s$} \\State Generate $\\hat{Q}_{jb} =$ \\Call{Resample}{$\\mathfrak{D}^{tr}, f(\\cdot;\\theta_j), M_b(\\mathfrak{D}^{tr}, B)$} \\State Update candidate subset $\\Theta^b$ by applying the filtering analysis \\Endfor \\Endfor \\State Determine $\\hat{\\theta}_{opt}$ that optimizes $\\hat{Q}_j^k$. \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nAlgorithm 11.2: Using racing for grid search tuning parameter optimization.\n\n\n\nThe details are in the analysis used to discard tuning parameter combinations. For simplicity, we’ll focus on a basic method that uses a form of analysis of variance (ANOVA), a standard statistical tool for determining whether there are differences between conditions (Kutner, Nachtsheim, and Neter 2004). In our context, the conditions are the tuning parameter candidates; we ignore their values and treat them as qualitative samples from the distribution of all possible tuning parameter combinations. The ANOVA model uses the candidate conditions as the predictor and the metric of choice as the numeric outcome.\nThere are two statistical considerations that we need to understand when using an ANOVA model for evaluating Racing results.\nFirst, the ANOVA model is used for statistical inference. Specifically, we’ll use it to compute confidence intervals on differences in performance. This means that the probabilistic assumptions about our data matter. The ANOVA method requires that the errors follow a Gaussian distribution. That distribution can be strongly influenced by the distribution of the outcome, and, in our case, this is a performance metric. Previously, we used the Brier score, which has non-negative values and might be prone to follow a right-skewed statistical distribution. However, each resampled Brier score is the average of differences, and the Central Limit Theorem suggests that as the number of data points used to compute the score increases, the sampling distribution will become more Gaussian. If we use this approach to assess parameter candidates, then we should check the normality assumptions of the errors.\nThe second statistical complication is related to the resamples. Basic ANOVA methods require the data to be independent of one another, which is definitely not the case for resampling results. A “within-resample” correlation occurs since some resamples are “easier” to predict than others. This means that the metrics associated with each resample are more similar to one another than to the metrics from other resamples.\nThis extra correlation means that a simple ANOVA model cannot be used. Instead, our interim analysis should instead use a hierarchical random effects model. This is the same methodology used in Section 6.4.3 for effect encodings. We’ll treat our set of resamples as a random sample of possible resampling indices. The ANOVA model itself is:\n\\[\nQ_{ij} =(\\beta_0 + \\beta_{0i}) + \\beta_1x_{i1} + \\ldots +  \\beta_{s-1}x_{i(s-1)}+ \\epsilon_{ij}\n\\tag{11.1}\\]\nfor \\(i=1,\\ldots, B\\) resamples and \\(j=1, \\ldots, s\\) candidates. This random intercept model assumes that the ranking of candidates is the same across resamples and only the magnitude of the pattern changes from resample to resample.\nThis model’s form is the reference cell parameterization discussed in Section 6.2. For each interim analysis, the reference cell will be set to the current best candidate. This means that the \\(\\hat{\\beta}_j\\) parameter estimates represent the loss of performance relative to the current best. A one-sided confidence interval is constructed to determine if a candidate should be removed. We can stop considering candidates whose interval does not contain zero.\nTo demonstrate the racing procedure, we re-evaluated our larger a space-filling design. The order of the folds was randomized, and after 3 resamples, the ANOVA method was used to analyze the results. Figure 11.5 shows the one-sided 95% confidence intervals for the loss of Brier score relative to the current best configuration (2030 trees and a learning rate of 10-2.78). Sixty-Six were eliminated at this round.\n\n\n\n\n\n\n\n\nFigure 11.5: The first interim analysis in racing using a larger space-filling design for the boosted tree model.\n\n\n\n\n\nIn the end, the racing process eliminated all but 7 candidates. The process eliminated candidates at iterations three (66 configurations), four (14 configurations), five (7 configurations), six (1 configuration), seven (2 configurations), eight (2 configurations), and nine (1 configuration). In all, 404 models were fit out of the possible 1,000 (40.4%). The numerically best candidate set for racing and basic grid search were the same: 576 trees and a learning rate of 10-2.3.\nThe model in Equation 11.1 allows us to estimate the within-resample correlation coefficient. This estimates how similar the performance metric values are to one another relative to values between resamples. In our example, the estimate of within resample correlation is 0.6, and is a good example of why we should use Equation 11.1 when culling parameter candidates.\nIn summary, racing can be an effective tool for screening a large number of models while using comprehensive grids.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-nested-resampling",
    "href": "chapters/grid-search.html#sec-nested-resampling",
    "title": "11  Grid Search",
    "section": "11.4 Optimization Bias and Nested Resampling",
    "text": "11.4 Optimization Bias and Nested Resampling\nIn the last section, we found that the numerically best candidate was 576 trees and a learning rate of 10-2.3. The corresponding resampling estimate of the Brier score was 0.0783 with a standard error of 0.0053. If someone were to ask us how well the optimal boosted tree performs, we would probably give them these performance estimates.\nHowever, there is an issue in doing so that may affect our decision for selecting the optimal tuning parameters. We are using grid search and resampling to find the best combination of parameters and to estimate the corresponding performance. The problem is that we don’t know how accurate or precise the estimate of the optimal candidate may be. In some situations, this dual use of the model tuning process can introduce optimization bias where, to some degree, our performance statistics are optimistic.\nThis issue has been studied extensively in the high-dimensional biology literature, where there could be dozens of samples but thousands of predictors. In this case, feature selection is of paramount importance. However, the process of selecting important predictors in this situation can be very difficult and often leads to unstable models or preprocessing methods that overfit the predictor set to the data. Discussions of these issues can be found in Varma and Simon (2006), Boulesteix and Strobl (2009), Bischl et al. (2012), and Bernau, Augustin, and Boulesteix (2013).\nIn this section, we’ll introduce a few methods to quantify and correct for optimization bias. These approaches are not specific to traditional grid search; they can be used with racing, iterative search, or any algorithm that we use to optimize a model. Some of the concepts can be difficult and, for this reason, we’ll use a simplified grid search scenario. Let’s optimize our boosted tree by fixing the number of boosting iterations to 500 and only optimize the learning rate. But the tools that we are about to discuss are not limited to a single tuning parameter. Simplifying the task in this way allows us to visualize and explain the process5.\nUsing the same 10-fold cross-validation scheme, Figure 11.6 shows the results of a conventional grid search over the learning rate where 100 values of that parameter were evaluated. As usual, the line indicates the average of the Brier scores produced by the 10 assessment sets. The results of this process indicates that the smallest Brier score is achieved with a learning rate of 10-2.23. The Brier score that corresponds to this tuning parameter value is estimated to be 0.0782 with corresponding standard error of 0.00536.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.6: Basic grid search for learning rate with 100 grid points. The dotted vertical line shows the optimal learning rate using the overall resampling estimates. The colored rug values show the learning rates that were optimal within each resample. There were 2 learning rates selected more than once: 10-2.27 and 10-1.91.\n\n\n\nHow can we estimate the potential bias in the process that led to this performance metric? Tibshirani and Tibshirani (2009) describes a simple analytical estimator applicable when multiple resamples are used to estimate performance. First, we obtain the performance estimates corresponding to the optimal tuning parameter candidate within each fold. Let’s call these \\(\\hat{Q}_j^*\\) (where \\(j=1\\ldots B\\)). They are represented in Figure 11.6 as the colored “rug” lines at the bottom of the figure. We know that our conventional analysis of this grid search finds that a learning rate of 10-2.23 to be optimal. We can find the metric values associated with the global optimum for each resample (denoted as \\(\\hat{Q}_j^{opt}\\)). For each resample, we now have matched performance estimates for the local optima as well as the global optimum. The difference between these values is an estimate of the bias in optimization. If different resamples have very different optimal performance metrics compared to the optimal performance determined using the averages of the resamples (as shown in Figure 11.6), bias can increase. The estimate of the bias is then:\n\\[\\widehat{bias} = \\frac{1}{B}\\sum_{j=1}^B\\left[\\hat{Q}_j^{opt} - \\hat{Q}_j^*\\right]\\] For this particular example, the bias is fairly small (0.000931 with standard error 0.000304). To correct the conventional estimator, we add the bias; the Brier score is adjusted from 0.0782 to 0.0791. A more complex analytical method can be found in Tibshirani and Rosset (2019).\nWe can also estimate optimization bias with more complex resampling schemes. Recall that the issue is that we are overextending the conventional resampling scheme by doing too much with the same data (i.e., estimating overall performance assessment and the optimal performance value). Nested resampling (Varma and Simon 2006; Boulesteix and Strobl 2009) prevents this overuse by using two layers of resampling: the “inner resamples” are used to estimate the optimal candidate values, and the “outer resamples” estimate performance at those values.\nFor example, our current resampling scheme is 10-fold cross-validation. Since the training set has 2,000 samples, each fold uses 1,800 to fit the model and a separate 200 for assessment. Nested resampling would create 10 more independent resampling schemes within each of the 1,800-point analysis sets. If we once again used 10-fold cross-validation for the inner resamples, each would contain 10 analysis sets of size 1,620 with corresponding assessment sets of 180 data points.\nThe process starts with the inner resamples. The same model tuning procedure is used (basic grid search in our example), and each of the inner resamples estimates its own optimal candidate \\(\\hat{\\theta}_k\\). The outer resample takes this candidate value and estimates its performance using the assessment sets from the outer resampling loop. These outer estimates, whose data were never used to tune the model, are averaged to produce the nested resampling estimate of performance.\nConsider Figure 11.7. The colored lines in the left-hand panel show the results of the 10 inner resamples. Each line is made up of the averages of the inner 10 assessment sets of size 180. The filled circles along these lines indicate the optimal learning rate for that inner resample. Each outer resample takes its corresponding learning rate, trains that model using its analysis set of 1,800 points, and computes performance using its 200 assessment samples. These 10 statistics are averaged to get the final performance estimate, which should be free of any optimization bias.\n\n\n\n\n\n\n\n\nFigure 11.7: Each line corresponds to a complete resampled grid search that uses the inner resamples. The points indicate the estimates of the optimal learning rate as determined by the inner results. The panel on the right contrasts the Brier score for the inner assessment set with the outer assessment sets.\n\n\n\n\n\nWe can see that the resample-specific optimal learning rates vary but are within a consistent region. There are a few resamples that found the same optimal value. There is some variation in the y-axis too; different assessment sets produce different values. The standard error of these inner statistics, 0.000537, is much smaller than the value of the conventional estimate (0.00536).\nThe panel on the left shows boxplots of the Brier scores for the inner resamples and the corresponding outer resample estimates. The standard error of the outer resamples is very similar to the the level of noise in the conventional estimate.\nIn the end, the nested resampling estimate of the Brier score was estimated as 0.0784; a value very close to the single 10-fold cross-validation result shown in Figure 11.6.\n\nIt is important to emphasize that nested resampling is for verification, not optimization. It provides a better estimate of our model optimization process; it does not replace it.\n\nWe might best communicate the results of nested resampling like this:\n\nWe tuned the learning rate of our boosted classification tree using grid search and found that a rate of 10-2.23 to be best. We think that this model has a corresponding Brier score, measured without optimization bias, of 0.0784.\n\nWe can look at the candidates produced by the inner resamples to understand the stability of the optimization process and potentially diagnose other issues. We would not choose “the best” inner resampling result and move forward with its candidate value6.\nIt is a good idea to use nested resampling when the training set is small, the predictor set is very large, or both. The lack of significant bias in the analysis above does not discount the problem of optimization bias. It exists but can sometimes be within the experimental noise.\nIt is a good idea to choose an inner resampling scheme that is as close as possible to the single resampling scheme that you used for optimization. The outer scheme can vary depending on your needs. Care should be used when the outer resampling method is the bootstrap. Since it replicates training set points in its analysis sets, the inner resamples need to use the unique rows of the original training set. Otherwise the same data point might end up in both the inner analysis and assessment sets.\nThe primary downside to nested resampling is the computational costs. Two layers of resampling have a quadratic cost, and only the inner resamples can be executed in parallel. Using 10 workers, the analysis in Figure 11.7 took 6.6-fold longer to compute than the basic grid search that produced Figure 11.6.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#sec-parameter-ranges",
    "href": "chapters/grid-search.html#sec-parameter-ranges",
    "title": "11  Grid Search",
    "section": "11.5 Setting Parameter Ranges",
    "text": "11.5 Setting Parameter Ranges\nSpecifying the range of parameters used to define the grid may involve some guesswork. For some parameter types, there is a well understood and defined range. For others, such as learning rate, there is a lower bound (zero) and a loosely defined upper bound mostly based on convention and prior experience. For grids, we want to avoid configurations with few unique values and a wide range. This might not sample the section of the parameter space that includes nonlinearity and the region of optimal results. Figure 9.6 shows an example of this for the learning rate. The initial grid included three points that missed the regions of interest. Figure 9.5 did a better job with more grid points. In high dimensional parameter space, the likelihood of a poor grid increases, especially for relatively “small” grid sizes.\nWe often envision the relationship between a predictor and model performance as being a sharp peak. If we cannot find the pinnacle, the model optimization would fail. Luckily, as seen in Figure 11.7, there are often substantial regions of parameter space with good performance. Model optimization may not be superficially easy but it is often not impossible. We only need to sample the optimal region once to be successful.\nIf we worry that our tuning grid did not produce good results, another strategy is to increase the parameter ranges and use an iterative approach that can naturally explore the parameter space and let the current set of results guide the exploration about the space. These methods are discussed in the next chapter.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#chapter-references",
    "href": "chapters/grid-search.html#chapter-references",
    "title": "11  Grid Search",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAudze, P, and V Eglais. 1977. “New Approach to Planning Out of Experiments.” Problems of Dynamics and Strengths 35: 104–7.\n\n\nBa, S, R Myers, and W Brenneman. 2015. “Optimal Sliced Latin Hypercube Designs.” Technometrics 57 (4): 479–87.\n\n\nBates, S, J Sienz, and V Toropov. 2004. “Formulation of the Optimal Latin Hypercube Design of Experiments Using a Permutation Genetic Algorithm.” In 45th AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics & Materials Conference, 2011.\n\n\nBergman, E, L Purucker, and F Hutter. 2024. “Don’t Waste Your Time: Early Stopping Cross-Validation.” arXiv.\n\n\nBernau, C, T Augustin, and AJ Boulesteix. 2013. “Correcting the Optimal Resampling-Based Error Rate by Estimating the Error Rate of Wrapper Algorithms.” Biometrics 69 (3): 693–702.\n\n\nBischl, B, O Mersmann, H Trautmann, and C Weihs. 2012. “Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation.” Evolutionary Computation 20 (2): 249–75.\n\n\nBoulesteix, AL, and C Strobl. 2009. “Optimal Classifier Selection and Negative Bias in Error Rate Estimation: An Empirical Study on High-Dimensional Prediction.” BMC Medical Research Methodology 9: 1–14.\n\n\nBox, GEP, W Hunter, and J. Hunter. 1978. Statistics for Experimenters. New York: Wiley.\n\n\nFang, KT, D Lin, P Winker, and Y Zhang. 2000. “Uniform Design: Theory and Application.” Technometrics 42 (3): 237–48.\n\n\nGramacy, R. 2020. Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.\n\n\nHusslage, B, G Rennen, E Van Dam, and D Den Hertog. 2011. “Space-Filling Latin Hypercube Designs for Computer Experiments.” Optimization and Engineering 12: 611–30.\n\n\nJoseph, V. 2016. “Space-Filling Designs for Computer Experiments: A Review.” Quality Engineering 28 (1): 28–35.\n\n\nJoseph, V, E Gul, and S Ba. 2015. “Maximum Projection Designs for Computer Experiments.” Biometrika 102 (2): 371–80.\n\n\nKrueger, T, D Panknin, and ML Braun. 2015. “Fast Cross-Validation via Sequential Testing.” Journal of Machine Learning Research 16 (1): 1103–55.\n\n\nKuhn, M. 2014. “Futility Analysis in the Cross-Validation of Machine Learning Models.” arXiv.\n\n\nKuhn, M, and J Silge. 2022. Tidy Modeling with R. O’Reilly Media, Inc.\n\n\nKutner, M, C Nachtsheim, and J Neter. 2004. Applied Linear Regression Models. McGraw-Hill/Irwin.\n\n\nMaron, O, and AW Moore. 1997. “The Racing Algorithm: Model Selection for Lazy Learners.” Artificial Intelligence Review 11: 193–225.\n\n\nMckay, M, R Beckman, and W Conover. 2000. “A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code.” Technometrics 42 (1): 55–61.\n\n\nPronzato, L. 2017. “Minimax and Maximin Space-Filling Designs: Some Properties and Methods for Construction.” Journal de La Société Française de Statistique 158 (1): 7–36.\n\n\nQian, P. 2012. “Sliced Latin Hypercube Designs.” Journal of the American Statistical Association 107 (497): 393–99.\n\n\nSantner, T, B Williams, and W Notz. 2018. The Design and Analysis of Computer Experiments. Springer.\n\n\nShewry, M, and H Wynn. 1987. “Maximum Entropy Sampling.” Journal of Applied Statistics 14 (2): 165–70.\n\n\nTibshirani, RJ, and S Rosset. 2019. “Excess Optimism: How Biased Is the Apparent Error of an Estimator Tuned by SURE?” Journal of the American Statistical Association 114 (526): 697–712.\n\n\nTibshirani, RJ, and R Tibshirani. 2009. “A Bias Correction for the Minimum Error Rate in Cross-Validation.” The Annals of Applied Statistics 3 (2): 822–29.\n\n\nVarma, S, and R Simon. 2006. “Bias in Error Estimation When Using Cross-Validation for Model Selection.” BMC Bioinformatics 7: 1–8.\n\n\nViana, F. 2016. “A Tutorial on Latin Hypercube Design of Experiments.” Quality and Reliability Engineering International 32 (5): 1975–85.\n\n\nWang, Y, F Sun, and H Xu. 2022. “On Design Orthogonality, Maximin Distance, and Projection Uniformity for Computer Experiments.” Journal of the American Statistical Association 117 (537): 375–85.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/grid-search.html#footnotes",
    "href": "chapters/grid-search.html#footnotes",
    "title": "11  Grid Search",
    "section": "",
    "text": "This process was illustrated in Algorithm 10.1.↩︎\nOther applications may want the points to have properties such as orthogonality, symmetry, etc.↩︎\nUsing Euclidean distance.↩︎\nNote that racing requires multiple assessment sets. A single validation set could not be used with racing.↩︎\nUnfortunately, this is an example that is not prone to optimization bias. We’ll see examples later on this website where we can more effectively use these tools.↩︎\nTo reiterate, the nested resampling process is used to characterize our optimization process. If it becomes our optimization process, we would need to nest it inside another nested resample.↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grid Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html",
    "href": "chapters/iterative-search.html",
    "title": "12  Iterative Search",
    "section": "",
    "text": "12.1 Example: Predicting Barley Amounts using Support Vector Machines\nGrid search is a static procedure; we predetermine which candidates will be evaluated before beginning. How can we adaptively optimize tuning parameters in a sequential manner? Perhaps more importantly, when is this a good approach?\nPreviously, we’ve seen that a plateau of good performance in the parameter space is possible. This is often the case but will not always be true. If the region of optimal performance is small, we must create a large space-filling design to find it. When there are many tuning parameters, the computational expense can be unreasonable. An even worse situation is one where the previously described speed-up techniques (such as the submodel trick from Section 11.3.1) are not applicable. Racing can be very efficient, but if the training set is very large, it may be that using a validation set is more appropriate than multiple resamples; in this case, racing cannot be used. Finally, there are practical considerations when the data set is very large. Most parallel processing techniques require the data to be in memory for each parallel worker, restricting their utility.\nGenerally, we are not often constrained by these issues for models used for tabular data. However, there are a few models that might be better optimized sequentially. The first two that come to mind are large neural networks1. and support vector machines2 (SVMs) (Schölkopf and Smola 2001; Shawe-Taylor and Cristianini 2004). For the former, as the number of layers increases, so does the number of tuning parameters. Neural networks also have many important tuning parameters associated with the model’s training, such as the learning rate, regularization parameters, etc. These models also require more preprocessing than others; they are suspectable to the effects of uninformative predictors, missing data, and collinearity. Depending on the data set, there can be many pre-model tuning parameters.\nSupport vector machines are models with fewer tuning parameters than neural networks but with similar preprocessing requirements. Unfortunately, the tuning parameter space can have large areas of poor performance with “islands” where the model works well. The location of these will change from data set to data set. We’ll see an example of this shortly where two model parameters are tuned.\nThese two models, in particular, are more likely to benefit from an optimization method that chooses candidates as the process evolves.\nIn theory, any iterative optimization procedures can be used. In general, gradient methods, such as steepest descent or Newton’s method, are the most commonly used technique for nonlinear optimization. These tools are suboptimal when it comes to parameter tuning but play important parts in other areas of machine learning and, for this reason, they will be discussed later.\nDerivative-free techniques can be helpful for model tuning. Traditional examples are genetic algorithms, simulated annealing, particle swarm optimization, etc. We’ll consider the first two of these in Sections 12.3 and 12.4. Currently, the most well-known iterative tool for tuning models is Bayesian optimization. This will be examined in Section 12.6. However, before this, Section 12.5 will flesh out what it means for a model to be Bayesian.\nTo get started, let’s revisit a data set.\nWe’ll return to the data previously seen in Section 7.1 where we want to predict the percentage of barley oil in a mixture. Recall that the predictors are extremely correlated with one another. In Chapter 7, we considered embedding methods like PCA to preprocess the data and these models were able to achieve RMSE values of about 6% (shown in Figure 7.6).\nIn this chapter, we’ll model these data in two different scenarios. First, we’ll use them as a “toy problem” where only two tuning parameters are optimized for a support vector machine model. This is a little unrealistic, but it allows us to visualize how iterative search methods work in a 2D space. Second, in Section 12.7, we’ll get serious and optimize a larger group of parameters for neural networks simultaneously with additional parameters for a specialized preprocessing technique.\nLet’s start with the toy example that uses an SVM regression model. This highly flexible nonlinear model can represent the predictor data in higher dimensions using a kernel transformation (Hofmann, Schölkopf, and Smola 2008). This type of function combines numeric predictor vectors from two data points using a dot product. There are many different types of kernel functions, but for a polynomial kernel, it is:\n\\[\nk(\\boldsymbol{x}_1, \\boldsymbol{x}_2) = (a\\boldsymbol{x}_1'\\boldsymbol{x}_2 + b)^q\n\\tag{12.1}\\]\nwhere \\(a\\) is called the scaling factor, \\(b\\) is a constant offset value, and \\(q\\) is the polynomial degree. The dot product of predictor vectors (\\(\\boldsymbol{x}_i\\)) measures both angle and distance between points. Note that for the kernel function to work in an appropriate way, the two vectors must have elements with consistent units (i.e., they have been standardized).\nThe kernel function operates much like a polynomial basis expansion; it projects a data point into a much larger, nonlinear space. The idea is that a more complex representation of the predictors might enable the model to make better predictions.\nFor our toy example, we’ll take the 550 predictors, project them to a reduced space of 10 principal components, and standardize those features to have the same mean and standard deviation. A quartic polynomial with zero offset is applied, and the scale parameter, \\(a\\), will be tuned. This parameter helps define how much influence the dot product has in the polynomial expansion.\nThe most commonly adjusted parameter in SVMs is the cost value, which is independent of the chosen kernel function. This parameter determines how strongly the model is penalized for incorrect predictions on the training set. A higher cost value pushes the SVM to create a more complex model. As shown earlier in Figure 9.1, small cost values result in the SVM making less effort to classify samples correctly, leading to underfitting. In contrast, extremely high cost values cause the model to overfit to the training set.\nJust as in Chapter 7, the RMSE will be computed from a validation set and these statistics will be used to guide our efforts.\nLet’s define a wide space for our two tuning parameters: cost will vary from 2-10 to 210 and the scale factor3 is allowed to range from 10-10 to 10-0.1.\nFigure 12.1 visualizes the RMSE across these ranges4 where darker colors indicate smaller RMSE values. The lower left diagonal area is a virtual “dead zone” with very large RMSE results that don’t appear to change much. There is also a diagonal wedge of good performance (symbolized by the darker colors). The figure shows the location of the smallest RMSE value and a diagonal ridge of parameter combinations with nearly equal results. Any points on this line would produce good models (at least for this toy example). Note that there is a small locale of excessively large RMSE values in the upper right. Therefore, increasing both tuning parameters to their upper limits is a bad idea.\nFigure 12.1: A visualization of model performance when only the SVM cost and scaling factor parameters are optimized. It shows the combination with the smallest RMSE and a ridge of candidate values with nearly equivalent performance.\nIn the next section, we explore gradient descent optimization and explain why it may not be suitable for model tuning. Following that, we delve into two traditional global search methods—simulated annealing and genetic algorithms—and their application in parameter space exploration. We then shift our focus to Bayesian optimization, concluding with an in-depth analysis of the barley prediction problem.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-gradient-opt",
    "href": "chapters/iterative-search.html#sec-gradient-opt",
    "title": "12  Iterative Search",
    "section": "12.2 Sidebar: Gradient-Based Optimization",
    "text": "12.2 Sidebar: Gradient-Based Optimization\nTo formalize the concept of optimization, we need an objective function that defines what we are trying to optimize. This function, often called a loss function (specifically to be minimized), is denoted by \\(\\psi()\\). The parameters that modify \\(\\psi()\\) are represented by \\(\\boldsymbol{\\theta}\\), a \\(p \\times 1\\) vector of real numbers. For simplicity, we will assume that \\(\\psi()\\) is smooth and generally differentiable. Additionally, without loss of generality, we will assume that smaller values of \\(\\psi()\\) are better.\nWe’ll denote the first derivative (\\(\\psi'(\\boldsymbol{\\theta})\\)), for simplicity, as \\(g(\\boldsymbol{\\theta})\\) (\\(p\\times 1\\)). The matrix of second deriviatives, called the Hessian matrix, is symbolized as \\(H(\\boldsymbol{\\theta})\\) (\\(p\\times  p\\)).\nWe start with an initial guess, \\(\\boldsymbol{\\theta}_0\\), and compute the gradient at this point, yielding a \\(p\\)-dimensional directional vector. To get to our next parameter value, simple gradient descent uses the update:\n\\[\n\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_i - \\alpha\\:g(\\boldsymbol{\\theta}_i)\n\\tag{12.2}\\]\nThe value \\(\\alpha\\) defines how far to move in the chosen direction. It can either be a fixed constant5 or adjusted using a secondary method called a line search. In a line search, \\(\\alpha\\) is incrementally increased until the objective function worsens.\nWe proceed to iterate this process until some measure of convergence is achieved. For example, the optimization could be halted if the objective function does not improve more than a very small value. Lu (2022) and Zhang (2019) are excellent introductions to gradient-based optimization that is focused on training models.\nAs a simple demonstration, Figure 12.2 shows \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\) for values of \\(\\theta\\) between \\(\\pm 10.0\\). We want to minimize this function. There is a global minimum at about \\(\\theta \\approx 6.85\\) while there are local minima at \\(\\theta = -10.0\\) and \\(\\theta \\approx -1.72\\). These are false solutions where some search procedures might become trapped.\n\n\n\n\n\n\n\n\n#| label: shiny-grad-descent\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-gd.R\")\n\napp\n\n\n\nFigure 12.2: An example of simple gradient descent for the function \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\).\n\n\n\n\n\n\n\nThe figure enables the choice of \\(\\theta_0\\), the value of \\(\\alpha\\), and how many iterations to used. Consider a few configurations:\n\nStarting at \\(\\theta = 3\\), a learning rate of \\(\\alpha = 2.0\\) is inappropriate. The search initially moves towards the global minimum but then reverses course and jumps past the best value, then becomes trapped around one of the local optima.\nIf we keep \\(\\theta = 3\\) and decrease learning rate to \\(\\alpha = 1.0\\), we quickly find the best result.\nHowever, if we tried decreasing the learning rate too low, say \\(\\alpha = 0.01\\), the optimization moves too slowly.\n\nThe learning rate plays a crucial role in the optimization process. Additionally, the starting value is significant; values below 2.0 consistently fail to reach the optimum.\nThis gradient descent algorithm outlined above is extremely basic. There are far more complex versions, the most well-known of which is the Newton-Raphson method (a.k.a. Newton’s Method) that incorporates the second derivative matrix \\(H(\\boldsymbol{\\theta})\\) in the updating formula6.\nWe often know when gradient-based optimization will work well. If we know the equation for the objective function, we can determine its properties, such as whether it is a convex function, and theory can tell us if a global optimum can be found via the use of gradients. For example, squared error loss \\(\\psi(\\boldsymbol{\\theta}) = (y-\\hat{y})^2\\), is convex when the relationship for the predicted value \\(\\hat{y}\\) is a well-behaved function of \\(\\boldsymbol{\\theta}\\) (such as in linear regression).\nHowever, there are additional considerations when it comes to optimizations for predictive models. First, since data are not deterministic, the loss function is not only a random variable but can be excessively noisy. This noise can have a detrimental effect on how well the optimization proceeds.\nSecond, our objective function is often a performance metric, such as RMSE. However, not all metrics are mathematically well-behaved—they may lack smoothness or convexity. In the case of deep neural networks, even when the objective function is simple (e.g., squared error loss), the model equations can create a non-convex optimization landscape. As a result, the optimized parameters may correspond to a local optimum rather than a global one.\nThis issue arises because traditional gradient-based methods are inherently greedy. They optimize by moving in the direction that appears most favorable based on the current estimates of \\(\\boldsymbol{\\theta}\\). While this approach is generally effective, it can lead to the optimization process becoming trapped in a local optimum, particularly with complex or non-standard objective functions.\nThere are additional complications when the tuning parameters, \\(\\boldsymbol{\\theta}\\), are not real numbers. For instance, the number of spline terms is an integer, but the update equation might produce a fractional value for this number. Other parameters are qualitative, such as the choice of activation function in a neural network, which cannot be represented as continuous numerical values.\nDuring model tuning, we know to avoid repredicting the training set (due to overfitting). Procedures such as resampling make the evaluation of \\(\\psi()\\) computationally expensive since multiple models should be trained. Unless we use symbolic gradient equations, the numerical process of approximating the gradient vector \\(g(\\boldsymbol{\\theta})\\) can require a large number of function evaluations.\nLater, we’ll discuss stochastic gradient descent (SGD) (Prince 2023, chap. 6). This method is commonly used to train complex networks with large amounts of training data. SGD approximates the gradient by using only a small subset of the data at a time. Although this introduces some variation in the direction of descent, it can be beneficial as it helps the algorithm escape from local minima. Additionally, when dealing with large datasets, computer memory may not be able to store all the data at once. In such cases, SGD is often the only viable option because it doesn’t require the entire dataset to be loaded into memory and can also be much faster.\nUntil then, the next three chapters will describe different stochastic optimization, gradient-free methods that are well-suited for parameter tuning since they can sidestep some of the issues described above.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-sim-anneal",
    "href": "chapters/iterative-search.html#sec-sim-anneal",
    "title": "12  Iterative Search",
    "section": "12.3 Simulated Annealing",
    "text": "12.3 Simulated Annealing\nNon-greedy search methods are not constrained to always proceed in the absolute best direction (as defined by the gradient). One such method is simulated annealing (SA)(Kirkpatrick, Gelatt, and Vecchi 1983; Spall 2005). It is a controlled random search that moves in random directions but with some amount of control over the path. It can also incorporate restarts if the algorithm moves into clearly poor regions.\nGiven an initial solution, simulated annealing (SA) creates a random perturbation of the current candidate solution, typically within a small local neighborhood. The objective function is then evaluated for the new candidate and compared to the previous solution. If the new candidate results in an improvement, the process moves forward by using it to make the next step. If the new candidate is worse, there are two options:\n\nWe can accept the current solution as “suboptimal” and use it as the basis for the next perturbation, or\nwe can discard the current solution and treat it as if it never occurred. The next candidate point will then be a perturbation of the last “acceptable” solution.\n\nFor our SVM example, suppose we start with a candidate where\n\\[x_0 = \\left[log_2 (cost), log_{10} (scale)\\right] = [-10, -0.1]\\]\nand had an associated RMSE value of 6.10%. For the next candidate, a perturbation of this values is created, say \\(x_1 = [-7.11, -0.225]\\). Suppose that the corresponding RMSE was measured at 5.97%. Since \\(x_1\\) has a better performance metric it is automatically accepted and is used to create the next parameter.\nHowever, suppose that the RMSE value for \\(x_1\\) was 7.00%, meaning that the new candidate did worse than the initial one. In this case, simulated annealing generates a probability threshold for accepting the worse solution. Typically, the probability depends on two quantities:\nSimulated annealing generates a probability threshold for accepting the worse solution. Typically, the probability depends on two quantities:\n\nThe difference between the current and previous objective function values. If the new candidate is nearly as good as the current solution, the probability of acceptance will be higher compared to a candidate that is significantly worse.\nThe probability of acceptance should decrease over time as the search progresses. This is often achieved using an exponentially decaying function, known as the “cooling schedule.”\n\nAn often used equation for the probability, assuming that smaller values are better, is\n\\[\nPr[accept] = \\exp\\Bigl[-i\\bigl(\\hat{Q}(\\boldsymbol{\\theta}_{i}) - \\hat{Q}(\\boldsymbol{\\theta}_{i-1})\\bigr)\\Bigr]\n\\]\nwhere \\(i\\) is the iteration number. To compare 6.1% versus 7.0%, the acceptance probability is 0.407. To make the determination, a random uniform number \\(\\mathcal{U}\\) is generated and, if \\(\\mathcal{U} \\le Pr[accept]\\), we accept \\(\\boldsymbol{\\theta}_{i}\\) and use it to make \\(\\boldsymbol{\\theta}_{i+1}\\). Note that the probability “cools” over time; if this difference were to occur at a later iteration, say \\(i = 5\\), the probability would drop to 0.0111.\nOne small matter is related to the scale of the objective function. The difference in the exponent is very sensitive to scale. If, for example, instead of percentages we were to use the proportions 0.61 and 0.70, the probability of acceptance would change from 40.7% to 91.4%. One way to mitigate this issue is to use a normalized difference by dividing the raw difference by the previous objective function (i.e., (0.70-0.61) / 0.70). This is the approach used in the SA analyses here.\nThis process continues until either a pre-defined number of iterations is reached or there is no improvement after a certain number of iterations. The best result found during the optimization process is used as the final value, as there is no formal concept of “convergence” for this method. Additionally, as mentioned earlier, a restart rule can prevent simulated annealing from getting stuck in suboptimal regions if no better results have been found within a certain timeframe. When the process is restarted, it can either continue from the best candidate found in previous iterations or start from a random point in the parameter space.\nHow should the candidates be perturbed from iteration to iteration? When the tuning parameters are all numeric, we can create a random distance and angle from the current values. A similar process can be used for integers by “flooring” them to the nearest whole number. For qualitative tuning parameters, a random subset of parameters is chosen to change to a different value chosen at random. The amount of change should be large enough to search the parameters space and potentially get out of a local optimum.\nIt is important to perform computations on the parameters in their transformed space to ensure that the full range of possible values is treated equally. For example, when working with the SVM cost parameter, we use its log (base 2) scale. When perturbing the parameter values, we make sure to adjust them in the log space rather than the original scale. This approach applies to all other search methods discussed in this chapter.\nTo illustrate, a very small initial space-filling design with three candidates was generated and evaluated. The results are in Table 12.1. To start the SA search7, we will start with the candidate with the smallest RMSE and proceed for 50 iterations without a rule for early stopping. A restart to the last known best results was enforced after eight suboptimal iterations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRMSE (%)\nCost (log-2)\nScale (log-10)\n\n\n\n\n6.10\n-10\n-0.1\n\n\n7.33\n10\n-5.0\n\n\n17.04\n0\n-10.0\n\n\n\n\n\n\n\n\nTable 12.1: The initial set of candidates used for iterative search with the toy example from Figure 12.1. One candidate does poorly while the other two have relatively similar results.\n\n\n\n\n\n\n\nFigure 12.3 contains an animation of the results of the SA search. In the figure, the initial points are represented by open circles, and a grey diagonal line shows the ridge of values that corresponds to the best RMSE results.\n\n\n\n\n\n\n\n\n#| label: shiny-sa-example\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A two-dimensional parameter space with axes for the scale parameter and the SVM cost is shown. Three initial points are shown. The SA algorithm progresses from a point with low cost and a large value of the scale factor to meander to the ridge of optimal performance, starting several times along the way. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-sa.R\")\n\napp\n\n\n\nFigure 12.3: An example of how simulated annealing can investigate the tuning parameter space. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n\n\n\n\n\n\nDuring the search, there were 4 iterations where a new global best result was discovered (iterations 1, 14, 15, and 40). There were also 5 restarts at iterations 9, 23, 31, 39, and 48. In the end, the best results occurred with a cost value of 2-2.5 and a scale factor of 10-0.98. The corresponding validation set RMSE was 5.78%. With this random seed, the search gets near the ridge of best performance shown in Figure 12.1 but only lingers there for short times. It does spend a fair amount of time meandering in regions of poor performance.\nSimulated annealing has several attractive qualities. First, the process of generating new candidates works with any type of parameter, whether real, integer, or qualitative. This is not true for the other two iterative methods we’ll discuss. Additionally, the perturbation process is very fast, meaning there is minimal computational overhead to compute the next objective function value. Since we are effectively generating new candidate sets, we can also apply constraints to individual parameters or groups of parameters. For example, if a tuning parameter is restricted to odd integers, this would not pose a significant problem for simulated annealing.\nThere are a few downsides to this method. Compared to the other search methods, SA makes small incremental changes. If we start far away from the optimum, many iterations might be required to reach it. One way to mitigate this issue is to do a small space-filling design and start from the best point (as we did). In fact, applying SA search after a grid search (perhaps using racing) can be a good way to verify that the grid search was effective.\nAnother disadvantage is that a single candidate is processed at a time. If the training set size is not excessive, we could parallel process the multiple candidates simultaneously. We could make a batch of perturbations and pick the best value to keep or apply the probabilistic process of accepting a poor value.\nThis optimization took 31.5s per candidate to execute for these data.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-genetic-algo",
    "href": "chapters/iterative-search.html#sec-genetic-algo",
    "title": "12  Iterative Search",
    "section": "12.4 Genetic Algorithms",
    "text": "12.4 Genetic Algorithms\nGenetic algorithms (GAs) (Mitchell 1996; Eiben and Smith 2015) are an optimization method that mimics the process of evolution through natural selection. While GAs are not ideally suited for parameter tuning in our case—since a standard GA search typically requires hundreds to millions of objective function evaluations—we will limit the search to a smaller scale for practical reasons. Nevertheless, genetic algorithms can often find solutions near the optimal value fairly quickly. Additionally, as previously mentioned, there is usually a region of acceptable candidates. Finally, GAs are highly unlikely to become trapped in a locally optimal solution.\nInstead of search iterations, genetic algorithms are counted in generations. A generation is a group of candidate values that are evaluated at the same time (as a batch)8. Once their corresponding performance metrics are computed, a small set of the best candidates is selected and is used to create the next generation via reproduction. Reproduction would entail combining a pair of “parent” candidates by swapping information9, and then random mutations can be applied. Once the next generation is created, the process continues for some pre-defined time limit or maximum number of generations.\nHow, exactly, does this work? The first step is to pick a numerical representation for each candidate. Let’s consider methods for our main types of data.\n\nReal-Valued Parameters\nFor real numbers, there are two encoding methods: one option is to keep them as-is (i.e., floating point values) and another converts the values to a binary encoding.\nWhen keeping the values as real numbers, we can make two children from a pair of well-performing candidates via a linear combination. For candidate vectors \\(\\boldsymbol{\\theta}_j\\), we can use:\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}^{\\:kid}_1 &= \\alpha \\boldsymbol{\\theta}^{\\:par}_1 + (1 - \\alpha) \\boldsymbol{\\theta}^{\\:par}_2 \\notag \\\\\n\\boldsymbol{\\theta}^{\\:kid}_2 &= (1-\\alpha) \\boldsymbol{\\theta}^{\\:par}_1 + \\alpha \\boldsymbol{\\theta}^{\\:par}_2 \\notag \\\\\n\\end{align}\n\\]\nwhere \\(\\alpha\\) is a random standard uniform number. Notice that the two children’s candidate values will always be in-between the values of their parents.\nFor mutation, a candidate’s values are either locally perturbed or simulated with rate \\(\\pi_m\\). For example, we might mutate the log cost value by simulating a random number across the range that defines its search space.\nBinary encodings for real numbers were suggested in the early days of genetic algorithms. In this case, each candidate value is encoded as a set of binary integers. For example, consider a log2 cost value of -2.34. To convert this to binary, we multiply it by 100 (assuming a limit of two decimal places) to convert it to an integer. If we use 8 binary digits (a.k.a. “bits”) to represent 234, we get 11101010. If the candidates could have both positive and negative numbers, we can add an extra bit at the start that is 1 when the value is positive, yielding: 011101010.\nThe method of cross-over was often used for the reproduction of binary representations. For a single cross-over, a random location between digits was created for each candidate value, and the binary digits were swapped. For example, a representation with five bits might have parents ABCDE and VWXYZ. If they were crossed over between the second and third elements, the children would be ABXYZ and VWCDE. There are reproduction methods that use multiple cross-over points to create a more granular sharing of information.\nThere are systematic bias that can occur when crossing binary representations as described by Rana (1999) and Soule (2009). For example, there are positional biases. For example, if the first bits (A and V) capture the sign of the value, the sign is more likely to follow the initial bits than the later bits to an offspring.\nFor mutating a binary representation, each child’s bit would be flipped at a rate of \\(\\pi_m\\).\nAfter these reproduction and mutation, the values are decoded into real numbers.\n\n\nInteger Parameters\nFor integers, the same approach can be used as real numbers, but after the usual operations, the decimal values are coerced to integers via rounding. If a binary encoding is used, the same process can be used for integers as real numbers; they are all just bits to the encoding process.\n\n\nQualitative Parameters\nFor qualitative tuning parameters, one (inelegant) approach is to encode them into values on the real line or as integers. For example, for a parameter with values “red,”, “blue,” and “green”, we could map them to bins of [0, 1/3), [1/3, 2/3), and [2/3, 1]. From here, we reproduce and mutate them as described above, then convert them back to their non-numeric categories. This is more palatable when there is a natural ordering of the values but is otherwise a workable but unfortunate approach. Mutation is simple though; at rate \\(\\pi_m\\), a value is flipped to a random selection of the possible values.\n\n\n12.4.1 Assembling Generations\nNow that we know how to generate new candidates, we can form new generations. The first step is to determine the population size. Typically, the minimum population size within a generation is 25 to 50 candidates. However, this may be infeasible depending on the computational cost of the model and the characteristics of the training set, such as its size. While fewer candidates can be used, the risk is that we may fail to sample any acceptable results. In such cases, combining the best results will only produce more mediocre candidates. Increasing the mutation rate can help mitigate this issue. For the first iteration, random sampling of the parameter space is commonly used. It is highly recommended to use a space-filling design for the initial candidate set to ensure a more comprehensive exploration.\nFinally, there is the choice of which candidates to reproduce at each generation. There are myriad techniques for selecting which candidates are used to make the next generation of candidates. The simplest is to pick two parents by randomly selecting them with probabilities that are proportional to their performance (i.e., the best candidates are chosen most often). There is also the idea of elitism in selection. Depending on the size of the generation, we could retain a few of the best-performing candidates from the previous generation. The performance values of these candidates would not have to be recomputed (saving time), and their information is likely to persist in a few good candidates for the next generation.\n\n\n12.4.2 Two Parameter Example\nTo illustrate genetic algorithms in two dimensions, a population size of 8 was used for 7 generations. These values are not optimal defaults but were selected to align with the previous SA search and the optimization method discussed in the next section. The two tuning parameters were kept as floating-point values. Parental selection was based on sampling weights proportional to the RMSE values (with smaller values being better). The mutation rate was set at 10%, and elitism was applied by retaining the best candidate from each generation. All computations within a generation were performed in parallel. On average, the optimization took 17.2s per candidate. Figure 12.4 illustrates the results of the search.\n\n\n\n\n\n\n\n\n#| label: shiny-ga-example\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A two-dimensional parameter space with axes for the scale parameter and the SVM cost is shown. Eight initial points are shown, one near the ridge of best results. As generations increase, new generations are gathered around the ridge. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-ga.R\")\n\napp\n\n\n\nFigure 12.4: Several generations of a genetic algorithm that search tuning parameter space. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n\n\n\n\n\n\nA space-filling design was used to ensure that the initial population was diverse. Fortuitously, one design point was very close to the ridge, with an RMSE of 5.83%. After this, there were 4 generations with better candidates (with almost identical performance): 5.82%, 5.772%, 5.765%, and 5.764%. We can see that, after three generations, the search is concentrated around the ridge of optimal performance. In later generations, some candidates have outliers in one dimension; this is the effect of mutation during reproduction.\n\n\n12.4.3 Summary\nThis small toy example is not ideal for demonstrating genetic algorithms. The example’s low parameter dimensionality and the relatively low computational cost per candidate might give the impression that genetic algorithms are a universal solution to optimization problems. While genetic algorithms are versatile and powerful tools for global optimization, they come with limitations. Like simulated annealing, they have minimal overhead between generations, but in most real-world applications, there are more than just two parameters. This typically means that larger populations and more generations are needed. If the dataset is not excessively large, parallel processing can help manage the increased computational demands.10.\n\nNow we’ll take a look at what makes a Bayesian model Bayesian.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes",
    "href": "chapters/iterative-search.html#sec-bayes",
    "title": "12  Iterative Search",
    "section": "12.5 Sidebar: Bayesian Models",
    "text": "12.5 Sidebar: Bayesian Models\nWe’ve superficially described an application of Bayesian analysis in Section 6.4.3. Before discussing Bayesian optimization, we should give a general description of Bayesian analysis, especially since it will appear again several times after this chapter.\nMany models make probabilistic assumptions about their data or parameters. For example, a linear regression model has the form\n\\[\ny_i = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p + \\epsilon_i\n\\]\nUsing ordinary least squares estimation, we can make assumptions regarding the model errors (\\(\\epsilon_i\\)). We can assume that the residuals are independent of one another and follow a Gaussian distribution with zero mean and a constant standard deviation. From there, it follows that the regression parameters (\\(\\beta\\) coefficients) also follow Gaussian distributions.\nBased on these assumptions, our objective function is the Gaussian likelihood function, generally denoted as \\(\\ell(z|\\theta)\\), although we often maximize the log of this value. We fix the outcome and predictor data and try to find values of \\(\\sigma\\) and the \\(\\beta\\) parameters that maximize the objective function (log \\(\\ell(z|\\theta)\\)). This process is maximum likelihood estimation.\n\nOne important point is that each model parameter is treated as a single value. Our maximum likelihood estimate (MLE) is a point estimate and, based on our assumptions about the residuals, we know the distribution of the MLEs.\n\nThe consequence of this is that we cannot make inferences about the true, unknown model parameters since they are considered to be single points. Instead, our inference focuses on the MLEs. This leads to the circuitous explanation of hypothesis tests and confidence intervals. For example, the explaination of a 90% confidence interval is:\n\n“We believe that if we were to repeat this experiment a large number of times, the true parameter value would fall between \\(L\\) and \\(U\\) 90% of the time.”\n\nA Bayesian approach takes a different perspective on probability assumptions. It assumes that the unknown parameters are drawn from a prior distribution that represents our beliefs or knowledge about them before observing the data. We denote this prior distribution as \\(\\pi(\\theta)\\). The term “prior” is crucial because the modeler should define this distribution before seeing the observed data.\nFor example, consider our model for the time to deliver food in Equation 1.1. A reasonable prior for the regression parameter associated with distance would have a distribution that assigns zero probability to negative values, since we would never expect (log) delivery times to decrease with distance. If we worked at the restaurant, we could be more specific based on our experience. We might believe that, all other factors being equal, each additional mile from the restaurant doubles the delivery time. Using this assumption, we could define a probability distribution that reflects this belief.\nOnce we have our prior distributions for our parameters and assumptions regarding our data distributions, we can write down the equations required to estimate our results. Bayes’ Rule is a basic probability statement that combines the prior distribution with our likelihood function:\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) \\ell(x|\\theta)} {\\pi(x)}\n\\tag{12.3}\\]\n\\(\\pi(\\theta|x)\\) is the posterior distribution: the probability distribution of our parameters, given the observed data. This is the endpoint for any Bayesian analysis.\nAnother important point is that Bayesian estimation has a much more difficult goal than maximum likelihood estimation. The latter needs to find point estimates of its parameters while Bayesian estimation has to estimate the entire posterior distribution \\(\\pi(\\theta|x)\\). In a moment, we’ll look at a simple problem with a simple solution. However, in most other cases, the computational requirements for Bayesian estimation are considerably higher. That’s the bad news.\nThe good news is that, once we find the posterior distribution, it is incredibly useful. First, we can make direct statements about parameter values. Unlike confidence intervals, Bayesian methods allow us to say things like\n\n“We believe that there is a 90% probability that the true value of the parameter is between \\(L\\) and \\(U\\).”\n\nIt also lets us easily make similar statements regarding more complex combinations of parameters (such as ratios, etc).\nFinally, we should mention the effect of the prior on the computations. It does pull the likelihood \\(\\ell(x|\\theta)\\) towards it. For example, suppose that for Equation 1.1 we used a highly restrictive prior for \\(\\beta_1\\) that was a uniform distribution between [0.7, 0.9]. In that case, the posterior would be confined to this range11. That said, the effect of the prior on the posterior decreases as our training set size increases. We saw this in Section 6.4.3 where two travel agents were contrasted; one with many reservations in the data and another with very few.\nTo illustrate, let’s look at a very simple example.\n\n12.5.1 A Single Proportion\nFor the forestry data discussed earlier in Section 10.8, the outcome is categorical, with the values “yes” and “no” representing the question “Is this location forested?” We can use the training dataset to estimate the probability (\\(\\pi\\)) of the event occurring. The simplest estimate is the sample proportion, which is calculated by dividing the number of occurrences of the event (e.g., “yes” responses) by the total number of data points. This gives the estimate \\(\\hat{\\pi}\\) = 56.167%).\nTo estimate this rate using maximum likelihood estimation, we might assume that the data follow a binomial distribution with theoretical probability \\(\\pi\\). From there we can solve equations that find a value of \\(\\pi\\) that correspond to the largest likelihood. It turns out that the sample proportion is also the maximum likelihood estimate.\nInstead of treating the unknown parameter as a single value, Bayesian methods propose that the parameter comes from a distribution of possible values. This distribution reflects our prior understanding or belief about the parameter based on what we know of the situation. For this reason, it is called a prior distribution. If we think that locations in Washington state are very likely to be forested, we would choose a distribution that has more area for higher probabilities.\nFor a binomial model, an example of a prior is the Beta distribution. The Beta distribution is very flexable and has values range between zero and one. It is indexed by two parameters: \\(\\alpha\\) and \\(\\beta\\). Figure 12.5 shows different versions of the Beta distribution for different values.\nTo settle on a specific prior, we would posit different questions such as:\n\n“Are there rate values that we would never believe possible?”\n“What is the most likely value that we would expect and how certain are we of that?”\n“Do we think that the distribution of possible values is symmetric?”\n\nand so on.\nFrom these answers, we would experiment with different values of \\(\\alpha\\) and \\(\\beta\\) until we find a combination that encapsulates what we believe. If we think that larger probabilities of forestation are more likely, we might choose a Beta prior that has values of \\(\\alpha\\) that are larger than \\(\\beta\\), which places more mass on larger values (as seen below). This is an example of an informative prior, albeit a weak one. As our prior distribution is more peaked it reflects that, before seeing any data, we have strong beliefs. If we honestly have no idea, we could choose a uniform distribution between zero and one using \\(\\alpha = \\beta = 1\\).\n\n\n\n\n\n\n\n\nFigure 12.5: Examples of the Beta distribution for different values of \\(\\alpha\\) and \\(\\beta\\).\n\n\n\n\n\nWe’ll use values of \\(\\alpha = 5\\) and \\(\\beta = 3\\). If the training set has \\(n_{tr}\\) points and \\(n_+\\) of them are known to be forested, the ordinary sample proportion estimate is \\(\\hat{p} = n_+ / n_{tr}\\). In a Bayesian analysis, the final estimate is a function of both the prior distribution and our observed data. For a Beta prior, the Bayesian estimate is\n\\[\n\\hat{p}_{BB} = \\frac{n_+ +\\alpha}{n_{tr} + \\alpha + \\beta}\n\\tag{12.4}\\]\nIf, before seeing the data, we had chosen \\(\\alpha = 5\\) and \\(\\beta = 1\\), we estimate that \\(\\hat{p}_{BB} = 56.201\\)%. This is pretty close to our simple estimate because there is so much data in the training set (\\(n_{tr}\\) = 4,832) that the influence of the prior is severely diminished. As a counter-example, suppose we took a very small sample that resulted in \\(n_y\\) = 1 and \\(n_{tr}\\) = 3, the prior would pull the estimate from the MLE of 33.3% to 54.5%.\nNow suppose that for regulatory purposes, we were required to produce an interval estimate for our parameter. From these data and our prior, we could say that the true probability of forestation has a 90% chance of being between 55% and 57.3%.\nWe did mention that it can be very difficult to compute the posterior distribution. We deliberately chose the Beta distribution because, if we assume a binomial distribution for our data and a \\(Beta(\\alpha, \\beta)\\) prior, the posterior is \\(Beta(\\alpha + n_y, n_{tr} - n_y + \\beta)\\). More often than not, we will not have a simple analytical solution for the posterior. That said, we’ll see this happen again in Section 12.6.2.\nFor a Bayesian model, the predictions also have a posterior distribution, reflecting the probability of a wide range of values. As with non-Bayesian models, we often summarize the posterior using the most likely value (perhaps using the mean or mode of the distribution). We can also measure the uncertainty in the prediction using the estimated standard deviation.\n\n\n12.5.2 What is not Bayesian?\nMost other methods fall into the category of “Frequentist.” The terms “Frequentist” and “Bayesian” correspond to different philosophies of interpreting probability. They are not the only two philosophies, but between them, they account for the vast majority of data analytic methods.\nMaximum likelihood estimation is a good example of a Frequentist approach. As we said, when training a model, the optimal parameters are the ones that maximize the probability that we actually did get the data that we observed. If we flip a coin 100 times and 90 of them come up heads, MLE would not choose parameter values that are inconsistent with the results from our 100 observed results.\nCoin flipping does present a good window into how both philosophies operate. Consider the probability of getting heads when tossing a specific coin. The two points of view are:\nFrequentists:\n\nThe probability of being heads is a single unknown value representing the long-term rate at which one would get heads with that coin after infinite tosses. Once we toss the coin, there is no uncertainty. If it is heads, the probability that it is heads is exactly 100%.\n\nBayesians:\n\nThe probability of heads is a random variable with an unknown distribution. We can incorporate our beliefs about that distribution into our analyses, often making them somewhat biased. We update our beliefs with data so that our inferences and predictions are a combination of both. We can make direct probabilistic statements about unknown parameters so if a coin was heads, we can still estimate the probability that it would be heads.\n\nIf you are unsure about the tool that you are using, terms like “maximum likelihood estimation”, “confidence intervals”, and “hypothesis testing” should tell you that it takes a Frequentist view. Terms such as “prior,” “posterior,” or “credible interval” are more indicative of a Bayesian methodology.\nBland and Altman (1998) and Fornacon-Wood et al. (2022) are fairly non-technical discussions of the two camps, while Wagenmakers et al. (2008) discusses more mathematical differences. Discussions on this subject can be vigorous. Gill, Sabin, and Schmid (2005)’s discussion of how medical doctors make decisions is interesting and elicited numerous letters to the British Medical Journal (Chitty 2005; Hutchon 2005; McCrossin 2005). Others have had pragmatic, if not emphatic, thoughts on the subject (Breiman 1997).\n\nTo summarize:\n\nBayesian models require a prior distribution for all of our model parameters.\nThe prior and observed data are combined into a posterior distribution.\nDifferent statistics can be estimated from the posterior, including individual predictions.\n\n\n\nWe’ll encounter even more Bayesian methods in subsequent chapters. For example, in Chapter 14, both Frequentist and Bayesian approaches to inference are discussed. In ?sec-cls-metrics, we’ll describe the process of developing a prior for muticlass problem based on Wordle scores and how priors can affect our results.\nNow that we know more about Bayesian statistics, let’s see how they can be used to tune models.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes-opt",
    "href": "chapters/iterative-search.html#sec-bayes-opt",
    "title": "12  Iterative Search",
    "section": "12.6 Bayesian Optimization",
    "text": "12.6 Bayesian Optimization\nBayesian optimization (BO) is a search procedure that uses an overarching statistical model to predict the next set of candidate values based on past performance. It is currently the most used iterative search routine for optimizing models. The technique originated with Močkus (1975) and was further developed by Jones, Schonlau, and Welch (1998). It is often tied to a particular Bayesian predictive model: the Gaussian process (GP) regression model (Binois and Wycoff 2022).\nBayesian optimization and Gaussian proceses have been vibrant research areas and our discussions will scratch the surface. More recent surveys, Gramacy (2020) and Garnett (2023), are comprehensive resources for these methodologies.\nHowever, before diving into the details of Gaussian proceseses, we should discuss how any Bayesian method can be used for optimization. Similar to the racing model from Equation 11.1, the Bayesian model’s outcome will be our estimated performance statistic (i.e., \\(\\hat{Q}\\)). The predictors for this model will be the tuning parameter values (\\(\\boldsymbol{\\theta}\\)):\n\\[\nQ_j = f(\\boldsymbol{\\theta}_j)+ \\epsilon_{j}\n\\tag{12.5}\\]\nwhere \\(j\\) is the iteration number, \\(f(\\cdot)\\) is a surrogate model, and \\(\\epsilon\\) is an error term. For the barley data, we’ve used RMSE as our primary metric. Using a Bayesian model for \\(f(\\cdot)\\) enables us to predict what we expected the RMSE to be for different candidate points.\nSince a Bayesian model produces a posterior (predictive) distribution, we can obtain the distribution of the performance metric for a potential new candidate. We often summarize this predictive distribution with the mean and standard deviation. Figure 12.6 shows two hypothetical candidates and their respective predictive distributions. The first candidate has a slightly better predicted RMSE (on average). The area under the distribution curve to the right of the vertical line reflects the probability of being worse than the current solution which, for this candidate, is 16%.\n\n\n\n\n\n\n\n\nFigure 12.6: An example of a choice between two candidates where performance is gauged by RMSE. One candidate has slightly better predicted mean performance and another where the mean RMSE is predicted to be much lower but with high uncertainty.\n\n\n\n\n\nThe second candidate has a mean RMSE that is predicted to have about a 2-fold improvement over the first but comes with considerably large uncertainty. On average, this is a better choice but there is more area to the right of the vertical line. The probability of the second candidate having worse results than the current best is 26%.\nWhich option is better? The first candidate is a safer choice, while the second comes with higher risk but also a greater potential reward. The decision often depends on our perspective. However, it’s important to understand why the second candidate has such a large variance in outcomes.\nWe’ve previously discussed the variance-bias tradeoff and demonstrated that large uncertainty in the performance metric can often be explained by model variance. Recall that models might vary due to predictive instability caused by a model that is far more complex than is required, such as the polynomial example in Section 8.4.\nHowever, there is another reason that the noise in our performance statistic can be large. Back in Section 3.9, we described spatial autocorrelation, where objects are more similar to closer objects than to objects further away. If our Bayesian model reflects spatial variability, the increased uncertainty can be attributed to how each candidate relates spatially to the existing set. If our model’s variance in Figure 12.6 is based on spatial effects, it implies that the first candidate is closer to the current collection of tuning parameter candidates. Conversely, the large uncertainty for the second candidate suggests that it is placed far away from existing results.\nThis example illustrates two different search strategies: exploitation and extrapolation.\n\nExploitation involves focusing the search near existing data, leveraging known results to refine and optimize solutions within familiar territory.\nExtrapolation emphasizes exploring new, untested areas more aggressively in search of novel candidates that could lead to model improvements.\n\nUsing the Bayesian model in Equation 12.5 is beneficial because it allows for both mean and variance predictions, which can be weighted differently when evaluating potential new candidates. If our surrogate Bayesian model effectively captures spatial effects, we can leverage this to optimize the model. The next section outlines the overall optimization process.\n\n12.6.1 How Does Bayesian Optimization Work?\nThe search process begins with an initial set of candidates (\\(s_0\\)) and their corresponding performance statistics (_j$). These data are used to build a surrogate model, where the performance metric serves as the outcome and the candidate values act as predictors. The model then predicts the mean and standard deviation of the metric for new candidate values.\nTo guide the search, we use an acquisition function—an objective function that combines the predicted mean and standard deviation into a single value (Garnett 2023, chap. 7). The next candidate to be evaluated is the one that optimizes this acquisition function. This process is then repeated iteratively.\nOne of the most well-known acquisition functions is based on the notion of expected improvement (Jones, Schonlau, and Welch 1998, sec. 4). If the current best metric value is denoted as \\(\\hat{Q}_{opt}\\), the expected improvement is the positive part of \\(\\delta(\\hat{\\boldsymbol{\\theta}}) = Q_{opt} - \\mu(\\hat{\\boldsymbol{\\theta}})\\), where \\(\\mu(\\hat{\\boldsymbol{\\theta}})\\) is the mean of the posterior prediction (assuming that smaller metrics are better). They found that, probabilistically, the expected improvement is\n\\[\nEI(\\hat{\\boldsymbol{\\theta}}; Q_{opt}) = \\delta(\\hat{\\boldsymbol{\\theta}}) \\Phi\\left(\\frac{\\delta(\\hat{\\boldsymbol{\\theta}})}{\\sigma(\\hat{\\boldsymbol{\\theta}})}\\right) + \\sigma(\\hat{\\boldsymbol{\\theta}}) \\phi\\left(\\frac{\\delta(\\hat{\\boldsymbol{\\theta}})}{\\sigma(\\hat{\\boldsymbol{\\theta}})}\\right)\n\\tag{12.6}\\]\nwhere \\(\\Phi(\\cdot)\\) is the cumulative standard normal and \\(\\phi(\\cdot)\\) is the standard normal density.\nExpected improvement balances both mean and variance, but their influence can shift throughout the optimization process. Consider a parameter space where most regions have already been sampled to some extent. If the Bayesian model is influenced by spatial variation, the remaining unexplored areas will generally have low variance (\\(\\sigma\\)), causing one term in Equation 12.6 to dominate the optimization. In contrast, during the early iterations, variance tends to be much higher. Unless there is a strong mean effect, spatial variance will dominate, leading the search to focus more on extrapolation.\nAdditionally, the expected improvement can be altered to include a tradeoff factor \\(\\xi\\) so that improvement is the positive part of\n\\[\n\\delta(\\hat{\\boldsymbol{\\theta}}) = Q_{opt} - \\mu(\\hat{\\boldsymbol{\\theta}}) + \\xi\n\\]\nThis effectively inflates the value of the current best and the resulting value of \\(\\delta(\\hat{\\boldsymbol{\\theta}})\\) helps emphasize extrapolation. \\(\\xi\\) could be a single constant or a function that changes the tradeoff value as a function of iterations.\nSimilar to simulated annealing, sampling continues until reaching a predefined limit on iterations or computational time. Additionally, an early stopping rule can be used to end the search early if no new best result is found within a set number of iterations.\nLet’s see how this approach works in a single dimension. Figure 12.8 shows how expected improvement can be used to find the global minimum of \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\). The light grey line in the lower panel represents the true value of \\(\\psi(\\theta)\\) across the parameter space. The process starts with \\(s_0 = 2\\) candidate points at -8.0 and 3.0. From these two points, the surrogate model is fit and the dark line in the lower panel shows the predicted objective function values across the space. The magnitude of the line represents the mean of the posterior. Notice that the predicted line fails to capture the true trend, which is not unexpected with two data points.\nIf we optimized only on the mean prediction, we would select the next point near \\(\\theta = 3\\) since it has the lowest predicted value. This approach relies entirely on exploitation.\nThe color of the line depicts the predicted variance, which drops to zero near the existing points and explodes in the regions in between our two points. If we focused solely on the variance of the objective function, we would choose the next point in an area far from the two sampled points, such as the interval \\(-6 \\le \\theta \\le 1\\) or \\(\\theta \\ge 4\\).\nBy combining the mean and variance predictions through expected improvement, we strike a balance between the two. In the first iteration, two areas near (but not exactly at) \\(\\theta = 3.0\\) are predicted to have large expected improvements. The region on the left is sampled first, and once \\(\\psi(\\theta = 3)\\) is evaluated, the next point moves closer to the true global optimum. This process continues, alternating between sampling near the best result and exploring regions with few nearby candidate values. After 15 iterations, the search is very close to the true global optimum.\n\n\n\n\n#| label: shiny-bayes-opt\n#| viewerHeight: 600\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A single curve is shown with alternating peaks and valleys. The x-axis is the parameter, and the y-axis shows the objective function. Above this is another curve where the y-axis is expected improvement. As the optimization proceeds, the top curve shows several peaks of expected improvement, and the candidate points begin to cluster around the global minimum. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\nload(url(\"https://raw.githubusercontent.com/aml4td/website/main/RData/bayesian_opt_1d.RData\"))\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-bo-1d.R\")\n\napp\n\n\n\nFigure 12.7: An example of Bayesian optimization for the function \\(\\psi(\\theta) = \\theta\\: cos(\\theta/2)\\). The top plot shows the expected improvement for the current iteration. The bottom panel shows the true curve in grey and the mean prediction line (colored by the predicted standard deviation). The vertical line indicates the location of the next candidate.\n\n\n\nNumerous other acquisition functions can be used. For example, instead of expected improvement, we can maximize the probability of improvement:\n\\[\nPI(\\hat{\\boldsymbol{\\theta}}) = \\Phi\\left(\\frac{\\tau - \\mu(\\hat{\\boldsymbol{\\theta}})}{\\sigma(\\hat{\\boldsymbol{\\theta}})}\\right)\n\\]\nwhere \\(\\tau\\) is some performance metric goal (e.g., an R2 of 80%). This is generally deprecated in favor of EI for a few reasons. Most importantly, PI does not directly incorporate the current best value. Additionally, the value of \\(\\tau\\) is arbitrary and the search can be inconsistent across values of \\(\\tau\\) (Jones 2001).\nAnother acquisition function is based on confidence bounds (Cox and John 1992). For minimizing the performance statistic, the lower bound would be computed:\n\\[\nCB(\\hat{\\boldsymbol{\\theta}}) = \\mu(\\hat{\\boldsymbol{\\theta}}) - \\kappa\\: \\sigma(\\hat{\\boldsymbol{\\theta}})\n\\]\nwhere \\(\\kappa\\) sets the confidence level and often has values that are much smaller than range used for statistical inference (e.g., centered around 1.64). This multiplier also modulates between extrapolation (large \\(\\kappa\\)) and exploitation (small \\(\\kappa\\)) for the search.\nBayesian optimization offers more than just different acquisition functions; there are also various other modifications that can enhance its efficiency. For instance, similar to simulated annealing, the default approach samples one candidate at a time. While this works, it can be computationally inefficient, especially when models can be fitted in parallel. To improve this, modifications allow for multiple candidates to be simulated at once. In this approach, the candidate with the highest predicted acquisition value is chosen, and its predicted performance is treated as if it were derived from an actual assessment. A new GP model is then created using this imputed data point, and the process continues by acquiring the next point. This cycle repeats until the desired number of new candidates is obtained (see Azimi, Fern, and Fern (2010)). Alternatively, we can focus on the most promising areas of the parameter space and sample several different points. For example, in iteration five of Figure 12.7, where there are three peaks showing improvement, we could sample each of these peaks and create a batch of three new candidates to evaluate all at once, rather than individually.\nNow that the general optimization process has been outlined, let’s focus on the most popular model associated with Bayesian optimization.\n\n\n12.6.2 Gaussian Process Models\nGPs (Williams and Rasmussen 2006; Garnett 2023) are often motivated as special cases of stochastic processes, which are the collection of random variables that are indexed by one or more variables. The most common example of a stochastic process is the stock market. A stock price is a random variable that is indexed by time. A Gaussian process is just a stochastic process whose data are assumed to be multivariate normal.\nGaussian processes are different from basic multivariable normal distributions. Let’s say that we collected the heights and weights of 100 students in a specific grade. If we collect the heights and weights into a two-dimensional vector \\(\\boldsymbol{x} = [height, weight]\\), we might assume that the data is multivariate normal, with a 2D population mean vector and a 2 \\(\\times\\) 2 covariance matrix, i.e., \\(\\boldsymbol{x} \\sim N(\\boldsymbol{\\mu}, \\Sigma)\\). In this case, the data are a sample of people at a static time point.\nHowever, a Gaussian process is a sequence of data so we have to specify their means and covariances dynamically; there are mean and covariance functions instead of vectors and matrices. The mean function is often denoted as \\(\\mu(\\boldsymbol{x})\\). One way of writing the covariance function is \\(\\Sigma(\\boldsymbol{x}, \\boldsymbol{x}') = k(\\boldsymbol{x}, \\boldsymbol{x}') + \\sigma^2_{\\epsilon}I\\) where \\(k(\\cdot, \\cdot)\\) is a kernel function and \\(\\sigma_{\\epsilon}\\) is a constant error term.\nWe’ve seen kernels before, back in Equation 12.1 when they were associated with support vector machine models. For Gaussian processes, the kernel will define the variance between two data points in the process. Some kernels can be written as a function of the difference \\(\\delta_{ii'} = ||\\boldsymbol{x} - \\boldsymbol{x}'||\\). A common kernel is the squared exponential:\n\\[\nk(\\boldsymbol{x}, \\boldsymbol{x}') = \\exp\\left(-\\sigma d_{ii'}^2 \\right)\n\\tag{12.7}\\]\nNote that this covariance function has it’s own parameter \\(\\sigma\\), similar to \\(a\\), \\(b\\), and \\(q\\) in Equation 12.1 (\\(\\sigma\\) is unrelated to \\(\\sigma_{\\epsilon}\\)).\nThe squared exponential function shows that as the two vectors become more different from one another (i.e., further away), the exponentiated difference becomes large, indicating high variance (apart from \\(\\sigma^2_{\\epsilon}\\)). For two identical vectors, this covariance function yields a variance of \\(\\sigma^2_{\\epsilon}\\). Now, we can see why the previous section had a strong emphasis on spatial variability for Bayesian optimization. Figure 12.7 used this kernel function.\nA more general covariance function uses the Matern kernel:\n\\[\nk_{\\nu }(\\delta_{ii'})=\n{\\frac {2^{1-\\nu }}{\\Gamma (\\nu )}}{\\Bigg (}{\\sqrt {2\\nu }}{\\frac {\\delta_{ii'}}{\\rho }}{\\Bigg )}^{\\nu }\\mathcal{K}_{\\nu }{\\Bigg (}{\\sqrt {2\\nu }}{\\frac {\\delta_{ii'}}{\\rho }}{\\Bigg )}\n\\tag{12.8}\\]\nwhere \\(\\mathcal{K}(\\cdot)\\) is the Bessel function, \\(\\Gamma(\\cdot)\\) is the Gamma function, and \\(\\rho\\) and \\(\\nu\\) are tuning parameters.\nSo far, our covariance functions have assumed that all tuning parameters are numeric. A common, though simplistic, approach for incorporating qualitative parameters into a Gaussian process is to convert them into binary indicators and treat them as generic numeric predictors. However, this method is problematic because it’s unlikely that these binary indicators follow a Gaussian distribution. Additionally, if the qualitative tuning parameters have many levels, this approach can rapidly increase the dimensionality of the Gaussian space, which in turn raises the computational costs of training the GP model.\nAlternatively, special “categorical kernels” take integers corresponding to levels of a categorical predictor as inputs. For example, one based on an exchangeable correlation structure is:\n\\[\nk(x, x') = \\rho I(x \\ne x') + I(x = x')\n\\tag{12.9}\\]\nwhere \\(\\rho &lt; 1\\) (Joseph and Delaney 2007). More extensive discussions of models with mixed types can be found in Qian and Wu (2008), Zhou, Qian, and Zhou (2011), and Saves et al. (2023).\nMultiple kernels can be used to accommodate a pipeline with both quantitative and qualitative tuning parameters. Depending on the situation, an overall kernel can be created via addition or multiplication.\nGenton (2001), Part II of Shawe-Taylor and Cristianini (2004), and Chapter 4 of Garnett (2023) contain overviews of kernel functions, their different classes, and how they can be used and combined for data analysis.\nHow does this relate to iterative tuning parameter optimization? Suppose that our multivariate random variable is the concatenation of an outcome variable and one or more predictors, our vector of random variable could be thought of as \\(\\boldsymbol{d} = [y, x_1, ..., x_p]\\). In a model, we’d like to predict the value of \\(y\\) is for a specific set of \\(x\\) values. Mathematically, this is \\(Pr[y | \\boldsymbol{x}]\\) and, since this collection of data is multivariate normal, there are some nice linear algebra equations that can be used to compute this. This ties into Bayesian analysis since we can write Equation 12.3 in terms of probabilities:\n\\[\nPr[y | \\boldsymbol{x}] = \\frac{Pr[y] Pr[\\boldsymbol{x} | y]}{Pr[\\boldsymbol{x}]} = \\frac{(prior)\\times (likelihood)}{(evidence)}\n\\tag{12.10}\\]\nThe GP model is Bayesian because we have specified the distributions for the outcome and predictors (\\(Pr[y]\\) and \\(Pr[\\boldsymbol{x}]\\), respectively) simultaneously. The kernel function is the primary driver of the prior distribution of Gaussian processes.\nIn theory, each of these is easy to compute with a simple multivariate normal. However, we don’t know the value of \\(\\sigma^2_{\\epsilon}\\) nor do we know the values of any kernel parameters used by \\(k(\\cdot,\\cdot)\\). The error term can be estimated a variety of ways, including maximum likelihood, cross-validation, and others (see Ameli and Shadden (2022) for a survey of methods). For the kernel tuning parameters, it is common to maximize the marginal likelihood to estimate these structural parameters (Williams and Rasmussen 2006, chap. 5). The Gaussian nature of this model enables a number of relatively straightforward procedures for these purposes (that does not involve another layer of resampling).\nHowver, training Gaussian process models can be difficult and the computation cost of training these models increases cubically with the number of data points. In our application, the training set size is the number of tuning parameter candidates with performance statistics. However, after the initial GP model has been trained, there are updating algorithms that can prevent the model from being completely re-estimated from scratch thus reducing training time.\n\nIn summary, Gaussian process models are nonlinear models that can use tuning parameter values to make predictions on future performance statistics. They can predict the mean and variance of performance, the latter being mostly driven by spatial variability.\n\nIn Figure 12.7 we’ve seen how well Bayesian optimization works for a simple one-dimensional function. Let’s apply it in context by using it to optimize our two SVM tuning parameters.\n\n\n12.6.3 A Two Dimensional Illustration\nReturning the the SVM example, we initialize the GP model using the same three points shown in Table 12.1. We again use the squared exponential kernel in Equation 12.7 to measure variability.\nFigure 12.8 shows the results that help us select the first new candidate value. Due to the small number of points used to fit the GP, the EI surface is virtually flat. The flatness of the acquisition function results in a fairly uninformative choice: the first point is very close to the best candidate in the initial set.\nOnce this new point is sampled, the surrogate GP model produces a clear trend in EI indicating a strong preference for higher cost values (and no effect of the scale parameter). The selected candidate is closer to the ridge of optimal performance. After acquiring this point, the next iteration shows that EI is focused on the scale parameter value and ignores the cost parameter. By iteration 13, a candidate is chosen that is nearly optimal.\nThis example shows that Bayesian optimization can make enormous leaps in parameter space, frequently choosing values of parameters at their minimum or maximum values. This is the opposite of SA, which meanders through local parameter space.\n\n\n\n\n\n\n\n\n#| label: shiny-bo-example\n#| viewerHeight: 630\n#| viewerWidth: \"100%\"\n#| standalone: true\n#| fig-alt: A two-dimensional parameter space with axes for the scale parameter and the SVM cost is shown. Three initial points are shown. The BO algorithm progresses from a point with low cost and a large scale factor value, making large jumps to different regions of the parameter space; several iterations come close to the optimal ridge. \nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(scales)\n\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-setup.R\")\nsource(\"https://raw.githubusercontent.com/aml4td/website/main/R/shiny-bo.R\")\n\napp\n\n\n\nFigure 12.8: An illustration of Bayesian optimization. The open circles represent a small initial grid. The thick diagonal grey light is a ridge where the model has the smallest RMSE. The asterisk denotes the current best candidate.\n\n\n\n\n\n\n\nThe complete search produced new optimal values at iterations iterations 4, 9, 13, 42, and 44. The decrease in RMSE shows few meaningful improvements among negligible decreases: 6.098%, 5.993%, 5.947%, 5.773%, 5.771%, and 5.77%. The search process was able to effectively tune the SVM model and find very good parameters.\nThe cost of the Gaussian process fit does become more expensive as new candidates are sampled, making the process somewhat less efficient than the previous approaches. The average time to compute each candidate was 56.2s, although the cost of each increases with iterations.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-bayes-opt-nnet",
    "href": "chapters/iterative-search.html#sec-bayes-opt-nnet",
    "title": "12  Iterative Search",
    "section": "12.7 Example: Tuning a Neural Network",
    "text": "12.7 Example: Tuning a Neural Network\nWe revisit the barley data, where the percentage of barley oil in a mixture is estimated using spectrographic measurements. Our previous work on these data addressed the high correlation between predictors via feature engineering with principal components.\nIn this chapter, we’ll take a more sophisticated approach to preprocessing this type of data using signal processing techniques, specifically the Savitzky-Golay (SG) method (Rinnan, Van Den Berg, and Engelsen 2009; Schafer 2011). This technique incorporates two main goals: to smooth the data from location to location (i.e., across the x-axis in Figure 7.1(b)) and to difference the data. Smoothing attempts to reduce measurement noise without diminishing the data patterns that could predict the outcome. Differencing can remove the correlation between locations and might also mitigate other issues, such as systematic background noise.\nFor smoothing, the SG procedure uses a moving window across a sample’s location and, within each window, uses a polynomial basis expansion to produce a smoother representation of the data. The window width and the polynomial degree are both tuning parameters.\nDifferencing has a single tuning parameter: the difference order. A zero-order difference leaves the data as-is (after smoothing). A first-order difference is the smoothed value minus the smoothed value for the next location (and so on). Savitzky and Golay’s method uses the polynomial estimates to estimate the differences/derivatives, instead of manually subtracting columns after smoothing.\nFigure 12.9 shows an example of a small set of locations from a single spectra in Figure 7.1. The top row shows the effect of smoothing when differencing is not used. For a linear smoother (i.e., a polynomial degree of one), the process can over-smooth the data such that the nonlinear portions of the raw data lose significant amounts of information. This issue goes away as the polynomial order is increased. However, the higher degree polynomial nearly interpolates between data points and may not be smoothing the data in any meaningful way.\nThe bottom panel shows the data when a first-order difference is used. The pattern is completely different since the data now represent the rate of change in the outcome rather than the absolute measurement. Again, larger window sizes oversmooth the data, and the highest degree of polynomial provides minimal smoothing.\n\n\n\n\n\n\n\n\nFigure 12.9: Demonstrations of the Savitzky-Golay preprocessing method for a small region for one specific sample previously shown in Figure 7.1.\n\n\n\n\n\nWe’ll want to optimize these preprocessing parameters simultaneously with the parameters for the supervised model. With this example, the supervised model of choice is a neural network with a single layer of hidden units. We’ll tune over the following parameters, described at length in ?sec-cls-nnet:\n\nThe number of hidden units in the layer (2 to 100).\nThe type of nonlinear function that connects the input layer and the hidden layer, called the “activation function”. Possible functions are “tanh”, exponential linear unit (“ELU”) functions, rectified linear unit (“ReLU”) functions, and log-sigmoidal functions.\nThe number of epochs of degraded performance before early stopping is enacted (3 to 20).\nThe total amount of penalization for the model (-10 to 0, in log-10 units).\nThe proportion of \\(L_1\\) penalty (a.k.a. Lasso penalization) (0 to 1).\nThe initial learning rate used during gradient descent (-2 to -0.5, in log-10 units).\nThe “scheduling” function to adjust the learning rate over iterations: “constant”, “cyclic”, or “exponential decay”.\n\nIn all, there are ten tuning parameters, two of which are qualitative.\nA space-filling design with 12 candidate points were initially processed to use as the starting point for Bayesian optimization and simulated annealing. When constructing the grid, there are two constraints on the preprocessing parameters:\n\nThe smoothing polynomial degree must be greater or equal to the differentiation order.\nThe window size must be greater than the polynomial degree.\n\nAn initial grid was created without constraints, and candidates who violated these rules were eliminated. The two constraints will also be applied when simulated annealing and Bayesian optimization create new candidates.\nTable 12.2 shows the settings and their corresponding validation set RMSE values, which ranged between 4.3% and 14.4%. At least two of the settings show good performance results, especially compared to the RMSE values previously achieved when embeddings and/or SVMs were used.\n\n\n\n\n\n\n\n\n\nRMSE (%)\nWindow\nDiff. Order\nDegree\nUnits\nPenalty (log-10)\nActivation\nLearn Rate (log-10)\nRate Schedule\nStop Iter.\nL1 Mixture\n\n\n\n\n4.32\n19\n2\n10\n27\n-7.89\nelu\n-1.84\nnone\n12\n42.11\n\n\n4.57\n19\n3\n7\n84\n-9.47\ntanh\n-0.82\ndecay\n8\n31.58\n\n\n4.72\n15\n2\n3\n2\n-2.63\ntanh\n-1.45\nnone\n16\n5.26\n\n\n4.87\n15\n1\n1\n69\n-5.79\nelu\n-1.29\ndecay\n4\n0.00\n\n\n4.91\n9\n2\n8\n100\n-7.37\nelu\n-1.05\ncyclic\n5\n94.74\n\n\n5.88\n17\n0\n3\n79\n-6.84\nrelu\n-1.92\ncyclic\n19\n73.68\n\n\n6.16\n11\n4\n6\n94\n-3.68\nelu\n-1.21\ndecay\n20\n10.53\n\n\n6.47\n13\n0\n9\n38\n-5.26\nlog sigmoid\n-0.74\ncyclic\n14\n15.79\n\n\n7.01\n13\n0\n5\n7\n-6.32\ntanh\n-1.37\ndecay\n3\n89.47\n\n\n7.72\n21\n1\n5\n53\n-1.58\nlog sigmoid\n-0.50\nnone\n15\n78.95\n\n\n10.57\n5\n1\n4\n17\n-0.53\nelu\n-1.68\ndecay\n13\n68.42\n\n\n14.38\n21\n3\n7\n58\n0.00\nrelu\n-1.53\ncyclic\n4\n52.63\n\n\n\n\n\n\n\n\nTable 12.2: The initial set of candidates used for iterative search methods.\n\n\n\nCan we improve on these results using iterative optimization? To begin, a simulated annealing search was initialized using the best results in Table 12.2 and ran for 50 iterations with no early stopping. If a new globally best result was not found within eight iterations, the search restarted from the previously best candidate. The search was restarted 3 times at iterations 15, 31, and 47 and improved candidates were found at iterations 2, 7, 23, 39. The overall best candidate had an RMSE value of 3.65%, with a 90% confidence interval of (3.41%, 3.9%). The improvement appears to be a legitimate improvement over the initial best configuration. The top panel of Figure 12.10 shows the RMSE values over iterations and Table 12.3 lists the best candidate value.\n\n\n\n\n\n\n\n\nFigure 12.10: Validation set RMSE results for the optimization. The validation set RMSE is shown on the y-axis with 90% confidence intervals. Orange points indicate iterations where a new optimal value was found and vertical lines indicate where the algorithm restarted from the previous best candidate.\n\n\n\n\n\nA Bayesian optimization was also used with a squared exponential kernel for the covariance function and used expected improvement to select candidates at each of the 50 iterations. This search found new best results at 3 iterations: 1, 10, and 47. Like the SA search, the best RMSE was substantially different than the one found in the initial grid: 3.58% with 90% confidence interval, (3.3%, 3.87%). The iterations and final values also be seen in Figure 12.10 and Table 12.3, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInitial Grid\nSimulated Annealing\nBayesian Optimzation\n\n\n\n\nDifferentiation Order\n2\n2\n1\n\n\nPolynomial Degree\n10\n4\n8\n\n\nWindow Size\n19\n19\n21\n\n\nHidden Units\n27\n27\n53\n\n\nActivation\nelu\nrelu\ntanh\n\n\nPenalty (log10)\n-7.89\n-9.00\n-8.22\n\n\nL1 Proportion\n0.421\n0.212\n0.112\n\n\nLearning Rate (log10)\n-1.84\n-1.39\n-1.97\n\n\nRate Schedule\nnone\ncyclic\ncyclic\n\n\nStopping Rule\n12\n14\n14\n\n\nRMSE\n4.32%\n3.65%\n3.58%\n\n\nConf. Int.\n(4.03%, 4.6%)\n(3.41%, 3.9%)\n(3.3%, 3.87%)\n\n\n\n\n\n\n\n\nTable 12.3: The best results from the initial space-filling design and the two iterative searches for a neural network using Savitzky-Golay preprocessing.\n\n\n\n\n\n\n\nMany of the parameter values for the final candidates from each search are very similar. In particular, the learning rates, regularization penalties, and SG window sizes where nearly the same.\nIn terms of computing time, simulated annealing took 1m 55.9s on average for each candidate and compared to 3m 22.3s required for BO (a 1.7-fold difference).",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#sec-iterative-summary",
    "href": "chapters/iterative-search.html#sec-iterative-summary",
    "title": "12  Iterative Search",
    "section": "12.8 Summary",
    "text": "12.8 Summary\nThere are several different ways to tune models sequentially. For tabular data, these approaches are rarely the only efficient method for model optimization, but they may be preferable for certain models and large data sets.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#chapter-references",
    "href": "chapters/iterative-search.html#chapter-references",
    "title": "12  Iterative Search",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmeli, S, and S Shadden. 2022. “Noise Estimation in Gaussian Process Regression.” arXiv.\n\n\nAzimi, J, A Fern, and X Fern. 2010. “Batch Bayesian Optimization via Simulation Matching.” Advances in Neural Information Processing Systems 23.\n\n\nBinois, M, and N Wycoff. 2022. “A Survey on High-Dimensional Gaussian Process Modeling with Application to Bayesian Optimization.” ACM Transactions on Evolutionary Learning and Optimization 2 (2): 1–26.\n\n\nBland, J, and D Altman. 1998. “Bayesians and Frequentists.” Bmj 317 (7166): 1151–60.\n\n\nBreiman, L. 1997. “No Bayesians in Foxholes.” IEEE Expert 12 (6): 21–24.\n\n\nChitty, R. 2005. “Why Clinicians Are Natural Bayesians: Is There a Bayesian Doctor in the House?” BMJ: British Medical Journal 330 (7504): 1390.\n\n\nCox, D, and S John. 1992. “A Statistical Method for Global Optimization.” In [Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics, 1241–46. IEEE.\n\n\nEiben, A, and J Smith. 2015. Introduction to Evolutionary Computing. Springer.\n\n\nFornacon-Wood, I, H Mistry, C Johnson-Hart, C Faivre-Finn, J O’Connor, and G Price. 2022. “Understanding the Differences Between Bayesian and Frequentist Statistics.” International Journal of Radiation Oncology, Biology, Physics 112 (5): 1076–82.\n\n\nGarnett, R. 2023. Bayesian Optimization. Cambridge University Press.\n\n\nGenton, M. 2001. “Classes of Kernels for Machine Learning: A Statistics Perspective.” Journal of Machine Learning Research 2 (Dec): 299–312.\n\n\nGill, C, L Sabin, and C Schmid. 2005. “Why Clinicians Are Natural Bayesians.” Bmj 330 (7499): 1080–83.\n\n\nGramacy, R. 2020. Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences. Chapman Hall/CRC.\n\n\nHofmann, T, B Schölkopf, and A Smola. 2008. “Kernel Methods in Machine Learning.” The Annals of Statistics 36 (3): 1171–1220.\n\n\nHutchon, D. 2005. “Why Clinicians Are Natural Bayesians: Bayesian Confusion.” BMJ: British Medical Journal 330 (7504): 1390.\n\n\nJones, D. 2001. “A Taxonomy of Global Optimization Methods Based on Response Surfaces.” Journal of Global Optimization 21: 345–83.\n\n\nJones, D, M Schonlau, and W Welch. 1998. “Efficient Global Optimization of Expensive Black-Box Functions.” Journal of Global Optimization 13: 455–92.\n\n\nJoseph, R, and J Delaney. 2007. “Functionally Induced Priors for the Analysis of Experiments.” Technometrics 49 (1): 1–11.\n\n\nKirkpatrick, S, D Gelatt, and M Vecchi. 1983. “Optimization by Simulated Annealing.” Science 220 (4598): 671–80.\n\n\nLu, J. 2022. Gradient Descent, Stochastic Optimization, and Other Tales. Eliva Press.\n\n\nMcCrossin, R. 2005. “Why Clinicians Are Natural Bayesians: Clinicians Have to Be Bayesians.” BMJ: British Medical Journal 330 (7504): 1390.\n\n\nMitchell, M. 1996. An Introduction to Genetic Algorithms. MIT Press.\n\n\nMočkus, J. 1975. “On Bayesian Methods for Seeking the Extremum.” In Optimization Techniques IFIP Technical Conference, 400–404. Springer.\n\n\nPrince, S. 2023. Understanding Deep Learning. MIT press.\n\n\nQian, P, and CF Jeff Wu. 2008. “Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments.” Technometrics 50 (2): 192–204.\n\n\nRana, S. 1999. “The Distributional Biases of Crossover Operators.” In Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation-Volume 1, 549–56.\n\n\nRinnan, A, F Van Den Berg, and S Engelsen. 2009. “Review of the Most Common Pre-Processing Techniques for Near-Infrared Spectra.” Trends in Analytical Chemistry 28 (10): 1201–22.\n\n\nSaves, P, Y Diouane, N Bartoli, T Lefebvre, and J Morlier. 2023. “A Mixed-Categorical Correlation Kernel for Gaussian Process.” Neurocomputing 550: 126472.\n\n\nSchafer, R. 2011. “What Is a Savitzky-Golay Filter?” IEEE Signal Processing Magazine 28 (4): 111–17.\n\n\nSchölkopf, B, and A Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. The MIT Press.\n\n\nShawe-Taylor, J., and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.\n\n\nSoule, T. 2009. “Crossover and Sampling Biases on Nearly Uniform Landscapes.” In Genetic Programming Theory and Practice VI, 1–15. Springer US.\n\n\nSpall, J. 2005. Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control. John Wiley & Sons.\n\n\nWagenmakers, E, M Lee, T Lodewyckx, and G Iverson. 2008. “Bayesian Evaluation of Informative Hypotheses.” In, edited by H Hoijtink, I Klugkist, and P Boelen, 181–207. New York, NY: Springer New York.\n\n\nWilliams, C, and C Rasmussen. 2006. Gaussian Processes for Machine Learning. MIT press Cambridge, MA.\n\n\nZhang, J. 2019. “Gradient Descent Based Optimization Algorithms for Deep Learning Models Training.” arXiv.\n\n\nZhou, Q, P Qian, and S Zhou. 2011. “A Simple Approach to Emulation for Computer Models with Qualitative and Quantitative Factors.” Technometrics 53 (3): 266–73.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/iterative-search.html#footnotes",
    "href": "chapters/iterative-search.html#footnotes",
    "title": "12  Iterative Search",
    "section": "",
    "text": "Sections ?sec-cls-svm and ?sec-reg-svm↩︎\nSections ?sec-cls-nnet and ?sec-reg-nnet↩︎\nWhy do these two parameters use different bases? It’s mostly a convention. Both parameters have valid values that can range across many orders of magnitude. The change in cost tends to occur more slowly than the scaling factor, so a smaller base of 2 is often used.↩︎\nThese are not simulated data, so this surface is an approximation of the true RMSE via the validation set using a very large regular grid. The RMSE results are estimates and would change if we used different random numbers for data splitting.↩︎\nWe’ve seen \\(\\alpha\\) before when it was called the learning rate (and will revisit it later in this chapter).↩︎\nHowever, computing the Hessian matrix increases the computational cost by about 3-fold.↩︎\nThese values will be also used as the initial substrate for Bayesian optimization.↩︎\nThink of the candidates within a generation as a population of people.↩︎\nAnalogous to chromosomes.↩︎\nIf you have access to high-performance computing infrastructure, the parallel processing can run the individuals in the population in parallel and, within these, any resampling iterations can also be parallelized.↩︎\nWe’ll see how the prior can affect some calculations later in ?sec-naive-bayes. ↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iterative Search</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html",
    "href": "chapters/feature-selection.html",
    "title": "13  Feature Selection",
    "section": "",
    "text": "13.1 The Cost of Irrelevant Predictors\nIn Chapters 5 through 8, we described different methods for adding new predictors to the model (e.g., interactions, spline columns, etc.). We can add any predictors (or functions of predictors) based on first principles or our intuition or contextual knowledge.\nWe are also motivated to remove unwanted or irrelevant predictors. This can decrease model complexity, training time, computer memory, the cost of acquiring unimportant predictors, and so on. For some models, the presence of non-informative predictors can decrease performance (as we’ll see in Figure 13.1).\nWe may want to remove predictors solely based on their distributions; unsupervised feature selection focuses on just the predictor data.\nMore often, though, we are interested in supervised feature selection1: the removal of predictors that do not appear to have a relationship to the outcome. This implies that we have a statistic that can compute some measure of each predictor’s utility for predicting the outcome.\nWhile it is easy to add predictors to a model, it can be very difficult (and computationally expensive) to properly determine which predictors to remove. The reason comes back to the misuse (or over-use) of data. We’ve described the problem of using the same data to fit and evaluate a model. By naively re-predicting the training set, we can accidentally get overly optimistic performance statistics. To compensate for this, resampling repeatedly allocates some data for model fitting and other data for model evaluation.\nFor feature selection, we have the same problem when the same data are used to measure importance, filter the predictors, and then train the model on the smaller feature set. It is similar to repeatedly taking the same test and then measuring our knowledge using the grade for the last test repetition. It doesn’t accurately measure the quantity that is important to us. This is the reason that this chapter is situated here instead of in Part 2; understanding overfitting is crucial to effectively remove predictors.\nThis chapter will summarize the most important aspects of feature selection. This is a broad topic and a more extensive discussion can be found in Kuhn and Johnson (2019). First, we’ll use an example to quantify the effect of irrelevant predictors on different models. Next, the four main approaches for feature selection are described and illustrated.\nDoes it matter if the model is given extra predictors that are unrelated to the outcome? It depends on the model. To quantify this, data were simulated using the equation described in Hooker (2004) and used in Sorokina et al. (2008):\n\\[\ny_i = \\pi^{x_{i1} x_{i2}}  \\sqrt{ 2 x_{i3} } - asin(x_{i4}) + \\log(x_{i3}  + x_{i5}) - (x_{i9} / x_{i10}) \\sqrt{x_{i7} / x_{i8}} - x_{i2} x_{i7} + \\epsilon_i\n\\tag{13.1}\\]\nwhere predictors 1, 2, 3, 6, 7, and 9 are standard uniform while the others are uniform on [0.6, 1.0]. Note that \\(x_6\\) is not used in the equation. The errors \\(\\epsilon_i\\) are Gaussian with mean zero and a standard deviation2 of 0.25. Training and testing data sizes where simulated to be 1,000 and 100,000, respectively.\nTo assess the effect of useless predictors, between 10 and 100 extra columns of standard normal data were generated (unrelated to the outcome). Models were fit, tuned, and the RMSE of the test set was computed for each model. Twenty simulations were run for each combination of models and extra features3.\nThe percent difference from baseline RMSE was computed (baseline being the model with no extra columns). Figure 13.1 shows the results for the following models:\nThese models are described in detail in subsequent chapters. The colors of the points/lines signifies whether the model automatically removes unused predictors from the model equation (see Section 13.6 below).\nFigure 13.1: For different regression models, the percent increase in RMSE is shown as a function of the number of additional non-informative predictors.\nThe results show that a few models, K-nearest neighbors, neural networks, and support vector machines, have severely degraded performance as the number of predictors increases. This is most likely due to the use of cross- and/or dot-products of the predictor columns in their calculations. The extra predictors add significant noise to these calculations, and that noise propagates in a way that inhibits the models from accurately determining the underlying relationship with the outcome.\nHowever, several other models use these same calculations without being drastically affected. This is due to the models automatically performing feature selection during model training. Let’s briefly describe two sets of models that were largely unaffected by the extra columns.\nThe other unaffected models are tree-based. As seen in Figure 2.5, these types of models make different rules by selecting predictors to split the data. If a predictor was not used in any split, it is functionally independent of the outcome in the prediction equation. This provides some insensitivity to the presence of non-informative predictors, although there is slight degradation in RMSE as the number of columns increases.\nIn summary, irrelevant columns minimally affect ML models that automatically select features. Models that do not have this attribute can be crippled under the same circumstances.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-irrelevant-predictors",
    "href": "chapters/feature-selection.html#sec-irrelevant-predictors",
    "title": "13  Feature Selection",
    "section": "",
    "text": "Bagged regression trees\nBayesian additive regression trees (BART)\nBoosted trees (via lightGBM)\nCubist\nGeneralized additive models (GAMs)\nK-nearest neighbors (KNN)\nMultivariate adaptive regression splines (MARS)\nPenalized linear regression\nRandom forest\nRuleFit\nSingle-layer neural networks\nSupport vector machines (radial basis function kernel)\n\n\n\n\n\n\nThe first set includes penalized linear regression, generalized additive models, and RuleFit. These add a penalty to their calculations, restricting the model parameters from becoming abnormally large unless the underlying data warrant a large coefficient. This effectively reduces the impact of extra predictors by keeping their corresponding model coefficient at or near zero.\nThe other set o model models include MARS and Cubist. They selectively include predictors in their regression equations, only including those specifically selected as having value.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-feature-levels",
    "href": "chapters/feature-selection.html#sec-feature-levels",
    "title": "13  Feature Selection",
    "section": "13.2 Different Levels of Features",
    "text": "13.2 Different Levels of Features\nWhen we talk about selecting features, we should be clear about what level of computation we mean. There are often two levels:\n\nOriginal predictors are the unmodified version of the data.\nDerived predictors refer to the set of columns that are present after feature engineering.\n\nFor example, if our model requires all numeric inputs, a categorical predictor is often converted to binary indicators. A date-time column is the original predictor, while the binary indicators for individual days of the week are the derived predictor. Similarly, the 550 measurement predictors in the barley data are the original predictors, and embedded values, such as the PCA components, are the derived predictors.\nDepending on the context, our interest in feature selection could be at either level. If the original predictors are expensive to estimate, we would be more interested in removing them at the original level; if any of their derived predictors are important, we need the original.\nThis idea will arise again in ?sec-importance when we discuss variable importance scores for explaining models.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-selection-overfitting",
    "href": "chapters/feature-selection.html#sec-selection-overfitting",
    "title": "13  Feature Selection",
    "section": "13.3 Overfitting the Feature Set",
    "text": "13.3 Overfitting the Feature Set\nWe’ve previously discussed overfitting, where a model finds patterns in the training data that are not reproducible. It is also possible to overfit the predictor set by finding the training set predictors that appear to be connected to the outcome but do not show the same relationship in other data.\nThis is most likely to occur when the number of predictors (\\(p\\)) is much larger than the training set size (\\(n_{tr}\\)) or when no external validation is used to verify performance. Ambroise and McLachlan (2002) shows reanalyses of high-dimensional biology experiments using a backward selection algorithm for removing predictors. In some cases, they could find a model and predictor subset with perfect accuracy even when the training set outcome data were randomly shuffled.\nRecall back in Section 1.5 an argument was made that preprocessing methods and the supervised model encompassed the entire model pipeline. Technically, feature selection (supervised or unsupervised) falls into the definition of preprocessing. It is important to use data that is different from the data used to train the pipeline (i.e., select the predictors) is different from the data used to evaluate how well it worked.\nFor example, if we use a resampling scheme, we have to repeat the feature selection process within each resample. If we used 10-fold cross-validation, each of the 10 analysis sets (90% of the training set) would have its own set of selected predictors.\nAmbroise and McLachlan (2002) demonstrated that when feature selection was enacted once, and then the model with this subset was resampled, they could produce models with excessively optimistic performance metrics. However, when the performance statistics were computed with multiple realizations of feature selection, there was very little false optimism.\nAnother improper example of data usage that can be found in the literature, as well as in real-life analyses, occurs when the entire data set is used to select predictors then the initial split is used to create the training and test sets. This “bakes in” the false optimism from the start, the epitome of information leakage4.\nHowever, performing feature selection in each resample can multiplicatively increase the computational cost, especially if the model has many tuning parameters. For this reason, we tend to focus on methods that automatically select features during the normal process of training the model (as discussed below).\nOne scheme to understand if we are overfitting to the predictors is to create one or more artificial “sentinel predictors” that are random noise. Once the analysis that ranks/selects predictors is finished, we can see how high the algorithm ranks these deliberately irrelevant predictors. It is also possible to create a rule where predictors that rank higher than the sentinels are selected to be included in the model.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-unsupervised-selection",
    "href": "chapters/feature-selection.html#sec-unsupervised-selection",
    "title": "13  Feature Selection",
    "section": "13.4 Unsupervised Selection",
    "text": "13.4 Unsupervised Selection\nThere are occasions where the values of the predictors are unfavorable or possibly detrimental to the model. The simplest example is a zero-variance column, where all of the values of a predictor in the training set have the same value. There is no information in these predictors and, although the model might be able to tolerate such a predictor, taking the time to identify and remove such columns is advisable.\nBut what about situations where only one or two rows have non-zero values? It is difficult to filter on variance values since the predictors could have different units or ranges. The underlying problem occurs when very few unique values exist (i.e., “coarse data”), and one or two predictor values capture most of the rows. Perhaps we can identify this situation.\nConsider a hypothetical training set of 500 data points with a count-based predictor that has mostly zero counts. However, there are three data points with a count of 1 and one instance with a count of 2. The variance of these data is low, but it would be difficult to determine a cutpoint for removal. Kuhn (2008) developed an algorithm to find near-zero variance predictors, as defined by two criteria:\n\nThe freqeuncy ratio is the frequency of the most prevalent value over the second most frequent value. For our example, the ratio was 496/3 = 165.3.\nThe proportion of unique values, which is 3 / 500 = 0.006.\n\nWe can determine thresholds for both of these criteria to remove the predictor. The default behavior is to remove the column of the frequency ratio is greater than 19 and the proportion of unique values is less than 0.1. For the example, the predictor would be eliminated.\nAnother example of an unsupervised filter reduces between-predictor correlations. A high degree of correlation can reduce the effectiveness of some models. We can filter out predictors by choosing a threshold for the (absolute) pairwise correlations and finding the smallest subset of predictors to meet this criterion.\nA good example is Figure 13.2, which visualizes the correlation matrix for the fifteen predictors in the forestry data set (from Section 3.9). Predictors for the January minimum temperature and the annual mean temperature are highly correlated (corr = 0.93), and only one might be sufficient to produce a better model. Similar issues occur with other predictor pairs. If we apply a threshold of 0.50, then six predictors (dew temperature, annual precipitation, annual maximum temperature, annual mean temperature, annual minimum temperature, and maximum vapor) would be removed before model training.\n\n\n\n\n\n\n\n\nFigure 13.2: A heatmap of a correlation matrix for fifteen predictors spatial, ordered using a hierarchical clustering algorithm.\n\n\n\n\nSimilar redundancy can occur in categorical predictors. In this case, measures of similarity can be used to filter the predictors in the same way.\nUnsupervised filters are only needed in specific circumstances. Some models (such as trees) are resistant to the distributions of the predictor’s data5, while others are excessively sensitive.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-selection-methods",
    "href": "chapters/feature-selection.html#sec-selection-methods",
    "title": "13  Feature Selection",
    "section": "13.5 Classes of Supervised Methods",
    "text": "13.5 Classes of Supervised Methods\nThere are three main strategies for supervised feature selection. The first and most effective was discussed: automatic selection occurs when a subset of predictors is determined during model training. We’ll discuss this more in the next section.\nThe second supervised strategy is called wrappers. In this case, a sequential algorithm proposes feature subsets, fits the model with these subsets, and then determines a better subset from the results. This sounds similar to iterative optimization because it is. Many of the same tools can be used. Wrappers are the most thorough approach to searching the space of feature subsets, but this can come with an enormous computational cost.\nA third strategy is to use a filter to screen predictors before adding them to the model. For example, the analysis shown in Figure 2.3 computed how each item in a food order impacted the delivery time. We could have applied a filter where we only used food item predictors whose lower confidence interval for the increase in time was greater than one. The idea behind the filter is that it is applied once prior to the model fit.\nThe next few sections will describe the mechanics of these techniques. However, how we utilize these algorithms is tricky. We definitely want to avoid overfitting the predictor set to the training data.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-automatic-selection",
    "href": "chapters/feature-selection.html#sec-automatic-selection",
    "title": "13  Feature Selection",
    "section": "13.6 Automatic Selection",
    "text": "13.6 Automatic Selection\nTo more closely demonstrate how automatic feature selection works, let’s return to the Cubist model fit to the delivery data in Section 2.4. That model created a series of regression trees, and converted them to rules (i.e., a path through the tree), then fit regression models for every rule. Recall that there was a sizable number of rules (2,007) each with its own linear regression. Despite the size of the rule set, only 13 of the original 30 predictors in the data set were used in rules or model fits; 17 were judged to be irrelevant for predicting delivery times.\n\nThis shows that we can often reduce the number of features for free.\n\nDifferent models view the training data differently, and the list of relevant predictors will likely change from model to model. When we fit a MARS model (?sec-mars-reg) to the data, only 9 original predictors were used. If we use automatic selection methods to understand what is important, we should fit a variety of different models and create a consensus estimate of which predictors are “active” for the data set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-wrappers",
    "href": "chapters/feature-selection.html#sec-wrappers",
    "title": "13  Feature Selection",
    "section": "13.7 Wrapper Methods",
    "text": "13.7 Wrapper Methods\nWrapper methods (Kohavi and John 1998) use an overarching search method to optimize binary indicators, one for each potential predictor, that control which predictors are given to the model.\nThe most popular approach is recursive feature elimination (RFE), which starts with the complete set of predictors and uses a mechanism to rank each in terms of their utility for predicting the outcome (Guyon et al. 2002). Using this ordering, predictors are successively removed while performance is monitored. The performance profile determines the optimal subset size, and the most important predictors are included in the model. RFE is greedy, though; it predetermines the order that the predictors are eliminated and will never consider a subset that is inconsistent with this order.\nTwo global search methods described in Chapter 12, genetic algorithms (GA) and simulated annealing, can also be used. In this instance, the search space is the predictor space represented as binary indicators. The same process applies, though. For example, a GA would contain a population of different subsets, each producing a fitted model. The performance values enable the creation of the next generation via selection, reproduction, and mutation. In the case of simulated annealing, an initial subset is created and evaluated, and perturbations are created by randomly altering a small set of indicators (to create a “nearby” subset). Suboptimal subsets can be accepted similarly to our application for tuning parameters.\nHowever, for each of these cases, the details matter. First, we should probably use separate data to\n\nestimate performance for each candidate subset and\ndetermining how far the search should proceed.\n\nIf we have large data sets, we can partition them into separate validation sets for each purpose. Otherwise, resampling is probably the answer to solve one or both of these problems.\nThe need to optimize tuning parameters can greatly complicate this entire process. A set of optimal parameter values for large subset sizes may perform very poorly for small subsets. In some cases, such as \\(m_{try}\\) and the number of embedding dimensions, the tuning parameter depends on the subset size.\nIn this case, the traditional approach uses a nested resampling procedure similar to the one shown in Section 11.4. An inner resampling scheme tunes the model, and the optimized model is used to predict the assessment set from the outer resample. Even with parallel processing, this can become an onerous computational task.\nHowever, let’s illustrate a wrapper method using an artificial data set from the simulations described in Section 13.1. We’ll use Equation 13.1 and supplement the eight truly important predictors with twenty one noise columns. For simplicity, we’ll use a neural network to predict the outcome with a static set of tuning parameters. The training set consisted of 1,000 data points.\nSince feature selection is part of the broader model pipeline, 10-fold cross-validation was used to measure the effectiveness of 150 iterations of simulated annealing (Kuhn and Johnson 2019 Sect 12.2). A separate SA run is created for each resample; we use the analysis set to train the model on the current subset, and the assessment set is used to measure the RMSE.\nFigure 13.3 shows the results, where each point is the average of the 10 RMSE values for the SA at each iteration. The results clearly improve as the selection process mostly begins to include important predictors and discards a few of the noise columns. After approximately 120 iterations, the solution stabilizes. This appears to be a very straightforward search process that clearly indicates when to stop the SA search.\n\n\n\n\n\n\n\n\nFigure 13.3: The progress of using simulated annealing to select features for.a neural network. The dashed green line indicators the best RMSE value achievable for these simulated data.\n\n\n\n\n\nHowever, looks can be deceiving. The feature selection process is incredibly variable, and adding or removing an important predictor can cause severe changes in performance. Figure 13.3 is showing the average change in performance, and the relative smoothness of this curve is somewhat deceiving.\nFigure 13.4 shows what could have happened by visualizing each of the 10 searches. The results are much more dynamic and significantly vary from resample to resample. Most (but not all) searches contain one or two precipitous drops in the RMSE with many spikes of high errors (caused by a bad permutation of the solution). These cliffs and spikes do not reproducibly occur at the same iteration or in the same way.\n\n\n\n\n\n\n\n\nFigure 13.4: The individual SA runs that make up the data in Figure 13.3.\n\n\n\n\n\nWe should keep in mind that Figure 13.3 lets us know where, on average, the search should stop, but our actual search result (that uses the entire training set) is more likely to look like what we see in Figure 13.4.\nFor this particular search, the numerically best result occurred at iteration 113 and the final model contained 17 predictors where eight of the twenty noise columns are still contained in the model. Our resampled estimate of the RMSE, at this iteration, was 0.302 (a perfect model would have a value of 0.25). However, the plots of the SA performance profiles indicate a range, probably above 125 iterations, where the results are equivocal.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#sec-filters",
    "href": "chapters/feature-selection.html#sec-filters",
    "title": "13  Feature Selection",
    "section": "13.8 Filter Methods",
    "text": "13.8 Filter Methods\nFilters can use any scoring function that quantifies how effectively the predictor predicts the outcome (Kohavi and John 1998; Duch 2006). One method is to use a univariate statistic to evaluate each predictor independently. For example, the area under the ROC curve (?sec-roc) can be computed for a binary classification outcome and a set of numeric predictors. The filter could be applied using these values by pre-defining a threshold for the statistic that defines “important enough.” Alternatively, the top \\(p^*\\) predictors can be retained and used in the supervised model. This works well unless predictors have ties in their statistics.\nThe example of using ROC curves raises an interesting question. Should we rank the predictors by their area under the curve or should we convert that analysis to a p-value to see if the area under the curve is greater than 0.5 (Hanley and McNeil 1983)? Our advice is to do the former. Hypothesis tests and p-values have their uses but the AUC estimates themselves are probably more numerically stable. To factor in uncertainty, we could filter on the lower confidence bound of the AUC. This is generally true for almost any ranking statistic that has a corresponding p-value.\nMultivariate methods can also be used. These simultaneously compute measures of importance for all predictors. This is usually preferred since univariate statistics can be compromised when the predictors are highly correlated and/or have important interaction effects.\nA variety of models have built-in methods for measuring importance. For example:\n\nMARS (Friedman 1991) and decision trees can compute the importance of each predictor by aggregating the improvement in the objective function when each predictor is used in a split or an artificial feature.\nSome neural networks can use the values of their coefficients in each layer to compute an aggregate measure of effectiveness for each feature (Garson 1991; Gevrey, Dimopoulos, and Lek 2003; Olden, Joy, and Death 2004).\nSimilarly, partial least squares models can compute importance scores from their loading estimates (Wold, Johansson, and Cocchi 1993; Farrés et al. 2015).\nAs mentioned in Section 8.1.2, BART (and H-statistics) can measure the importance of individual predictors using the same techniques to discover interaction effects.\n\nThere are also model-free tools for computing importance scores from an existing model based on permuting each predictor and measuring how performance metrics change. This approach was initially popularized by the random forest model (?sec-rand-forest-cls) but can be easily extended to any model. See Breiman (2001), Strobl et al. (2007), and ?sec-importance for more details. There are a few other methods worth mentioning:\n\nThe Relief allgorithm (Kira and Rendell 1992; Kononenko 1994) and its descendant methods randomly select training set points and use the outcome values of their nearest neighbors to aggregate statistics to measure importance.\nMinimum redundancy, maximum relevance analysis (MRMR) combines two statistics, one for importance and another for between-predictor correlation, to find a subset that has the best of both quantities (Peng, Long, and Ding 2005).\n\nThere are also tools to help combine multiple filters into a compound score (Karl et al. 2023). For example, we may want to blend the number of active predictors and multiple performance characteristics into a single score used to rank different tuning parameter candidates. One way to do this is to use desirability functions. These will be described in ?sec-combined-metrics, Also, Section 12.3.2 of Kuhn and Johnson (2019) shows an example of how desirability functions can be incorporated into feature selection.\nOnce an appropriate scoring function is determined, a tuning parameter can be added for how many predictors are to be retained. This is like RFE, but we optimize all the tuning parameters simultaneously and evaluate subsets in a potentially less greedy way.\nThere are two potential downsides, though. First, there is still the potential for enormous computational costs, especially if the ranking process is expensive. For example, if we use a grid search, there is significant potential for the same subset size to occur more than once in the grid6. In this case, we are repeating the same importance calculation multiple times.\nThe second potential problem is that we will use the same data set to determine optimal parameters and how many features to retain. This could very easily lead to overfitting and/or optimization bias, especially if our training set is not large.\nWe measured the predictors using the standard random forest importance score using the same data as the previous simulated annealing search. The number of features to retain was treated as an additional tuning parameter. Cross-validation was used to select the best candidate from the space-filling design with 50 candidate points. Figure 13.5(a) shows the tuning results. The number of selected predictors appears to have the largest effect on the results; performance worsens as we remove too many features (which probably includes informative ones). It also appears that too much penalization has a negative effect on reducing the RMSE.\nThe model with the smallest RMSE selected the top 9 most important predictors, 13 hidden units, a penalty value of 10-8.78, a learning rate of 10-1.47, and tanh activation. The resampled RMSE associated with the numerically best results was 0.28. When that model was fit on the entire training set, the holdout RMSE estimate was 0.282. One reason that these two estimates of performance are so close is the relatively high \\(n_{tr}/p\\) ratio (1,000 / 29 \\(\\approx\\) 34.5).\n\n\n\n\n\n\n\n\nFigure 13.5: Panel (a): The results of tuning a variance importance filter and the parameters from a single-layer neural network for predicting the simulated data from Equation 13.1. Panel (b): The frequency distribution of predictor selection in the 10 folds for the model with the smallest RMSE.\n\n\n\n\n\nRecall that the importance scores are computed on different analysis sets. For the best model, Figure 13.5(b) contains the frequency at which each predictor was selected. For the most part, the importance scores are pretty accurate; almost all informative features are consistently selected across folds. When the final model was fit on the entire training set, a single irrelevant predictor was included in the model (by random change) along with true predictors \\(x_{1}\\), \\(x_{2}\\), \\(x_{3}\\), \\(x_{4}\\), \\(x_{5}\\), \\(x_{7}\\), \\(x_{9}\\), and \\(x_{10}\\). These results are unusually clean, mostly due to the large sample size and relatively small predictor set.\nSimilar to the discussion in Section 10.10, the different predictor sets discovered in the resamples can give us a sense of our results’ consistency. They are “realizations” of what could have occurred. Once we fit the final model, we get the actual predictor set.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#chapter-references",
    "href": "chapters/feature-selection.html#chapter-references",
    "title": "13  Feature Selection",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nAmbroise, C, and G McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66.\n\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nDuch, W. 2006. “Filter Methods.” In Feature Extraction: Foundations and Applications, 89–117. Springer.\n\n\nFarrés, M, S Platikanov, S Tsakovski, and R Tauler. 2015. “Comparison of the Variable Importance in Projection (VIP) and of the Selectivity Ratio (SR) Methods for Variable Selection and Interpretation.” Journal of Chemometrics 29 (10): 528–36.\n\n\nFriedman, J. 1991. “Multivariate Adaptive Regression Splines.” The Annals of Statistics 19 (1): 1–67.\n\n\nGarson, D. 1991. “Interpreting Neural Network Connection Weights.” Artificial Intellegence Expert.\n\n\nGevrey, M, I Dimopoulos, and S Lek. 2003. “Review and Comparison of Methods to Study the Contribution of Variables in Artificial Neural Network Models.” Ecological Modelling 160 (3): 249–64.\n\n\nGuyon, I, J Weston, S Barnhill, and V Vapnik. 2002. “Gene Selection for Cancer Classification Using Support Vector Machines.” Machine Learning 46: 389–422.\n\n\nHanley, J, and B McNeil. 1983. “A Method of Comparing the Areas Under Receiver Operating Characteristic Curves Derived from the Same Cases.” Radiology 148 (3): 839–43.\n\n\nHooker, G. 2004. “Discovering Additive Structure in Black Box Functions.” In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 575–80.\n\n\nKarl, F, T Pielok, J Moosbauer, F Pfisterer, S Coors, M Binder, L Schneider, et al. 2023. “Multi-Objective Hyperparameter Optimization in Machine Learning—an Overview.” ACM Transactions on Evolutionary Learning and Optimization 3 (4): 1–50.\n\n\nKira, K, and L Rendell. 1992. “A Practical Approach to Feature Selection.” In Machine Learning Proceedings 1992, 249–56. Elsevier.\n\n\nKohavi, R, and G John. 1998. “The Wrapper Approach.” In Feature Extraction, Construction and Selection: A Data Mining Perspective, 33–50. Springer.\n\n\nKononenko, I. 1994. “Estimating Attributes: Analysis and Extensions of RELIEF.” In European Conference on Machine Learning, 171–82. Springer.\n\n\nKuhn, M. 2008. “Building Predictive Models in R Using the caret Package.” Journal of Statistical Software 28: 1–26.\n\n\nKuhn, M, and K Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nOlden, J, M Joy, and R Death. 2004. “An Accurate Comparison of Methods for Quantifying Variable Importance in Artificial Neural Networks Using Simulated Data.” Ecological Modelling 178 (3-4): 389–97.\n\n\nPeng, H, F Long, and C Ding. 2005. “Feature Selection Based on Mutual Information Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy.” IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (8): 1226–38.\n\n\nSorokina, D, R Caruana, M Riedewald, and D Fink. 2008. “Detecting Statistical Interactions with Additive Groves of Trees.” In Proceedings of the 25th International Conference on Machine Learning, 1000–1007.\n\n\nStrobl, C, AL Boulesteix, A Zeileis, and T Hothorn. 2007. “Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution.” BMC Bioinformatics 8: 1–21.\n\n\nWold, S, E Johansson, and M Cocchi. 1993. “PLS: Partial Least Squares Projections to Latent Structures.” In 3D QSAR in Drug Design: Theory, Methods and Applications., 523–50. Kluwer ESCOM Science Publisher.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/feature-selection.html#footnotes",
    "href": "chapters/feature-selection.html#footnotes",
    "title": "13  Feature Selection",
    "section": "",
    "text": "Recall that we use the terms “predictors” and “features” interchangeably. We favor the former term, but “features” is more commonly used when discussing the filtering of columns.↩︎\nFor this value of the simulated standard deviation, the best possible RMSE value is also 0.25.↩︎\nDetails and code can be found at https://github.com/topepo/noise_features_sim.↩︎\nRecall out mantra that “Always have a separate piece of data that can contradict what you believe.”↩︎\nZero-variance predictors do not harm these models, but determining such predictors computationally will speed up their training.↩︎\nThe same issue can also occur with iterative search↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html",
    "href": "chapters/comparing-models.html",
    "title": "14  Comparing Models",
    "section": "",
    "text": "14.1 Resampled Data Sets\nSo far, we have established several key fundamentals of building predictive models. To create an effective model, we must employ strategies to reduce overfitting, use empirical validation techniques to accurately measure predictive performance, and systematically search a tuning parameter space to identify values that optimize model quality. By following these approaches, we can have confidence that our model generalizes well to new, unseen data and provides a realistic estimate of its efficacy.\nWith this foundation in place, the next step is to select the best model for a given data set. When presented with new data, we typically evaluate multiple machine learning models, each with different sets of tuning parameters. Our goal is to select the model and parameter combination that yields the best results. But how do we determine which is truly the best?\nMaking this decision requires considering several factors. As briefly discussed in Chapter 2, practical characteristics such as model complexity, interpretability, and deployability play an important role. Predictive ability, however, is an objective measure that can be evaluated quantitatively. Comparing metrics across models raises an important question: How can we be sure that one model’s quality is truly superior to another’s? Or, conversely, how can we conclude that multiple models have no significant difference in performance?\nTo address these questions, we will examine both graphical and statistical techniques for comparing the performance statistics of different models. During the model tuning and training process, these comparisons may involve assessing various configurations of tuning parameters within a single model or comparing fundamentally different modeling approaches.\nEmpirical validation (e.g., resampling) serves as the backbone of the tuning and training process. It systematically partitions the data into multiple subsets, enabling repeated estimation of performance metrics. The variability from the different assessment sets provides critical information for selecting optimal tuning parameters or identifying the best-performing model.\nIn the first part of this chapter, we will focus on methods for evaluating models when replicate estimates of performance metrics are available. This includes techniques grounded in Frequentist and Bayesian frameworks. After that, we will explore the case of a single data set. This is relevant when a single holdout set is used during model development (i.e., a validation set) and also when the test set is evaluated.\nAs a refresher, resampling produces multiple variations of the training set. These are used to metric estimates, such as accuracy or R2, that should mimic the results we would see on data sets similar to the training set.\nThe tools in this section are very similar to those described for model racing in Section 11.3.3, and we’ll use the same notation as that section. Similarly, we’ve discussed the two main philosophies of statistical analysis (Frequentist and Bayesian) in Section 6.4.3 and Section 12.5.2. Many of those ideas play out again in this chapter.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-resamples",
    "href": "chapters/comparing-models.html#sec-compare-resamples",
    "title": "14  Comparing Models",
    "section": "",
    "text": "14.1.1 Statistical Foundations\nRegardless of whether we take a Frequentist or Bayesian philosophies to comparing model performance, both methods aim to identify key factors that influence the outcome, such as model-to-model effects. We seek to determine whether the effect of these factors is statistically significant—that is, unlikely to be due to random chance.\nTo illustrate these statistical techniques, we will use the same data that will be used in later chapters on classification: the Washington State forestation data introduced in Chapter 3. Our goal is to predict whether a location is classified as forested or non-forested. The data set includes predictors such as elevation, annual precipitation, longitude, latitude, and so on.\nWithout going into much detail, we’ll focus on comparing three modeling strategies that will be described in Part 4:\n\nA boosted tree ensemble using C5.0 trees as the base learner. This model was tuned over the number of trees in the ensemble and the amount of data required to make additional splits (a.k.a. \\(n_{min}\\)). Grid search found that 60 trees in the ensemble were optimal, in conjunction with \\(n_{min}\\) = 20 training set points.\nA logistic regression model that was trained using penalization (?sec-logistic-penalized). Feature engineering included an effect encoding parameterization of the county associated with locations, a small set of interaction terms (described in ?sec-forestation-eda), and ten spline terms for several of its numeric predictors. The optimal tuning parameter values were a penalization value of 10-4.67 and 0% Lasso penalization.\nA naive Bayes model (?sec-naive-bayes]) was used where the conditional distributions were modeled using a nonparametric density estimate. No feature preprocessing was used.\n\nOne difference between these analyses and those to come in Part 4 is that we’ve chosen to use classification accuracy as our metric1. Some of these results may differ from upcoming analyses. Previously, the spatially aware resampling scheme that was used generated 10 resamples of the training set. For this reason, there are 10 different accuracy “replicates” for each candidate value that was evaluated.\nSimilar to previous sections, we’ll produce models where the replicated performance statistics (denoted as \\(\\widehat{Q}_{ij}\\)) are the outcomes of the model (for resample \\(i\\) and model \\(j\\)). This is the same general approach shown in Equation 11.1 and Equation 12.5. We’ll assume that there are \\(M\\) models to compare (three in our case).\n\n\n14.1.2 Frequentist Hypothesis Testing Methods\nLet’s begin by building towards an appropriate statistical model that will enable us to formally compare the efficacy of models. To begin this illustration, let’s suppose that each of these models was built separately using different 10-fold partitions of the training data. In this scenario, the metrics on the first holdout fold would be unique, or independent, for each model. If this were the case, then we could represent the desired performance metric with the following linear statistical model:\n\\[\nQ_{ij} = \\beta_0 + \\sum_{j=1}^{M-1}\\beta_jx_{ij} + \\epsilon_{ij}\n\\tag{14.1}\\]\nIn this equation \\(\\beta_0\\) is the overall average performance statistic across all assessment sets and models2. The term \\(\\beta_{j}\\) represents the incremental impact on the outcome due to the \\(j^{th}\\) model (\\(j\\) = 1, 2, …, \\(M-1\\) = total number of models), and \\(x_{ij}\\) is an indicator function for the \\(j^{th}\\) model. In this example, the \\(\\beta_j\\) will be considered as a fixed effect. This indicates that these are the only models that we are considering. If we were to have taken a random sample of models, then the \\(\\beta_j\\) would be treated a random effect which would have a corresponding statistical distribution. The distinction between fixed and random effects will come into play shortly. Finally, \\(\\epsilon_{ij}\\)) represents the part of the performance that is not explainable by the \\(i^{th}\\) assessment set and \\(j^{th}\\) model. We’ll assume these residuals follow a Gaussian distribution with a mean of zero and an unknown variance3. The model, along with the statistical assumption about the distribution of the residuals, provides the statistical foundation for formally comparing models.\nFigure 14.1 illustrates these separate components for the assessment set with the largest accuracy value for the boosting ensemble, where the overall intercept is determined by the logistic regression. The horizontal dashed line represents the overall mean of the accuracy estimates, and the blue bar shows the mean of the boosting estimates alone. It is slightly higher than the average. For that model, the orange line shows the residual (\\(\\epsilon_{ij}\\) for the largest boosted tree accuracy replicate. The totality of such errors is used to measure the overall error in the model.\n\n\n\n\n\n\n\n\nFigure 14.1: An illustration of how variation is partitioned based on the effect of the model (blue) and unexplained causes (orange) when distinct cross-validation partitions are used to train each model.\n\n\n\n\n\nEquation 14.1 enables us to distinctly partition the total variability in the outcome data into two components:\n\nExplained variability: The portion attributable to differences among models, represented by the blue bar. This is the signal in the data set.\nUnexplained variability: The portion due to random or unaccounted-for factors, represented by the orange bar, is commonly called the noise.\n\nOur key question is whether the observed differences in the mean performance for each model reflect genuine differences among models or if they could simply be due to random variation. The Frequentist approach addresses this question using hypothesis testing. In this framework:\n\nThe null hypothesis (\\(H_0\\)) represents the assumption that all models have the same average performance. Formally, this can be written as:\n\\[\nH_0: \\beta_k = \\beta_m,\n\\] for all \\(k = m\\).\nThe alternative hypothesis (\\(H_A\\)) asserts that at least two models have significantly different average performance:\n\\[\nH_A: \\beta_{k} \\neq \\beta_{m},\n\\]\nfor any \\(k \\neq m\\).\n\nTo determine whether we have sufficient evidence to reject the null hypothesis, we compare:\n\nBetween-group variability (signal): Differences in mean performance across models.\n\nWithin-group variability (noise): Residual variation not explained by the models.\n\nThis comparison is conducted using a test whose statistic follows an F distribution (Kutner, Nachtsheim, and Neter 2004), which assesses whether the signal is large enough relative to the noise to conclude that differences among models are statistically significant.\nIn this case, the F-statistic is (3.06, 2, 27)4, with a corresponding p-value of 0.0633. The p-value is small. However, it is not small enough to conclude that there is strong evidence to reject the null hypothesis. Therefore, we would not conclude that there are differences in accuracy among the three models. However, remember that we treated these data as if the resamples were completely different, which was not the case. As you’ll see shortly, this is a seriously flawed analysis of our data.\nIn practice, when tuning and training models, we use the same resampling sets to evaluate their performance. This is a more efficient approach since we only need to set up the resampling scheme once. Moreover, the structure generated by using the same assessment sets will enable us to detect differences in quality between models more effectively.\nIn this setting, each assessment set forms what is known as a block, which is a group of units that are similar to each other. This grouping is an important piece of information that will aid in assessing whether models have statistically different performance. The predictive performance when tuning multiple models using the same cross-validation assessment sets can now be represented as:\n\\[\nQ_{ij} = (\\beta_0 + \\beta_{0i}) + \\sum_{j=1}^{M-1}\\beta_jx_{ij} + \\epsilon_{ij}\n\\tag{14.2}\\]\nNotice that Equation 14.2 is nearly the same as Equation 14.1, with the additional term \\(\\beta_{0i}\\). This term represents the incremental impact on performance due to the \\(i^{th}\\) assessment set (\\(i\\) = 1, 2,…, \\(B\\) resamples). In this model, the impact of the \\(B\\) resamples, represented by \\(\\beta_{0i}\\) are a random effect. The reason the resamples are considered as random effects is because the 10 folds were selected at random from an overall distribution of resamples. The impacts of each model are still considered as fixed effects. When a model contains both fixed and random effects, it is called a mixed effect model.\nFigure 14.2 illustrates these separate components for the assessment set with the largest accuracy value for the boosted tree. In this figure, the points are connected based on the fold. The green line shows that the residual for the best boosting accuracy can be explained by the fact that it comes from this specific fold (i.e., resample). For whatever reason, this specific assessment set was easier to predict since all models had the highest accuracies. The resample that is associated with each accuracy is a systematic and explainable effect in these data. It’s also a “nuisance parameter” because we don’t care about this specific trend.\nThe mixed model can quantify the effect of this nuisance parameter; we can subtract its effect from the overall error, yielding a much smaller value of \\(\\epsilon_{ij}\\) as evidenced by the updated vertical orange bar. Notice that the total amount of variability explained by the model is identical to Figure 14.1. However, the previously unexplainable variability from Figure 14.2 is now mostly explained by the variation due to the fold, thus reducing the unexplainable variability. We have made our analysis much more precise by accounting for the resample effect5.\n\n\n\n\n\n\n\n\nFigure 14.2: An illustration of how variation is partitioned based on the effect of the model (blue), folds (green), and unexplainable causes (orange) when the same cross-validation partitions are used to train each model. The lines connecting the points indicate the results for the same cross-validation fold.\n\n\n\n\n\nTo understand if there are differences between models, we again compare the variability explained by the different models with the unexplainable variability. The signal relative to the noise is now much stronger, enabling us to more easily see differences in model performance. The F-statistic from this model is (20.42, 2, 18) and has a corresponding p-value of &lt; 10-4. By utilizing the same cross-validation scheme across models, there is now strong evidence of a difference in performance among the models.\nThis result does not tell us which models are different. To understand that, we need to perform additional tests to compare each pair of models. These tests are referred to as post hoc tests because they are performed after the initial hypothesis test among all means. To learn more details about post hoc tests and the field of multiple comparisons, see Hsu (1996) and/or Dudoit and Van Der Laan (2008) .\n\n\n14.1.2.1 Post Hoc Pairwise Comparisons and Protecting Against False Positive Findings\nAfter detecting a significant effect in a one-way analysis of variance (ANOVA), the next step is to determine which specific groups differ from each other. This is achieved using post hoc pairwise comparisons, which assess all possible pairwise differences while controlling for false positive findings.\nWhen conducting multiple comparisons, the risk of making at least one Type I error (false positive) increases. If we compare many pairs of groups separately, the probability of incorrectly rejecting at least one null hypothesis accumulates, leading to spurious findings. If this issue is not addressed, then we will eventually conclude that models are different when they actually are not. To mitigate this, statistical procedures adjust for multiple comparisons to ensure that the overall false positive rate remains controlled. Two methods for this purpose are Tukey’s Honestly Significant Difference (HSD) (Tukey 1949) and the False Discovery Rate (FDR) procedures (Efron and Hastie 2016, chap. 15).\nTukey’s HSD test focuses on controlling the probability of making at least one false discovery; this is known as the familywise error rate (FWER). The HSD test is specifically designed for one-way ANOVA post hoc comparisons and ensures that the FWER remains at a pre-determined significance level (\\(\\alpha\\) = 0.05).\nThe test considers all possible pairwise differences while maintaining a constant error rate across comparisons. To contrast models \\(i\\) and \\(j\\), the test statistic is calculated as:\n\\[\nT_{HSD} = \\frac{\\bar{Q}_i - \\bar{Q}_j}{\\sqrt{MS_{within} / n}}\n\\]\nwhere, \\(\\bar{Q}_i\\) and \\(\\bar{Q}_j\\) are the sample means of each model’s outcome data, \\(MS_{within}\\) is the mean square error from the ANOVA, and \\(n\\) is the number of observations per group. The critical value for \\(T_{HSD}\\) is obtained from the studentized range distribution, which depends on the number of groups and the degrees of freedom from the ANOVA error term.\nWhen we desire to compare the performance of each pair of models, then Tukey’s HSD procedure will provide the appropriate adjustments to the p-values to protect against false positive findings as it becomes overly conservative.\nIn settings where many hypotheses are tested simultaneously, we may be more interested in the false discovery rate (FDR): the expected proportion of false positives among all rejected hypotheses. With many comparisons, this can be more appropriate than controlling the familywise error rate.\nThere are numerous FDR procedures in the literature. The method by Benjamini and Hochberg (1995) is the simplest. It ranks the p-values from all pairwise comparisons from smallest to largest (\\(p_1\\), \\(p_2\\), …, \\(p_m\\)) and compares them to an adjusted threshold:\n\nRank the p-values in ascending order, assigning each a rank \\(i\\).\nCompute the critical value for each comparison: \\[\np_{crit} = \\frac{i}{m} \\alpha\n\\] where \\(m\\) is the total number of comparisons and \\(\\alpha\\) is the desired FDR level.\nFind the largest \\(i\\) where \\(p_i \\leq p_{crit}\\), and reject all hypotheses with p-values at or below this threshold.\n\nUnlike Tukey’s method, FDR control is more appropriate when performing many comparisons. This procedure offers a balance between sensitivity (detecting true differences) and specificity (limiting false positives).\nNow, let’s compare how these methods perform when evaluating each pair of models in our example. Table 14.1 presents the estimated differences in accuracy, standard error, degrees of freedom, raw, unadjusted p-values, and the p-values adjusted using the Tukey and FDR methods for each pairwise comparison. Notice that the Tukey adjustment results in less significant p-values than the FDR adjustment. Regardless of the adjustment method, the results indicate that each model’s performance differs statistically significantly from the others. Specifically, the boosting model has a higher accuracy than the other two models.\n\n\n\n\n\n\n\n\n\n\n\nComparison\nEstimate\nSE\nDF\n\np-value Adjustment\n\n\n\nUnadjusted\nTukey\nFDR\n\n\n\n\nBoosting - Logistic\n0.79%\n0.67%\n18\n0.25544\n0.48276\n0.25544\n\n\nBoosting - Naive Bayes\n4.04%\n0.67%\n18\n0.00001\n0.00003\n0.00003\n\n\nLogistic - Naive Bayes\n3.25%\n0.67%\n18\n0.00013\n0.00036\n0.00019\n\n\n\n\n\n\n\n\nTable 14.1: Pairwise comparisons among the three models for predicting forestation classification. The table includes the estimated difference in accuracy, standard error, degrees of freedom, the unadjusted p-value, and the corresponding Tukey and FDR adjusted p-values.\n\n\n\n\nIn this table, we can see that we have no evidence that boosting and logistic regression have different accuracies, but both of these models are statistically different from naive Bayes (no matter the adjustment type).\n\n\n\n14.1.2.2 Comparing Performance using Equivalence Tests\nIn the previous section, we explored how to determine whether one model significantly outperforms another in a metric of choice. However, there are cases where we may instead want to assess whether the models perform similarly in a practically meaningful way. To do this, we use an equivalence test, which differs from the traditional hypothesis test, which aimed to detect differences between models.\nEquivalence testing is based on the concept that two models can be considered practically equivalent if their difference falls within a predefined margin of indifference, often denoted as \\(\\theta\\). This threshold represents the maximum difference that is considered practically insignificant. To illustrate equivalence testing, let’s begin by reviewing the hypothesis testing context for assessing differences between models. Let \\(\\beta_1\\) and \\(\\beta_2\\) represent the mean quality of two models. The null hypothesis for a standard two-sample test of difference is:\n\\[\nH_O:  \\beta_1 = \\beta_2\n\\]\nwhile the alternative hypothesis is:\n\\[\nH_A:  \\beta_1 \\neq \\beta_2\n\\]\nHowever, an equivalence test reverses this logic. The null hypothesis is that the models are not equivalent, and the alternative hypothesis is that the models are practically insignificant:\n\\[\nH_0:  |\\beta_1 - \\beta_2| \\geq \\theta\n\\tag{14.3}\\]\n\\[\nH_A:  |\\beta_1 - \\beta_2| \\lt \\theta\n\\tag{14.4}\\]\nUsing this framework, we might reject the null hypothesis and conclude that the models are equivalent within the predefined margin of indifference.\nA commonly used approach for equivalence testing is the Two One-Sided Test (TOST) procedure (Schuirmann 1987). This method involves conducting two separate one-sided hypothesis tests as defined in Equation 14.3 and Equation 14.4. Specifically, one test assesses the lower bound comparison while the other test assesses the upper bound comparison as follows:\nLower bound:\n\\[\nH_0:  \\beta_1 - \\beta_2 \\geq -\\theta\n\\]\n\\[\nH_A:  \\beta_1 - \\beta_2 &gt; -\\theta\n\\] Upper bound:\n\\[\nH_0:  \\beta_1 - \\beta_2 \\leq \\theta\n\\] \\[\nH_A:  \\beta_1 - \\beta_2 &gt; \\theta\n\\]\nEach test is conducted at a desired significance level \\(\\alpha\\), and if both null hypotheses are rejected, we conclude that the model performance is equivalent based on the predefined margin of indifference.\nSchuirmann showed that the two one-sided hypotheses can be practically evaluated by constructing a \\(100(1-2\\alpha)\\%\\) confidence interval for \\(\\beta_1 - \\beta_2\\). If this interval is contained within the boundaries of (\\(-\\theta\\), \\(\\theta\\)), then we can reject the null hypothesis and conclude that the two models have equivalent performance.\nSelecting an appropriate margin of indifference is crucial and should be determined prior to conducting the statistical test. What value should we choose? The answer to this question is problem-specific and depends on what we believe makes a sensible difference in performance.\nFrom the previous section, we saw that the largest expected difference between any pair of models was 4%. For the sake of this illustration, let’s suppose that the margin of practical indifference between models is at least \\(\\pm\\) 3 percent in accuracy.6\nTo perform the equivalence tests, we use the same ANOVA model as when testing for differences in Equation 14.2 and estimate the 90% confidence intervals about each pairwise difference. Because we are making multiple pairwise comparisons, the intervals need to be adjusted to minimize the chance of making a false positive finding. This adjustment can be done using either the Tukey or FDR procedures.\nFigure 14.3 presents the Tukey-adjusted 90% confidence intervals for each pairwise difference between models. For the forestation data, we would only conclude that the boosting ensemble and the logistic regression are equivalent.\n\n\n\n\n\n\n\n\nFigure 14.3: Pairwise comparisons of models using equivalence testing. The Tukey-adjusted 90% confidence interval of each pairwise difference is illustrated with vertical bands. For these comparisons, the margin of practical equivalence was set to +/- 3% accuracy.\n\n\n\n\n\n\n\n\n\n14.1.3 Comparisons Using Bayesian Models\nHow would we use a Bayesian approach to fit the model from Equation 14.2? To start, we can again assume a Gaussian distribution for the errors with standard deviation \\(\\sigma\\). One commonly used prior for this parameter is the exponential distribution; it is highly right-skewed and has a single parameter. Our strategy here is to use very uninformative priors so that the data have more say in the final posterior estimates than the prior. If our model were useless, the standard deviation would be very large and would probably be bounded by the standard deviation of the outcome. For our data, that value for the accuracy is 4.1%. We’ll specify an exponential prior to use this as the mean of its distribution.\nWe’ll also need to specify priors for the three \\(\\beta\\) parameters. A fairly wide Gaussian would suffice. We’ll use \\(\\beta_j \\sim N(0, 5)\\) for each.\nFor the random effects \\(\\beta_{0i}\\), the sample size is fairly low (10 resamples per model). Let’s specify a bell-shaped distribution with a wide tail: a \\(t\\)-distribution with a single degree of freedom. This would enable the Bayesian model to have some resample-to-resample effects that might be considered outliers if we assumed a Gaussian distribution.\nWe’ll discuss training Bayesian models in ?sec-logistic-bayes, so we’ll summarize here. Given our priors, there is no analytically defined distribution for the posterior. To train the model, we’ll use a Markov-chain Monte Carlo (MCMC) method to slowly iterate the posterior to an optimal location, not unlike the simulated annealing process previously described. To do this, we create chains of parameters. These are a controlled random walk from a starting point. Once that walk appears to converge, we continue generating random data points that are consistent with the optimized posterior. These random points will numerically approximate the posterior, and from these, we can make inferences. We’ll create 10 independent chains, and each will have a warm-up phase of 5,000 iterations and then another 5,000 samples to approximate the posterior. If all goes well, we will have a random sample of 50,000 posterior samples.\nThe resulting posterior distributions for the mean accuracy for each of the three models are shown in Figure 14.4 along with 90% credible intervals. The posteriors are relatively bell-shaped, and although there are discernible shifts between the models, there are overlaps in these distributions.\nWe can also use the posteriors to estimate how much of the error in the data was associated with the resample-to-resample effect (i.e., the previously described nuisance parameter). The percentage of total error attributable to the resample-to-resample effects is 82.5%. This illustrates how important it is to account for all of the systematic effects, wanted or unwanted, in the model.\n\n\n\n\n\n\n\n\nFigure 14.4: Posterior distributions of accuracy for each of the models. The horizontal bars represent the 90% credible intervals about the posterior accuracy values for each model.\n\n\n\n\n\nSince we are interested in contrasting models, we can get a sample of the posterior and take the difference between the parameters that correspond to the model terms (i.e., \\(\\beta\\) parameters). We can create a large number of these samples to form posterior distributions of the pairwise contrasts and compute credible intervals. Bayesian modeling does not use the null hypothesis testing paradigm as Frequentist methods, so it is unclear whether post hoc adjustments are needed or how that would be computed. See Gelman, Hill, and Yajima (2012) and Ogle et al. (2019) for discussions. Figure 14.5 illustrates posteriors for the pairwise contrasts.\n\n\n\n\n\n\n\n\nFigure 14.5: Posterior distributions of pairwise differences in accuracy for each pair of models. The vertical dashed lines represent the 90% credible intervals for each contrast.\n\n\n\n\n\nBased on these results, we would reach the same conclusion as the Frequentist results; the boosting and logistic regression models are the only pair to appear to have comparable accuracies. We can compute probability statements such as “There is a 85.4% probability that the boosting ensemble exhibits a higher accuracy than logistic regression.”\nIn addition to utilizing the posterior distribution to compute probabilities of difference, we can also use this distribution to calculate probabilities of similarity or equivalence. The region of practical equivalence (ROPE) defines a range of metric values that we believe contains practically insignificant results (Kruschke 2014). When using the equivalence testing from Section 14.1.2.2, we considered models equivalent if the accuracy was within 3%. Using this region, the posterior distribution of the parameter of interest is used to generate a probability that the parameter is within this region. If a large proportion of the posterior distribution falls within the ROPE, we can conclude that the difference is minimal.\nThe primary difference between the Frequentist and Bayesian equivalence testing lies in interpretation and flexibility. Frequentist equivalence testing is based on pre-specified significance levels and long-run error rates. Bayesian ROPE, on the other hand, provides a probability-based assessment of practical equivalence. This is particularly useful in predictive modeling, where slight differences in performance may not be operationally relevant.\nFigure 14.6 illustrates the ROPE for the forestation data, defined as \\(\\pm\\) 3% accuracy. The probability mass within this region, shown in blue, represents the likelihood that the models have equivalent performance based on this criterion. Table 14.2 summarizes the probabilities for each model comparison.\n\n\n\n\n\n\n\n\nFigure 14.6: Posterior distributions of pairwise differences in accuracy for each pair of models. The vertical dashed lines represent the region of practical equivalence. The probability mass within this region represents the probability that the models are practically equivalent within +/- 3% accuracy.\n\n\n\n\n\nFor instance, there is an overwhelming probability that boosting and logistic regression are practically equivalent. The other two comparisons (with naive Bayes) show scant evidence of the same.\n\n\n\n\n\n\n\n\n\n\n\nComparison\nProbability of Practical Equivalence\n\n\n\n\nBoosting vs Logistic\n0.997\n\n\nBoosting vs Naive Bayes\n0.081\n\n\nLogistic vs Naive Bayes\n0.362\n\n\n\n\n\n\n\n\nTable 14.2: Probability of practical equivalence between each pair of models’ performance.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-holdout",
    "href": "chapters/comparing-models.html#sec-compare-holdout",
    "title": "14  Comparing Models",
    "section": "14.2 Single Holdout Data Sets",
    "text": "14.2 Single Holdout Data Sets\nUnlike resampled data sets, validation sets and test sets offer a single “look” at how well the model operates. In the case of the validation set, we would repeatedly use it to guide our model development process, much like how we would rely on resamples. For the test set, it should only be evaluated once, hopefully on a single model, but perhaps to make a final choice between a select few. It is not intended for repeat use. In the discussions below, we’ll use the term “holdout set” to describe the validation or test set, without loss of generality. In this scenario, the focus will be on Frequentist methods by way of bootstrap sampling (Efron 1979, 2003), which we have seen previously.\nYou were first introduced to the bootstrap in Section 10.6, where it was applied to estimate model performance during tuning. In essence, the bootstrap involves resampling with replacement from the available data to estimate the sampling variability of a statistic. This variability can then be used to construct confidence intervals of the performance metrics of interest, offering a range of plausible values for model performance on similar validation sets.\nFigure 10.7 illustrates the bootstrap process during the model-building tuning phase. Recall that resampling with replacement from the training data generates multiple bootstrap samples of the same size as the original. Because sampling is done with replacement, some observations may appear multiple times within a sample, while others may be omitted. The omitted samples are used as holdout sets to evaluate model performance.\nTo adapt this idea to make predictions on the single holdout set, then repeatedly draw bootstrap samples from it. Each bootstrap sample is the same size as the holdout set and is created via random sampling with replacement (as illustrated in Figure 14.7). We compute our metric(s) of interest on each analysis set, which are just the bootstrap samples. The corresponding assessment sets are not used in this process.\n\n\n\n\n\n\n\n\nFigure 14.7: A schematic of bootstraps resamples created from a validation set.\n\n\n\n\n\nThis process yields a distribution of performance metrics, from which bootstrap confidence intervals can be derived. These intervals offer an estimate of the uncertainty around the holdout set’s performance and give us a sound way to express the expected range of performance across other holdout sets drawn from the same population.\nThere are several methods for computing bootstrap confidence intervals. See Davison and Hinkley (1997) and Chapter 11 of Efron and Hastie (2016). We’ll describe one that is straightforward and often used: percentile intervals. The idea is simple: for an interval with \\(100 (1 - \\alpha)\\)% confidence, we compute the \\(\\alpha/2\\) quantiles on both ends of the bootstrap distribution. For our 90% intervals, we determine the 5% and 95% quantiles. Since the method relies on estimating very small areas in the distributional tails, it is important to create many bootstrap samples (i.e., thousands) to guarantee dependable results.\nLet’s suppose that we considered a wide variety of models and feature sets, and still found our boosted tree and logistic regression to be the best. We’ve previously seen that, while the ensemble appears to perform best, the results are fairly similar, and the logistic model is simpler and perhaps more explainable. We might take both of these pipelines to the test set to make one final comparison before making a choice.\nWe fit the final models on the entire training set and then predict the test set of 1,371 locations. The accuracies for the boosting and logistic models were very close; they were 87.7% and 87.3%, respectively. Is this difference within the experimental noise or could there be a (statistical) difference?\nWe arrange the data so that there are separate columns for the boosted tree and logistic regression predictions (i.e., they are merged by the row number of the test set). Using this data, 5,000 bootstrap samples were created. From each of these, we can compute intervals for the accuracy of each and the difference in accuracy between them.\nFigure 14.8 shows the bootstrap distributions of the individual model results. They are fairly similar, with a small shift between them. Above each histogram are two 90% confidence intervals. One is the bootstrap percentile interval, and the other is a theoretically based asymmetric interval that assumes that the correct/incorrect data for each model follows a Bernoulli distribution. For each model, these intervals are almost identical. We computed each to demonstrate that the bootstrap has very good statistical performance (compared to its theoretical counterpart).\n\n\n\n\n\n\n\n\nFigure 14.8: For both models, the bootstrap distributions of the accuracy statistics are shown as histograms. The intervals at the top of each panel show the 90% confidence intervals computed using the bootstrap and via the traditional analytical method.\n\n\n\n\n\n\nThe confidence intervals between models substantially overlap. Ordinarily, we should take that as reliable information that there is no statistical difference between the two models. However, this is not true for this particular situation.\n\nThe issue is that the two sets of predictions are highly correlated. On the test set, they agree 93.1% of the time. Unlike our previous mixed model approach from Equation 14.2, the individual confidence intervals have no way to correct for this issue (since they evaluate one model at a time).\nWhen computing the difference between two things (A and B), the variance of their difference is:\n\\[\nVar[A-B] = Var[A] + Var[B] - 2Cov[A, B]\n\\tag{14.5}\\]\nWhen A and B are related, their covariance will be very high and, if we ignore it, the power of any statistical test on that difference can be severely underpowered. This is the same issue as shown in Section 14.1.2. When we didn’t account for the resampling effect, we accidentally sabotaged the test’s ability to find a difference (if there was one).\nFor the test set, we compute the difference in accuracy by resampling the matched pairs of predictions. From this, the bootstrap estimate of the difference was 0.4% with 90% bootstrap interval (-0.8%, 1.5%). This does indicate that the models have statistically indistinguishable accuracies, but the lower bound is very close to zero. With slightly different models or a different random split of the data, they might very well be considered different7. There is an overwhelmingly strong case to make that they are practically equivalent, though.\nAlso, as the data set size increases, the variation we expect to see in the summary statistic will decrease. This concept is illustrated in Figure 14.9. In this figure, we have randomly selected subsets of varying size from the data set and then computed a 90% confidence interval using these smaller sets. Notice that as the sample size decreases, the width of the confidence interval increases. Therefore, the smaller the data set, the higher the variability will be in the estimate of model quality, and the more important it is to understand the range of potential uncertainty. This pattern is true of any interval procedure.\n\n\n\n\n\n\n\n\nFigure 14.9: 90% confidence interval widths of accuracy for data sets of varying size, assuming the same performance as the boosted ensemble.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#sec-compare-conclusion",
    "href": "chapters/comparing-models.html#sec-compare-conclusion",
    "title": "14  Comparing Models",
    "section": "14.3 Conclusion",
    "text": "14.3 Conclusion\nSelecting the best predictive model involves assessing performance metrics and accounting for practical considerations. Thus far, this chapter has outlined both Frequentist and Bayesian approaches to comparing model performance, emphasizing the importance of appropriate statistical techniques when evaluating differences. The Frequentist approach provides a formal hypothesis testing framework, while the Bayesian approach offers a more direct probability-based interpretation of model differences. When there is just one data set for evaluation, like a validation set or test set, then the bootstrap technique can be used to create an interval about the performance metric (or differences between models).\nUltimately, while statistical significance can indicate meaningful differences between models, practical significance should also guide decision-making. Equivalence testing and Bayesian ROPE analysis help determine whether models perform similarly within a defined threshold of practical relevance. In our forestation classification example, these methods suggested that while the boosting model demonstrated superior predictive performance overall, differences between some models may not always be large enough to drive a clear choice.\nIn practice, model selection is rarely based on performance alone. Factors such as interpretability, computational cost, and deployment feasibility must also be considered. By combining quantitative performance assessment with practical domain knowledge, we can make informed decisions that balance accuracy with real-world constraints.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#chapter-references",
    "href": "chapters/comparing-models.html#chapter-references",
    "title": "14  Comparing Models",
    "section": "Chapter References",
    "text": "Chapter References\n\n\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300.\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nDudoit, S, and M Van Der Laan. 2008. Multiple Testing Procedures with Applications to Genomics. Springer.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nEfron, B. 2003. “Second Thoughts on the Bootstrap.” Statistical Science, 135–40.\n\n\nEfron, B, and T Hastie. 2016. Computer Age Statistical Inference. Cambridge University Press.\n\n\nGelman, A, J Hill, and M Yajima. 2012. “Why We (Usually) Don’t Have to Worry about Multiple Comparisons.” Journal of Research on Educational Effectiveness 5 (2): 189–211.\n\n\nHsu, J. 1996. Multiple Comparisons: Theory and Methods. Chapman; Hall/CRC.\n\n\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press.\n\n\nKutner, M, C Nachtsheim, and J Neter. 2004. Applied Linear Regression Models. McGraw-Hill/Irwin.\n\n\nOgle, K, D Peltier, M Fell, J Guo, H Kropp, and J Barber. 2019. “Should We Be Concerned about Multiple Comparisons in Hierarchical Bayesian Models?” Methods in Ecology and Evolution 10 (4): 553–64.\n\n\nSchuirmann, Donald J. 1987. “A Comparison of the Two One-Sided Tests Procedure and the Power Approach for Assessing the Equivalence of Average Bioavailability.” Journal of Pharmacokinetics and Biopharmaceutics 15: 657–80.\n\n\nTukey, John W. 1949. “Comparing Individual Means in the Analysis of Variance.” Biometrics, 99–114.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  },
  {
    "objectID": "chapters/comparing-models.html#footnotes",
    "href": "chapters/comparing-models.html#footnotes",
    "title": "14  Comparing Models",
    "section": "",
    "text": "This isn’t normally our first choice. However, the outcome classes are fairly balanced. Also, accuracy is intuitively understandable, and the broader set of classification metrics has yet to be described in detail.↩︎\nThis is another example of an analysis of variance (ANOVA) model.↩︎\nIt may seem odd that we would assume a Gaussian distribution for a variable with values between zero and one. Almost all performance statistics are means; accuracy is the average of 0/1 data. The Central Limit Theorem tells us that, despite the original data distribution, the mean tends to normality as the sample size increases. Assessment and test sets often have enough data points to trust this assumption.↩︎\nThis is common notation for reporting the F-statistic, where the first number represents the ratio of the mean square of the model to the mean square of the error. The second and third numbers represent the degrees of freedom of the numerator and denominator, respectively.↩︎\nThis again echoes the discussion of racing from Section 11.3.3.↩︎\nWhen performing an equivalence test, the margin of practical indifference should be specified before the test is performed.↩︎\nOne reason to avoid using metrics based on qualitative predictions (e.g., “yes” or “no”) is that they have a diminished capability to detect differences. They tend to contain far less information than their corresponding class probabilities. A metric such as the Brier score would have additional precision and might be able to tell the two models apart.↩︎",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing Models</span>"
    ]
  }
]